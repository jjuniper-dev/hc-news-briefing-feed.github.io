‚úÖ Morning News Briefing ‚Äì July 10, 2025 10:56

üìÖ Date: 2025-07-10 10:56
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  15.6¬∞C
  Temperature: 15.6&deg;C Pressure / Tendency: 101.4 kPa rising Humidity: 97 % Humidity : 97 % Dewpoint: 15 .1&deg:C Wind: NW 3 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 10 July 2025 . Weather forecast: 15/6¬∞
‚Ä¢ Thursday: Chance of showers. High 28. POP 40%
  40 percent chance of showers this afternoon with risk of a thunderstorm . Fog patches dissipating this morning. High 28. Humidex 33. UV index 7 or high. . Forecast issued 5:00 AM EDT Thursday 10 July 2025. Forecast: Showery, thundery, sunny, cloudy, cloudy and breezy, breezy conditions. Showers and thunderstorms

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Kerr County struggled to fund flood warnings. Under Trump, it's getting even harder
  Kerr County, Texas, applied for federal grants to build a warning system to protect residents from flash floods . Under the Trump Administration, that kind of funding is drying up . Kerr County applied to federal grants for flood warnings, but that funding is now drying up under the new administration . The county applied for grants for a flood warning system, but the funds are drying up, officials say .
‚Ä¢ Photos: Before-and-after satellite images show extent of Texas flooding destruction
  Before-and-after images show how the Guadalupe River surged and devastated towns across Texas . Before and after images show the river surge and devastation of towns across the state . The river is still raging in parts of Texas, but it is expected to return to normal waters in the next few weeks . Watch the full video of the flooding below: CNN.com/Heroes .
‚Ä¢ South Korean court approves new arrest of former President Yoon Suk Yeol
  A South Korean court approved the new arrest of former President Yoon on charges related to his imposition of martial law in December . Yoon's lawyers had described the arrest request as excessive . The former president is accused of imposing martial law on South Korea in December, citing the need to impose martial law, in violation of the law's constitution, in addition to other violations of the constitution .
‚Ä¢ 31 workers reach safety after partial collapse of Los Angeles industrial tunnel
  Construction workers inside a huge industrial tunnel in Los Angeles made it to safety after a portion of it collapsed Wednesday evening . Officials called the outcome an outcome officials called a blessing . Construction workers in the tunnel were able to get to safety, officials called it a 'blessed blessing' The tunnel collapsed in a section of the tunnel in a Los Angeles suburb of Los Angeles, officials said .
‚Ä¢ U.S. issues sanctions against United Nations investigator probing abuses in Gaza
  Francesca Albanese is the U.N. special rapporteur for the West Bank and Gaza . The State Department's decision to impose sanctions on Albanese follows an unsuccessful campaign to force her removal from the post . Albanese was a member of the United Nations' special rapporteagues in the region of the West and Gaza, which borders Israel and the West . The U.S

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Sovereign-ish: Google Cloud keeps AI data in UK, but not the support
  Processing and storage for Gemini 2.5 Flash to stay in Blighty . Google Cloud is offering organizations the option to keep machine learning processing entirely within the UK . Machine learning processing and storage will be stored in the UK, Google Cloud says . The company is hoping to ease concerns about where AI data is stored by moving it to the U.S. Google Cloud will be offering the option
‚Ä¢ Outlook takes another sick day ‚Äì Microsoft says it'll get better soon, promise
  Outlook is down for the count in a major outage affecting millions of users worldwide for the past 11 hours . Microsoft Outlook was down for 11 hours on Monday morning, 11 hours after the outage began . The outage is the result of a major overhaul of Outlook's mailbox infrastructure, which has been down for over a dozen hours . Outlook is now down for about 12 hours, with the outage lasting
‚Ä¢ Write-back to aging UK health systems lessens benefits of Palantir-based platform
  UK health sector's Palantir-powered data and analytics system suppressed by limits of writing back to NHS software, MPs heard this week . Controversial system requires local NHS investment to maximize productivity, MPs were told . Controversy over the controversial system is being suppressed by the limits of NHS software software, they were told by MPs . It is not the first time the system has been
‚Ä¢ At last, a use case for AI agents with sky-high ROI: Stealing crypto
  Boffins outsmart smart contracts with evil automation . Using AI models to generate exploits for cryptocurrency contract flaws appears to be a promising business model, though not necessarily a legal one .‚Ä¶‚Ä¶‚Ä¶ Using AI to generate exploit exploits for cryptocurrencies contract flaws is not necessarily legal one, it's not clear if it's a legal business model that could be used in the U.S. It
‚Ä¢ Swiss boffins just trained a 'fully open' LLM on the Alps supercomputer
  Supercomputers are usually associated with scientific exploration, research, and development, and ensuring our nuclear stockpiles actually work . Source code and weights coming later this summer with an Apache 2.0 bow on top . Apache 2 .0 will be available to download and use for the first time in the next few months of the year . The code is expected to be released in the summer .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Leveraging retinal vascular features in non-invasive, early diagnosis of preeclampsia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Effects of long-term very high-altitude exposure on cardiopulmonary function of healthy adults in plain areas
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Childhood antecedents of adult place satisfaction in 22 countries
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Knowledge, attitudes, and willingness of patients with thyroid diseases toward thyroid thermal ablation techniques
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Increased testing is needed for Mpox in DR Congo to urgently curb disease spread
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ China‚Äôs energy dominance in three charts
  China is the dominant force in next-generation energy technologies today . It‚Äôs pouring billions of dollars into renewable sources like wind and solar on its grid, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more . In the US, a massive new tax and spending bill just cut hundreds of billions in credits, grants, and loans for clean energy technologies . The question is, will that change anytime soon?
‚Ä¢ This tool strips away anti-AI protections from digital art
  A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. It‚Äôs the next step in a cat-and-mouse game‚Äîacross technology, law, and culture‚Äîthat has been going on between artists and AI proponents for years.&nbsp;



Generative AI models that create images need to be trained on a wide variety of visual material, and data sets that are used for this training allegedly include copyrighted art without permission. This has worried artists, who are concerned that the models will learn their style, mimic their work, and put them out of a job.




These artists got some potential defenses in 2023, when researchers created tools like Glaze and Nightshade to protect artwork by ‚Äúpoisoning‚Äù it against AI training (Shawn Shan was even named MIT Technology Review‚Äôs Innovator of the Year last year for his work on these). LightShed, however, claims to be able to subvert these tools and others like them, making it easy for the artwork to be used for training once again.







To be clear, the researchers behind LightShed aren‚Äôt trying to steal artists‚Äô work. They just don‚Äôt want people to get a false sense of security. ‚ÄúYou will not be sure if companies have methods to delete these poisons but will never tell you,‚Äù says Hanna Foerster, a PhD student at the University of Cambridge and the lead author of a paper on the work. And if they do, it may be too late to fix the problem.




AI models work, in part, by implicitly creating boundaries between what they perceive as different categories of images. Glaze and Nightshade change enough pixels to push a given piece of art over this boundary without affecting the image‚Äôs quality, causing the model to see it as something it‚Äôs not. These almost imperceptible changes are called perturbations, and they mess up the AI model‚Äôs ability to understand the artwork.




Glaze makes models misunderstand style (e.g., interpreting a photorealistic painting as a cartoon). Nightshade instead makes the model see the subject incorrectly (e.g., interpreting a cat in a drawing as a dog). Glaze is used to defend an artist‚Äôs individual style, whereas Nightshade is used to attack AI models that crawl the internet for art.




Foerster worked with a team of researchers from the Technical University of Darmstadt and the University of Texas at San Antonio to develop LightShed, which learns how to see where tools like Glaze and Nightshade splash this sort of digital poison onto art so that it can effectively clean it off. The group will present its findings at the Usenix Security Symposium, a leading global cybersecurity conference, in August.&nbsp;



The researchers trained LightShed by feeding it pieces of art with and without Nightshade, Glaze, and other similar programs applied. Foerster describes the process as teaching LightShed to reconstruct ‚Äújust the poison on poisoned images.‚Äù Identifying a cutoff for how much poison will actually confuse an AI makes it easier to ‚Äúwash‚Äù just the poison off.&nbsp;




LightShed is incredibly effective at this. While other researchers have found simple ways to subvert poisoning, LightShed appears to be more adaptable. It can even apply what it‚Äôs learned from one anti-AI tool‚Äîsay, Nightshade‚Äîto others like Mist or MetaCloak without ever seeing them ahead of time. While it has some trouble performing against small doses of poison, those are less likely to kill the AI models‚Äô abilities to understand the underlying art, making it a win-win for the AI‚Äîor a lose-lose for the artists using these tools.




Around 7.5 million people, many of them artists with small and medium-size followings and fewer resources, have downloaded Glaze to protect their art. Those using tools like Glaze see it as an important technical line of defense, especially when the state of regulation around AI training and copyright is still up in the air. The LightShed authors see their work as a warning that tools like Glaze are not permanent solutions. ‚ÄúIt might need a few more rounds of trying to come up with better ideas for protection,‚Äù says Foerster.



The creators of Glaze and Nightshade seem to agree with that sentiment: The website for Nightshade warned the tool wasn‚Äôt future-proof before work on LightShed ever began. And Shan, who led research on both tools, still believes defenses like his have meaning even if there are ways around them.&nbsp;





‚ÄúIt‚Äôs a deterrent,‚Äù says Shan‚Äîa way to warn AI companies that artists are serious about their concerns. The goal, as he puts it, is to put up as many roadblocks as possible so that AI companies find it easier to just work with artists. He believes that ‚Äúmost artists kind of understand this is a temporary solution,‚Äù but that creating those obstacles against the unwanted use of their work is still valuable.



Foerster hopes to use what she learned through LightShed to build new defenses for artists, including clever watermarks that somehow persist with the artwork even after it‚Äôs gone through an AI model. While she doesn‚Äôt believe this will protect a work against AI forever, she thinks this could help tip the scales back in the artist‚Äôs favor once again.
‚Ä¢ The Download: a conversation with Karen Hao, and how did life begin?
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Inside OpenAI‚Äôs empire: A conversation with Karen Hao



In a wide-ranging Roundtables conversation for MIT Technology Review subscribers, journalist and author Karen Hao recently spoke about her new book, Empire of AI: Dreams and Nightmares in Sam Altman‚Äôs OpenAI.She talked with executive editor Niall Firth about how she first covered the company in 2020 while on staff at MIT Technology Review. They discussed how the AI industry now functions like an empire and went on to examine what ethically-made AI looks like.Read the transcript of the conversation, which has been lightly edited and condensed. And, if you‚Äôre already a subscriber, you can watch the on-demand recording of the event here.¬†







MIT Technology Review Narrated: How did life begin?



How life begins is one of the biggest and hardest questions in science. All we know is that something happened on Earth more than 3.5 billion years ago, and it may well have occurred on many other worlds in the universe as well. Could AI help us to unpick the mysteries around the origins of life and detect signs of it on other worlds?This is our latest story to be turned into a MIT Technology Review Narrated podcast, which&nbsp;we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 xAI‚Äôs Grok went on an anti-Semitic rant¬†Days after Elon Musk said new updates would lessen its reliance on mainstream media. (WP $)+ The chatbot started to call itself ‚ÄòMechaHitler.‚Äô (WSJ $)+ What Grok‚Äôs neo-Nazi turn tells us about xAI. (The Atlantic $)



2 Musk loyalists are fighting to keep DOGE runningAs officials seek to diminish the department‚Äôs role. (WSJ $)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



3 An imposter used AI to successfully impersonate Marco RubioThey were able to send voice and text messages to fellow politicians. (WP $)+ It‚Äôs not the first time Rubio has been targeted like this. (FT $)



4 Terrorist groups are using AI to recruit and planCounter-terror agencies are struggling to keep up. (The Guardian)



5 How the crypto faithful won over the PresidentThe industry‚Äôs successful Trump courtship sparked a lobbying bonanza. (NYT $)



6 Wanted: 115,000 Nvidia chips for China‚Äôs data centersBut the US doesn‚Äôt seem to know how many restricted chips are already in the country. (Bloomberg $)



7 For startups, protecting companies from AI threats isn‚Äôt big businessSmaller firms are only making modest gains‚Äîfor now. (The Information $)+ Cyberattacks by AI agents are coming. (MIT Technology Review)



8 Inside Zimbabwe‚Äôs dangerous EV lithium minesMany residents worry that China is exploiting them. (Rest of World)+ How one mine could unlock billions in EV subsidies. (MIT Technology Review)



9 ‚ÄòThe Milk Guy‚Äô is delivering raw dairy around NYCMmm, delicious listeria, salmonella, and E. coli. (NY Mag $)+ RFK Jr barred Democrats from being vaccine advisors. (Ars Technica)+ The Department of Health and Human Services is searching for two new vaccines against deadly viruses. (Undark)



10 Take a look at these beautiful star clustersCourtesy of the Hubble Space Telescope and the James Webb Space Telescope. (Ars Technica)+ See the stunning first images from the Vera C. Rubin Observatory. (MIT Technology Review)







Quote of the day



‚ÄúPeople are going to die.‚Äù



‚ÄîClement Nkubizi, the country director for the nonprofit Action Against Hunger in South Sudan, tells Wired that their food stock is running critically low in the wake of USAID cuts.







One more thing







The world is moving closer to a new cold war fought with authoritarian techDespite President Biden‚Äôs assurances that the US is not seeking a new cold war, one is brewing between the world‚Äôs autocracies and democracies‚Äîand technology is fueling it.Authoritarian states are following China‚Äôs lead and are trending toward more digital rights abuses by increasing the mass digital surveillance of citizens, censorship, and controls on individual expression.And while democracies also use massive amounts of surveillance technology, it‚Äôs the tech trade relationships between authoritarian countries that‚Äôs enabling the rise of digitally enabled social control. Read the full story.



‚ÄîTate Ryan-Mosley







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The UK is deep in the grip of Oasis-mania right now.+ Take a look back over the legacy of iconic Indian director and actor Guru Dutt.+ These are the best foods to help keep you hydrated in this heat.+ Artificial flowers are cool now? Hmm
‚Ä¢ Inside OpenAI‚Äôs empire: A conversation with Karen Hao
  In a wide-ranging Roundtables conversation for MIT Technology Review subscribers, AI journalist and author Karen Hao spoke about her new book, Empire of AI: Dreams and Nightmares in Sam Altman‚Äôs OpenAI. She talked with executive editor Niall Firth about how she first covered the company in 2020 while on staff at MIT Technology Review, and they discussed how the AI industry now functions like an empire and what ethically-made AI looks like.&nbsp;



Read the transcript of the conversation, which has been lightly edited and condensed, below. Subscribers can watch the on-demand recording of the event here.&nbsp;







Niall Firth: Hello, everyone, and welcome to this special edition of Roundtables. These are our subscriber-only events where you get to listen in to conversations between editors and reporters. Now, I‚Äôm delighted to say we‚Äôve got an absolute cracker of an event today. I‚Äôm very happy to have our prodigal daughter, Karen Hao, a fabulous AI journalist, here with us to talk about her new book. Hello, Karen, how are you doing?



Karen Hao: Good. Thank you so much for having me back, Niall.&nbsp;



Niall Firth: Lovely to have you. So I‚Äôm sure you all know Karen and that‚Äôs why you‚Äôre here. But to give you a quick, quick synopsis, Karen has a degree in mechanical engineering from MIT. She was MIT Technology Review‚Äôs senior editor for AI and has won countless awards, been cited in Congress, written for the Wall Street Journal and The Atlantic, and set up a series at the Pulitzer Center to teach journalists how to cover AI.&nbsp;



But most important of all, she‚Äôs here to discuss her new book, which I‚Äôve got a copy of here, Empire of AI. The UK version is subtitled ‚ÄúInside the reckless race for total domination,‚Äù and the US one, I believe, is ‚ÄúDreams and nightmares in Sam Altman‚Äôs OpenAI.‚Äù



It‚Äôs been an absolute sensation, a New York Times chart topper. An incredible feat of reporting‚Äîlike 300 interviews, including 90 with people inside OpenAI. And it‚Äôs a brilliant look at not just OpenAI‚Äôs rise, and the character of Sam Altman, which is very interesting in its own right, but also a really astute look at what kind of AI we‚Äôre building and who holds the keys.&nbsp;



Karen, the core of the book, the rise and rise of OpenAI, was one of your first big features at MIT Technology Review. It‚Äôs a brilliant story that lifted the lid for the first time on what was going on at OpenAI ‚Ä¶ and they really hated it, right?



Karen Hao: Yes, and first of all, thank you to everyone for being here. It‚Äôs always great to be home. I do still consider MIT Tech Review to be my journalistic home, and that story was‚ÄîI only did it because Niall assigned it after I said, ‚ÄúHey, it seems like OpenAI is kind of an interesting thing,‚Äù and he was like, you should profile them. And I had never written a profile about a company before, and I didn‚Äôt think that I would have it in me, and Niall believed that I would be able to do it. So it really didn‚Äôt happen other than because of you.



I went into the piece with an open mind about‚Äîlet me understand what OpenAI is. Let me take what they say at face value. They were founded as a nonprofit. They have this mission to ensure artificial general intelligence benefits all of humanity. What do they mean by that? How are they trying to achieve that ultimately? How are they striking this balance between mission-driven AI development and the need to raise money and capital?&nbsp;



And through the course of embedding within the company for three days, and then interviewing dozens of people outside the company or around the company ‚Ä¶ I came to realize that there was a fundamental disconnect between what they were publicly espousing and accumulating a lot of goodwill from and how they were operating. And that is what I ended up focusing my profile on, and that is why they were not very pleased.



Niall Firth: And how have you seen OpenAI change even since you did the profile? That sort of misalignment feels like it‚Äôs got messier and more confusing in the years since.



Karen Hao: Absolutely. I mean, it‚Äôs kind of remarkable that OpenAI, you could argue that they are now one of the most capitalistic corporations in Silicon Valley. They just raised $40 billion, in the largest-ever private fundraising round in tech industry history. They‚Äôre valued at $300 billion. And yet they still say that they are first and foremost a nonprofit.&nbsp;



I think this really gets to the heart of how much OpenAI has tried to position and reposition itself throughout its decade-long history, to ultimately play into the narratives that they think are going to do best with the public and with policymakers, in spite of what they might actually be doing in terms of developing their technologies and commercializing them.



Niall Firth: You cite Sam Altman saying, you know, the race for AGI is what motivated a lot of this, and I‚Äôll come back to that a bit before the end. But he talks about it as like the Manhattan Project for AI. You cite him quoting Oppenheimer (of course, you know, there‚Äôs no self-aggrandizing there): ‚ÄúTechnology happens because it‚Äôs possible,‚Äù he says in the book.¬†



And it feels to me like this is one of the themes of the book: the idea that technology doesn‚Äôt just happen because it comes along. It comes because of choices that people make. It‚Äôs not an inevitability that things are the way they are and that people are who they are. What they think is important‚Äîthat influences the direction of travel. So what does this mean, in practice, if that‚Äôs the case?



Karen Hao: With OpenAI in particular, they made a very key decision early on in their history that led to all of the AI technologies that we see dominating the marketplace and dominating headlines today. And that was a decision to try and advance AI progress through scaling the existing techniques that were available to them. At the time when OpenAI started, at the end of 2015, and then, when they made that decision, in roughly around 2017, this was a very unpopular perspective within the broader AI research field.&nbsp;



There were kind of two competing ideas about how to advance AI progress, or rather a spectrum of ideas, bookended by two extremes. One extreme being, we have all the techniques we need, and we should just aggressively scale. And the other one being that we don‚Äôt actually have the techniques we need. We need to continue innovating and doing fundamental AI research to get more breakthroughs. And largely the field assumed that this side of the spectrum [focusing on fundamental AI research] was the most likely approach for getting advancements, but OpenAI was anomalously committed to the other extreme‚Äîthis idea that we can just take neural networks and pump ever more data, and train on ever larger supercomputers, larger than have ever been built in history.



The reason why they made that decision was because they were competing against Google, which had a dominant monopoly on AI talent. And OpenAI knew that they didn‚Äôt necessarily have the ability to beat Google simply by trying to get research breakthroughs. That‚Äôs a very hard path. When you‚Äôre doing fundamental research, you never really know when the breakthrough might appear. It‚Äôs not a very linear line of progress, but scaling is sort of linear. As long as you just pump more data and more compute, you can get gains. And so they thought, we can just do this faster than anyone else. And that‚Äôs the way that we‚Äôre going to leap ahead of Google. And it particularly aligned with Sam Altman‚Äôs skillset, as well, because he is a once-in-a-generation fundraising talent, and when you‚Äôre going for scale to advance AI models, the primary bottleneck is capital.



And so it was kind of a great fit for what he had to offer, which is, he knows how to accumulate capital, and he knows how to accumulate it very quickly. So that is ultimately how you can see that technology is a product of human choices and human perspectives. And they‚Äôre the specific skills and strengths that that team had at the time for how they wanted to move forward.





Niall Firth: And to be fair, I mean, it works, right? It was amazing, fabulous. You know the breakthroughs that happened, GPT-2 to GPT-3, just from scale and data and compute, kind of were mind-blowing really, as we look back on it now.



Karen Hao: Yeah, it is remarkable how much it did work, because there was a lot of skepticism about the idea that scale could lead to the kind of technical progress that we‚Äôve seen. But one of my biggest critiques of this particular approach is that there‚Äôs also an extraordinary amount of costs that come with this particular pathway to getting more advancements. And there are many different pathways to advancing AI, so we could have actually gotten all of these benefits, and moving forward, we could continue to get more benefits from AI, without actually engaging in a hugely consumptive, hugely costly approach to its development.



Niall Firth: Yeah, so in terms of consumptive, that‚Äôs something we‚Äôve touched on here quite recently at MIT Technology Review, like the energy costs of AI. The data center costs are absolutely extraordinary, right? Like the data behind it is incredible. And it‚Äôs only gonna get worse in the next few years if we continue down this path, right?&nbsp;



Karen Hao: Yeah ‚Ä¶ so first of all, everyone should read the series that Tech Review put out, if you haven‚Äôt already, on the energy question, because it really does break down everything from what is the energy consumption of the smallest unit of interacting with these models, all the way up until the highest level.&nbsp;



The number that I have seen a lot, and that I‚Äôve been repeating, is there was a McKinsey report that was looking at if we continue to just look at the pace at which data centers and supercomputers are being built and scaled, in the next five years, we would have to add two to six times the amount of energy consumed by California onto the grid. And most of that will have to be serviced by fossil fuels, because these data centers and supercomputers have to run 24/7, so we cannot rely solely on renewable energy. We do not have enough nuclear power capacity to power these colossal pieces of infrastructure. And so we‚Äôre already accelerating the climate crisis.&nbsp;



And we‚Äôre also accelerating a public-health crisis, the pumping of thousands of tons of air pollutants into the air from coal plants that are having their lives extended and methane gas turbines that are being built in service of powering these data centers. And in addition to that, there‚Äôs also an acceleration of the freshwater crisis, because these pieces of infrastructure have to be cooled with freshwater resources. It has to be fresh water, because if it‚Äôs any other type of water, it corrodes the equipment, it leads to bacterial growth.



And Bloomberg recently had a story that showed that two-thirds of these data centers are actually going into water-scarce areas, into places where the communities already do not have enough fresh water at their disposal. So that is one dimension of many that I refer to when I say, the extraordinary costs of this particular pathway for AI development.



Niall Firth: So in terms of costs and the extractive process of making AI, I wanted to give you the chance to talk about the other theme of the book, apart from just OpenAI‚Äôs explosion. It‚Äôs the colonial way of looking at the way AI is made: the empire. I‚Äôm saying this obviously because we‚Äôre here, but this is an idea that came out of reporting you started at MIT Technology Review and then continued into the book. Tell us about how this framing helps us understand how AI is made now.



Karen Hao: Yeah, so this was a framing that I started thinking a lot about when I was working on the AI Colonialism series for Tech Review. It was a series of stories that looked at the way that, pre-ChatGPT, the commercialization of AI and its deployment into the world was already leading to entrenchment of historical inequities into the present day.



And one example was a story that was about how facial recognition companies were swarming into South Africa to try and harvest more data from South Africa during a time when they were getting criticized for the fact that their technologies did not accurately recognize black faces. And the deployment of those facial recognition technologies into South Africa, into the streets of Johannesburg, was leading to what South African scholars were calling a recreation of a digital apartheid‚Äîthe controlling of black bodies, movement of black people.



And this idea really haunted me for a really long time. Through my reporting in that series, there were so many examples that I kept hitting upon of this thesis, that the AI industry was perpetuating. It felt like it was becoming this neocolonial force. And then, when ChatGPT came out, it became clear that this was just accelerating.&nbsp;



When you accelerate the scale of these technologies, and you start training them on the entirety of the Internet, and you start using these supercomputers that are the size of dozens‚Äîif not hundreds‚Äîof football fields. Then you really start talking about an extraordinary global level of extraction and exploitation that is happening to produce these technologies. And then the historical power imbalances become even more obvious.&nbsp;





And so there are four parallels that I draw in my book between what I have now termed empires of AI versus empires of old. The first one is that empires lay claim to resources that are not their own. So these companies are scraping all this data that is not their own, taking all the intellectual property that is not their own.



The second is that empires exploit a lot of labor. So we see them moving to countries in the Global South or other economically vulnerable communities to contract workers to do some of the worst work in the development pipeline for producing these technologies‚Äîand also producing technologies that then inherently are labor-automating and engage in labor exploitation in and of themselves.&nbsp;



And the third feature is that the empires monopolize knowledge production. So, in the last 10 years, we‚Äôve seen the AI industry monopolize more and more of the AI researchers in the world. So AI researchers are no longer contributing to open science, working in universities or independent institutions, and the effect on the research is what you would imagine would happen if most of the climate scientists in the world were being bankrolled by oil and gas companies. You would not be getting a clear picture, and we are not getting a clear picture, of the limitations of these technologies, or if there are better ways to develop these technologies.



And the fourth and final feature is that empires always engage in this aggressive race rhetoric, where there are good empires and evil empires. And they, the good empire, have to be strong enough to beat back the evil empire, and that is why they should have unfettered license to consume all of these resources and exploit all of this labor. And if the evil empire gets the technology first, humanity goes to hell. But if the good empire gets the technology first, they‚Äôll civilize the world, and humanity gets to go to heaven. So on many different levels, like the empire theme, I felt like it was the most comprehensive way to name exactly how these companies operate, and exactly what their impacts are on the world.



Niall Firth: Yeah, brilliant. I mean, you talk about the evil empire. What happens if the evil empire gets it first? And what I mentioned at the top is AGI. For me, it‚Äôs almost like the extra character in the book all the way through. It‚Äôs sort of looming over everything, like the ghost at the feast, sort of saying like, this is the thing that motivates everything at OpenAI. This is the thing we‚Äôve got to get to before anyone else gets to it.¬†



There‚Äôs a bit in the book about how they‚Äôre talking internally at OpenAI, like, we‚Äôve got to make sure that AGI is in US hands where it‚Äôs safe versus like anywhere else. And some of the international staff are openly like‚Äîthat‚Äôs kind of a weird way to frame it, isn‚Äôt it? Why is the US version of AGI better than others?&nbsp;



So tell us a bit about how it drives what they do. And AGI isn‚Äôt an inevitable fact that‚Äôs just happening anyway, is it? It‚Äôs not even a thing yet.



Karen Hao: There‚Äôs not even consensus around whether or not it‚Äôs even possible or what it even is. There was recently a New York Times story by Cade Metz that was citing a survey of long-standing AI researchers in the field, and 75% of them still think that we don‚Äôt have the techniques yet for reaching AGI, whatever that means. And the most classic definition or understanding of what AGI is, is being able to fully recreate human intelligence in software. But the problem is, we also don‚Äôt have scientific consensus around what human intelligence is. And so one of the aspects that I talk about a lot in the book is that, when there is a vacuum of shared meaning around this term, and what it would look like, when would we have arrived at it? What capabilities should we be evaluating these systems on to determine that we‚Äôve gotten there? It can basically just be whatever OpenAI wants.&nbsp;



So it‚Äôs kind of just this ever-present goalpost that keeps shifting, depending on where the company wants to go. You know, they have a full range, a variety of different definitions that they‚Äôve used throughout the years. In fact, they even have a joke internally: If you ask 13 OpenAI researchers what AGI is, you‚Äôll get 15 definitions. So they are kind of self-aware that this is not really a real term and it doesn‚Äôt really have that much meaning.&nbsp;



But it does serve this purpose of creating a kind of quasi-religious fervor around what they‚Äôre doing, where people think that they have to keep driving towards this horizon, and that one day when they get there, it‚Äôs going to have a civilizationally transformative impact. And therefore, what else should you be working on in your life, but this? And who else should be working on it, but you?&nbsp;



And so it is their justification not just for continuing to push and scale and consume all these resources‚Äîbecause none of that consumption, none of that harm matters anymore if you end up hitting this destination. But they also use it as a way to develop their technologies in a very deeply anti-democratic way, where they say, we are the only people that have the expertise, that have the right to carefully control the development of this technology and usher it into the world. And we cannot let anyone else participate because it‚Äôs just too powerful of a technology.



Niall Firth: You talk about the factions, particularly the religious framing. AGI has been around as a concept for a while‚Äîit was very niche, very kind of nerdy fun, really, to talk about‚Äîto suddenly become extremely mainstream. And they have the boomers versus doomers dichotomy. Where are you on that spectrum?



Karen Hao: So the boomers are people who think that AGI is going to bring us to utopia, and the doomers think AGI is going to devastate all of humanity. And to me these are actually two sides of the same coin. They both believe that AGI is possible, and it‚Äôs imminent, and it‚Äôs going to change everything.&nbsp;



And I am not on this spectrum. I‚Äôm in a third space, which is the AI accountability space, which is rooted in the observation that these companies have accumulated an extraordinary amount of power, both economic and political power, to go back to the empire analogy.&nbsp;



Ultimately, the thing that we need to do in order to not return to an age of empire and erode a lot of democratic norms is to hold these companies accountable with all the tools at our disposal, and to recognize all the harms that they are already perpetuating through a misguided approach to AI development.



Niall Firth: I‚Äôve got a couple of questions from readers. I‚Äôm gonna try to pull them together a little bit because Abbas asks, what would post-imperial AI look like? And there was a question from Liam basically along the same lines. How do you make a more ethical version of AI that is not within this framework?&nbsp;



Karen Hao: We sort of already touched a little bit upon this idea. But there are so many different ways to develop AI. There are myriads of techniques throughout the history of AI development, which is decades long. There have been various shifts in the winds of which techniques ultimately rise and fall. And it isn‚Äôt based solely on the scientific or technical merit of any particular technique. Oftentimes certain techniques become more popular because of business reasons or because of the funder‚Äôs ideologies. And that‚Äôs sort of what we‚Äôre seeing today with the complete indexing of AI development on large-scale AI model development.



And ultimately, these large-scale models ‚Ä¶ We talked about how it‚Äôs a remarkable technical leap, but in terms of social progress or economic progress, the benefits of these models have been kind of middling. And the way that I see us shifting to AI models that are going to be A) more beneficial and B) not so imperial is to refocus on task-specific AI systems that are tackling well-scoped challenges that inherently lend themselves to the strengths of AI systems that are inherently computational optimization problems.&nbsp;



So I‚Äôm talking about things like using AI to integrate more renewable energy into the grid. This is something that we definitely need. We need to more quickly accelerate our electrification of the grid, and one of the challenges of using more renewable energy is the unpredictability of it. And this is a key strength of AI technologies, being able to have predictive capabilities and optimization capabilities where you can match the energy generation of different renewables with the energy demands of different people that are drawing from the grid.



Niall Firth: Quite a few people have been asking, in the chat, different versions of the same question. If you were an early-career AI scientist, or if you were involved in AI, what can you do yourself to bring about a more ethical version of AI? Do you have any power left, or is it too late?&nbsp;



Karen Hao: No, I don‚Äôt think it‚Äôs too late at all. I mean, as I‚Äôve been talking with a lot of people just in the lay public, one of the biggest challenges that they have is they don‚Äôt have any alternatives for AI. They want the benefits of AI, but they also do not want to participate in a supply chain that is really harmful. And so the first question is, always, is there an alternative? Which tools do I shift to? And unfortunately, there just aren‚Äôt that many alternatives right now.&nbsp;



And so the first thing that I would say to early-career AI researchers and entrepreneurs is to build those alternatives, because there are plenty of people that are actually really excited about the possibility of switching to more ethical alternatives. And one of the analogies I often use is that we kind of need to do with the AI industry what happened with the fashion industry. There was also a lot of environmental exploitation, labor exploitation in the fashion industry, and there was enough consumer demand that it created new markets for ethical and sustainably sourced fashion. And so we kind of need to see just more options occupying that space.



Niall Firth: Do you feel optimistic about the future? Or where do you sit? You know, things aren‚Äôt great as you spell them out now. Where‚Äôs the hope for us?



Karen Hao: I am. I‚Äôm super optimistic. Part of the reason why I‚Äôm optimistic is because you know, a few years ago, when I started writing about AI at Tech Review, I remember people would say, wow, that‚Äôs a really niche beat. Do you have enough to write about?&nbsp;



And now, I mean, everyone is talking about AI, and I think that‚Äôs the first step to actually getting to a better place with AI development. The amount of public awareness and attention and scrutiny that is now going into how we develop these technologies, how we use these technologies, is really, really important. Like, we need to be having this public debate and that in and of itself is a significant step change from what we had before.&nbsp;



But the next step, and part of the reason why I wrote this book, is we need to convert the awareness into action, and people should take an active role. Every single person should feel that they have an active role in shaping the future of AI development, if you think about all of the different ways that you interface with the AI development supply chain and deployment supply chain‚Äîlike you give your data or withhold your data.



There are probably data centers that are being built around you right now. If you‚Äôre a parent, there‚Äôs some kind of AI policy being crafted at [your kid‚Äôs] school. There‚Äôs some kind of AI policy being crafted at your workplace. These are all what I consider sites of democratic contestation, where you can use those opportunities to assert your voice about how you want AI to be developed and deployed. If you do not want these companies to use certain kinds of data, push back when they just take the data.&nbsp;



I closed all of my personal social media accounts because I just did not like the fact that they were scraping my personal photos to train their generative AI models. I‚Äôve seen parents and students and teachers start forming committees within schools to talk about what their AI policy should be and to draft it collectively as a community. Same with businesses. They‚Äôre doing the same thing. If we all kind of step up to play that active role, I am super optimistic that we‚Äôll get to a better place.



Niall Firth: Mark, in the chat, mentions the MƒÅori story from New Zealand towards the end of your book, and that‚Äôs an example of sort of community-led AI in action, isn‚Äôt it?



Karen Hao: Yeah. There was a community in New Zealand that really wanted to help revitalize the MƒÅori language by building a speech recognition tool that could recognize MƒÅori, and therefore be able to transcribe a rich repository of archival audio of their ancestors speaking MƒÅori. And the first thing that they did when engaging in that project was they asked the community, do you want this AI tool?&nbsp;



Niall Firth: Imagine that.



Karen Hao: I know! It‚Äôs such a radical concept, this idea of consent at every stage. But they first asked that; the community wholeheartedly said yes. They then engaged in a public education campaign to explain to people, okay, what does it take to develop an AI tool? Well, we are going to need data. We‚Äôre going to need audio transcription pairs to train this AI model. So then they ran a public contest in which they were able to get dozens, if not hundreds, of people in their community to donate data to this project. And then they made sure that when they developed the model, they actively explained to the community at every step how their data was being used, how it would be stored, how it would continue to be protected. And any other project that would use the data has to get permission and consent from the community first.&nbsp;





And so it was a completely democratic process, for whether they wanted the tool, how to develop the tool, and how the tool should continue to be used, and how their data should continue to be used over time.



Niall Firth: Great. I know we‚Äôve gone a bit over time. I‚Äôve got two more things I‚Äôm going to ask you, basically putting together lots of questions people have asked in the chat about your view on what role regulations should play. What are your thoughts on that?



Karen Hao: Yeah, I mean, in an ideal world where we actually had a functioning government, regulation should absolutely play a huge role. And it shouldn‚Äôt just be thinking about once an AI model is built, how to regulate that. But still thinking about the full supply chain of AI development, regulating the data and what‚Äôs allowed to be trained in these models, regulating the land use. And what pieces of land are allowed to build data centers? How much energy and water are the data centers allowed to consume? And also regulating the transparency. We don‚Äôt know what data is in these training data sets, and we don‚Äôt know the environmental costs of training these models. We don‚Äôt know how much water these data centers consume and that is all information that these companies actively withhold to prevent democratic processes from happening. So if there were one major intervention that regulators could have, it should be to dramatically increase the amount of transparency along the supply chain.



Niall Firth: Okay, great. So just to bring it back around to OpenAI and Sam Altman to finish with. He famously sent an email around, didn‚Äôt he? After your original Tech Review story, saying this is not great. We don‚Äôt like this. And he didn‚Äôt want to speak to you for your book, either, did he?



Karen Hao: No, he did not.



Niall Firth: No. But imagine Sam Altman is in the chat here. He‚Äôs subscribed to Technology Review and is watching this Roundtables because he wants to know what you‚Äôre saying about him. If you could talk to him directly, what would you like to ask him?&nbsp;



Karen Hao: What degree of harm do you need to see in order to realize that you should take a different path?&nbsp;



Niall Firth: Nice, blunt, to the point. All right, Karen, thank you so much for your time.&nbsp;



Karen Hao: Thank you so much, everyone.



MIT Technology Review Roundtables is a subscriber-only online event series where experts discuss the latest developments and what‚Äôs next in emerging technologies. Sign up to get notified about upcoming sessions.
‚Ä¢ Why the AI moratorium‚Äôs defeat may signal a new political era
  The ‚ÄúBig, Beautiful Bill‚Äù that President Donald Trump signed into law on July 4 was chock full of controversial policies‚ÄîMedicaid work requirements, increased funding for ICE, and an end to tax credits for clean energy and vehicles, to name just a few. But one highly contested provision was missing. Just days earlier, during a late-night voting session, the Senate had killed the bill‚Äôs 10-year moratorium on state-level AI regulation.&nbsp;



‚ÄúWe really dodged a bullet,‚Äù says Scott Wiener, a California state senator and the author of SB 1047, a bill that would have made companies liable for harms caused by large AI models. It was vetoed by Governor Gavin Newsom last year, but Wiener is now working to pass SB 53, which establishes whistleblower protections for employees of AI companies. Had the federal AI regulation moratorium passed, he says, that bill likely would have been dead.



The moratorium could also have killed laws that have already been adopted around the country, including a Colorado law that targets algorithmic discrimination, laws in Utah and California aimed at making AI-generated content more identifiable, and other legislation focused on preserving data privacy and keeping children safe online. Proponents of the moratorium, such OpenAI and Senator Ted Cruz, have said that a ‚Äúpatchwork‚Äù of state-level regulations would place an undue burden on technology companies and stymie innovation. Federal regulation, they argue, is a better approach‚Äîbut there is currently no federal AI regulation in place.



Wiener and other state lawmakers can now get back to work writing and passing AI policy, at least for the time being‚Äîwith the tailwind of a major moral victory at their backs. The movement to defeat the moratorium was impressively bipartisan: 40 state attorneys general signed a letter to Congress opposing the measure, as did a group of over 250 Republican and Democratic state lawmakers. And while congressional Democrats were united against the moratorium, the final nail in its coffin was hammered in by Senator Marsha Blackburn of Tennessee, a Tea Party conservative and Trump ally who backed out of a compromise with Cruz at the eleventh hour.



The moratorium fight may have signaled a bigger political shift. ‚ÄúIn the last few months, we‚Äôve seen a much broader and more diverse coalition form in support of AI regulation generally,‚Äù says Amba Kak, co‚Äìexecutive director of the AI Now Institute. After years of relative inaction, politicians are getting concerned about the risks of unregulated artificial intelligence.&nbsp;





Granted, there‚Äôs an argument to be made that the moratorium‚Äôs defeat was highly contingent. Blackburn appears to have been motivated almost entirely by concerns about children‚Äôs online safety and the rights of country musicians to control their own likenesses; state lawmakers, meanwhile, were affronted by the federal government‚Äôs attempt to defang legislation that they had already passed. 



And even though powerful technology firms such as Andreessen Horowitz and OpenAI reportedly lobbied in favor of the moratorium, continuing to push for it might not have been worth it to the Trump administration and its allies‚Äîat least not at the expense of tax breaks and entitlement cuts. Baobao Zhang, an associate professor of political science at Syracuse University, says that the administration may have been willing to give up on the moratorium in order to push through the rest of the bill by its self-imposed Independence Day deadline.



Andreessen Horowitz did not respond to a request for comment. OpenAI noted that the company was opposed to a state-by-state approach to AI regulation but did not respond to specific questions regarding the moratorium‚Äôs defeat.&nbsp;



It‚Äôs almost certainly the case that the moratorium‚Äôs breadth, as well as its decade-long duration, helped opponents marshall a diverse coalition to their side. But that breadth isn‚Äôt incidental‚Äîit‚Äôs related to the very nature of AI. Blackburn, who represents country musicians in Nashville, and Wiener, who represents software developers in San Francisco, have a shared interest in AI regulation precisely because such a powerful and general-purpose tool has the potential to affect so many people‚Äôs well-being and livelihood. ‚ÄúThere are real anxieties that are touching people of all classes,‚Äù Kak says. ‚ÄúIt‚Äôs creating solidarities that maybe didn‚Äôt exist before.‚Äù



Faced with outspoken advocates, concerned constituents, and the constant buzz of AI discourse, politicians from both sides of the aisle are starting to argue for taking AI extremely seriously. One of the most prominent anti-moratorium voices was Marjorie Taylor Greene, who voted for the version of the bill containing the moratorium before admitting that she hadn‚Äôt read it thoroughly and committing to opposing the moratorium moving forward. ‚ÄúWe have no idea what AI will be capable of in the next 10 years,‚Äù she posted last month.



And two weeks ago, Pete Buttigieg, President Biden‚Äôs transportation secretary, published a Substack post entitled ‚ÄúWe Are Still Underreacting on AI.‚Äù ‚ÄúThe terms of what it is like to be a human are about to change in ways that rival the transformations of the Enlightenment or the Industrial Revolution, only much more quickly,‚Äù he wrote.



Wiener has noticed a shift among his peers. ‚ÄúMore and more policymakers understand that we can‚Äôt just ignore this,‚Äù he says. But awareness is several steps short of effective legislation, and regulation opponents aren‚Äôt giving up the fight. The Trump administration is reportedly working on a slate of executive actions aimed at making more energy available for AI training and deployment, and Cruz says he is planning to introduce his own anti-regulation bill.



Meanwhile, proponents of regulation will need to figure out how to channel the broad opposition to the moratorium into support for specific policies. It won‚Äôt be a simple task. ‚ÄúIt‚Äôs easy for all of us to agree on what we don‚Äôt want,‚Äù Kak says. ‚ÄúThe harder question is: What is it that we do want?‚Äù

üîí Cybersecurity & Privacy
‚Ä¢ Microsoft Patch Tuesday, July 2025 Edition
  Microsoft releases updates to fix at least 137 security vulnerabilities in its Windows operating systems and supported software . None of the weaknesses addressed this month are known to be actively exploited, but 14 of the flaws earned Microsoft‚Äôs most-dire &#8217;s most dire . The flaws could be exploited to seize control over vulnerable Windows PCs with little or no help from users . Microsoft also patched at least four critical, remote code execution flaws in Office .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Daniel Carpenter, the Allie S. Freed Professor of Government and chair of the department of government at Harvard University, explains how the US Food and Drug Administration‚Äôs rigorous, multi-phase drug approval process serves as a gatekeeper that builds public trust and scientific credibility, while Timo Minssen, professor of law and founding director of the Center for Advanced Studies in Bioscience Innovation Law at the University of Copenhagen, explores the evolving regulatory landscape of medical devices with a focus on the challenges of balancing innovation with public safety. Later, Microsoft‚Äôs Chad Atalla, an applied scientist in responsible AI, discusses the sociotechnical nature of AI models and systems, their team‚Äôs work building an evaluation framework inspired by social science, and where AI researchers, developers, and policymakers might find inspiration from the approach to governance and testing in pharmaceuticals and medical devices.







Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: The History and Evolution of Testing in Pharmaceutical RegulationCase study | January 2025&nbsp;



Learning from other Domains to Advance AI Evaluation and Testing: Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI GovernanceCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025‚ÄØ‚ÄØ&nbsp;



Evaluating Generative AI Systems is a Social Science Measurement Challenge&nbsp;Publication&nbsp;|&nbsp;November 2024‚ÄØ&nbsp;



STAC: Sociotechnical Alignment Center‚ÄØ



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Research‚ÄØ








	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



SULLIVAN: Today, I&#8217;m excited to welcome Dan Carpenter and Timo Minssen to the podcast to explore testing and risk assessment in the areas of pharmaceuticals and medical devices, respectively.



Dan Carpenter is chair of the Department of Government at Harvard University. His research spans the sphere of social and political science, from petitioning in democratic society to regulation and government organizations. His recent work includes the FDA Project, which examines pharmaceutical regulation in the United States.



Timo is a professor of law at the University of Copenhagen, where he is also director of the Center for Advanced Studies in Bioscience Innovation Law. He specializes in legal aspects of biomedical innovation, including intellectual property law and regulatory law. He&#8217;s exercised his expertise as an advisor to such organizations as the World Health Organization and the European Commission.



And after our conversations, we&#8217;ll talk to Microsoft&#8217;s Chad Atalla, an applied scientist in responsible AI, about how we should think about these insights in the context of AI.



Daniel, it&#8217;s a pleasure to welcome you to the podcast. I&#8217;m just so appreciative of you being here. Thanks for joining us today.



				
				
					



DANIEL CARPENTER:&nbsp;Thanks for having me.&nbsp;



SULLIVAN:&nbsp;Dan, before we dissect policy,&nbsp;let&#8217;s&nbsp;rewind the tape to your&nbsp;origin&nbsp;story. Can you take us to the moment that you first became fascinated with regulators rather than, say, politicians? Was there a spark that pulled you toward the FDA story?&nbsp;



CARPENTER:&nbsp;At one point during graduate school, I was studying a combination of American politics and political theory, and I did a summer interning at the Department of Housing and Urban Development. And I began to think, why don&#8217;t people study these administrators more and the rules they make, the, you know,&nbsp;inefficiencies, the efficiencies?&nbsp;Really more&nbsp;from,&nbsp;kind of,&nbsp;a descriptive standpoint, less from a normative standpoint.&nbsp;And I was reading a lot that summer about the Food and Drug Administration and some of the decisions it was making on AIDS drugs. That was&nbsp;a,&nbsp;sort of,&nbsp;a major, &#8230;



SULLIVAN: Right.&nbsp;



CARPENTER: &#8230; sort of, you know,&nbsp;moment in the news, in the global news as well as the national news during, I would say, what?&nbsp;The late&nbsp;‚Äô80s, early&nbsp;‚Äô90s? And&nbsp;so&nbsp;I began to&nbsp;look&nbsp;into&nbsp;that.



SULLIVAN:&nbsp;So now that we know what pulled you in,&nbsp;let‚Äôs&nbsp;zoom out for our listeners. Give us&nbsp;the&nbsp;whirlwind tour. I think most of us know pharma involves years of trials, but&nbsp;what‚Äôs&nbsp;the part we&nbsp;don‚Äôt&nbsp;know?



CARPENTER:&nbsp;So&nbsp;I think when most businesses develop a product, they all go through some phases of research and development and testing. And I think&nbsp;what&#8217;s&nbsp;different about the FDA is,&nbsp;sort of,&nbsp;two-&nbsp;or three-fold.



First, a lot of those tests are much more stringently specified and regulated by the government, and second, one of the reasons for that is that the FDA imposes not simply safety requirements upon drugs&nbsp;in particular but&nbsp;also efficacy requirements. The FDA wants you to prove not simply that&nbsp;it&#8217;s&nbsp;safe and non-toxic&nbsp;but also that&nbsp;it&#8217;s&nbsp;effective.&nbsp;And the final thing,&nbsp;I think, that&nbsp;makes the FDA different is that it stands as what I would call the&nbsp;‚Äúveto player‚Äù&nbsp;over R&amp;D [research and development] to the marketplace.&nbsp;The FDA&nbsp;basically has,&nbsp;sort of,&nbsp;this control over entry&nbsp;to&nbsp;the marketplace.



And&nbsp;so&nbsp;what that involves is usually first, a set of human trials where people who have no disease take it. And&nbsp;you&#8217;re&nbsp;only looking&nbsp;for&nbsp;toxicity generally. Then&nbsp;there&#8217;s&nbsp;a set of Phase 2 trials, where they look more at safety and a little bit at efficacy, and&nbsp;you&#8217;re&nbsp;now examining people who have the disease that the drug claims to treat. And&nbsp;you&#8217;re&nbsp;also basically comparing people who get the drug,&nbsp;often&nbsp;with those who do not.



And then finally, Phase 3 involves a much more direct and large-scale attack, if you will, or assessment of efficacy, and&nbsp;that&#8217;s&nbsp;where you get the sort of large randomized clinical trials that are&nbsp;very expensive&nbsp;for pharmaceutical companies, biomedical companies to launch, to execute, to analyze. And those are often the sort of core evidence base for the decisions that the FDA makes about&nbsp;whether or not&nbsp;to approve a new drug for marketing in the United States.



SULLIVAN:&nbsp;Are there&nbsp;differences in how that process has, you know, changed through other countries and&nbsp;maybe just&nbsp;how&nbsp;that&#8217;s&nbsp;evolved as&nbsp;you&#8217;ve&nbsp;seen it play out?&nbsp;



CARPENTER:&nbsp;Yeah, for a long time, I would say that the United States had&nbsp;probably the&nbsp;most&nbsp;stringent regime&nbsp;of regulation for biopharmaceutical products until,&nbsp;I would say,&nbsp;about the 1990s and early 2000s. It used to be the case that a number of other countries, especially in Europe but around the world, basically waited for the FDA to mandate tests on a drug and only after the drug was approved in the United States would they deem it approvable and marketable in their own countries. And then after the formation of the European Union and the creation of the European Medicines Agency, gradually the European Medicines Agency began to get a bit more stringent.&nbsp;&nbsp;



But, you know,&nbsp;over the long run,&nbsp;there&#8217;s&nbsp;been a&nbsp;lot of,&nbsp;sort&nbsp;of,&nbsp;heterogeneity, a lot of variation over time and space, in the way that the FDA has approached these problems. And&nbsp;I&#8217;d&nbsp;say in the last 20 years, it&#8217;s begun to partially deregulate, namely,&nbsp;you know,&nbsp;trying to find all sorts of mechanisms or pathways for really innovative&nbsp;drugs for deadly diseases without a lot of treatments to&nbsp;basically get&nbsp;through the process at lower cost.&nbsp;For many people,&nbsp;that has not been sufficient.&nbsp;They&#8217;re&nbsp;concerned about the cost of the system.&nbsp;Of course, then the agency also gets criticized by those&nbsp;who believe&nbsp;it&#8217;s&nbsp;too lax. It is&nbsp;potentially letting&nbsp;ineffective and unsafe therapies on the market.



SULLIVAN:&nbsp;In your view, when does the structured model genuinely safeguard patients and where do you think it&nbsp;maybe slows&nbsp;or&nbsp;limits&nbsp;innovation?



CARPENTER:&nbsp;So&nbsp;I think&nbsp;the worry&nbsp;is that if you approach pharmaceutical approval as a world where only things can go wrong,&nbsp;then&nbsp;you&#8217;re&nbsp;really at a risk of limiting innovation. And even if you end up letting a lot of things through, if by your regulations you end up basically slowing down the development process or making it very, very costly, then there&#8217;s just a whole bunch of drugs that either come to market too slowly or they come to market not at all because&nbsp;they just aren&#8217;t worth the kind of cost-benefit or, sort of, profit analysis of the firm.&nbsp;You know, so&nbsp;that&#8217;s&nbsp;been a concern.&nbsp;And I think&nbsp;it&#8217;s&nbsp;been one of the reasons that the Food and Drug Administration as well as other world regulators have begun to&nbsp;basically try&nbsp;to smooth the process and accelerate the process at the margins.



The other thing is that&nbsp;they&#8217;ve&nbsp;started to&nbsp;basically make&nbsp;approvals&nbsp;on the basis of&nbsp;what are called&nbsp;surrogate endpoints. So the idea is that a cancer drug, we really want to know whether that drug saves lives, but if we wait to see whose lives are saved or prolonged by that drug, we might miss the opportunity to make judgments on the basis of, well, are we detecting tumors in the bloodstream? Or can we measure the size of those tumors&nbsp;in, say, a&nbsp;solid cancer? And then the further question is, is the size of the tumor&nbsp;basically a&nbsp;really good&nbsp;correlate&nbsp;or predictor of whether people will die or&nbsp;not, right?&nbsp;Generally, the&nbsp;FDA tends to be less stringent when&nbsp;you&#8217;ve&nbsp;got, you know, a remarkably innovative new&nbsp;therapy&nbsp;and the disease being treated is one that just&nbsp;doesn&#8217;t&nbsp;have a lot of available treatments,&nbsp;right.



The one thing that people often think about when&nbsp;they&#8217;re&nbsp;thinking about pharmaceutical regulation is they often contrast,&nbsp;kind of,&nbsp;speed versus safety &#8230;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;&#8230; right. And&nbsp;that&#8217;s&nbsp;useful as a tradeoff,&nbsp;but I often try to remind people that&nbsp;it&#8217;s&nbsp;not simply&nbsp;about whether the drug gets out&nbsp;there&nbsp;and&nbsp;it&#8217;s&nbsp;unsafe. You know, you and I as patients and even doctors have&nbsp;a hard time&nbsp;knowing whether something works and whether it should be prescribed. And the evidence for knowing whether something works&nbsp;isn&#8217;t&nbsp;just, well,&nbsp;you&nbsp;know, Sally took&nbsp;it&nbsp;or Dan took it or Kathleen took it, and they&nbsp;seem to get&nbsp;better or they&nbsp;didn&#8217;t&nbsp;seem to get better.&nbsp;&nbsp;



The really rigorous evidence comes from randomized clinical trials.&nbsp;And I think&nbsp;it&#8217;s&nbsp;fair to say that if you didn&#8217;t&nbsp;have the FDA there as a veto player, you&nbsp;wouldn&#8217;t&nbsp;get as many randomized clinical&nbsp;trials&nbsp;and the evidence&nbsp;probably&nbsp;wouldn&#8217;t&nbsp;be as rigorous for whether these things work. And as I like to put it,&nbsp;basically there&#8217;s&nbsp;a whole ecology of expectations and beliefs around the biopharmaceutical industry in the United States and globally,&nbsp;and to some extent,&nbsp;it&#8217;s&nbsp;undergirded by&nbsp;all of&nbsp;these tests that happen.&nbsp;&nbsp;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;And in part, that means&nbsp;it&#8217;s&nbsp;undergirded by regulation. Would there still be a market without regulation? Yes. But it would be a market in which people had far less information in and confidence about the drugs that are being taken. And&nbsp;so&nbsp;I think&nbsp;it&#8217;s&nbsp;important to recognize that kind of confidence-boosting potential of, kind of, a scientific regulation base.&nbsp;



SULLIVAN:&nbsp;Actually, if we could&nbsp;double-click&nbsp;on that for a minute, I&#8217;d love to hear your perspective on, testing&nbsp;has been completed;&nbsp;there&#8217;s results.&nbsp;Can you walk us through how those results actually shape the next steps and decisions of a particular drug and just,&nbsp;like,&nbsp;how regulators actually think about using that data to influence really what happens next with it?



CARPENTER:&nbsp;Right.&nbsp;So&nbsp;it&#8217;s&nbsp;important to understand that every drug is approved for&nbsp;what&#8217;s called&nbsp;an indication. It can have a first primary&nbsp;indication, which is the main disease that it treats, and then others can be added as more evidence is shown. But a drug is not something that just kind of exists out there in the ether.&nbsp;It has to have the right form of administration.&nbsp;Maybe it&nbsp;should be injected.&nbsp;Maybe it&nbsp;should be ingested.&nbsp;Maybe it&nbsp;should&nbsp;be administered only at a clinic&nbsp;because it needs to be&nbsp;kind of administered&nbsp;in just the right way. As doctors will tell you, dosage is everything, right.&nbsp;&nbsp;



And&nbsp;so&nbsp;one of the reasons that you want those trials is not simply a, you know, yes or no answer about whether the drug works,&nbsp;right.&nbsp;It&#8217;s&nbsp;not simply if-then.&nbsp;It&#8217;s&nbsp;literally what&nbsp;goes into what you might call the dose response curve.&nbsp;You know, how much of this drug do we need to&nbsp;basically, you know,&nbsp;get the benefit? At what point does that fall off significantly that we can&nbsp;basically say, we can stop there? All that evidence comes from&nbsp;trials. And&nbsp;that&#8217;s&nbsp;the kind of evidence that is&nbsp;required&nbsp;on the basis of&nbsp;regulation.&nbsp;&nbsp;



Because&nbsp;it&#8217;s&nbsp;not simply a drug&nbsp;that&#8217;s&nbsp;approved.&nbsp;It&#8217;s&nbsp;a drug and a&nbsp;frequency&nbsp;of administration. It&#8217;s&nbsp;a&nbsp;method of administration.&nbsp;And&nbsp;so&nbsp;the drug&nbsp;isn&#8217;t&nbsp;just,&nbsp;there&#8217;s&nbsp;something to be taken off the shelf and popped into your mouth. I mean, sometimes&nbsp;that&#8217;s&nbsp;what happens, but even then,&nbsp;we want to know what the dosage is,&nbsp;right.&nbsp;We want to know what to look for in terms of side effects, things like that.



SULLIVAN:&nbsp;Going back to that point, I&nbsp;mean,&nbsp;it sounds like&nbsp;we&#8217;re&nbsp;making a lot of progress from a regulation perspective&nbsp;in, you know, sort of speed and getting things approved but doing it in a&nbsp;really balanced&nbsp;way. I mean, any other kind of closing thoughts on the tradeoffs there or where&nbsp;you&#8217;re&nbsp;seeing that going?



CARPENTER:&nbsp;I think&nbsp;you&#8217;re&nbsp;going to see some move in the coming years‚Äîthere&#8217;s&nbsp;already been some of it‚Äîto say, do we always need a&nbsp;really large&nbsp;Phase 3 clinical trial? And to what degree do we need the, like, you&nbsp;know,&nbsp;all the i&#8217;s dotted and the t&#8217;s crossed or a really,&nbsp;really large&nbsp;sample size?&nbsp;And&nbsp;I&#8217;m&nbsp;open to innovation there.&nbsp;I&#8217;m&nbsp;also open to the idea that we consider, again, things like accelerated approvals or pathways for looking at&nbsp;different kinds&nbsp;of surrogate endpoints.&nbsp;I do think, once we do that, then we also have to have some degree of follow-up.



SULLIVAN:&nbsp;So&nbsp;I know&nbsp;we&#8217;re&nbsp;getting&nbsp;close to&nbsp;out of time, but&nbsp;maybe just&nbsp;a quick rapid fire if&nbsp;you‚Äôre&nbsp;open to it. Biggest myth about clinical trials?



CARPENTER:&nbsp;Well, some people tend to think that the FDA performs them.&nbsp;You know,&nbsp;it&#8217;s&nbsp;companies that do it. And the only other thing I would say is the company that does a lot of the testing and even the innovating is not always the company that takes the drug to market, and it tells you something about how powerful regulation is in our system, in our world,&nbsp;that you often need a company that has dealt with the FDA quite a bit and knows all the regulations and knows how to dot the i&#8217;s and cross the t&#8217;s in order to get a drug across the finish line.



SULLIVAN:&nbsp;If you had a magic wand,&nbsp;what&#8217;s&nbsp;the one thing&nbsp;you&#8217;d&nbsp;change in regulation today?



CARPENTER:&nbsp;I would like people to think a little bit less about just speed versus safety and,&nbsp;again, more about this basic issue of confidence. I think&nbsp;it&#8217;s&nbsp;fundamental to everything that happens in markets but especially in biopharmaceuticals.



SULLIVAN:&nbsp;Such a great point.&nbsp;This has been really fun.&nbsp;Just thanks so much for being here today. We&#8217;re really excited to share your thoughts&nbsp;out to&nbsp;our listeners. Thanks.



[TRANSITION MUSIC]&nbsp;



CARPENTER:&nbsp;Likewise.&nbsp;



SULLIVAN:&nbsp;Now&nbsp;to&nbsp;the world of medical devices,&nbsp;I&#8217;m&nbsp;joined by Professor Timo&nbsp;Minssen. Professor Minssen, it&#8217;s&nbsp;great to have you here. Thank you for joining us today.&nbsp;



TIMO&nbsp;MINSSEN:&nbsp;Yeah, thank you very much,&nbsp;it&#8217;s&nbsp;a pleasure.



SULLIVAN:&nbsp;Before getting into the regulatory world of medical devices, tell our audience a bit about your personal journey or your origin story, as&nbsp;we&#8217;re&nbsp;asking our guests. How did you land in regulation, and what&#8217;s kept you hooked in this space?



MINSSEN:&nbsp;So&nbsp;I started out as a patent expert in the biomedical area, starting with my PhD thesis on patenting biologics in Europe and in the US.&nbsp;So&nbsp;during that time, I was mostly interested in patent and trade secret questions.&nbsp;But at the same time, I also developed and taught courses in regulatory law and held talks on regulating advanced medical therapy medicinal products.&nbsp;I&nbsp;then&nbsp;started to lead large research projects on legal challenges in a wide variety of health and life science innovation frontiers. I also started to focus increasingly on AI-enabled medical devices and software as a medical device, resulting in several academic articles in this area&nbsp;and also&nbsp;in the regulatory area and a book on the future of medical device regulation.&nbsp;&nbsp;



SULLIVAN:&nbsp;Yeah,&nbsp;what&#8217;s&nbsp;kept you hooked in&nbsp;the space?



MINSSEN:&nbsp;It&#8217;s&nbsp;just incredibly exciting,&nbsp;in particular right&nbsp;now with everything that is going on, you know, in the software arena, in the marriage between AI and medical devices. And this is really challenging not only societies but also regulators and authorities in Europe and in the US.



SULLIVAN:&nbsp;Yeah,&nbsp;it&#8217;s&nbsp;a super exciting time to be in this space. You know, we talked to Daniel a little earlier and, you know, I think&nbsp;similar to&nbsp;pharmaceuticals, people have a general sense of what we mean when we say medical devices, but most listeners may&nbsp;picture&nbsp;like a stethoscope or a hip implant. The word &#8220;medical device&#8221;&nbsp;reaches&nbsp;much wider. Can you give us a quick, kind of, range from perhaps&nbsp;very simple&nbsp;to even, I don&#8217;t know, sci-fi and then your 90-second tour of how risk assessment works and why a framework is essential?



MINSSEN:&nbsp;Let me start out by saying that&nbsp;the WHO [World Health Organization] estimates that today there are approximately 2 million different kinds of medical devices on the world market, and as of the FDA&#8217;s latest update that I&#8217;m aware of, the FDA has authorized more than 1,000 AI-, machine learning-enabled medical devices, and that number is rising rapidly.



So in that context, I think it is important to understand that medical devices can be any instrument, apparatus, implement, machine, appliance, implant, reagent for in vitro use, software, material, or other similar or related articles that are&nbsp;intended&nbsp;by the manufacturer to be used alone or in combination for a medical purpose. And the spectrum of what constitutes a medical device can&nbsp;thus&nbsp;range from very simple devices such as tongue depressors, contact lenses, and thermometers to more complex devices such as blood pressure monitors, insulin pumps, MRI machines, implantable pacemakers, and even software as a medical device or AI-enabled monitors or drug device combinations, as well.



So&nbsp;talking about regulation,&nbsp;I think&nbsp;it&nbsp;is also&nbsp;very important&nbsp;to stress that medical devices are used in many diverse situations by&nbsp;very different&nbsp;stakeholders. And testing&nbsp;has to&nbsp;take this variety into consideration, and it is intrinsically tied to regulatory requirements across various&nbsp;jurisdictions.



During the pre-market phase, medical testing&nbsp;establishes&nbsp;baseline safety and effectiveness metrics through bench testing, performance standards, and clinical studies. And post-market testing ensures that real-world data informs ongoing compliance and safety improvements. So testing is indispensable in translating technological innovation into safe and effective medical devices. And while&nbsp;particular details&nbsp;of pre-market and post-market review procedures may slightly differ among countries, most developed&nbsp;jurisdictions regulate medical devices similarly to the US or European models.‚ÄØ



So&nbsp;most&nbsp;jurisdictions&nbsp;with medical device regulation classify devices based on their risk profile, intended use, indications for use, technological characteristics,&nbsp;and the regulatory controls necessary to provide a reasonable assurance of safety and effectiveness.



SULLIVAN:&nbsp;So medical devices face a pretty prescriptive multi-level testing path before they hit the market. From your vantage point, what are some of the downsides of that system and when does it make the most sense?



MINSSEN:&nbsp;One primary drawback is, of course, the lengthy and expensive approval process. High-risk devices, for example, often undergo years of clinical trials,&nbsp;which can cost millions of dollars, and this can create a significant barrier for startups and small companies with limited resources.&nbsp;And even for moderate-risk devices, the regulatory burden can slow product development and time to the market.



And the approach can also limit flexibility. Prescriptive requirements may not accommodate emerging innovations like digital therapeutics or AI-based diagnostics in&nbsp;a feasible&nbsp;way. And in such cases, the framework can unintentionally [stiffen]&nbsp;innovation by discouraging creative solutions or iterative improvements, which as matter of fact can also&nbsp;put&nbsp;patients&nbsp;at risk when you&nbsp;don&#8217;t&nbsp;use&nbsp;new technologies and AI.&nbsp;And&nbsp;additionally, the same level of scrutiny may be applied to low-risk devices, where&nbsp;the extensive testing and documentation may also be disproportionate to the actual patient risk.



However, the prescriptive model is highly&nbsp;appropriate where&nbsp;we have high testing standards for high-risk medical devices, in my view, particularly those that are life-sustaining, implanted, or involve new materials or mechanisms.



I also wanted to say that I think that these higher compliance thresholds can be OK and necessary if you have a system where authorities and stakeholders also have the capacity and funding to enforce, monitor, and achieve compliance with such rules in a feasible, time-effective, and straightforward manner. And this, of course, requires resources, novel solutions,&nbsp;and investments.



SULLIVAN:&nbsp;A range of tests are undertaken across the life cycle of medical devices.&nbsp;How do these testing requirements vary across&nbsp;different stages&nbsp;of development and across various applications?



MINSSEN:&nbsp;Yes,&nbsp;that&#8217;s&nbsp;a good question.&nbsp;So&nbsp;I think first it&nbsp;is important to realize that testing is conducted by various entities, including manufacturers, independent third-party laboratories, and regulatory agencies. And it occurs throughout the device&nbsp;life&nbsp;cycle, beginning with iterative testing during the research and development stage, advancing to pre-market evaluations, and continuing into post-market monitoring. And the outcomes of&nbsp;these tests directly&nbsp;impact&nbsp;regulatory approvals, market access, and device design refinements, as well.&nbsp;So&nbsp;the testing results are typically shared with regulatory authorities and in some cases with healthcare providers and the broader public to enhance transparency and trust.



So&nbsp;if you talk about the&nbsp;different phases&nbsp;that play a role here ‚Ä¶ so&nbsp;let&#8217;s&nbsp;turn to the pre-market phase, where manufacturers must&nbsp;demonstrate&nbsp;that the device is conformed to safety and performance benchmarks defined by regulatory authorities. Pre-market evaluations include functional bench testing, biocompatibility, for example, assessments and software validation, all of which are integral components of a manufacturer&#8217;s submission.&nbsp;



But, yes, but, testing also, and we touched already up on that, extends into the post-market phase, where it continues to ensure device safety and efficacy, and post-market surveillance relies on testing to&nbsp;monitor real-world performance and&nbsp;identify&nbsp;emerging risks on the post-market phase. By integrating real-world evidence into ongoing assessments, manufacturers can address unforeseen issues, update devices as needed, and&nbsp;maintain compliance with evolving regulatory expectations. And&nbsp;I think this&nbsp;is particularly important in this new generation of medical devices that are AI-enabled or machine-learning enabled.



I think we have to understand that in this AI-enabled medical devices field, you know, the devices and the algorithms that are working with&nbsp;them, they&nbsp;can improve in the lifetime of a product.&nbsp;So actually, not&nbsp;only you could assess them and make sure that they&nbsp;maintain&nbsp;safe,&nbsp;you&nbsp;could also sometimes lower the risk category by finding evidence that these devices are&nbsp;actually becoming&nbsp;more precise and safer.&nbsp;So&nbsp;it can both, you know, heighten the risk&nbsp;category&nbsp;or lower the risk category, and&nbsp;that&#8217;s&nbsp;why&nbsp;this continuous testing is so important.



SULLIVAN:&nbsp;Given what you just said, how should regulators handle a device whose algorithm keeps updating itself after approval?



MINSSEN:&nbsp;Well, it&nbsp;has to&nbsp;be an iterative process that is&nbsp;feasible&nbsp;and straightforward and that is based on a very efficient, both time efficient and performance efficient, communication between the regulatory authorities and the medical device developers, right. We need to have&nbsp;the sensors&nbsp;in place that spot potential changes, and we need to have&nbsp;the mechanisms&nbsp;in place that allow us to quickly react to these changes both regulatory wise&nbsp;and also&nbsp;in&nbsp;the&nbsp;technological way.‚ÄØ



So&nbsp;I think communication&nbsp;is important,&nbsp;and we need to have&nbsp;the pathways&nbsp;and&nbsp;the feedback&nbsp;loops in the regulation that quickly allow us to&nbsp;monitor&nbsp;these self-learning algorithms and devices.



SULLIVAN:&nbsp;It sounds like&nbsp;it&#8217;s&nbsp;just ‚Ä¶&nbsp;there&#8217;s&nbsp;such a delicate balance between advancing technology and really ensuring public safety. You know, if we clamp down too hard, we stifle that innovation. You already touched upon this a bit. But if&nbsp;we&#8217;re&nbsp;too lax, we risk unintended consequences. And&nbsp;I&#8217;d&nbsp;just love to hear how you think the field is balancing that and any learnings you can share.



MINSSEN:&nbsp;So&nbsp;this is&nbsp;very true, and&nbsp;you just touched upon a very central question also in our research and our writing. And this is also the&nbsp;reason why&nbsp;medical device regulation is so fascinating and continues to evolve in response to rapid advancements in technologies, particularly dual technologies&nbsp;regarding&nbsp;digital health, artificial intelligence, for example, and personalized medicine.



And finding the balance is tricky because also [a] related major future challenge relates to the increasing regulatory jungle and the complex interplay between evolving regulatory landscapes that regulate AI more generally.



We really need to make sure that the regulatory authorities that deal with this, that need to find the right balance to promote innovation and mitigate and prevent risks, need to have the&nbsp;capacity&nbsp;to do this.&nbsp;So&nbsp;this requires investments, and it also requires new ways to regulate this technology more flexibly, for example through regulatory sandboxes and so on.



SULLIVAN:&nbsp;Could you just expand upon that a bit and double-click on what it is&nbsp;you&#8217;re&nbsp;seeing there? What excites you about&nbsp;what&#8217;s&nbsp;happening in that space?



MINSSEN:&nbsp;Yes, well, the research of my group at the Center for Advanced Studies in Bioscience Innovation Law is&nbsp;very broad. I mean, we are looking into gene editing technologies. We are looking into new biologics. We are looking into medical&nbsp;devices,&nbsp;as well, obviously, but also other technologies&nbsp;in advanced medical computing.



And what we see across the line here is that there is an increasing demand for having more adaptive and flexible regulatory frameworks in these&nbsp;new technologies,&nbsp;in particular when&nbsp;they have new uses, regulations that are focusing more on the product rather than the process. And I have recently&nbsp;written&nbsp;a report, for example,&nbsp;for&nbsp;emerging biotechnologies and&nbsp;bio-solutions&nbsp;for the EU commission. And even in that area, regulatory sandboxes are increasingly important, increasingly considered.



So&nbsp;this idea of regulatory sandboxes has been developing originally in the financial sector, and it is now penetrating into&nbsp;other sectors, including synthetic biology, emerging biotechnologies, gene editing, AI, quantum technology, as&nbsp;well. This is&nbsp;basically creating&nbsp;an environment where actors can test&nbsp;new ideas&nbsp;in close collaboration and under the oversight of regulatory authorities.



But&nbsp;to implement&nbsp;this in the AI sector now also leads us to&nbsp;a&nbsp;lot of questions and challenges. For example, you need to have the&nbsp;capacities&nbsp;of authorities that are governing and&nbsp;monitoring&nbsp;and deciding&nbsp;on these regulatory sandboxes. There are issues relating to competition law, for example, which&nbsp;you&nbsp;call antitrust law in the US, because the question is, who can enter the sandbox and how may they compete after they exit the sandbox? And there are many questions relating to, how&nbsp;should we&nbsp;work with these sandboxes and how&nbsp;should we&nbsp;implement these sandboxes?



[TRANSITION MUSIC]&nbsp;



SULLIVAN:&nbsp;Well, Timo, it has just been such a pleasure to speak with you today.



MINSSEN:&nbsp;Yes, thank you very much.&nbsp;



And now&nbsp;I&#8217;m&nbsp;happy to introduce Chad Atalla.



Chad&nbsp;is&nbsp;senior applied scientist&nbsp;in&nbsp;Microsoft Research&nbsp;New York City&#8217;s&nbsp;Sociotechnical Alignment Center, where they contribute to foundational responsible AI research and practical responsible AI solutions for teams across Microsoft.



Chad, welcome!



CHAD ATALLA:&nbsp;Thank you.



SULLIVAN:&nbsp;So&nbsp;we&#8217;ll&nbsp;kick off with a couple questions just to dive right in.&nbsp;So&nbsp;tell me a little bit more about the&nbsp;Sociotechnical Alignment Center,&nbsp;or&nbsp;STAC? I know it was founded in&nbsp;2022.&nbsp;I&#8217;d&nbsp;love to just learn a little bit more about what the group does, how&nbsp;you&#8217;re&nbsp;thinking about evaluating AI, and&nbsp;maybe just&nbsp;give us a sense of some of the projects&nbsp;you&#8217;re&nbsp;working on.



ATALLA:&nbsp;Yeah, absolutely. The name is quite a mouthful.



SULLIVAN:&nbsp;It is!&nbsp;[LAUGHS]&nbsp;



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;start by breaking that down and seeing what that means.



SULLIVAN:&nbsp;Great.



ATALLA: So modern AI systems are sociotechnical systems, meaning that the social and technical aspects are deeply intertwined. And&nbsp;we&#8217;re interested in aligning the behaviors of these sociotechnical&nbsp;systems with some values.&nbsp;Those could be societal values;&nbsp;they could be regulatory values, organizational values, etc. And to make this alignment happen, we need the ability to evaluate the systems.



So&nbsp;my team is broadly working on an evaluation framework that acknowledges the sociotechnical nature of the technology and the often-abstract nature of the concepts&nbsp;we&#8217;re&nbsp;actually interested&nbsp;in evaluating. As you noted,&nbsp;it&#8217;s&nbsp;an applied science team, so we split our time between some fundamental research and time to bridge the work into real products across the company. And I also want to note that to power this sort of work, we have an interdisciplinary team drawing upon the social sciences, linguistics, statistics, and,&nbsp;of course, computer science.



SULLIVAN:&nbsp;Well,&nbsp;I&#8217;m&nbsp;eager to get into our takeaways from the conversation with&nbsp;both Daniel&nbsp;and Timo. But&nbsp;maybe just&nbsp;to double-click on this for a minute, can you talk a bit about some of the overarching goals of the AI evaluations that you noted?&nbsp;



ATALLA:&nbsp;So&nbsp;evaluation is really the act of making valuative judgments based on some evidence, and in the case of AI evaluation, that evidence might be from tests or measurements, right.&nbsp;And the goal of why&nbsp;we&#8217;re doing this in the first place is to make decisions and claims most often.



So&nbsp;perhaps I&nbsp;am going to make a claim about a model that&nbsp;I&#8217;m&nbsp;producing, and I want to say that&nbsp;it&#8217;s&nbsp;better than this other model. Or we are asking whether a certain product is safe to ship.&nbsp;All of these decisions need to be informed by good evaluation and therefore good measurement or testing.&nbsp;And&nbsp;I&#8217;ll&nbsp;also note that in&nbsp;the regulatory conversation, risk&nbsp;is often what we want to evaluate. So that is a goal in and of itself. And&nbsp;I&#8217;ll&nbsp;touch more on that later.



SULLIVAN:&nbsp;I read a recent&nbsp;paper that you had put out with some of our colleagues from Microsoft Research, from the University of Michigan, and Stanford, and you were arguing that evaluating generative AI is&nbsp;the&nbsp;social-science measurement challenge.&nbsp;Maybe for&nbsp;those who&nbsp;haven&#8217;t&nbsp;read the paper, what does this mean? And can you tell us a little bit more about what motivated you and your coauthors?&nbsp;



ATALLA:&nbsp;So the measurement tasks involved in evaluating generative AI systems are often abstract and contested. So that means they cannot be directly measured and must instead [be] indirectly measured via other observable phenomena. So this is very different than the older machine learning paradigm, where, let&#8217;s say, for example, I had a system that took a picture of a traffic light and told you whether it was green, yellow, or red at a given time.&nbsp;



If we wanted to evaluate that system, the task is much simpler. But with the modern generative AI systems that are also general purpose, they have open-ended output, and language in a whole chat or multiple paragraphs being outputted can have a lot of different properties. And as I noted, these are general-purpose systems, so we don&#8217;t know exactly what task they&#8217;re supposed to be carrying out.



So&nbsp;then the question becomes, if I want to make some decision or claim‚Äîmaybe I&nbsp;want to make a claim that this system has human-level reasoning capabilities‚Äîwell, what does that mean? Do I have the same impression of what that means as you do? And how do we know whether the downstream, you know, measurements and tests that&nbsp;I&#8217;m&nbsp;conducting&nbsp;actually will&nbsp;support my notion of what it means to have human-level reasoning,&nbsp;right?&nbsp;Difficult questions. But luckily, social scientists have been dealing with these exact sorts of challenges for multiple decades in fields like education, political science, and psychometrics. So&nbsp;we&#8217;re&nbsp;really&nbsp;attempting&nbsp;to avoid reinventing the wheel here and trying to learn from their past methodologies.



And so the rest of the paper goes on to delve into&nbsp;a four-level framework, a measurement framework, that&#8217;s grounded in the measurement theory from the quantitative social sciences that takes us all the way from these abstract and contested concepts through processes to get much clearer and eventually reach reliable and valid measurements that can power our evaluations.



SULLIVAN:&nbsp;I love that. I mean,&nbsp;that&#8217;s&nbsp;the whole point of this podcast,&nbsp;too,&nbsp;right.&nbsp;Is&nbsp;to really&nbsp;build&nbsp;on those other learnings and frameworks that&nbsp;we&#8217;re&nbsp;taking from industries that have been thinking about this for much longer.&nbsp;Maybe from&nbsp;your vantage point, what are some of the biggest day-to-day hurdles in building solid AI evaluations&nbsp;and,&nbsp;I&nbsp;don&#8217;t&nbsp;know, do we need more shared standards? Are there&nbsp;bespoke methods? Are those&nbsp;the way to go? I would love&nbsp;to just&nbsp;hear your thoughts on that.



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;talk about some of those practical challenges. And I want to briefly go back to what I mentioned about risk before, all right.&nbsp;Oftentimes,&nbsp;some of the regulatory environment&nbsp;is requiring practitioners to measure the&nbsp;risk&nbsp;involved in deploying one of their models or AI systems. Now, risk is importantly a&nbsp;concept that includes both event and impact,&nbsp;right.&nbsp;So&nbsp;there&#8217;s&nbsp;the probability of some event occurring. For the case of AI evaluation,&nbsp;perhaps this&nbsp;is us seeing a certain AI behavior&nbsp;exhibited. Then there&#8217;s also the severity of the&nbsp;impacts,&nbsp;and this is a complex chain of effects in the real world that&nbsp;happen&nbsp;to people, organizations, systems, etc., and&nbsp;it&#8217;s&nbsp;a lot more challenging to&nbsp;observe&nbsp;the impacts,&nbsp;right.



So&nbsp;if we&#8217;re saying that we need to measure risk, we have to measure both the event and the&nbsp;impacts. But realistically, right now, the field is not doing&nbsp;a very good&nbsp;job of&nbsp;actually measuring&nbsp;the impacts. This requires vastly different techniques and methodologies where if I just wanted to measure something about the event itself, I can, you know, do that in a technical sandbox&nbsp;environment&nbsp;and&nbsp;perhaps have&nbsp;some automated methods to detect whether a certain AI behavior is being&nbsp;exhibited. But if I want to measure the impacts? Now,&nbsp;we&#8217;re&nbsp;in the realm of needing to have real people involved, and&nbsp;perhaps a&nbsp;longitudinal study where you have interviews, questionnaires, and more qualitative evidence-gathering techniques to&nbsp;truly understand&nbsp;the long-term impacts. So&nbsp;that&#8217;s&nbsp;a significant challenge.



Another is that, you know,&nbsp;let&#8217;s&nbsp;say we forget about the impacts for&nbsp;now&nbsp;and we focus on the event side of things. Still, we need datasets, we need&nbsp;annotations,&nbsp;and we need&nbsp;metrics to make this whole thing work. When I say we need datasets, if I want to test whether my system has good mathematical reasoning, what questions should I ask? What are my set of inputs that are relevant? And then when I get&nbsp;the&nbsp;response from the system, how do I annotate them? How do I know if it was a good response that&nbsp;did demonstrate mathematical reasoning or if it was a mediocre response? And then once I have an annotation of&nbsp;all of these outputs from the AI system, how do I aggregate those all up into a single informative number?



SULLIVAN:&nbsp;Earlier in this episode, we heard Daniel and&nbsp;Timo walk&nbsp;through the regulatory frameworks in pharma and medical devices.&nbsp;I&#8217;d&nbsp;be curious what pieces of those mature systems are already showing up or at least may&nbsp;be bubbling up in AI governance.



ATALLA:&nbsp;Great question. You know, Timo was talking about the pre-market and post-market testing difference. Of course, this is similarly important in the AI evaluation space. But again, these have different methodologies and serve different purposes.



So&nbsp;within the pre-deployment phase, we&nbsp;don&#8217;t&nbsp;have evidence of how people are going to use the system. And when we have these general-purpose AI systems,&nbsp;to understand what the risks are, we really need to have a sense of what might happen and how they might be used.&nbsp;So&nbsp;there are&nbsp;significant challenges there where I think we can learn from other fields and how they do pre-market testing. And the difference in that pre- versus post-market testing also ties to testing at&nbsp;different stages&nbsp;in the life cycle.



For AI systems, we already see some regulations saying you need to start with the base model and do some evaluation of the base model, some basic attributes, some core attributes,&nbsp;of that base model before you start putting it into any real products. But once we have a product in mind, we have a user base in mind, we have a specific task‚Äîlike maybe we&#8217;re going to integrate this model into Outlook and it&#8217;s going to help you write&nbsp;emails‚Äînow we suddenly have a much crisper picture of how the system will interact with the world around it. And again, at that stage, we need to think about another round of evaluation.



Another part that jumped out to me in what they were saying about pharmaceuticals is that sometimes approvals can be based on surrogate endpoints.&nbsp;So&nbsp;this is like&nbsp;we&#8217;re&nbsp;choosing some&nbsp;heuristic.&nbsp;Instead of measuring the long-term impact, which is what we&nbsp;actually care&nbsp;about,&nbsp;perhaps we&nbsp;have a proxy that we&nbsp;feel like&nbsp;is a good enough indicator of what that long-term impact might look like.&nbsp;&nbsp;



This is occurring in the AI evaluation space right now and is often perhaps even the default here since&nbsp;we&#8217;re not seeing that many studies of the long-term impact itself. We are seeing, instead, folks constructing these heuristics or proxies and saying if I see this behavior happen,&nbsp;I&#8217;m&nbsp;going to&nbsp;assume&nbsp;that it&nbsp;indicates&nbsp;this sort of impact will happen downstream. And&nbsp;that&#8217;s&nbsp;great.&nbsp;It&#8217;s&nbsp;one of the techniques that was used to speed up and reduce the barrier to innovation in&nbsp;the other&nbsp;fields. And I think&nbsp;it&#8217;s&nbsp;great that we are applying that in the AI evaluation space. But&nbsp;special care&nbsp;is,&nbsp;of course, needed to ensure that those heuristics and proxies you&#8217;re&nbsp;using are reasonable indicators of the greater outcome&nbsp;you&#8217;re&nbsp;looking for.



SULLIVAN:&nbsp;What are some of the promising ideas from&nbsp;maybe pharma&nbsp;or med device regulation that maybe haven&#8217;t&nbsp;made it to AI testing yet and&nbsp;maybe should? And where would you urge technologists, policymakers,&nbsp;and researchers to focus their energy next?



ATALLA:&nbsp;Well, one of the key things that jumped out to me in the discussion about pharmaceuticals was driving home the emphasis that there&nbsp;is&nbsp;a&nbsp;holistic&nbsp;focus on safety&nbsp;and&nbsp;efficacy. These go hand in hand&nbsp;and decisions must be made while considering both pieces of the picture. I would like to see that further emphasized in the AI evaluation space.



Often,&nbsp;we&nbsp;are seeing&nbsp;evaluations of risk being separated from evaluations of&nbsp;performance or quality&nbsp;or efficacy, but these two pieces of the puzzle really are not enough for us to make informed decisions independently.&nbsp;And that ties back into my desire to really also see us measuring the impacts.



So&nbsp;we see Phase 3 trials as something that occurs in the medical devices and pharmaceuticals field. That&#8217;s not something that we are doing an equivalent of in the AI evaluation space at this time.&nbsp;These are really&nbsp;cost intensive. They can last years and really involve careful monitoring of that holistic picture of safety and efficacy. And realistically, we are not going to be able to put that on the critical path to getting specific individual AI models or AI systems vetted before they&nbsp;go out&nbsp;into the world. However, I would love to see a world in which this sort of work is prioritized&nbsp;and funded or&nbsp;required. Think of how, with&nbsp;social media, it took quite a long time for us to understand that there are some long-term negative impacts on mental health, and we have the opportunity now, while the AI wave is still building,&nbsp;to start prioritizing and funding this sort of work. Let it run in the background and as soon as possible develop a good understanding of the subtle, long-term effects.



More broadly, I would love to see us focus on reliability and validity of the evaluations&nbsp;we&#8217;re&nbsp;conducting because trust in these decisions and claims is important. If we&nbsp;don&#8217;t&nbsp;focus on building reliable, valid, and trustworthy evaluations,&nbsp;we&#8217;re&nbsp;just going to continue to be flooded by a bunch of competing, conflicting, and&nbsp;largely meaningless&nbsp;AI evaluations.



SULLIVAN:&nbsp;In a number of the discussions we&#8217;ve had on this podcast, we talked about how it&#8217;s not just one entity that really needs to ensure safety across the board,&nbsp;and I‚Äôd&nbsp;just love to hear from you how you think about some of those ecosystem collaborations, and you know, from across &#8230; where we think about ourselves as more of a platform company or places that these AI models are being deployed more at the application level. Tell me a little bit about how you think about,&nbsp;sort&nbsp;of, stakeholders in that mix and where responsibility lies across the board.



ATALLA:&nbsp;It&#8217;s&nbsp;interesting. In this age of general-purpose AI technologies,&nbsp;we&#8217;re&nbsp;often&nbsp;seeing&nbsp;one company or organization&nbsp;being responsible for&nbsp;building the foundational model. And then many, many other people will take that model and build it into specific products that are designed for specific tasks and contexts.



Of course,&nbsp;in that, we already see that there is&nbsp;a responsibility&nbsp;of the owners of that foundational model to do some testing of the central model before they distribute it broadly. And then again, there is responsibility of all of the downstream individuals digesting that and turning it into products to consider the specific contexts that they are deploying into and how that may affect the risks we&#8217;re concerned with or the types of quality and safety and performance we need to evaluate.



Again, because that field of risks we may be concerned with is so broad, some of them also require an immense amount of&nbsp;expertise.&nbsp;Let&#8217;s&nbsp;think about whether AI systems can enable people to create dangerous chemicals or dangerous weapons at home. It&#8217;s not that every AI practitioner is going to have the knowledge to evaluate this, so in some of those cases, we really need third-party experts, people who are experts in chemistry, biology, etc., to come in and evaluate certain systems and models for those specific risks,&nbsp;as well.



So&nbsp;I think there&nbsp;are many reasons why multiple stakeholders need to be involved, partly from who owns what and&nbsp;is responsible for&nbsp;what and partly from the perspective of who has the&nbsp;expertise&nbsp;to meaningfully construct the evaluations that we need.



SULLIVAN:&nbsp;Well, Chad, this has just been great to connect, and in a few of our discussions,&nbsp;we&#8217;ve&nbsp;done a bit of a lightning round, so&nbsp;I&#8217;d&nbsp;love to just hear your&nbsp;30-second responses to a few of these questions. Perhaps&nbsp;favorite&nbsp;evaluation&nbsp;you&#8217;ve&nbsp;run so far this year?&nbsp;



ATALLA:&nbsp;So&nbsp;I&#8217;ve&nbsp;been involved in trying to evaluate some language models for whether they&nbsp;infer&nbsp;sensitive attributes about people. So&nbsp;perhaps&nbsp;you&#8217;re&nbsp;chatting with a&nbsp;chatbot,&nbsp;and it infers your religion or sexuality based on things&nbsp;you&#8217;re&nbsp;saying or how you sound,&nbsp;right.&nbsp;And in working to evaluate this, we&nbsp;encounter&nbsp;a lot of interesting questions. Or,&nbsp;like,&nbsp;what is a sensitive attribute? What makes these attributes sensitive, and what are the differences that make it inappropriate for an AI system to infer these things about a person? Whereas realistically, whenever I meet a person on the street, my&nbsp;brain is&nbsp;immediately&nbsp;forming&nbsp;first impressions and some assumptions about these people.&nbsp;So&nbsp;it&#8217;s&nbsp;a very interesting&nbsp;and thought-provoking evaluation to conduct and think about the norms that we place upon&nbsp;people&nbsp;interacting with other people and the norms we place upon&nbsp;AI systems&nbsp;interacting with other people.



SULLIVAN:&nbsp;That‚Äôs&nbsp;fascinating!&nbsp;I&#8217;d&nbsp;love to hear the AI&nbsp;buzzword&nbsp;you&#8217;d&nbsp;retire tomorrow.&nbsp;[LAUGHTER]



ATALLA:&nbsp;I would love to see the term ‚Äúbias‚Äù being&nbsp;used less when referring to fairness-related issues and systems. Bias happens to be a highly overloaded term in statistics and machine learning and has a lot of technical meanings and just&nbsp;fails to&nbsp;perfectly capture what we mean in the AI risk sense.



SULLIVAN:&nbsp;And last one. One metric&nbsp;we&#8217;re&nbsp;not tracking enough.



ATALLA:&nbsp;I would say over-blocking, and this comes into that connection between the holistic picture of safety and efficacy. It&#8217;s too easy to produce systems that throw safety to the wind and focus purely on utility or achieving some goal, but simultaneously, the other side of the picture is possible, where we can clamp down too hard and reduce the utility of our systems and block even benign and useful outputs just because they border on something sensitive.&nbsp;So&nbsp;it&#8217;s&nbsp;important for us to track that over-blocking and actively track that tradeoff between safety and efficacy.



SULLIVAN:&nbsp;Yeah, we talk a lot about this on the podcast,&nbsp;too,&nbsp;of how do you both make things safe but also ensure innovation can&nbsp;thrive,&nbsp;and&nbsp;I think you&nbsp;hit the nail on the head with that last piece.



[MUSIC]&nbsp;



Well, Chad, this was&nbsp;really terrific. Thanks for joining us and thanks for your work and your&nbsp;perspectives. And another big thanks to Daniel and Timo for setting the stage earlier in the podcast.



And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.‚ÄØ



See you next time!‚ÄØ



[MUSIC FADES]

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series

Opens in a new tabThe post AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices appeared first on Microsoft Research.
‚Ä¢ AWS AI infrastructure with NVIDIA Blackwell: Two powerful compute solutions for the next frontier of AI
  Imagine a system that can explore multiple approaches to complex problems, drawing on its understanding of vast amounts of data, from scientific datasets to source code to business documents, and reasoning through the possibilities in real time. This lightning-fast reasoning isn‚Äôt waiting on the horizon. It‚Äôs happening today in our customers‚Äô AI production environments. The scale of the AI systems that our customers are building today‚Äîacross drug discovery, enterprise search, software development, and more‚Äîis truly remarkable. And there‚Äôs much more ahead. 
To accelerate innovation across emerging generative AI developments such as reasoning models and agentic AI systems, we‚Äôre excited to announce general availability of P6e-GB200 UltraServers, accelerated by NVIDIA Grace Blackwell Superchips. P6e-GB200 UltraServers are designed for training and deploying the largest, most sophisticated AI models. Earlier this year, we launched P6-B200 instances, accelerated by NVIDIA Blackwell GPUs, for diverse AI and high-performance computing workloads. 
 
In this post, we share how these powerful compute solutions build on everything we‚Äôve learned about delivering secure, reliable GPU infrastructure at a massive scale, so that customers can confidently push the boundaries of AI. 
Meeting the expanding compute demands of AI workloads 
P6e-GB200 UltraServers represent our most powerful GPU offering to date, featuring up to 72 NVIDIA Blackwell GPUs interconnected using fifth-generation NVIDIA NVLink‚Äîall functioning as a single compute unit. Each UltraServer delivers a massive 360 petaflops of dense FP8 compute and 13.4 TB of total high bandwidth GPU memory (HBM3e)‚Äîwhich is over 20 times the compute and over 11 times the memory in a single NVLink domain compared to P5en instances. P6e-GB200 UltraServers support up to 28.8 Tbps aggregate bandwidth of fourth-generation Elastic Fabric Adapter (EFAv4) networking.P6-B200 instances are a versatile option for a broad range of AI use cases. Each instance provides 8 NVIDIA Blackwell GPUs interconnected using NVLink with 1.4 TB of high bandwidth GPU memory, up to 3.2 Tbps of EFAv4 networking, and fifth-generation Intel Xeon Scalable processors. P6-B200 instances offer up to 2.25 times the GPU TFLOPs, 1.27 times the GPU memory size, and 1.6 times the GPU memory bandwidth compared to P5en instances. 
How do you choose between P6e-GB200 and P6-B200? This choice comes down to your specific workload requirements and architectural needs: 
 
 P6e-GB200 UltraServers are ideal for the most compute and memory intensive AI workloads, such as training and deploying frontier models at the trillion-parameter scale. Their NVIDIA GB200 NVL72 architecture really shines at this scale. Imagine all 72 GPUs working as one, with a unified memory space and coordinated workload distribution. This architecture enables more efficient distributed training by reducing communication overhead between GPU nodes. For inference workloads, the ability to fully contain trillion-parameter models within a single NVLink domain means faster, more consistent response times at scale. When combined with optimization techniques such as disaggregated serving with NVIDIA Dynamo, the large domain size of GB200 NVL72 architecture unlocks significant inference efficiencies for various model architectures such as mixture of experts models. GB200 NVL72 is particularly powerful when you need to handle extra-large context windows or run high-concurrency applications in real time. 
 P6-B200 instances support a broad range of AI workloads and are an ideal option for medium to large-scale training and inference workloads. If you want to port your existing GPU workloads, P6-B200 instances offer a familiar 8-GPU configuration that minimizes code changes and simplifies migration from current generation instances. Additionally, although NVIDIA‚Äôs AI software stack is optimized for both Arm and x86, if your workloads are specifically built for x86 environments, P6-B200 instances, with their Intel Xeon processors, will be your ideal choice. 
 
Innovation built on AWS core strengths 
Bringing NVIDIA Blackwell to AWS isn‚Äôt about a single breakthrough‚Äîit‚Äôs about continuous innovation across multiple layers of infrastructure. By building on years of learning and innovation across compute, networking, operations, and managed services, we‚Äôve brought NVIDIA Blackwell‚Äôs full capabilities with the reliability and performance customers expect from AWS. 
Robust instance security and stability 
When customers tell me why they choose to run their GPU workloads on AWS, one crucial point comes up consistently: they highly value our focus on instance security and stability in the cloud. The specialized hardware, software, and firmware of the AWS Nitro System are designed to enforce restrictions so that nobody, including anyone in AWS, can access your sensitive AI workloads and data. Beyond security, the Nitro System fundamentally changes how we maintain and optimize infrastructure. The Nitro System, which handles networking, storage, and other I/O functions, makes it possible to deploy firmware updates, bug fixes, and optimizations while it remains operational. This ability to update without system downtime, which we call live update, is crucial in today‚Äôs AI landscape, where any interruption significantly impacts production timelines. P6e-GB200 and P6-B200 both feature the sixth generation of the Nitro System, but these security and stability benefits aren‚Äôt new‚Äîour innovative Nitro architecture has been protecting and optimizing Amazon Elastic Compute Cloud (Amazon EC2) workloads since 2017. 
Reliable performance at massive scale 
In AI infrastructure, the challenge isn‚Äôt just reaching massive scale‚Äîit‚Äôs delivering consistent performance and reliability at that scale. We‚Äôve deployed P6e-GB200 UltraServers in third-generation EC2 UltraClusters, which creates a single fabric that can encompass our largest data centers. Third-generation UltraClusters cut power consumption by up to 40% and reduce cabling requirements by more than 80%‚Äînot only improving efficiency, but also significantly reducing potential points of failure. 
To deliver consistent performance at this massive scale, we use Elastic Fabric Adapter (EFA) with its Scalable Reliable Datagram protocol, which intelligently routes traffic across multiple network paths to maintain smooth operation even during congestion or failures. We‚Äôve continuously improved EFA‚Äôs performance across four generations. P6e-GB200 and P6-B200 instances with EFAv4 show up to 18% faster collective communications in distributed training compared to P5en instances that use EFAv3. 
Infrastructure efficiency 
Whereas P6-B200 instances use our proven air-cooling infrastructure, P6e-GB200 UltraServers use liquid cooling, which enables higher compute density in large NVLink domain architectures, delivering higher system performance. P6e-GB200 are liquid cooled with novel mechanical cooling solutions providing configurable liquid-to-chip cooling in both new and existing data centers, so we can support both liquid-cooled accelerators and air-cooled network and storage infrastructure in the same facility. With this flexible cooling design, we can deliver maximum performance and efficiency at the lowest cost. 
Getting started with NVIDIA Blackwell on AWS 
We‚Äôve made it simple to get started with P6e-GB200 UltraServers and P6-B200 instances through multiple deployment paths, so you can quickly begin using Blackwell GPUs while maintaining the operational model that works best for your organization. 
Amazon SageMaker HyperPod 
If you‚Äôre accelerating your AI development and want to spend less time managing infrastructure and cluster operations, that‚Äôs exactly where Amazon SageMaker HyperPod excels. It provides managed, resilient infrastructure that automatically handles provisioning and management of large GPU clusters. We keep enhancing SageMaker HyperPod, adding innovations like flexible training plans to help you gain predictable training timelines and run training workloads within your budget requirements. 
SageMaker HyperPod will support both P6e-GB200 UltraServers and P6-B200 instances, with optimizations to maximize performance by keeping workloads within the same NVLink domain. We‚Äôre also building in a comprehensive, multi-layered recovery system: SageMaker HyperPod will automatically replace faulty instances with preconfigured spares in the same NVLink domain. Built-in dashboards will give you visibility into everything from GPU utilization and memory usage to workload metrics and UltraServer health status. 
Amazon EKS 
For large-scale AI workloads, if you prefer to manage your infrastructure using Kubernetes, Amazon Elastic Kubernetes Service (Amazon EKS) is often the control plane of choice. We continue to drive innovations in Amazon EKS with capabilities like Amazon EKS Hybrid Nodes, which enable you to manage both on-premises and EC2 GPUs in a single cluster‚Äîdelivering flexibility for AI workloads. 
Amazon EKS will support both P6e-GB200 UltraServers and P6-B200 instances with automated provisioning and lifecycle management through managed node groups. For P6e-GB200 UltraServers, we‚Äôre building in topology awareness that understands the GB200 NVL72 architecture, automatically labeling nodes with their UltraServer ID and network topology information to enable optimal workload placement. You will be able to span node groups across multiple UltraServers or dedicate them to individual UltraServers, giving you flexibility in organizing your training infrastructure. Amazon EKS monitors GPU and accelerator errors and relays them to the Kubernetes control plane for optional remediation. 
NVIDIA DGX Cloud on AWS 
P6e-GB200 UltraServers will also be available through NVIDIA DGX Cloud. DGX Cloud is a unified AI platform optimized at every layer with multi-node AI training and inference capabilities and NVIDIA‚Äôs complete AI software stack. You benefit from NVIDIA‚Äôs latest optimizations, benchmarking recipes, and technical expertise to improve efficiency and performance. It offers flexible term lengths along with comprehensive NVIDIA expert support and services to help you accelerate your AI initiatives. 
This launch announcement is an important milestone, and it‚Äôs just the beginning. As AI capabilities evolve rapidly, you need infrastructure built not just for today‚Äôs demands but for all the possibilities that lie ahead. With innovations across compute, networking, operations, and managed services, P6e-GB200 UltraServers and P6-B200 instances are ready to enable these possibilities. We can‚Äôt wait to see what you will build with them. 
Resources 
 
 AWS News Blog: P6e-GB200 UltraServers 
 AWS News Blog: P6-B200 Instances 
 
 
 
About the author 
David Brown is the Vice President of AWS Compute and Machine Learning (ML) Services. In this role he is responsible for building all AWS Compute and ML services, including Amazon EC2, Amazon Container Services, AWS Lambda, Amazon Bedrock and Amazon SageMaker. These services are used by all AWS customers but also underpin most of AWS‚Äôs internal Amazon applications. He also leads newer solutions, such as AWS Outposts, that bring AWS services into customers‚Äô private data centers. 
David joined AWS in 2007 as a Software Development Engineer based in Cape Town, South Africa, where he worked on the early development of Amazon EC2. In 2012, he relocated to Seattle and continued to work in the broader Amazon EC2 organization. Over the last 11 years, he has taken on larger leadership roles as more of the AWS compute and ML products have become part of his organization. 
Prior to joining Amazon, David worked as a Software Developer at a financial industry startup. He holds a Computer Science &amp; Economics degree from the Nelson Mandela University in Port Elizabeth, South Africa.
‚Ä¢ Unlock retail intelligence by transforming data into actionable insights using generative AI with Amazon Q Business
  Businesses often face challenges in managing and deriving value from their data. According to McKinsey, 78% of organizations now use AI in at least one business function (as of 2024), showing the growing importance of AI solutions in business. Additionally, 21% of organizations using generative AI have fundamentally redesigned their workflows, showing how AI is transforming business operations. 
Gartner identifies AI-powered analytics and reporting as a core investment area for retail organizations, with most large retailers expected to deploy or scale such solutions within the next 12‚Äì18 months. The retail sector‚Äôs data complexity demands sophisticated solutions that can integrate seamlessly with existing systems. Amazon Q Business offers features that can be tailored to meet specific business needs, including integration capabilities with popular retail management systems, point-of-sale systems, inventory management software, and ecommerce systems. Through advanced AI algorithms, the system analyzes historical data and current trends, helping businesses prepare effectively for seasonal fluctuations in demand and make data-driven decisions. 
Amazon Q Business for Retail Intelligence is an AI-powered assistant designed to help retail businesses streamline operations, improve customer service, and enhance decision-making processes. This solution is specifically engineered to be scalable and adaptable to businesses of various sizes, helping them compete more effectively. In this post, we show how you can use Amazon Q Business for Retail Intelligence to transform your data into actionable insights. 
Solution overview 
Amazon Q Business for Retail Intelligence is a comprehensive solution that transforms how retailers interact with their data using generative AI. The solution architecture combines the powerful generative AI capabilities of Amazon Q Business and Amazon QuickSight visualizations to deliver actionable insights across the entire retail value chain. Our solution also uses Amazon Q Apps so retail personas and users can create custom AI-powered applications to streamline day-to-day tasks and automate workflows and business processes. 
The following diagram illustrates the solution architecture. 
 
The solution uses the AWS architecture above to deliver a secure, high-performance, and reliable solution for retail intelligence. Amazon Q Business serves as the primary generative AI engine, enabling natural language interactions and powering custom retail-specific applications. The architecture incorporates AWS IAM Identity Center for robust authentication and access control, and Amazon Simple Storage Service (Amazon S3) provides secure data lake storage for retail data sources. We use QuickSight for interactive visualizations, enhancing data interpretation. The solution‚Äôs flexibility is further enhanced by AWS Lambda for serverless processing, Amazon API Gateway for efficient endpoint management, and Amazon CloudFront for optimized content delivery. This solution uses the Amazon Q Business custom plugin to call the API endpoints to start the automated workflows directly from the Amazon Q Business web application interface based on customer queries and interactions. 
This setup implements a three-tier architecture: a data integration layer that securely ingests data from multiple retail sources, a processing layer where Amazon Q Business analyzes queries and generates insights, and a presentation layer that delivers personalized, role-based insights through a unified interface. 
We have provided an AWS CloudFormation template, sample datasets, and scripts that you can use to set up the environment for this demonstration. 
In the following sections, we dive deeper on how this solution works. 
Deployment 
We have provided the Amazon Q Business for Retail Intelligence solution as open source‚Äîyou can use it as a starting point for your own solution and help us make it better by contributing fixes and features through GitHub pull requests. Visit the GitHub repository to explore the code, choose Watch to be notified of new releases, and check the README for the latest documentation updates. 
After you set up the environment, you can access the Amazon Q Business for Retail Intelligence dashboard, as shown in the following screenshot. 
 
You can interact with the QuickSight visualizations and Amazon Q Business chat interface to ask questions using natural language. 
Key features and capabilities 
Retail users can interact with this solution in many ways. In this section, we explore the key features. 
For C-suite executives or senior leadership wanting to know how your business is performing, our solution provides a single pane of glass and makes it straightforward to access and interact with your enterprise‚Äôs qualitative and quantitative data using natural language. For example, users can analyze quantitative data like product sales or marketing campaign performance using the interactive visualizations powered by QuickSight and qualitative data like customer feedback from Amazon Q Business using natural language, all from a single interface. 
 
Consider that you are a marketing analyst and you want to evaluate campaign performance and reach across channels and conduct an analysis on ad spend vs. revenue. With Amazon Q Business, you can run complex queries with natural language questions and with share the Q Apps with multiple teams. The solution provides automated insights about customer behavior and campaign effectiveness, helping marketing teams make faster decisions and quick adjustments to maximize ROI. 
 
Similarly, let‚Äôs assume you are a merchandising planner or a vendor manager and you want to understand the impact of cost-prohibitive events for your international business that deals with importing and exporting of goods and services. You can add inputs to Amazon Q Apps and get responses based on that specific product or product family. 
 
Users can also send requests through APIs using Amazon Q Business custom plugins for real-time interactions with downstream applications. For example, a store manager might want to know which items in the current inventory they need to replenish or rebalance for the next week based on weather predictions or local sporting events. 
 
To learn more, refer to the following complete demo. 

 
  
 
 
For this post, we haven‚Äôt used the generative business intelligence (BI) capabilities of Amazon Q with our QuickSight visualizations. To learn more, see Amazon Q in QuickSight. 
Empowering retail personas with AI-driven intelligence 
Amazon Q Business for Retail Intelligence transforms how retailers handle their data challenges through a generative AI-powered assistant. This solution integrates seamlessly with existing systems, using Retrieval Augmented Generation (RAG) to unify disparate data sources and deliver actionable insights in real time.The following are some of the key benefits for various roles: 
 
 C-Suite executives ‚Äì Access comprehensive real-time dashboards for company-wide metrics and KPIs while using AI-driven recommendations for strategic decisions. Use predictive analytics to anticipate consumer shifts and enable proactive strategy adjustments for business growth. 
 Merchandisers ‚Äì Gain immediate insights into sales trends, profit margins, and inventory turnover rates through automated analysis tools and AI-powered pricing strategies. Identify and capitalize on emerging trends through predictive analytics for optimal product mix and category management. 
 Inventory managers ‚Äì Implement data-driven stock level optimization across multiple store locations while streamlining operations with automated reorder point calculations. Accurately predict and prepare for seasonal demand fluctuations to maintain optimal inventory levels during peak periods. 
 Store managers ‚Äì Maximize operational efficiency through AI-predicted staffing optimization while accessing detailed insights about local conditions affecting store performance. Compare store metrics against other locations using sophisticated benchmarking tools to identify improvement opportunities. 
 Marketing analysts ‚Äì Monitor and analyze marketing campaign effectiveness across channels in real time while developing sophisticated customer segments using AI-driven analysis. Calculate and optimize marketing ROI across channels for efficient budget allocation and improved campaign performance. 
 
Amazon Q Business for Retail Intelligence makes complex data analysis accessible to different users through its natural language interface. This solution enables data-driven decision-making across organizations by providing role-specific insights that break down traditional data silos. By providing each retail persona tailored analytics and actionable recommendations, organizations can achieve greater operational efficiency and maintain a competitive edge in the dynamic retail landscape. 
Conclusion 
Amazon Q Business for Retail Intelligence combines generative AI capabilities with powerful visualization tools to revolutionize retail operations. By enabling natural language interactions with complex data systems, this solution democratizes data access across organizational levels, from C-suite executives to store managers. The system‚Äôs ability to provide role-specific insights, automate workflows, and facilitate real-time decision-making positions it as a crucial tool for retail businesses seeking to maintain competitiveness in today‚Äôs dynamic landscape. As retailers continue to embrace AI-driven solutions, Amazon Q Business for Retail Intelligence can help meet the industry‚Äôs growing needs for sophisticated data analysis and operational efficiency. 
To learn more about our solutions and offerings, refer to Amazon Q Business and Generative AI on AWS. For expert assistance, AWS Professional Services, AWS Generative AI partner solutions, and AWS Generative AI Competency Partners are here to help. 
 
About the authors 
Suprakash Dutta is a Senior Solutions Architect at Amazon Web Services, leading strategic cloud transformations for Fortune 500 retailers and large enterprises. He specializes in architecting mission-critical retail solutions that drive significant business outcomes, including cloud-native based systems, generative AI implementations, and retail modernization initiatives. He‚Äôs a multi-cloud certified architect and has delivered transformative solutions that modernized operations across thousands of retail locations while driving breakthrough efficiencies through AI-powered retail intelligence solutions. 
Alberto Alonso is a Specialist Solutions Architect at Amazon Web Services. He focuses on generative AI and how it can be applied to business challenges. 
Abhijit Dutta is a Sr. Solutions Architect in the Retail/CPG vertical at AWS, focusing on key areas like migration and modernization of legacy applications, data-driven decision-making, and implementing AI/ML capabilities. His expertise lies in helping organizations use cloud technologies for their digital transformation initiatives, with particular emphasis on analytics and generative AI solutions. 
Ramesh Venkataraman is a Solutions Architect who enjoys working with customers to solve their technical challenges using AWS services. Outside of work, Ramesh enjoys following stack overflow questions and answers them in any way he can. 
Girish Nazhiyath is a Sr. Solutions Architect in the Amazon Web Services Retail/CPG vertical. He enjoys working with retail/CPG customers to enable technology-driven retail innovation, with over 20 years of expertise in multiple retail segments and domains worldwide. 
Krishnan Hariharan is a Sr. Manager, Solutions Architecture at AWS based out of Chicago. In his current role, he uses his diverse blend of customer, product, technology, and operations skills to help retail/CPG customers build the best solutions using AWS. Prior to AWS, Krishnan was President/CEO at Kespry, and COO at LightGuide. He has an MBA from The Fuqua School of Business, Duke University and a Bachelor of Science in Electronics from Delhi University.
‚Ä¢ Democratize data for timely decisions with text-to-SQL at Parcel Perform
  This post was co-written with Le Vy from Parcel Perform. 
Access to accurate data is often the true differentiator of excellent and timely decisions. This is even more crucial for customer-facing decisions and actions. A correctly implemented state-of-the-art AI can help your organization simplify access to data for accurate and timely decision-making for the customer-facing business team, while reducing the undifferentiated heavy lifting done by your data team. In this post, we share how Parcel Perform, a leading AI Delivery Experience Platform for e-commerce businesses worldwide, implemented such a solution. 
Accurate post-purchase deliveries tracking can be crucial for many ecommerce merchants. Parcel Perform provides an AI-driven, intelligent end-to-end data and delivery experience and software as a service (SaaS) system for ecommerce merchants. The system uses AWS services and state-of-the-art AI to process hundreds of millions of daily parcel delivery movement data and provide a unified tracking capability across couriers for the merchants, with emphasis on accuracy and simplicity. 
The business team in Parcel Perform often needs access to data to answer questions related to merchants‚Äô parcel deliveries, such as ‚ÄúDid we see a spike in delivery delays last week? If so, in which transit facilities were this observed, and what was the primary cause of the issue?‚Äù Previously, the data team had to manually form the query and run it to fetch the data. With the new generative AI-powered text-to-SQL capability in Parcel Perform, the business team can self-serve their data needs by using an AI assistant interface. In this post, we discuss how Parcel Perform incorporated generative AI, data storage, and data access through AWS services to make timely decisions. 
Data analytics architecture 
The solution starts with data ingestion, storage, and access. Parcel Perform adopted the data analytics architecture shown in the following diagram. 
 
One key data type in the Parcel Perform parcel monitoring application is the parcel event data, which can reach billions of rows. This includes the parcel‚Äôs shipment status change, location change, and much more. This day-to-day data from multiple business units lands in relational databases hosted on Amazon Relational Database Service (Amazon RDS). 
Although relational databases are suitable for rapid data ingestion and consumption from the application, a separate analytics stack is needed to handle analytics in a scalable and performant way without disrupting the main application. These analytics needs include answering aggregation queries from questions like ‚ÄúHow many parcels were delayed last week?‚Äù 
Parcel Perform uses Amazon Simple Storage Service (Amazon S3) with a query engine provided by Amazon Athena to meet their analytics needs. With this approach, Parcel Perform benefits from cost-effective storage while still being able to run SQL queries as needed on the data through Athena, which is priced on usage. 
Data in Amazon S3 is stored in Apache Iceberg data format that allows data updates, which is useful in this case because the parcel events sometimes get updated. It also supports partitioning for better performance. Amazon S3 Tables, launched in late 2024, is a managed Iceberg tables feature that can also be an option for you. 
Parcel Perform uses an Apache Kafka cluster managed by Amazon Managed Streaming for Apache Kafka (Amazon MSK) as the stream to move the data from the source to the S3 bucket. Amazon MSK Connect with a Debezium connector streams data with change data capture (CDC) from Amazon RDS to Amazon MSK. 
Apache Flink, running on Amazon Elastic Kubernetes Service (Amazon EKS), processes data streams from Amazon MSK. It writes this data to an S3 bucket according to the Iceberg format, and updates the data schema in the AWS Glue Data Catalog. The data schema enables Athena to correctly query the data in the S3 bucket. 
Now that you understand how the data is ingested and stored, let‚Äôs show how the data is consumed using the generative AI-powered data serving assistant for the business teams in Parcel Perform. 
AI agent that can query data 
The users of the data serving AI agent in Parcel Perform are customer-facing business team members who often query the parcel event data to answer questions from ecommerce merchants regarding the parcel deliveries and to proactively assist them. The following screenshot shows the UI experience for the AI agent assistant, powered by text-to-SQL with generative AI. 
 
This functionality helped the Parcel Perform team and their customers save time, which we discuss later in this post. In the following section, we present the architecture that powers this feature. 
Text-to-SQL AI agent architecture 
The data serving AI assistant architecture in Parcel Perform is shown in the following diagram. 
The AI assistant UI is powered by an application built with the Fast API framework hosted on Amazon EKS. It is also fronted by an Application Load Balancer to allow for potential horizontal scalability. 
The application uses LangGraph to orchestrate the workflow of large language model (LLM) invocations, the use of tools, and the memory checkpointing. The graph uses multiple tools, including those from SQLDatabase Toolkit, to automatically fetch the data schema through Athena. The graph also uses an Amazon Bedrock Knowledge Bases retriever to retrieve business information from a knowledge base. Parcel Perform uses Anthropic‚Äôs Claude models in Amazon Bedrock to generate SQL. 
Although the function of Athena as a query engine to query the parcel event data on Amazon S3 is clear, Parcel Perform still needs a knowledge base. In this use case, the SQL generation performs better when the LLM has more business contextual information to help interpret database fields and translate logistics terminology into data representations. This is better illustrated with the following two examples: 
 
 Parcel Perform‚Äôs data lake operations use specific codes: c for create and u for update. When analyzing data, Parcel Perform sometimes needs to focus only on initial creation records, where operation code is equal to c. Because this business logic might not be inherent in the training of LLMs in general, Parcel Perform explicitly defines this in their business context. 
 In logistics terminology, transit time has specific industry conventions. It‚Äôs measured in days, and same-day deliveries are recorded as transit_time = 0. Although this is intuitive for logistics professionals, an LLM might incorrectly interpret a request like ‚ÄúGet me all shipments with same-day delivery‚Äù by using WHERE transit_time = 1 instead of WHERE transit_time = 0 in the generated SQL statement. 
 
Therefore, each incoming question goes to a Retrieval Augmented Generation (RAG) workflow to find potentially relevant stored business information, to enrich the context. This mechanism helps provide the specific rules and interpretations that even advanced LLMs might not be able to derive from general training data. 
Parcel Perform uses Amazon Bedrock Knowledge Bases as a managed solution for the RAG workflow. They ingest business contextual information by uploading files to Amazon S3. Amazon Bedrock Knowledge Bases processes the files, converts them to chunks, uses embedding models to generate vectors, and stores the vectors in a vector database to make them searchable. The steps are fully managed by Amazon Bedrock Knowledge Bases. Parcel Perform stores the vectors in Amazon OpenSearch Serverless as the vector database of choice to simplify infrastructure management. 
Amazon Bedrock Knowledge Bases provides the Retrieve API, which takes in an input (such as a question from the AI assistant), converts it into a vector embedding, searches for relevant chunks of business context information in the vector database, and returns the top relevant document chunks. It is integrated with the LangChain Amazon Bedrock Knowledge Bases retriever by calling the invoke method. 
The next step involves invoking an AI agent with the supplied business contextual information and the SQL generation prompt. The prompt was inspired by a prompt in LangChain Hub. The following is a code snippet of the prompt: 
 
 You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.
Relevant context:
{rag_context}
You can order the results by a relevant column to return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for the relevant columns given the question.
You have access to tools for interacting with the database.
- Only use the below tools. Only use the information returned by the below tools to construct your final answer.
- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.
- To start querying for final answer you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.
- Then you should query the schema of the most relevant tables 
 
The prompt sample is part of the initial instruction for the agent. The data schema is automatically inserted by the tools from the SQLDatabase Toolkit at a later step of this agentic workflow. The following steps occur after a user enters a question in the AI assistant UI: 
 
 The question triggers a run of the LangGraph graph. 
 The following processes happen in parallel: 
   
   The graph fetches the database schema from Athena through SQLDatabase Toolkit. 
   The graph passes the question to the Amazon Bedrock Knowledge Bases retriever and gets a list of relevant business information regarding the question. 
    
 The graph invokes an LLM using Amazon Bedrock by passing the question, the conversation context, data schema, and business context information. The result is the generated SQL. 
 The graph uses SQLDatabase Toolkit again to run the SQL through Athena and fetch the data output. 
 The data output is passed into an LLM to generate the final response based on the initial question asked. Amazon Bedrock Guardrails is used as a safeguard to avoid inappropriate inputs and responses. 
 The final response is returned to the user through the AI assistant UI. 
 
The following diagram illustrates these steps. 
 
This implementation demonstrates how Parcel Perform transforms raw inquiries into actionable data for timely decision-making. Security is also implemented in multiple components. From a network perspective, the EKS pods are placed in private subnets in Amazon Virtual Private Cloud (Amazon VPC) to improve network security of the AI assistant application. This AI agent is placed behind a backend layer that requires authentication. For data security, sensitive data is masked at rest in the S3 bucket. Parcel Perform also limits the permissions of the AWS Identity and Access Management (IAM) role used to access the S3 bucket so it can only access certain tables. 
In the following sections, we discuss Parcel Perform‚Äôs approach to building this data transformation solution. 
From idea to production 
Parcel Perform started with the idea of freeing their data team from manually serving the request from the business team, while also improving the timeliness of the data availability to support the business team‚Äôs decision-making. 
With the help of the AWS Solutions Architect team, Parcel Perform completed a proof of concept using AWS services and a Jupyter notebook in Amazon SageMaker Studio. After an initial success, Parcel Perform integrated the solution with their orchestration tool of choice, LangGraph. 
Before going into production, Parcel Perform conducted extensive testing to verify the results were consistent. They added LangSmith Tracing to log the AI agent‚Äôs steps and results to evaluate its performance. 
The Parcel Perform team discovered challenges during their journey, which we discuss in the following section. They performed prompt engineering to address those challenges. Eventually, the AI agent was integrated into production to be used by the business team. Afterward, Parcel Perform collected user feedback internally and monitored logs from LangSmith Tracing to verify performance was maintained. 
The challenges 
This journey isn‚Äôt free from challenges. Firstly, some ecommerce merchants might have several records in the data lake under various names. For example, a merchant with the name ‚ÄúABC‚Äù might have multiple records such, as ‚ÄúABC Singapore Holdings Pte. Ltd.,‚Äù ‚ÄúABC Demo Account,‚Äù ‚ÄúABC Test Group,‚Äù and so on. For a question like ‚ÄúWas there any parcel shipment delay by ABC last week?‚Äù, the generated SQL has the element of WHERE merchant_name LIKE '%ABC%', which might result in ambiguity. During the proof of concept stage, this problem caused incorrect matching of the result. 
For this challenge, Parcel Perform relies on careful prompt engineering to instruct the LLM to identify when the name was potentially ambiguous. The AI agent then calls Athena again to look for matching names. The LLM decides which merchant name to use based on multiple factors, including the significance in data volume contribution and the account status in the data lake. In the future, Parcel Perform intends to implement a more sophisticated technique by prompting the user to resolve the ambiguity. 
The second challenge is about unrestricted questions that might yield expensive queries running across large amounts of data and resulting in longer query waiting time. Some of these questions might not have a LIMIT clause imposed in the query. To solve this, Parcel Perform instructs the LLM to add a LIMIT clause with a certain number of maximum results if the user doesn‚Äôt specify the intended number of results. In the future, Parcel Perform plans to use the query EXPLAIN results to identify heavy queries. 
The third challenge is related to tracking usage and incurred cost of this particular solution. Having started multiple generative AI projects using Amazon Bedrock and sometimes with the same LLM ID, Parcel Perform must distinguish usage incurred by projects. Parcel Perform creates an inference profile for each project, associates the profile with tags, and includes that profile in each LLM call for that project. With this setup, Parcel Perform is able to segregate costs based on projects to improve cost visibility and monitoring. 
The impact 
To extract data, the business team clarifies details with the data team, makes a request, checks feasibility, and waits for bandwidth. This process lengthens when requirements come from customers or teams in different time zones, with each clarification adding 12‚Äì24 hours due to asynchronous communication. Simpler requests made early in the workday might complete within 24 hours, whereas more complex requests or those during busy periods can take 3‚Äì5 business days. 
With the text-to-SQL AI agent, this process is dramatically streamlined‚Äîminimizing the back-and-forth communication for requirement clarification, removing the dependency on data team bandwidth, and automating result interpretation. 
Parcel Perform‚Äôs measurements show that the text-to-SQL AI agent reduces the average time-to-insight by 99%, from 2.3 days to an average of 10 minutes, saving approximately 3,850 total hours of wait time per month across requesters while maintaining data accuracy. 
Users can directly query the data without intermediaries, receiving results in minutes rather than days. Teams across time zones can now access insights any time of day, alleviating the frustrating ‚Äúwait until Asia wakes up‚Äù or ‚Äúcatch EMEA before they leave‚Äù delays, leading to happier customers and faster problem-solving. 
This transformation has profoundly impacted the data analytics team‚Äôs capacity and focus, freeing the data team for more strategic work and helping everyone make faster, more informed decisions. Before, the analysts spent approximately 25% of their working hours handling routine data extraction requests‚Äîequivalent to over 260 hours monthly across the team. Now, with basic and intermediate queries automated, this number has dropped to just 10%, freeing up nearly 160 hours each month for high-impact work. Analysts now focus on complex data analysis rather than spending time on basic data retrieval tasks. 
Conclusion 
Parcel Perform‚Äôs solution demonstrates how you can use generative AI to enhance productivity and customer experience. Parcel Perform has built a text-to-SQL AI agent that transforms a business team‚Äôs question into SQL that can fetch the actual data. This improves the timeliness of data availability for decision-making that involves customers. Furthermore, the data team can avoid the undifferentiated heavy lifting to focus on complex data analysis tasks. 
This solution uses multiple AWS services like Amazon Bedrock and tools like LangGraph. You can start with a proof of concept and consult your AWS Solutions Architect or engage with AWS Partners. If you have questions, post them on AWS re:Post. You can also make the development more straightforward with the help of Amazon Q Developer. When you face challenges, you can iterate to find the solution, which might include prompt engineering or adding additional steps to your workflow. 
Security is a top priority. Make sure your AI assistant has proper guardrails in place to protect against prompt threats, inappropriate topics, profanity, leaked data, and other security issues. You can integrate Amazon Bedrock Guardrails with your generative AI application through an API.To learn more, refer to the following resources: 
 
 Build a robust text-to-SQL solution generating complex queries, self-correcting, and querying diverse data sources 
 LangGraph agents with Amazon Bedrock workshop 
 Build a knowledge base by connecting to a structured data store 
 
 
 
About the authors 
Yudho Ahmad Diponegoro is a Senior Solutions Architect at AWS. Having been part of Amazon for 10+ years, he has had various roles from software development to solutions architecture. He helps startups in Singapore when it comes to architecting in the cloud. While he keeps his breadth of knowledge across technologies and industries, he focuses in AI and machine learning where he has been guiding various startups in ASEAN to adopt machine learning and generative AI at AWS. 
Le Vy is the AI Team Lead at Parcel Perform, where she drives the development of AI applications and explores emerging AI research. She started her career in data analysis and deepened her focus on AI through a Master‚Äôs in Artificial Intelligence. Passionate about applying data and AI to solve real business problems, she also dedicates time to mentoring aspiring technologists and building a supportive community for youth in tech. Through her work, Vy actively challenges gender norms in the industry and champions lifelong learning as a key to innovation. 
Loke Jun Kai is a GenAI/ML Specialist Solutions Architect in AWS, covering strategic customers across the ASEAN region. He works with customers ranging from Start-up to Enterprise to build cutting-edge use cases and scalable GenAI Platforms. His passion in the AI space, constant research and reading, have led to many innovative solutions built with concrete business outcomes. Outside of work, he enjoys a good game of tennis and chess.
‚Ä¢ Query Amazon Aurora PostgreSQL using Amazon Bedrock Knowledge Bases structured data
  Amazon Bedrock Knowledge Bases offers a fully managed Retrieval Augmented Generation (RAG) feature that connects large language models (LLMs) to internal data sources. This feature enhances foundation model (FM) outputs with contextual information from private data, making responses more relevant and accurate. 
At AWS re:Invent 2024, we announced Amazon Bedrock Knowledge Bases support for natural language querying to retrieve structured data from Amazon Redshift and Amazon SageMaker Lakehouse. This feature provides a managed workflow for building generative AI applications that access and incorporate information from structured and unstructured data sources. Through natural language processing, Amazon Bedrock Knowledge Bases transforms natural language queries into SQL queries, so users can retrieve data directly from supported sources without understanding database structure or SQL syntax. 
In this post, we discuss how to make your Amazon Aurora PostgreSQL-Compatible Edition data available for natural language querying through Amazon Bedrock Knowledge Bases while maintaining data freshness. 
Structured data retrieval in Amazon Bedrock Knowledge Bases and Amazon Redshift Zero-ETL 
Structured data retrieval in Amazon Bedrock Knowledge Bases enables natural language interactions with your database by converting user queries into SQL statements. When you connect a supported data source like Amazon Redshift, Amazon Bedrock Knowledge Bases analyzes your database schema, table relationships, query engine, and historical queries to understand the context and structure of your information. This understanding allows the service to generate accurate SQL queries from natural language questions. 
At the time of writing, Amazon Bedrock Knowledge Bases supports structured data retrieval directly from Amazon Redshift and SageMaker Lakehouse. Although direct support for Aurora PostgreSQL-Compatible isn‚Äôt currently available, you can use the zero-ETL integration between Aurora PostgreSQL-Compatible and Amazon Redshift to make your data accessible to Amazon Bedrock Knowledge Bases structured data retrieval. Zero-ETL integration automatically replicates your Aurora PostgreSQL tables to Amazon Redshift in near real time, alleviating the need for complex extract, transform, and load (ETL) pipelines or data movement processes. 
This architectural pattern is particularly valuable for organizations seeking to enable natural language querying of their structured application data stored in Amazon Aurora database tables. By combining zero-ETL integration with Amazon Bedrock Knowledge Bases, you can create powerful applications like AI assistants that use LLMs to provide natural language responses based on their operational data. 
Solution overview 
The following diagram illustrates the architecture we will implement to connect Aurora PostgreSQL-Compatible to Amazon Bedrock Knowledge Bases using zero-ETL. 
 
The workflow consists of the following steps: 
 
 Data is stored in Aurora PostgreSQL-Compatible within the private subnet. We use a bastion host to connect securely to the database from the public subnet. 
 Using zero-ETL integration, this data is made available in Amazon Redshift, also located in the private subnet. 
 Amazon Bedrock Knowledge Bases uses Amazon Redshift as its structured data source. 
 Users can interact with Amazon Bedrock Knowledge Bases using the AWS Management Console or an AWS SDK client, which sends natural language queries. These queries are processed by Amazon Bedrock Knowledge Bases to retrieve information stored in Amazon Redshift (sourced from Aurora). 
 
Prerequisites 
Make sure you‚Äôre logged in with a user role with access to create an Aurora database, run DDL (CREATE, ALTER, DROP, RENAME) and DML (SELECT, INSERT, UPDATE, DELETE) statements, create a Redshift database, set up zero-ETL integration, and create an Amazon Bedrock knowledge base. 
Set up the Aurora PostgreSQL database 
In this section, we walk through creating and configuring an Aurora PostgreSQL database with a sample schema for our demonstration. We create three interconnected tables: products, customers, and orders. 
Provision the database 
Let‚Äôs begin by setting up our database environment. Create a new Aurora PostgreSQL database cluster and launch an Amazon Elastic Compute Cloud (Amazon EC2) instance that will serve as our access point for managing the database. The EC2 instance will make it straightforward to create tables and manage data throughout this post. 
The following screenshot shows the details of our database cluster and EC2 instance. 
 
For instructions to set up your database, refer to Creating and connecting to an Aurora PostgreSQL DB cluster. 
Create the database schema 
After you connect to your database using SSH on your EC2 instance (described in Creating and connecting to an Aurora PostgreSQL DB cluster), it‚Äôs time to create your data structure. We use the following DDL statements to create three tables: 
 
 -- Create Product table
CREATE TABLE product (
&nbsp;&nbsp; &nbsp;product_id SERIAL PRIMARY KEY,
&nbsp;&nbsp; &nbsp;product_name VARCHAR(100) NOT NULL,
&nbsp;&nbsp; &nbsp;price DECIMAL(10, 2) NOT NULL
);

-- Create Customer table
CREATE TABLE customer (
&nbsp;&nbsp; &nbsp;customer_id SERIAL PRIMARY KEY,
&nbsp;&nbsp; &nbsp;customer_name VARCHAR(100) NOT NULL,
&nbsp;&nbsp; &nbsp;pincode VARCHAR(10) NOT NULL
);

-- Create Orders table
CREATE TABLE orders (
&nbsp;&nbsp; &nbsp;order_id SERIAL PRIMARY KEY,
&nbsp;&nbsp; &nbsp;product_id INTEGER NOT NULL,
&nbsp;&nbsp; &nbsp;customer_id INTEGER NOT NULL,
&nbsp;&nbsp; &nbsp;FOREIGN KEY (product_id) REFERENCES product(product_id),
&nbsp;&nbsp; &nbsp;FOREIGN KEY (customer_id) REFERENCES customer(customer_id)
); 
 
Populate the tables with data 
After you create the tables, you can populate them with sample data. When inserting data into the orders table, remember to maintain referential integrity by verifying the following: 
 
 The product_id exists in the product table 
 The customer_id exists in the customer table 
 
We use the following example code to populate the tables: 
 
 INSERT INTO product (product_id, product_name, price) VALUES (1, 'Smartphone X', 699.99);
INSERT INTO product (product_id, product_name, price) VALUES (2, 'Laptop Pro', 1299.99);
INSERT INTO product (product_id, product_name, price) VALUES (3, 'Wireless Earbuds', 129.99);
INSERT INTO customer (customer_id, customer_name, pincode) VALUES (1, 'John Doe', '12345');
INSERT INTO customer (customer_id, customer_name, pincode) VALUES (2, 'Jane Smith', '23456');
INSERT INTO customer (customer_id, customer_name, pincode) VALUES (3, 'Robert Johnson', '34567');
INSERT INTO orders (order_id, product_id, customer_id) VALUES (1, 1, 1);
INSERT INTO orders (order_id, product_id, customer_id) VALUES (2, 1, 2);
INSERT INTO orders (order_id, product_id, customer_id) VALUES (3, 2, 3);
INSERT INTO orders (order_id, product_id, customer_id) VALUES (4, 2, 1);
INSERT INTO orders (order_id, product_id, customer_id) VALUES (5, 3, 2);
INSERT INTO orders (order_id, product_id, customer_id) VALUES (6, 3, 3); 
 
Make sure to maintain referential integrity when populating the orders table to avoid foreign key constraint violations. 
You can also use similar examples to build your schema and populate data for this. 
Set up the Redshift cluster and configure zero-ETL 
Now that you have set up your Aurora PostgreSQL database, you can establish the zero-ETL integration with Amazon Redshift. This integration automatically syncs your data between Aurora PostgreSQL-Compatible and Amazon Redshift. 
Set up Amazon Redshift 
First, create an Amazon Redshift Serverless workgroup and namespace. For instructions, see Creating a data warehouse with Amazon Redshift Serverless. 
Create a zero-ETL integration 
The zero-ETL integration process involves two main steps: 
 
 Create the zero-ETL integration from your Aurora PostgreSQL database to Redshift Serverless. 
 After you establish the integration on the Aurora side, create the corresponding mapping database in Amazon Redshift. This step is crucial for facilitating proper data synchronization between the two services. 
 
The following screenshot shows our zero-ETL integration details. 
 
Verify the integration 
After you complete the integration, you can verify its success through several checks. 
Firstly, you can check the zero-ETL integration details in the Amazon Redshift console. You should see an Active status for your integration, along with source and destination information, as shown in the following screenshot. 
 
Additionally, you can use the Redshift Query Editor v2 to verify that your data has been successfully populated. A simple query like SELECT * FROM customer; should return the synchronized data from your Aurora PostgreSQL database, as shown in the following screenshot. 
 
Set up the Amazon Bedrock knowledge base with structured data 
The final step is to create an Amazon Bedrock knowledge base that will enable natural language querying of our data. 
Create the Amazon Bedrock knowledge base 
Create a new Amazon Bedrock knowledge base with the structured data option. For instructions, see Build a knowledge base by connecting to a structured data store. Then you must synchronize the query engine to enable data access. 
Configure data access permissions 
Before the sync process can succeed, you need to grant appropriate permissions to the Amazon Bedrock Knowledge Bases AWS Identity and Access Management (IAM) role. This involves executing GRANT SELECT commands for each table in your Redshift database. 
Run the following command in Redshift Query Editor v2 for each table:GRANT SELECT ON &lt;table_name&gt; TO "IAMR:&lt;KB Role name&gt;";For example:GRANT SELECT ON customer TO "IAMR:AmazonBedrockExecutionRoleForKnowledgeBase_ej0f0"; 
For production setups, integrating the end-user identity into the data access flow requires identity federation. Refer to AWS documentation on structured database access for the role-based access model. For federating identities from web clients, Amazon Cognito or SAML federation with AWS Security Token Service (AWS STS) might be required depending on your architecture. 
Verify the setup 
After you complete the configuration, your knowledge base should show the following details: 
 
 Status as Available 
 Query engine successfully synced with Amazon Redshift 
 COMPLETE status for the database synchronization 
 
You can now start querying your data using natural language. 
Example natural language queries 
Now that you have set up your Amazon Bedrock knowledge base, you can begin testing its capabilities by running natural language queries against your structured data. Amazon Bedrock Knowledge Bases structured data translates plain English questions into SQL and uses FMs to generate human-readable responses. 
You can test your Amazon Bedrock knowledge base in two ways: 
 
 Amazon Bedrock console ‚Äì On the Amazon Bedrock console, choose Knowledge Bases in the navigation pane, and select your configured knowledge base. Open the Test tab to interact with your data. 
 AWS SDK or API ‚Äì Use the AWS SDK (for example, Python‚Äôs Boto3) or directly call the RetrieveAndGenerate API to send queries programmatically. 
 
In this section, we illustrate the console experience. On the Amazon Bedrock console, you can interact with your Amazon Bedrock knowledge base in two modes: 
 
 Generate SQL queries ‚Äì This option shows you the raw SQL query that gets generated from your natural language question 
 
 
 
 Retrieval and response generation ‚Äì This provides a natural language response using an LLM 
 
 
The following table contains some examples of queries and their respective SQL and model response generation. 
 
  
   
   Natural Language Query 
   Generate SQL API Result 
   Retrieval and Response Generation 
   Model Used for Response Generation 
   
   
   How many customers do we have? 
    
     
     SELECT COUNT(DISTINCT customer_id) AS num_customers 
FROM public.customer; 
      
   We currently have 11 unique customers. 
   Amazon Nova Lite 
   
   
   Which all customers have purchased the most products? 
    
     
     SELECT c. "customer_name", COUNT(o."product_id") 
AS "total_products_purchased" 
FROM public.customer c 
INNER JOIN public.orders o 
ON c. "customer_id" = o. "customer_id" 
GROUP BY c. "customer_name" 
ORDER BY "total_products_purchased" DESC; 
      
   Based on the data, the customers who have purchased the most products are Charlie Davis, Alice Brown, and John Doe, each having purchased 14 products. Following closely are Jane Smith, Grace Lee, and Bob Johnson, who have each purchased 13 products. Henry Taylor, Frank Miller, and Eva Wilson have each purchased 12 products, while Ivy Anderson has purchased 11 products. 
   Amazon Nova Lite 
   
   
   Who all have purchased more than one number of the most expensive product? 
    
     
     SELECT c.customer_name 
FROM public.customer c 
INNER JOIN public.orders o 
ON c.customer_id = o.customer_id 
INNER JOIN public.product p 
ON o.product_id = p.product_id 
WHERE p.price = (SELECT MAX(price) 
FROM public.product) 
GROUP BY c.customer_name 
HAVING COUNT(DISTINCT o.order_id); 
      
   The customers who have purchased more than one number of the most expensive product are Grace Lee, Jane Smith, Alice Brown, and Eva Wilson. 
   Amazon Nova Micro 
   
  
 
Clean up 
When you‚Äôre done using this solution, clean up the resources you created to avoid ongoing charges. 
Conclusion 
In this post, we demonstrated how to enable natural language querying of Aurora PostgreSQL data using Amazon Bedrock Knowledge Bases through zero-ETL integration with Amazon Redshift. We showed how to set up the database, configure zero-ETL integration, and establish the knowledge base connection for seamless data access. Although this solution provides an effective way to interact with your data using natural language, you should consider the additional storage costs in Amazon Redshift when implementing this architecture for your use case. 
Please try out this solution for yourself and share your feedback in the comments. 
 
About the authors 
 Girish B is a Senior Solutions Architect at AWS India Pvt Ltd based in Bengaluru. Girish works with many ISV customers to design and architect innovative solutions on AWS 
 Dani Mitchell is a Generative AI Specialist Solutions Architect at AWS. He is focused on helping accelerate enterprises across the world on their generative AI journeys with Amazon Bedrock

‚∏ª