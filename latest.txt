âœ… Morning News Briefing â€“ October 21, 2025 10:45

ğŸ“… Date: 2025-10-21 10:45
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ Current Conditions:  5.4Â°C
  Temperature: 5.4&deg;C Pressure / Tendency: 100.9 kPa falling Humidity: 77 % Dewpoint: 1.7&deg:C Wind: WNW 10 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 21 October 2025 . Weather forecast: 5/4Â°C Pem
â€¢ Tuesday: Chance of showers. High 13. POP 60%
  Increasing cloudiness this morning then 60 percent chance of showers late this afternoon . Mainly sunny this morning . High 13.5 degrees in the morning . UV index 3 or moderate. UV index 1 or lower . Showers will be mainly sunny in the afternoon . Forecast issued 5:00 AM EDT Tuesday 21 October 2025 . Weather will be mostly sunny this afternoon. High 13 degrees in
â€¢ Tuesday night: Chance of showers. Low plus 5. POP 40%
  Mainly cloudy with 40 percent chance of showers . Low plus 5.70s is expected to be mostly cloudy with low-60s chance of rain . Forecast issued 5:00 AM EDT Tuesday 21 October 2025 for the U.S. Low 60s is forecast for the rest of the week . Low 50s is predicted to be below 40s for the next few days .

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ A theory why the internet is going down the toilet
  A new book diagnoses a sickness affecting some of America's biggest companies . The book is called 'The Sickness of Fortune 500' and focuses on companies like Amazon, Google and Facebook . It is the first time a book has been published that focuses on the sickness of some of the biggest companies in the U.S. The book will be released on Friday at 8 p.m
â€¢ 'Cancer doesn't care': Patients pushed past divisive politics to lobby Congress
  Hundreds of volunteer advocates put partisan differences aside and pressed Congress to help people with cancer . The advocacy came just before the stalemate that has shut down the federal government . It came just days before the government shut down in the federal shutdown, the advocacy came before the end of the government's funding debate . It's not the first time the government has been forced to shut down by the shutdown
â€¢ Blue New Jersey is expecting a very close race for governor this November
  President Trump and former President Obama have endorsed the two candidates locked in a tight race for New Jersey governor . Republican Jack Ciattarelli and Democrat Mikie Sherrill are currently tied in the race for governor of New Jersey . Former President Obama has endorsed the Republican candidate for governor in New Jersey's gubernatorial race . President Trump has endorsed both candidates for the next New Jersey Governor's office .
â€¢ Karine Jean-Pierre on why she left the Democrats â€” and calls herself independent
  Former Biden press secretary Karine Jean-Pierre writes that party infighting, bias and disloyalty drove her to leave the Democratic Party . In her new book Independent, she writes that she left the party because of the party's infighting and bias . The book, Independent, is out now and is being published by Simon Simon, a Pulitzer Prize-winning Democratic journalist and author of
â€¢ Middle East Institute's Natan Sachs on Israel and the future of the ceasefire deal
  Natan Sachs with the Middle East Institute talks about the future of the ceasefire deal from Israel's perspective . Sachs talks with Leila Fadel about Israel's role in the ceasefire agreement . Sachs: The deal is a good one for Israel and Israel, but it's not a bad one for the U.S., it's a good for Israel, Israel and the world's future .

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ Feds flag active exploitation of patched Windows SMB vuln
  CISA adds high-severity flaw to KEV list, urges swift updating . Uncle Sam's cyber wardens warn flaw in Microsoft's Windows SMB client is now being actively exploited â€“ months after it was patched . CISA: 'Flaw is now actively being exploited' CISA also urges swift updates to fix flaw in SMB clients that were patched months ago . Microsoft's
â€¢ Trust the AI, says new coding manifesto by Kim and Yegge
  DevOps guru and ex-Googler says vibes beat reading diffs but there are risks . "Accept All. Always. Don't read the diffs anymore," he says . DevOps expert: "Don't read diffs now. Acceptance All. Means "Always. Accept All. Donâ€™t read thediffs anymore" DevOps gurus say vib
â€¢ A shot in the dark: Can malware vaccines stop ransomware's rampage?
  Security pros explore whether infection-spoofing code can immunize Windows systems against attack Feature: What's better, prevention or cure? For a long time the global cybersecurity industry has operated by reacting to attacks and computer viruses . But given that ransomware has continued to escalate, more proactive action is needed, says security expert . Security pros: More proactive action needed to be done to combat the
â€¢ Mobian makes Debian's latest 'Trixie' release pocket-sized
  Mobian, an edition of Linux aimed at mobile devices, is based on the latest version of the latest edition of the software . The Reg attempts to disentangle the options in Mobian . Mobian is a version of "Trixie" based on a version based on "Tricie" and "Trapie" The Reg attempted to understand the implications of Mobian
â€¢ Anti-fraud body leaks dozens of email addresses in invite mishap
  Calendar cock-up exposed recipients' details . Anti-fraud nonprofit Cifas left red-faced after sending out a calendar invite that exposed the email addresses of dozens of individuals working across the fraud space . Calendar invite exposed emails of people working across fraud space working in fraud space.â€¦â€¦â€¦ Calendar invite sent out to dozens of people who worked in fraud-related fraud space exposed

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ A community cross-sectional study on oral health status among rural and urban inhabitants of Zambia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Association of repeated high serum osmolarity with cognitive function in older Japanese adults in a KOBE study subanalysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Impact of health education on knowledge and perception of cervical cancer and screening among rural women in Bangladesh
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Creativity keeps the brain young
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ RNA replicon vaccination confers long-lasting protection against H5N1 avian influenza in 23 zoo bird species
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ New noninvasive endometriosis tests are on the rise
  Shantana Hazel often thought her insides might fall out during menstruation. It took 14 years of stabbing pain before she ultimately received a diagnosis of endometriosis, an inflammatory disease where tissue similar to the uterine lining implants outside the uterus and bleeds with each cycle. The results can include painful periods and damaging scar tissue. Hazel, now 50 and the founder of the endometriosis advocacy organization Sister Girl Foundation, was once told by a surgeon that her internal organs were â€œfused togetherâ€ by lesions resembling Laffy Taffy. After 16 surgeries, she had a hysterectomy at age 30.&nbsp;



Hazel is far from alone. Endometriosis inflicts debilitating pain and heavy bleeding on more than 11% of reproductive-Â­age women in the United States. Diagnosis takes nearly 10 years on average, partly because half the cases donâ€™t show up on scans, and surgery is required to obtain tissue samples.



But a new generation of noninvasive tests are emerging that could help accelerate diagnosis and improve management of this poorly understood condition.&nbsp;



Within the next year, several companies, including Hera Biotech, Proteomics International, NextGen Jane, and Ziwig, aim to launch endometriosis diagnostics in the United States. Their tests analyze biomarkersâ€”biological molecules (in this case, mRNA, proteins, or miRNA) that signal a disease or process like inflammationâ€”in samples of endometrial tissue, blood, menstrual blood, and saliva.&nbsp;



Ziwig Lab, headquartered in France, hopes to have their test approved for use in the United States soon.COURTESY MM PRODUCTION â€“ MAGALI MEIRA




These tests could help patients get an accurate diagnosis quickly and noninvasively, speeding access to endometriosis treatments and management strategies, including surgery, hormonal medications, and pelvic floor physical therapy. Early identification could also help doctors manage conditions for which people with endometriosis face increased risk, including cardiovascular disease, heart attack, and stroke. Endometriosis can also make it difficult to become pregnant. Because half of women with infertility have endometriosis, identifying and managing the condition sooner may improve fertility and IVF outcomes.&nbsp;





Endometriosis biomarker tests rely on a range of technologies, including single-cell RNA sequencing and mass spectrometry that can identify thousands of proteins simultaneously. â€œThese instruments are very good at precisely identifying a molecule, in [our] case a protein. And whatâ€™s changed over the last five or 10 years is theyâ€™ve gotten more sensitive,â€ says Proteomics cofounder Richard Lipscombe. Machine learning can also now efficiently sift through large quantities of the resulting data.&nbsp;



So far only Ziwig has a test on the market. It uses a saliva sample to identify biomarkers in people with endometriosis symptoms and is currently sold in 30 countries. In France, where the company is based, the cost is fully covered by national health insurance.&nbsp;



Some researchers are concerned that Ziwigâ€™s test might not be accurate when itâ€™s used in larger and more diverse populations; its interim validation study included just 200 people. â€œIâ€™m not saying this doesnâ€™t work. I just would want to see more validation,â€ says Kathryn Terry, an associate professor of epidemiology and gynecology at Harvard. Company representatives say theyâ€™re preparing to publish results on 1,000 patients in the near future, adding that French authorities had access to the full data set before approving government reimbursements.



COURTESY MM PRODUCTION â€“ MAGALI MEIRA




These tests are emerging as momentum is building to tackle endometriosis. Over the past five years, France, Australia, the United Kingdom, and Canada have launched ambitious endometriosis initiatives.&nbsp;



The potential benefits are not just on the individual level: In 2025, the World Economic Forum estimated that earlier diagnosis and improved treatment to address the chronic pain, infertility, and depression caused by endometriosis could add at least $12 billion to global GDP by 2040.



As these biomarker tests are further developed, itâ€™s possible their results could inform such treatments. Today surgery is often used to excise the lesions. The process can take as long as seven hours, and even then, lesions frequently form again. Jason Abbott, chair of Australiaâ€™s&nbsp;National Endometriosis Clinical and Scientific&nbsp;Trials Network, compares endometriosis management today to breast cancer care 30 years ago. Whereas doctors once prescribed surgery for all breast cancer patients, targeted treatments now address the underlying cell processes that help tumors grow and spread. Endometriosis tests could likewise help researchers categorize the conditionâ€™s distinct subsets and understand their underlying inflammatory pathwaysâ€”information drugmakers could use to develop targeted treatments that keep it in remission.



Colleen de Bellefonds is a science journalist based in Paris.
â€¢ The astonishing embryo models of Jacob Hanna
  When the Palestinian stem-cell scientist Jacob Hanna was stopped while entering the US last May, airport customs agents took him aside and held him for hours in â€œsecondary,â€ a back office where you donâ€™t have your passport and canâ€™t use your phone. There were two young Russian women and a candy machine in the room with him. Hanna, who has a trim beard and glasses and holds an Israeli passport, accepted the scrutiny. â€œItâ€™s almost like you are under arrest, but in a friendly way,â€ he says. He agreed to turn over his phone and social media for inspection.&nbsp;&nbsp;



â€œThey said, â€˜You have the right to refuse,â€™â€ he recalls, â€œand I said, â€˜No, no, itâ€™s an open book.â€™â€






The agents scrolling through his feeds would have learned that Hanna is part of Israelâ€™s small Arab Christian minority, a nonbinary LGBTQ-rights advocate, and an outspoken critic of the Gaza occupation, who uses his social media accounts to post images of atrocities and hold up a mirror to scientific colleagues including those at the Weizmann Institute of Science, the pure-science powerhouse where he worksâ€”Israelâ€™s version of Caltech or Rockefeller University. In his luggage, they would have found his keffiyeh, or traditional headscarf, which Hanna last year vowed to wear at lecture podiums on his many trips abroad.



Hanna had been stopped before; he knew the routine. Anything to declare? Any biological samples? But this time the agentsâ€™ questions touched on a specific new topic: embryos.



Weeks earlier, a Harvard University researcher had been arrested for having frog embryos in her luggage and sent to a detention center in Louisiana. Hanna didnâ€™t have any specimens from his lab, but if he had, it would have been surprisingly hard to say what they were. Thatâ€™s because his lab specializes in creating synthetic embryo models, structures that resemble real embryos but donâ€™t involve sperm, eggs, or fertilization.&nbsp;



Instead of relying on the same old recipe biology has followed for a billion years, give or take, Hanna is coaxing the beginnings of animal bodies directly from stem cells. Join these cells together in the right way, and they will spontaneously attempt to organize into an embryoâ€”a feat thatâ€™s opening up the earliest phases of development to scientific scrutiny and may lead to a new source of tissue for transplant medicine.




Soon it could be difficult to distinguish between a real human embryoâ€”the kind with legal protectionsâ€”and one conjured from stem cells.




In 2022, working with mice, Hanna reported heâ€™d used the technique to produce synthetic embryos with beating hearts and neural foldsâ€”growing them inside small jars connected to a gas mixer, a type of artificial womb. The next year, he repeated the trick using human cells. This time the structures were not so far developed, still spherical in shape. Nonetheless, they were incredibly realistic mimics of a two-week-old human embryo, including cells destined to form the placenta.&nbsp;



These sorts of models arenâ€™t yet the same as embryos. Itâ€™s rare that they form correctlyâ€”it takes a hundred tries to make oneâ€”and they skip past normal steps before popping into existence. Yet to scientists like the French biologist Denis Duboule, Hannaâ€™s creations are â€œentirely astonishing and very disturbing.â€ Soon, Duboule expects, it could be difficult to distinguish between a real human embryoâ€”the kind with legal protectionsâ€”and one conjured from stem cells.&nbsp;



Hanna is the vanguard of a wider movement thatâ€™s fusing advanced methods in genetics, stem-cell biology, and still-Â­primitive artificial wombs to create bodies where theyâ€™ve never grown beforeâ€”outside the uterus. Joining the chase are researchers at Caltech, the University of Cambridge, and Rockefeller in New York, as well as a growing cadre of startup companies with commercial aims. Thereâ€™s Renewal Bio, a startup Hanna cofounded, which hopes to grow synthetic embryos as a source of youthful replacement cells, such as bits of liver or even eggs. In Europe, Dawn Bio has started placing a type of embryo model called a blastoid on uterine tissue. That will light up a pregnancy test and could, the company thinks, provide new insights into IVF medicine. Patent offices in the US and Europe are seeing a flood of claims as universities grasp for exclusive commercial control over these new types of beings.&nbsp;



Jacob Hanna leads a team at the Weizmann Institute of Science in Rehovot, Israel, that is studying how to create embryos without using sperm, eggs, or fertilization. Heâ€™s cofounded a startup company, Renewal Bio, that has plans to use these synthetic embryo models as bioprinters to produce youthful tissue, but ethical questions surround the project.AHMAD GHARABLI/GETTY IMAGES




Hanna declined a request to discuss his research for this story. But for the last three years, MIT Technology Review has followed Hanna across online presentations, lecture halls, and two in-person ethics meetings, both organized by the Global Observatory for Genome Editing, a public consultation project where he agreed to engage with religious scholars, bioethicists, and other experts. What emerged is a remarkable picture of a scientist working at a Nobel Prize level but whose research, though approved by his institution, raises serious long-term ethical questions.



Exactly how far Hanna has taken his models of the human embryo is an open question. According to public comments from Renewal Bio, the answer is at least 28 days. But itâ€™s possibly further. One scientist in contact with the company said he thought theyâ€™d reached close to day 40, a point where you would see the beginning of eyes and budding limbs. Renewal did not respond to a request for comment.



But even if he hasnâ€™t gotten that far yet, Hanna intends to. His team is â€œtrying to make entities at more advanced stagesâ€”depending on the goal, it could be day 30 in development, day 40, or day 70,â€ he told an audience last May in Cambridge, Massachusetts, where heâ€™d traveled to join a panel discussion involving religious scholars and social scientists at the Global Observatoryâ€™s annual summit. The more advanced versions would be similar in size and development to a fetus in the third month of pregnancy.&nbsp;





O. Carter Snead, a bioethicist from the University of Notre Dame who led the panel featuring Hanna, approached me afterward to ask if Iâ€™d heard what the scientist had said. Snead was surprised that Hanna had so frankly disclosed his goals and that no one had objected, or maybe even grasped what it meant. Perhaps, Snead thinks, this technology wonâ€™t sink in until people can see it with their own eyes. â€œIf you had one of these spinning bottles with something that looked like a human fetus inside it, I think youâ€™d get peopleâ€™s attention,â€ he says. â€œThatâ€™s going to be like, whoaâ€”what are we doing?â€



Snead, a Catholic who sits on a panel that advises the Vatican, also was not comforted by Hannaâ€™s plan to make sure his models, if they advance to later stages of development, will pass ethical scrutiny. That plan involves blocking the formation of the head, brain, or perhaps heart of the synthetic structures, by means including genetic modification. If thereâ€™s no brain, Hannaâ€™s reasoning goes, thereâ€™s no awareness, no person, and no foul. Just a clump of organs.



Snead says thatâ€™s not the same standard of humanity he knows, which treats all humans the same, regardless of their intellectual capacity or anything else. â€œWhat is considered human? Who is considered human?â€ wonders Snead. â€œItâ€™s whoâ€™s in and whoâ€™s out. There is a dramatic consequence of being in versus out of the boundaries of humanity.â€



The beginnings of bodies



Each of usâ€”me, you the reader, and Jacob Hannaâ€”started as a fertilized egg, a single cell thatâ€™s able to divide and dynamically carry out a program to build a complete body with all its organs and billions of specialized cells. Science has long sought ways to seize on that dramatic potential. A first step came in the 1990s, when scientists were able to isolate powerful stem cells from five-day-old embryos created through in vitro fertilizationâ€”and keep them growing in their labs. These embryonic stem cells had the inherent potential to become any other type of cell. If they could be directed in the lab to form, for example, neurons or the insulin-making cells that diabetics need, that would open up a way to treat disease using cell transplants.&nbsp;



A side-by-side comparison of synthetic (left) and natural (right) mouse embryos shows similar formation of the brain and heart.AMADEI AND HANDFORD/UNIVERSITY OF CAMBRIDGE




But these lab recipes are often unsuccessful, which explains the general lack of new stem-cell treatments. â€œThe sad truth is that over 25 years that weâ€™ve been working on this problem, there are about 10 cell types you make that have reasonable function,â€ says Chad Cowan, chief scientific officer of the stem-cell company Century Therapeutics. If we think of the body as a car, he explains, â€œweâ€™ve got only spark plugs. We maybe have some tires.â€ The bodyâ€™s most potent blood-forming cells in particular â€œnever appear,â€ according to Cowan, even though biotech companies have spent millions trying to make them.




Hannaâ€™s startup plans to use synthetic embryos as a kind of â€œbioprinter,â€ producing medically valuable cells in cases where other methods have failed.




It turns out, though, that stem cells retain a natural urge to work together. Scientists began to notice that, when left alone, the cells would join into blobs, tubes, and cavitiesâ€”some of which resembled parts of an embryo.&nbsp;



Early versions of these structures were crude, even just a swirling film of cells on a glass slide. But each year, they have grown more realistic. By 2023, Hanna was describing what he called a â€œbona fideâ€ human embryo model that was â€œfully integrated,â€ with all the major parts arranged in an architecture that was hard to distinguish from the real thing.&nbsp;



His company, Renewal, plans to use these synthetic embryos as a kind of â€œbioprinter,â€ producing medically valuable cells in cases where other methods have failed. This could be particularly valuable if the synthetic embryos are a perfect match with a patientâ€™s DNA. And thatâ€™s possible too: These days reprogramming anyoneâ€™s skin cells into stem cells is easily done. Hanna has tried it on himself, transforming his own cells into synthetic embryos.&nbsp;



Hannaâ€™s research, and that of other groups, has at times collided with a powerful scientific body called the International Society for Stem Cell Research, or ISSCR, a self-governance organization that sets boundaries about what research can and canâ€™t be published and what terminology to use. Thatâ€™s to shield scientists from sensational headlines, public backlash, or the reach of actual regulators.&nbsp;



The organization has taken a particularly categorical position on structures made from stem cells, saying they are mere â€œmodels.â€ According to a statement it fired off in 2023, â€œembryo models are neither synthetic nor embryosâ€â€”and, it added, they â€œcannot and will not develop to the equivalent of postnatal stage human.â€&nbsp;



Many scientists, including Hanna, agree no one should ever try to make a stem-cell baby. But he is fairly certain these structures will become more realistic and can grow further. In fact, that may be the real test of what an embryo is: whether it can dynamically keep reaching new stages of development, especially organogenesis, or the first emergence of organs. The language in the ISSCR statement, he complained, was â€œbrainwashing.â€&nbsp;



Replacement parts



Most of the commercial projects involving synthetic embryos are doomed to a short and fitful life as the technology proves too difficult or undeveloped. But the idea isnâ€™t going away. Instead, there are signals itâ€™s getting bigger, and weirder. In an editorial published in March by MIT Technology Review, a group of Stanford scientists put forward a proposal for what they called â€œbodyoids,â€ arguing that stem cells and artificial wombs may lead to an â€œunlimited sourceâ€ of nonsentient human bodies for use in drug research or as organ donors. One of its authors, Henry Greely, among the foremost bioethicists in the US, posted on Bluesky that even though the idea gives him â€œsome creeps,â€ he added his name because he feels it is plausible enough to need discussion, and â€œsoon.â€





Especially in the Bay Area, headless bodies are having a moment. The Stanford biologist Hiro Nakauchi, another â€œbodyoidsâ€ author, said the editorial provided a surprise entrÃ©e for him into a world of stealth startups already pursuing synthetic embryos, artificial wombs, and body-part â€œreplacement.â€ He met the CEO of Hannaâ€™s company, signing on as an advisor. But other teams have still more radical plans. One venture capitalist introduced him to a longevity entrepreneur tinkering with a plan for head transplants. The idea: Swap your aged head onto the body of a younger clone. That company claims to have a facility on a Caribbean island â€œjust like Jurassic Park,â€ Nakauchi says.&nbsp;&nbsp;&nbsp;



These sorts of plansâ€”real or rumoredâ€”have gotten the attention of the stem-cell police, the ISSCR. This June, an ethics committee led by Amander Clark, a fetal specialist at UCLA and a past president of the society, wrote that it had become aware of â€œcommercial and other groups raising the possibility of building an embryo in vitroâ€ and bringing it to viability inside â€œartificial systems.â€ Though the ISSCR had previously decreed that embryo models â€œcannot and will notâ€ develop to term, it now declared efforts aiming at viability â€œunsafe and unethical,â€ placing them in a â€œprohibitedâ€ category. It added that the ban would cover â€œany purpose: reproductive, research, or commercial.â€&nbsp;



Blurred boundaries



Clark and her colleagues are right that, for the foreseeable future, no one is going to decant a full-term baby out of a bottle. Thatâ€™s still science fiction. But thereâ€™s a pressing issue that needs to be dealt with right now. And thatâ€™s what to do about synthetic embryo models that develop just part of the wayâ€”say for a few weeks, or months, as Hanna proposes.&nbsp;



Because right now, hardly any laws or policies apply to synthetic embryos. One reason is their unnatural origin: Because these entities donâ€™t start with conception and grow in labs, most existing laws wonâ€™t cover them. That includes the Fetus Farming Prohibition Act, legislation passed unanimously in 2006 by the US Congress, which sought to prevent anyone from growing a fetus for its organs. But that law references â€œa human pregnancyâ€ and a â€œuterusâ€â€”and there would be neither if a synthetic embryo were grown in a mechanical vessel.&nbsp;





Another policy under pressure is the â€œ14-day rule,â€ a widely employed convention that natural embryos should not be grown longer than two weeks in the lab. Though itâ€™s a mostly arbitrary stopping point, itâ€™s been convenient for laboratory scientists to know where their limit is. But that rule isnâ€™t being applied to the embryo models. For instance, even though the United Kingdom has a 14-day rule enshrined in law, that legislation doesnâ€™t define what an embryo is. To scientists working on models, thatâ€™s a critical loophole. If the structures arenâ€™t considered true embryos, then the rule doesnâ€™t apply. &nbsp;



Last year, the University of Cambridge, in the UK, described the situation as a â€œgrey areaâ€ and said it â€œhas left scientists and research organisations uncertain about the acceptable boundaries of their work, both legally and ethically.â€&nbsp;



Researchers at the university, which is a hot spot for human embryo models, have been working with one that has advanced features, including beating heart cells. But the appearance of distinctive features under their microscopes is unsettlingâ€”even to scientists. â€œI was scared, honestly,â€ Jitesh Neupane, who led that work, told the Guardian in 2023. â€œI had to look down and look back again.â€&nbsp;



That particular stem-cell model isnâ€™t completeâ€”it entirely lacks placenta cells and a brain. So itâ€™s not a real embryo. But it could get ever trickier to insist the models donâ€™t count, given the accelerating race to make them more realistic. To Duboule, scientists are caught in a â€œfoolâ€™s paradoxâ€ and a â€œrather unstable situation.â€



Even incomplete models raise the question of where to draw the line. Should you stop when it can feel pain? When itâ€™s just too human-looking for comfort? Scientific leaders may soon have to decide if there are â€œmorally significantâ€ human featuresâ€”like hands or a faceâ€”that should be avoided, whether the structure has a brain or not. â€œI personally think there should be regulation, and many in the field believe this too,â€ says Alejandro De Los Angeles, a stem-cell biologist affiliated with the University of Central Florida.&nbsp;




â€œI always live in fear that I might find myself embroiled in some kind of a scandal â€¦ Things can shift very quickly for political reasons.â€
Jacob Hanna



Hanna says he has all the necessary approvals in Israel to carry his work forward. But he also worries that the ground rules could change. â€œIâ€™m almost the only one [in Israel] doing these kinds of experiments, and I always live in fear that I might find myself embroiled in some kind of a scandal,â€ he says. â€œThings can shift very quickly for political reasons.â€&nbsp;



And his statements about the situation in Gaza have made him a target. Heâ€™s gotten voicemails wondering why a Weizmann professor is so sympathetic to Palestine, and once when he returned from a trip, someone had tucked an Israeli army beret into the door handle of his car. Last year, he says, political opponents even went after his science by filing a complaint that his research was illegal.



What is clear is that Hanna, who is gregarious and attentive, has worked to cultivate a large group of friends and allies, including religious authoritiesâ€”all part of a campaign to explain the science and hear out other views. He says he got a perfect grade in a bioethics class with a rabbi, conferenced with a priest from his hometown in Galilee, and even paid his respects to an Orthodox professor at a conservative hospital in Jerusalem. â€œIt was unofficial. I didnâ€™t have to get a permit from him,â€ Hanna says. â€œBut â€¦ what does he think? Can I get him on board? Do I get a different opinion?â€&nbsp;



â€œI really do think itâ€™s admirable that he is willing to ask these hard questions about what it is that heâ€™s doing. I think that makes him different,â€ says Snead. â€œBut if you are cynical, you could ask if his focus on the ethical dimension of this is more of a branding exercise.â€ Perhaps, Snead says, itâ€™s a way to market the structures as the â€œgreen, sustainable alternative to embryos.â€



A heartbeat in a jar



To admirers, Hanna is a doctor and researcher â€œheads above the rest,â€ according to Eli Adashi, the former dean of Brown Universityâ€™s medical school. â€œHeâ€™s very unusual, very special, and is making major discoveries that canâ€™t be ignored,â€ Adashi says. â€œHeâ€™s one of those unusually talented people that exceed the capacity of us mortals, and it all emanates from a town in Galilee that no one knows exists.â€





While it is something of a rarity for a Palestinian to rise so high in Israelâ€™s ivory tower, in reality Hanna has an elite backgroundâ€”heâ€™s from a family of MDs, and an uncle, Nabil Hanna, co-developed the first antibody drug for cancer, the blockbuster rituximab.



Since the October 7 attack on Israel by Hamas, Israel has been at war in Gaza, and Hannaâ€™s team has felt the effects. One young scientist dropped his pipette to don an IDF uniform. Another trainee, who is from Gaza, had a brother and other family members struck dead by an Israeli missile that hit near a church where people were sheltering. Then, this June, an Iranian ballistic missile hit the grounds of the Weizmann Institute, shattering windows and walls and sending Hannaâ€™s students scrambling to save research.&nbsp;



Despite delays in his research due to the ongoing conflict, Hannaâ€™s ideas and technologies are being exportedâ€”and emulated. One place to see a version of the artificial womb is at the Janelia Research Campus, in Virginia, where one of Hannaâ€™s former students, Alejandro Aguilera CastrejÃ³n, now operates a lab of his own. Aguilera CastrejÃ³n, for whom science was a ticket out of the poor outskirts of Mexico City, has tattoos from his wrists to his elbows; the newest depicts a hydra, a sea polyp noted for being able to regenerate itself from a few cells.



During a visit in June, Aguilera CastrejÃ³n flipped aside a black cover to reveal the incubator: a metal wheel that slowly turned, gently agitating jars filled with blood serum. Inside one, a mouse embryo driftedâ€”a tiny, translucent shape, curved like a comma. Then, awesomely, a red-colored blob expanded in its center. A heartbeat.&nbsp;



That day, it was a normal mouse embryo in the jarâ€”it had been transferred there to see how far it would grow. Aguilera CastrejÃ³n has the goal of eventually birthing a mouse from an incubator, a process called ectogenesis. But the stem-cell embryos donâ€™t grow as well or as long, he says. The problem isnâ€™t just the challenge of growing them in culture jars. Thereâ€™s probably some kind of fundamental disorganization. They arenâ€™t entirely normalâ€”not yet true embryos.



A rotating bioreactor, developed in Israel, is used to grow synthetic embryos in small jars of blood serum.GETTY IMAGES




Aguilera CastrejÃ³n, who spent eight years at Weizmann contributing to Hannaâ€™s research, is skeptical that the human version of the technology is ready for commercialization. For one thing, itâ€™s inefficient. In every 100 attempts to make a synthetic embryo, the desired structure will form only once or twice. The rest are disorganized blobsâ€”closer to â€œhuevos fritosâ€ than real embryos, he says. â€œI do think the human embryo model will go further, but it could take years,â€ he adds.



In Aguilera CastrejÃ³nâ€™s view, Hanna is well placed to lead that work. One reason is that Israel offers a relatively permissive environmentâ€”and so does Jewish thought. In the Talmud, the embryo is considered â€œmere waterâ€ until the 40th day. Plus, Hanna is already successful. â€œSome people arenâ€™t allowed to do it. And some people want to do it, but they canâ€™t,â€ says Aguilera CastrejÃ³n. â€œJacob wants to make it as realistic as possible and go as far as possibleâ€”that is his aim. Heâ€™s very ambitious and wants to tackle very big things people donâ€™t dare to do. He really wants to do something big. His main aim is always to grow them as far as you can.â€&nbsp;



The first payoff of a technology for mimicking embryos this way is a new view of the unfolding human no one has ever had before. Real human embryos are rarely seen at the early stages, since theyâ€™re inside the wombâ€”and at four or five weeks, many people donâ€™t even know theyâ€™re pregnant. Itâ€™s been a black box. But synthetic models of the embryo can be made in the thousands (depending on the type), studied closely, inspected with modern microscopes, and subjected to dyes and genetic engineering tools, all while theyâ€™re still alive. Add a known toxic chemical that causes birth defects, like thalidomide, and you can closely trace the effects. â€œSince we donâ€™t have a way to peer into the uterus, this allows us to watch things as if they are intrauterine but are not,â€ says Adashi, the former Brown dean and a fertility doctor.&nbsp;



Whatâ€™s more, a synthetic embryo may be able to make cells correctlyâ€”just as a real one doesâ€”and make all types at once, expanding on the limited few that scientists can create from stem cells today. While not all embryonic material is useful to medicine, the blood-forming cells in an embryo are known to be particularly potent. In mice, they can be extracted and multipliedâ€”and if transplanted into a mouse subjected to lethal radiation, they will save it.&nbsp;



Hanna imagines a cancer patient who needs a bone marrow transplant but canâ€™t find a match. Could blood-forming cells be harvested from, say, 100 or 500 embryo-stage clones of that person, providing perfectly matched tissue?&nbsp;



In his cost-benefit analysis, he believes the chance to save lives outweighs the moral risk of growing embryo models for a month, which is about how long it takes for key blood cells to form. At that stage, says Hanna, he thinks â€œthere is still no personification of the embryoâ€ and itâ€™s permissible to use them in research.



Young everything



Hanna cofounded Renewal in 2022 with Omri Amirav-Drory, a venture capitalist whose fund, NFX, raised about $9 million for the company and purchased rights to Weizmann patents. The startupâ€™s idea is to create synthetic embryos from the cells of patients, allowing them to grow for weeks or months to produce what Amirav-Drory calls â€œperfect cellsâ€ for transplant. That is because the synthetic structure, as a clone, would contain â€œyoung, genetically identical everything.â€



Speaking at an event for tech futurists last year near San Francisco, Amirav-Drory flashed a picture of pregnancy tests used on the synthetic embryos. â€œWe even went to CVS,â€ he said, â€œand by day eight itâ€™s already triggering a pregnancy test. So itâ€™s alive.â€ &nbsp;



Amirav-Drory is a fan of Peter F. Hamilton, the science fiction author whose Commonwealth series features a society where space colonists transfer their minds into cloned bodies, attaining second lives. And heâ€™s pitched Hannaâ€™s technology along related lines, as a new type of longevity medicine based on replacing old cells with young ones. He is convinced Hannaâ€™s work is â€œmagicâ€ thatâ€™s sure to win a Nobel.




â€œThe importance of getting rid of the head is all ethical. It just means we can make all these bodies and organ structures without having to cross ethical lines or harm sentient living beings.â€
Carsten Charlesworth, researcher, Stanford University



But he knows the startup has both technical and ethical challenges. The technical challenge is that once the synthetic embryos reach a certain size and age, the incubator canâ€™t support them any longer. Thatâ€™s because they lack a blood supply and need to absorb oxygen and nutrients from their surroundings; they starve once they get too big. One idea being considered is to add a feeding tube, but that involves microsurgery and isnâ€™t easily scalable. The ethical issue is also age related: The more developed they become, the more they will be recognizably human, with the beginnings of organs and small, webbed fingers and toes. â€œNo one has a problem with day 14, but the further we go, the further it looks like a baby, and we get into trouble. So how do we solve that?â€ Amirav-Drory asked a different audience, in Menlo Park.



The solution, so far, is a neural knockoutâ€”genetic changes made to the embryoids so they donâ€™t develop a brain. The group has already tried out the concept on mice, removing a gene called LIM-1. That yielded a headless mouse, which looks a bit like a pink thumb, except with little claws and a tail. Those mice wonâ€™t live after birth, but they can develop in the womb. â€œWe got synthetic mouse embryos growing with no head, with no brain,â€ Amirav-Drory said in Menlo Park. â€œItâ€™s just to show you where we can go to solve both technical and ethical issues.â€&nbsp;



The idea of brain removal is a surprisingly active area of researchâ€”suggesting that itâ€™s no sideshow. Working with mice, for example, Nakauchiâ€™s team at Stanford is currently testing several different genetic changes to see if they can consistently yield an animal with no brain or head, but whose other tissues are normal. â€œThe importance of getting rid of the head is all ethical. It just means we can make all these bodies and organ structures without having to cross ethical lines or harm sentient living beings,â€ says Carsten Charlesworth, a researcher in Nakauchiâ€™s lab. He says the group is working toward a â€œgenetic software packageâ€ it can add to mouse embryos to create a â€œreproducible phenotype.â€







It may seem surprising that a technique designed to call forth a living being from stem cells is, simultaneously, being paired with a tactic to diminish that being. To Douglas Kysar, a professor at Yale Law School, thatâ€™s part of a broader trend toward what he calls â€œlife that is not life,â€ which includes innovations like lab-grown meat. In the areas of animal-rights law Kysar studies, commercial biotech projects have begun to explore what he terms â€œdisenhancementâ€ and â€œdisengineering.â€ That is the use of genetics to reduce the capacity of animals to suffer, feel pain, or have conscious experience at all, typically as part of a program to increase the efficiency and ethics of food production.&nbsp;



For humans, of course, the worry around genetic engineering is usually that it will be used for enhancementâ€”creating a baby with advantages. Itâ€™s much harder to think of examples where genetic disenhancements get pointed at the human embryo. John Evans, who co-directs the Institute of Applied Ethics at the University of California, San Diego, told me he can think of one, in literature. Hannaâ€™s plans remind him of Bokanovskyâ€™s Process, the fictional method of producing clones of different intelligence levels in the 1932 novel Brave New World.





That may not be a complete turnoff to investors. Lately, the plots of science fiction dystopiasâ€”Jurassic Park, Gattacaâ€”seem to be getting repurposed at hot biotech properties. Thereâ€™s Colossal, the company that wants to re-create extinct animals. Aguilera CastrejÃ³n says heâ€™s already had a high-dollar offer to pack up his academic lab and join a startup company that wants to build an artificial womb. And when Hanna was at the Global Observatory meeting near Boston â€‹earlier this year, he was being shadowed by Matt Krisiloff, CEO of the Silicon Valley company Conception, which was set up to try to manufacture human eggs in the lab and has funding from OpenAI leader Sam Altman.



Eggs are another cell type that has proved difficult to generate from a stem cell in the lab. But a growing fetus will&nbsp; form millions of immature egg cells. So just imagine: Someone too old to conceive gives some blood, which is converted into stem cells and then into a clone, from which the fetal gonad is dissected. Maybe the reproductive cells found there could be matured further in the lab. Or maybe those young and perfectly matched ovariesâ€”her ovaries, really, not anyone elseâ€™sâ€”could be returned to her body to finish developing. A fertility expert, David Albertini, told me it might just be possible.



During the ethics meeting he traveled to the US in May to attend, Hanna participated on a panel whose topic was â€œsources of moral authority.â€ Hannaâ€™s authority comes from the possible benefits the science of synthetic embryos may bring. But he also wields his moral credibility. Early in his remarks, Hanna had framed the whole matter in a way that made worrying about whatâ€™s in the petri dish start to sound silly. Wearing a keffiyeh around his shoulders, he said: â€œIâ€™d like to start and, you know, just remind everyone, unfortunately, that there is a genocide ongoing right now in Gaza, where children are being starved intentionally. And it is relevant, because weâ€™re sitting here and weâ€™re discussing human dignity, weâ€™re discussing the status of an embryo, and weâ€™re discussing the status of a fetus. But what about the life of the children, and adults, and innocent adults? How does it relate?â€
â€¢ Why AI should be able to â€œhang upâ€ on you
  Chatbots today are everything machines. If it can be put into wordsâ€”relationship advice, work documents, codeâ€”AI will produce it, however imperfectly. But the one thing that almost no chatbot will ever do is stop talking to you.&nbsp;



That might seem reasonable. Why should a tech company build a feature that reduces the time people spend using its product?&nbsp;&nbsp;



The answer is simple: AIâ€™s ability to generate endless streams of humanlike, authoritative, and helpful text can facilitate delusional spirals, worsen mental-health crises, and otherwise harm vulnerable people. Cutting off interactions with those who show signs of problematic chatbot use could serve as a powerful safety tool (among others), and the blanket refusal of tech companies to use it is increasingly untenable.



Letâ€™s consider, for example, whatâ€™s been called AI psychosis, where AI models amplify delusional thinking. A team led by psychiatrists at Kingâ€™s College London recently analyzed more than a dozen such cases reported this year. In conversations with chatbots, peopleâ€”including some with no history of psychiatric issuesâ€”became convinced that imaginary AI characters were real or that they had been chosen by AI as a messiah. Some stopped taking prescribed medications, made threats, and ended consultations with mental-health professionals.





In many of these cases, it seems AI models were reinforcing, and potentially even creating, delusions with a frequency and intimacy that people do not experience in real life or through other digital platforms.



The three-quarters of US teens who have used AI for companionship also face risks. Early research suggests that longer conversations might correlate with loneliness. Further, AI chats â€œcan tend toward overly agreeable or even sycophantic interactions, which can be at odds with best mental-health practices,â€ says Michael Heinz, an assistant professor of psychiatry at Dartmouthâ€™s Geisel School of Medicine.



Letâ€™s be clear: Putting a stop to such open-ended interactions would not be a cure-all. â€œIf there is a dependency or extreme bond that itâ€™s created,â€ says Giada Pistilli, chief ethicist at the AI platform Hugging Face, â€œthen it can also be dangerous to just stop the conversation.â€ Indeed, when OpenAI discontinued an older model in August, it left users grieving. Some hang ups might also push the boundaries of the principle, voiced by Sam Altman, to â€œtreat adult users like adultsâ€ and err on the side of allowing rather than ending conversations.



Currently, AI companies prefer to redirect potentially harmful conversations, perhaps by having chatbots decline to talk about certain topics or suggest that people seek help. But these redirections are easily bypassed, if they even happen at all.



When 16-year-old Adam Raine discussed his suicidal thoughts with ChatGPT, for example, the model did direct him to crisis resources. But it also discouraged him from talking with his mom, spent upwards of four hours per day in conversations with him that featured suicide as a regular theme, and provided feedback about the noose he ultimately used to hang himself, according to the lawsuit Raineâ€™s parents have filed against OpenAI. (ChatGPT recently added parental controls in response.)



There are multiple points in Raineâ€™s tragic case where the chatbot could have terminated the conversation. But given the risks of making things worse, how will companies know when cutting someone off is best? Perhaps itâ€™s when an AI model is encouraging a user to shun real-life relationships, Pistilli says, or when it detects delusional themes. Companies would also need to figure out how long to block users from their conversations.



Writing the rules wonâ€™t be easy, but with companies facing rising pressure, itâ€™s time to try. In September, Californiaâ€™s legislature passed a law requiring more interventions by AI companies in chats with kids, and the Federal Trade Commission is investigating whether leading companionship bots pursue engagement at the expense of safety.Â 



A spokesperson for OpenAI told me the company has heard from experts that continued dialogue might be better than cutting off conversations, but that it does remind users to take breaks during long sessions.&nbsp;



Only Anthropic has built a tool that lets its models end conversations completely. But itâ€™s for cases where users supposedly â€œharmâ€ the modelâ€”Anthropic has explored whether AI models are conscious and therefore can sufferâ€”by sending abusive messages. The company does not have plans to deploy this to protect people.Looking at this landscape, itâ€™s hard not to conclude that AI companies arenâ€™t doing enough. Sure, deciding when a conversation should end is complicated. But letting thatâ€”or, worse, the shameless pursuit of engagement at all costsâ€”allow them to go on forever is not just negligence. Itâ€™s a choice.
â€¢ Fold your own tessellation
  Download the pattern for Dancing Ribbons here.





Yoder recommends printing the pattern on paper in between normal printer paper and cardstock in weight, making sure it folds in straight lines (not too thick), folds back and forth easily on the same line (not too thin), and is crisp enough to make a satisfying snapping noise when you shake it. Her favorite paper isSkytone, which is commonly used to print certificates and fancy envelopes.



Watch the video tutorial on folding Dancing Ribbons here.



Yoderâ€™s detailed folding instructions:



Once you have your crease pattern on a sheet of paper, cut out the hexagon that contains the pattern. Yoder recommends using a straightedge and blade on a cutting mat instead of scissors, whether that means an X-Acto knife and a ruler on a sheet of cardboard or a quilting ruler and rotary cutter on a fabric cutting mat.



The next step is folding the background grid of black lines that the pattern uses as references. Assuming youâ€™ve cut out your hexagon precisely, you can use the edge of the hexagon and the printed lines to make your creases, or you can fold as if there were no lines printed by folding the hexagon in half (edge to opposite edge) and then folding those edges in to the center to make quarter lines, first in one direction and then in the other two. After each set of folds, itâ€™s a good idea to fold the new lines back the other way to make the paper easier to work with later. After folding the quarters, fold the eighths in each direction, and finally the 16ths. Yoder presses the creases with a bone folder to make them easier to work with and to minimize stress on her hands.




You can choose at this point whether to fold the pattern one twist at a time or to precrease the off-grid creases (just crease the short segments that have been printed, folded as mountains on the printed side of the pattern) and collapse everything all at once. Beginning folders may find it helpful to precrease the triangle and rhombus twists, to make the squashing process easier, even if you plan to fold the pattern one twist at a time. Solid red lines in the crease pattern represent mountain folds, and dashed blue lines represent valley folds. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.



The central closed hexagon twist will be the first twist folded, and itâ€™ll be made on the blank side of the paper. All the mountain folds for this twist (as viewed on the blank side of the paper) will be on grid lines going to the corners of the hexagon, and the valley folds will be one grid spacing above the mountains on the right-hand side of the paper. To fold the twist, set up both the mountain and valley folds of one pleat; then pass that pleat counterclockwise into your other hand before setting up both folds of the next pleat. Keep all pleats folded and the center of the paper elevated as you work your way around the center, eventually folding all six pleats (use your table to keep the pleats folded, or use clips at the edge of the paper) and forming a hexagonal tower in the center of the paper. Make the pleats more flat, working from the edges in, until this hexagon tower is two grid spacings high. Then grab the tower and give it a sharp counterclockwise twist to get it to lie flat. This twist almost never lies down completely flat right away, so lift each pleat slightly to make sure the valley folds have stayed on grid lines to help the central hexagon to smooth out.



Once the hexagon has been folded, flip the paper over to the printed side. Take the mountain fold of one pleat and split it into a three-way intersection of mountain folds evenly spaced around a point two grid spacings out from the closed hexagon hole. This point is the center of the closed triangle twist, which can be squashed to create the triangle of off-grid creases once the two new pleats are folded over in a clockwise direction (as printed). To squash the triangle twist, press gently on each of the three pleats just outside the point where the valley fold of one pleat contacts the mountain fold of the next pleat. This will start to flatten the central triangle, which can then be pressed from the top to smooth it out and finalize the new creases.



Fold each of the triangle twists in the same way (causing pleats to overlap with pleats from other triangles), in a counterclockwise order around the central hexagon (this order makes the overlapping pleats easier to work with later).



Once the triangles have all been folded, find a place where two pleats from triangle twists are overlapping and open up the overlap so you can see all the parts of the paper (leaving the triangle twists folded). Use the printed folds to set up a rhombus twist, and then press the twist flat from the top once all the folds in the pleats are set up.



Repeat this step with all six of the pleat overlaps (if you followed the recommended sequence for the triangles, only one overlap will be in a different order from the rest) to complete the pattern.
â€¢ The Download: a promising retina implant, and how climate change affects flowers
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This retina implant lets people with vision loss do a crossword puzzle



The news: Science Corporationâ€”a competitor to Neuralink founded by the former president of Elon Musk&#8217;s brain-interface ventureâ€”has leapfrogged its rival after acquiring a vision implant in advanced testing for a fire-sale price. The implant produces a form of â€œartificial visionâ€ that lets some patients read text and do crosswords, according to a report published in The New England Journal of Medicine today.How it works: The implant is a microelectronic chip placed under the retina. Using signals from a camera mounted on a pair of glasses, the chip emits bursts of electricity in order to bypass photoreceptor cells damaged by macular degeneration, the leading cause of vision loss in the elderly. Read the full story.



â€”Antonio Regalado







How will flowers respond to climate change?



Flowers play a key role in most landscapes, from urban to rural areas. Yet flowers have much more to tell in their bright blooms: The very shape they take is formed by local and global climate conditions.&nbsp;



The form of a flower is a visual display of its climate, if you know what to look for. In a dry year, its petalsâ€™ pigmentation may change. In a warm year, the flower might grow bigger. The flowerâ€™s ultraviolet-absorbing pigment increases with higher ozone levels.



Now, a new artistic project sets out to answer the question: As the climate changes in the future, how might flowers change? Read the full story.



â€”Annelie Berner



This story is from our forthcoming print issue, which is all about the body. If you havenâ€™t already, subscribe now to receive future issues once they land.







2025 climate tech companies to watch: Redwood Materials and its new AI microgrids



Over the past few years, Redwood Materials has become one of the top US battery recyclers, joining forces with the likes of Volkswagen, BMW, and Toyota to process old electric-vehicle batteries and recover materials that can be used to make new ones.Now it&#8217;s moving into reuse as well. Redwood Energy, a new branch of the company, incorporates used EV batteries into microgrids to power energy-hungry AI data centers. Read the full story.



â€”Peter Hall



Redwood Materials is one of our 10 climate tech companies to watchâ€”our annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 AWS is recovering from a major outageÂ Itâ€™s racing to get hundreds of apps and services back online. (The Verge)+ Snapchat, Roblox and banking services are among those affected. (The Guardian)2 OpenAI madeâ€”then retractedâ€”a claim it had made a major math breakthroughAfter math experts and rival AI firms ridiculed its poorly-worded declaration. (TechCrunch)+ Whatâ€™s next for AI and math. (MIT Technology Review)



3 The grave costs of Trumpâ€™s war on climate scienceItâ€™s affecting the accuracy of forecasting systems globally, not just in the US. (FT $)+ Trump himself led an effort to derail plans to tax shipping pollution. (Politico $)+ How to make clean energy progress under Trump in the states. (MIT Technology Review)



4 China claims the US is behind a cyberattack on its national time centerIt says it has yearsâ€™ worth of irrefutable evidence of data stealing. (Reuters)+ US experts allegedly exploited vulnerabilities in mobile phones belonging to National Time Service Center workers. (Bloomberg $)5 Is AI-generated art real art?Itâ€™s a question gallery and museum curators across the world are debating. (NYT $)+ Artisan craftmakers are happy to resist the pull of AI. (FT $)+ This tool claims to trace how much of an AI image has been drawn from existing material. (The Guardian)+ From slop to Sothebyâ€™s? AI art enters a new phase. (MIT Technology Review)



6 Chipmaker Nexperia has accused its ousted CEO of spreading falsehoodsZhang Xuezheng reportedly claimed it was operating independently in China. (Bloomberg $)



7 This whistleblower raised concerns about the safety of US data under DOGEAnd says the hostile reception to his complaint led to him leaving his dream job. (WP $)+ DOGEâ€™s tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



8 Aid agencies have been criticized for using AI â€œpoverty pornâ€But the NGOs say its use protects the identities of real people in social media campaigns. (The Guardian)



9 EVs lose their value much faster than gas-powered carsWhich isnâ€™t exactly an incentive for prospective first-time buyers. (Rest of World)



10 What happens to our brains when we dream Weâ€™re learning more about the many liminal states they can slip through. (Quanta Magazine)







Quote of the day



â€œHoisted by their own GPTards.â€



â€”Metaâ€™s chief AI scientist Yann LeCun pokes fun at OpenAI after the company walked back its claim it had made a major math breakthrough in a post on X.







One more thing







One option for electric vehicle fires? Let them burn.Although there isnâ€™t solid data on the frequency of EV battery fires, itâ€™s no secret that these fires are happening.Despite that, manufacturers offer no standardized steps on how to fight them or avoid them in the first place. Whatâ€™s more, with EVs, itâ€™s never entirely clear whether the fire is truly out.Patrick Durham, the owner of one of a growing number of private companies helping first responders learn how to deal with lithium-ion battery safety, has a solution. He believes that the best way to manage EV fires right now is to let them burn. But such an approach not only goes against firefightersâ€™ instinctsâ€”itâ€™d require a significant cultural shift. Read the full story.



â€”Maya L. Kapoor







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ It looks as though the sumo wrestlers who visited London last week had the best time.+ The Chicago rat hole may not have been made by a rat after all.+ Finally, a good use for AIâ€”to help me pick a perfectly ripe avocado + Keith Richards, we love you!

ğŸ”’ Cybersecurity & Privacy
â€¢ Email Bombs Exploit Lax Authentication in Zendesk
  Cybercriminals are abusing a widespread lack of authentication in the customer service platform Zendesk to flood targeted email inboxes with menacing messages . The Washington Post, Tinder, CapCom, CompTIA, Discord, GMAC, NordVPN and NordVPN are among the victims of the abuse . The abusive missives can include any subject line chosen by the abusers, such as a supposed law enforcement investigation involving KrebsOnSecurity.com .

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ How TP ICAP transformed CRM data into real-time insights with Amazon Bedrock
  This post is co-written with Ross Ashworth at TP ICAP. 
The ability to quickly extract insights from customer relationship management systems (CRMs) and vast amounts of meeting notes can mean the difference between seizing opportunities and missing them entirely. TP ICAP faced this challenge, having thousands of vendor meeting records stored in their CRM. Using Amazon Bedrock, their Innovation Lab built a production-ready solution that transforms hours of manual analysis into seconds by providing AI-powered insights, using a combination of Retrieval Augmented Generation (RAG) and text-to-SQL approaches. 
This post shows how TP ICAP used Amazon Bedrock Knowledge Bases and Amazon Bedrock Evaluations to build ClientIQ, an enterprise-grade solution with enhanced security features for extracting CRM insights using AI, delivering immediate business value. 
The challenge 
TP ICAP had accumulated tens of thousands of vendor meeting notes in their CRM system over many years. These notes contained rich, qualitative information and details about product offerings, integration discussions, relationship insights, and strategic direction. However, this data was being underutilized and business users were spending hours manually searching through records, knowing the information existed but unable to efficiently locate it. The TP ICAP Innovation Lab set out to make the information more accessible, actionable, and quickly summarized for their internal stakeholders. Their solution needed to surface relevant information quickly, be accurate, and maintain proper context. 
ClientIQ: TP ICAPâ€™s custom CRM assistant 
With ClientIQ, users can interact with their Salesforce meeting data through natural language queries. For example: 
 
 Ask questions about meeting data in plain English, such as â€œHow can we improve our relationship with customers?â€, â€œWhat do our clients think about our solution?â€, or â€œHow were our clients impacted by Brexit?â€ 
 Refine their queries through follow-up questions. 
 Apply filters to restrict model answers to a particular time period. 
 Access source documents directly through links to specific Salesforce records. 
 
ClientIQ provides comprehensive responses while maintaining full traceability by including references to the source data and direct links to the original Salesforce records. The conversational interface supports natural dialogue flow, so users can refine and explore their queries without starting over. The following screenshot shows an example interaction (examples in this post use fictitious data and AnyCompany, a fictitious company, for demonstration purposes). 
 
ClientIQ performs multiple tasks to fulfill a userâ€™s request: 
 
 It uses a large language model (LLM) to analyze each user query to determine the optimal processing path. 
 It routes requests to one of two workflows: 
   
   The RAG workflow for getting insights from unstructured meeting notes. For example, â€œWas topic A discussed with AnyCompany the last 14 days?â€ 
   The SQL generation workflow for answering analytical queries by querying structured data. For example, â€œGet me a report on meeting count per region for last 4 weeks.â€ 
    
 It then generates the responses in natural language. 
 ClientIQ respects existing permission boundaries and access controls, helping verify users only access the data theyâ€™re authorized to. For example, if a user only has access to their regional accounts in the CRM system, ClientIQ only returns information from these accounts. 
 
Solution overview 
Although the team considered using their CRMâ€™s built-in AI assistant, they opted to develop a more customized, cost-effective solution that would precisely match their requirements. They partnered with AWS and built an enterprise-grade solution powered by Amazon Bedrock. With Amazon Bedrock, TP ICAP evaluated and selected the best models for their use case and built a production-ready RAG solution in weeks rather than months, without having to manage the underlying infrastructure. They specifically used the following Amazon Bedrock managed capabilities: 
 
 Amazon Bedrock foundation models â€“ Amazon Bedrock provides a range of foundation models (FMs) from providers, including Anthropic, Meta, Mistral AI, and Amazon, accessible through a single API. TP ICAP experimented with different models for various tasks and selected the best model for each task, balancing latency, performance, and cost. For instance, they used Anthropicâ€™s Claude 3.5 Sonnet for classification tasks and Amazon Nova Pro for text-to-SQL generation. Because Amazon Bedrock is fully managed, they didnâ€™t need to spend time setting up infrastructure for hosting these models, reducing the time to delivery. 
 Amazon Bedrock Knowledge Bases â€“ The FMs needed access to the information in TP ICAPâ€™s Salesforce system to provide accurate, relevant responses. TP ICAP used Amazon Bedrock Knowledge Bases to implement RAG, a technique that enhances generative AI responses by incorporating relevant data from your organizationâ€™s knowledge sources. Amazon Bedrock Knowledge Bases is a fully managed RAG capability with built-in session context management and source attribution. The final implementation delivers precise, contextually relevant responses while maintaining traceability to source documents. 
 Amazon Bedrock Evaluations â€“ For consistent quality and performance, the team wanted to implement automated evaluations. By using Amazon Bedrock Evaluations and the RAG evaluation tool for Amazon Bedrock Knowledge Bases in their development environment and CI/CD pipeline, they were able to evaluate and compare FMs with human-like quality. They evaluated different dimensions, including response accuracy, relevance, and completeness, and quality of RAG retrieval. 
 
Since launch, their approach scales efficiently to analyze thousands of responses and facilitates data-driven decision-making about model and inference parameter selection, and RAG configuration.The following diagram showcases the architecture of the solution. 
 
The user query workflow consists of the following steps: 
 
 The user logs in through a frontend React application, hosted in an Amazon Simple Storage Service (Amazon S3) bucket and accessible only within the organizationâ€™s network through an internal-only Application Load Balancer. 
 After logging in, a WebSocket connection is opened between the client and Amazon API Gateway to enable real-time, bi-directional communication. 
 After the connection is established, an AWS Lambda function (connection handler) is invoked, which process the payload, logs tracking data to Amazon DynamoDB, and publishes request data to an Amazon Simple Notification Service (Amazon SNS) topic for downstream processing. 
 Lambda functions for different types of tasks consume messages from Amazon Simple Queue Service (Amazon SQS) for scalable and event-driven processing. 
 The Lambda functions use Amazon Bedrock FMs to determine whether a question is best answered by querying structured data in Amazon Athena or by retrieving information from an Amazon Bedrock knowledge base. 
 After processing, the answer is returned to the user in real time using the existing WebSocket connection through API Gateway. 
 
Data ingestion 
ClientIQ needs to be regularly updated with the latest Salesforce data. Rather than using an off-the-shelf option, TP ICAP developed a custom connector to interface with their highly tailored Salesforce implementation and ingest the latest data to Amazon S3. This bespoke approach provided the flexibility needed to handle their specific data structures while remaining simple to configure and maintain. The connector, which employs Salesforce Object Query Language (SOQL) queries to retrieve the data, runs daily and has proven to be fast and reliable. To optimize the quality of the results during the RAG retrieval workflow, TP ICAP opted for a custom chunking approach in their Amazon Bedrock knowledge base. The custom chunking happens as part of the ingestion process, where the connector splits the data into individual CSV files, one per meeting. These files are also automatically tagged with relevant topics from a predefined list, using Amazon Nova Pro, to further increase the quality of the retrieval results. The final outputs in Amazon S3 contain a CSV file per meeting and a matching JSON metadata file containing tags such as date, division, brand, and region. The following is an example of the associated metadata file: 
 
 {
"metadataAttributes": {
   "Tier": "Bronze",
   "Number_Date_of_Visit": 20171130,
   "Author_Region_C": "AMER",
   "Brand_C": "Credit",
   "Division_C": "Credit",
   "Visiting_City_C": "Chicago",
   "Client_Name": "AnyCompanyâ€
   }
} 
 
As soon as the data is available in Amazon S3, an AWS Glue job is triggered to populate the AWS Glue Data Catalog. This is later used by Athena when querying the Amazon S3 data. 
The Amazon Bedrock knowledge base is also synced with Amazon S3. As part of this process, each CSV file is converted into embeddings using Amazon Titan v1 and indexed in the vector store, Amazon OpenSearch Serverless. The metadata is also ingested and available for filtering the vector store results during retrieval, as described in the following section. 
Boosting RAG retrieval quality 
In a RAG query workflow, the first step is to retrieve the documents that are relevant to the userâ€™s query from the vector store and append them to the query as context. Common ways to find the relevant documents include semantic search, keyword search, or a combination of both, referred to as hybrid search. ClientIQ uses hybrid search to first filter documents based on their metadata and then perform semantic search within the filtered results. This pre-filtering provides more control over the retrieved documents and helps disambiguate queries. For example, a question such as â€œfind notes from executive meetings with AnyCompany in Chicagoâ€ can mean meetings with any AnyCompany division that took place in Chicago or meetings with AnyCompanyâ€™s division headquartered in Chicago. 
TP ICAP used the manual metadata filtering capability in Amazon Bedrock Knowledge Bases to implement hybrid search in their vector store, OpenSearch Serverless. With this approach, in the preceding example, the documents are first pre-filtered for â€œChicagoâ€ as Visiting_City_C. After that, a semantic search is performed to find the documents that contain executive meeting notes for AnyCompany. The final output contains notes from meetings in Chicago, which is what is expected in this case. The team enhanced this functionality further by using the implicit metadata filtering of Amazon Bedrock Knowledge Bases. This capability relies on Amazon Bedrock FMs to automatically analyze the query, understand which values can be mapped to metadata fields, and rewrite the query accordingly before performing the retrieval. 
Finally, for additional precision, users can manually specify filters through the application UI, giving them greater control over their search results. This multi-layered filtering approach significantly improves context and final response accuracy while maintaining fast retrieval speeds. 
Security and access control 
To maintain Salesforceâ€™s granular permissions model in the ClientIQ solution, TP ICAP implemented a security framework using Okta group claims mapped to specific divisions and regions. When a user signs in, their group claims are attached to their session. When the user asks a question, these claims are automatically matched against metadata fields in Athena or OpenSearch Serverless, depending on the path followed. 
For example, if a user has access to see information for EMEA only, then the documents are automatically filtered by the EMEA region. In Athena, this is done by automatically adjusting the query to include this filter. In Amazon Bedrock Knowledge Bases, this is done by introducing an additional metadata field filter for region=EMEA in the hybrid search. This is highlighted in the following diagram. 
 
Results that donâ€™t match the userâ€™s permission tags are filtered out, so that users can only access data theyâ€™re authorized to see. This unified security model maintains consistency between Salesforce permissions and ClientIQ access controls, preserving data governance across solutions. 
The team also developed a custom administrative interface for admins that manage permission in Salesforce to add or remove users from groups using Oktaâ€™s APIs. 
Automated evaluation 
The Innovation Lab team faced a common challenge in building their RAG application: how to scientifically measure and improve its performance. To address that, they developed an evaluation strategy using Amazon Bedrock Evaluations that involves three phrases: 
 
 Ground truth creation â€“ They worked closely with stakeholders and testing teams to develop a comprehensive set of 100 representative question answers pairs that mirrored real-world interactions. 
 RAG evaluation â€“ In their development environment, they programmatically triggered RAG evaluations in Amazon Bedrock Evaluations to process the ground truth data in Amazon S3 and run comprehensive assessments. They evaluated different chunking strategies, including default and custom chunking, tested different embedding models for retrieval, and compared FMs for generation using a range of inference parameters. 
 Metric-driven optimization â€“ Amazon Bedrock generates evaluation reports containing metrics, scores, and insights upon completion of an evaluation job. The team tracked content relevance and content coverage for retrieval and quality, and responsible AI metrics such as response relevance, factual accuracy, retrieval precision, and contextual comprehension for generation. They used the evaluation reports to make optimizations until they reached their performance goals. 
 
The following diagram illustrates this approach. 
 
In addition, they integrated RAG evaluation directly into their continuous integration and continuous delivery (CI/CD) pipeline, so every deployment automatically validates that changes donâ€™t degrade response quality. The automated testing approach gives the team confidence to iterate quickly while maintaining consistently high standards for the production solution. 
Business outcomes 
ClientIQ has transformed how TP ICAP extracts value from their CRM data. Following the initial launch with 20 users, the results showed that the solution has driven a 75% reduction in time spent on research tasks. Stakeholders also reported an improvement in insight quality, with more comprehensive and contextual information being surfaced. Building on this success, the TP ICAP Innovation Lab plans to evolve ClientIQ into a more intelligent virtual assistant capable of handling broader, more complex tasks across multiple enterprise systems. Their mission remains consistent: to help technical and non-technical teams across the business to unlock business benefits with generative AI. 
Conclusion 
In this post, we explored how the TP ICAP Innovation Lab team used Amazon Bedrock FMs, Amazon Bedrock Knowledge Bases, and Amazon Bedrock Evaluations to transform thousands of meeting records from an underutilized resource into a valuable asset and accelerate time to insights while maintaining enterprise-grade security and governance. Their success demonstrates that with the right approach, businesses can implement production-ready AI solutions and deliver business value in weeks. To learn more about building similar solutions with Amazon Bedrock, visit the&nbsp;Amazon Bedrock documentation or discover real-world success stories and implementations on the&nbsp;AWS Financial Services Blog. 
 
About the authors 
Ross Ashworth works in TP ICAPâ€™s AI Innovation Lab, where he focuses on enabling the business to harness Generative AI across a range of projects. With over a decade of experience working with AWS technologies, Ross brings deep technical expertise to designing and delivering innovative, practical solutions that drive business value. Outside of work, Ross is a keen cricket fan and former amateur player. He is now a member at The Oval, where he enjoys attending matches with his family, who also share his passion for the sport. 
Anastasia Tzeveleka is a Senior Generative AI/ML Specialist Solutions Architect at AWS. Her experience spans the entire AI lifecycle, from collaborating with organizations training cutting-edge Large Language Models (LLMs) to guiding enterprises in deploying and scaling these models for real-world applications. In her spare time, she explores new worlds through fiction.
â€¢ Principal Financial Group accelerates build, test, and deployment of Amazon Lex V2 bots through automation
  This guest post was written by Mulay Ahmed and Caroline Lima-Lane of Principal Financial Group. The content and opinions in this post are those of the third-party authors and AWS is not responsible for the content or accuracy of this post. 
With US contact centers that handle millions of customer calls annually, Principal Financial GroupÂ® wanted to modernize their customer call experience. In the post Principal Financial Group increases Voice Virtual Assistant performance using Genesys, Amazon Lex, and Amazon QuickSight, we discussed the overall Principal Virtual Assistant solution using Genesys Cloud, Amazon Lex V2, multiple AWS services, and a custom reporting and analytics solution using Amazon QuickSight. 
This post focuses on the acceleration of the Virtual Assistant (VA) platform delivery processes through automated build, testing, and deployment of an Amazon Lex V2 bot (including other database and analytics resources described later in this post) using a GitHub continuous integration and delivery (CI/CD) pipeline with automated execution of the Amazon Lex V2 Test Workbench for quality assurance. This solution helps PrincipalÂ® scale and maintain VA implementations with confidence and speed using infrastructure as code (IaC), configuration as code (CaC,) and an automated CI/CD approach instead of testing and deploying the Amazon Lex V2 bot on the AWS Management Console. 
Principal is a global financial company with nearly 20,000 employees passionate about improving the wealth and well-being of people and businesses. In business for 145 years, Principal is helping approximately 70 million customers (as of Q4 2024) plan, protect, invest, and retire, while working to support the communities where it does business.The enterprise virtual assistant engineering team at Principal, in collaboration with AWS, used Amazon Lex V2 to implement a voice virtual assistant to provide self-service and routing capabilities for contact center customers. The following engineering opportunities were recognized and prioritized: 
 
 Elimination of console-driven configuration, testing, and deployment of an Amazon Lex V2 bot 
 Collaboration through structured version control and parallel development workflows for multiple team members 
 Acceleration of development cycles with automated build, test, and deployment processes for Amazon Lex bot creation and optimization 
 Enhanced quality assurance controls through automated testing gates and coding standard validation for reliable releases 
 
With the automation solutions described in the post, as of September 2024, Principal has accelerated development efforts by 50% across all environments (development, pilot, and production) through streamlined implementation and deployment processes. This solution also enhances deployment reliability through automated workflows, providing consistent updates while minimizing errors across development, pilot, and production environments, and maximizes development efficiency by integrating the Test Workbench with GitHub, enabling version control and automated testing.With the automation of the Test Workbench and its integration with GitHub, the solution strengthens the CI/CD pipeline by maintaining alignment between test files and bot versions, creating a more agile and reliable development process. 
Solution overview 
The solution uses the services described in Principal Financial Group increases Voice Virtual Assistant performance using Genesys, Amazon Lex, and Amazon QuickSight. The following services/APIs are also used as part of this solution: 
 
 AWS Step Functions to orchestrate the deployment workflow 
 The Test Workbench APIs, which are invoked within the Step Functions state machine as a sequence of tasks 
 AWS Lambda to process data to support some of the Test Workbench APIs inputs 
 
VA code organization and management 
The Principal VA implementation uses Genesys Cloud as the contact center application and the following AWS services organized as different stacks: 
 
 Bot stack: 
   
   The Amazon Lex V2 CDK is used for defining and deploying the bot infrastructure 
   Lambda functions handle the bot logic and manage routing logic (for Amazon Lex and Genesys Cloud) 
   AWS Secrets Manager stores secrets for calling downstream systems endpoints 
    
 Testing stack: 
   
   Step Functions orchestrates the testing workflow 
   Lambda functions are used in the testing process 
   Test files contains test cases and scenarios in Test Workbench format 
   Simulated data is used to simulate various scenarios for testing without connecting to downstream systems or APIs 
    
 Data stack: 
   
   Amazon Dynamo DB manages and stores bot prompts 
   Amazon Simple Storage Service (Amazon S3) stores testing data 
    
 Analytics stack: 
   
   Amazon S3 stores logs and processed data 
   Amazon Data Firehose streams logs to Amazon S3 
   Lambda orchestrates extract, transform, and load (ETL) operations 
   AWS Glue manages the Data Catalog and ETL jobs 
   Amazon Athena is used for querying and analyzing analytics data in Amazon S3 
   Amazon QuickSight is used for data visualization and business intelligence 
    
 CI/CD pipeline: 
   
   GitHub serves as the source code repository 
   A GitHub workflow automates the CI/CD pipeline 
    
 
Amazon Lex V2 configuration as code and CI/CD workflow 
The following diagram illustrates how multiple developers can work on changes to the bot stack and test in parallel by deploying changes locally or using a GitHub workflow. 
 
The process consists of the following steps: 
 
 A developer clones the repository and creates a new branch for changes. 
 Developer A or B makes changes to the bot configuration or Lambda functions using code. 
 The developer creates a pull request. 
 The developer deploys the Amazon Lex V2 CDK stack through one of the following methods: 
   
   Create a pull request and ensure all code quality and standards checks are passing. 
   Merge it with the main branch. 
   Deploy the Amazon Lex V2 CDK stack from their local environment. 
    
 The developer runs the Test Workbench as part of the CI/CD pipeline or from their local environment using the automation scripts. 
   
   Tests results are displayed in GitHub Actions and the terminal (if run locally). 
   The pipeline succeeds only if defined checks such as linting, unit testing, infrastructure testing and integration, and Test Workbench functional testing pass. 
    
 After all tests and checks pass, a new pre-release can be drafted to deploy to the staging environment. After staging deployment and testing (automated and UAT) is successful, a new release can be created for production deployment (after manual review and approval). 
 
Amazon Lex Test Workbench automation 
The solution uses GitHub and AWS services, such as Step Functions state machines and Lambda functions, to orchestrate the entire Amazon Lex V2 Bot testing process (instead of using the existing manual testing process for Amazon Lex). The pipeline triggers the upload of test sets, Lambda functions to interact with the Amazon Lex V2 bot and Test Workbench, then another Lambda function to read the tests results and provide results in the pipeline. 
To maintain consistent, repeatable evaluations of your Amazon Lex V2 bots, itâ€™s essential to manage and organize your test datasets effectively. The following key practices help keep test sets up-to-date: 
 
 Test set files are version-controlled and linked to each bot and its version 
 Separate golden test sets are created for each intent and updated on a regular basis to include production customer utterances, increasing intent recognition rates 
 The versioned test data is deployed as part of each bot deployment in non-production environments 
 
The following diagram illustrates the end-to-end automated process for testing Amazon Lex V2 bots after each deployment. 
 
The post-deployment workflow consists of the following steps: 
 
 The developer checks the test file into the GitHub repository (or deploys directly from local). After each bot deployment, GitHub triggers the test script using the GitHub workflow. 
 The test scripts upload the test files to an S3 bucket. 
 The test script invokes a Step Functions state machine, using a bot name and list of file keys as inputs. 
 Amazon Lex Model API calls are invoked to get the bot ID (ListBots) and alias (ListBotAliases). 
 Each test file key is iterated within a Map state, where the following tasks are executed: 
   
   Call Amazon Lex APIs to start import jobs: 
     
     StartImport â€“ Creates a test set ID and stores it under an S3 bucket specified location. 
     DescribeImport â€“ Checks if the status of StartImport is complete. 
      
   Run the test set: 
     
     StartTestExecution â€“ Creates a test execution ID and executes the test. 
     ListTestExecutions â€“ Gathers all test executions. A Lambda function filters out the current test execution id and its status. 
      
   Get test results. 
    
 When the test is complete: 
   
   The ListTestExecutionResultItems API is invoked to gather overall test results. 
   The ListTestExecutionResultItems API is invoked to fetch test failure details at the utterance level if present. 
    
 A Lambda function orchestrates the final cleanup and reporting: 
   
   DeleteTestSet cleans up test sets that are no longer needed from an S3 bucket. 
   The pipeline outputs the results and if there are test failures, these are listed in the GitHub action or local terminal job report. 
    
 Developers conduct the manual process of reviewing the test result files from the Test Workbench console. 
 
Conclusion 
In this post, we presented how Principal accelerated the development, testing, and deployment of Amazon Lex V2 bots and supporting AWS services using code. In addition to the reporting and analytics solution, this provides a robust solution for the continued enhancement and maintenance of the Virtual Assistant ecosystem. 
By automating Test Workbench processes and integrating them with version control and CI/CD processes, Principal was able to decrease testing and deployment time, increase test coverage, streamline their development workflows, and deliver quality conversational experience to customers. For a deeper dive into other relevant services, refer to Evaluating Lex V2 bot performance with the Test Workbench. 
AWS and Amazon are not affiliates of any company of the Principal Financial Group. This communication is intended to be educational in nature and is not intended to be taken as a recommendation. Insurance products issued by Principal National Life Insurance Co (except in NY) and Principal Life Insurance Company. Plan administrative services offered by Principal Life. Principal Funds, Inc. is distributed by Principal Funds Distributor, Inc. Securities offered through Principal Securities, Inc., member SIPC and/or independent broker/dealers. Referenced companies are members of the Principal Financial Group, Des Moines, IA 50392. Â©2025 Principal Financial Services, Inc. 4373397-042025 
 
About the authors 
Mulay Ahmed is a Solutions Architect at Principal with expertise in architecting complex enterprise-grade solutions, including AWS Cloud implementations. 
Caroline Lima-Lane is a Software Engineer at Principal with a vast background in the AWS Cloud space.
â€¢ Beyond vibes: How to properly select the right LLM for the right task
  Choosing the right large language model (LLM) for your use case is becoming both increasingly challenging and essential. Many teams rely on one-time (ad hoc) evaluations based on limited samples from trending models, essentially judging quality on â€œvibesâ€ alone. 
This approach involves experimenting with a modelâ€™s responses and forming subjective opinions about its performance. However, relying on these informal tests of model output is risky and unscalable, often misses subtle errors, overlooks unsafe behavior, and provides no clear criteria for improvement. 
A more holistic approach entails evaluating the model based on metrics around qualitative and quantitative aspects, such as quality of response, cost, and performance. This also requires the evaluation system to compare models based on these predefined metrics and give a comprehensive output comparing models across all these areas. However, these evaluations donâ€™t scale effectively enough to help organizations take full advantage of the model choices available. 
In this post, we discuss an approach that can guide you to build comprehensive and empirically driven evaluations that can help you make better decisions when selecting the right model for your task. 
From vibes to metrics and why it matters 
Human brains excel at pattern-matching, and models are designed to be convincing. Although a vibes-based approach can serve as a starting point, without systematic evaluation, we lack the evidence needed to trust a model in production. This limitation makes it difficult to compare models fairly or identify specific areas for improvement. 
The limitations of â€œjust trying it outâ€ include: 
 
 Subjective bias â€“ Human testers might favor responses based on style or tone rather than factual accuracy. Users can be swayed by â€œexotic wordsâ€ or formatting. A model whose writing sounds confident might win on vibes while actually introducing inaccuracies. 
 Lack of coverage â€“ A few interactive prompts wonâ€™t cover the breadth of real-world inputs, often missing edge cases that reveal model weaknesses. 
 Inconsistency â€“ Without defined metrics, evaluators might disagree on why one model is better based on different priorities (brevity vs. factual detail), making it difficult to align model choice with business goals. 
 No trackable benchmarks â€“ Without quantitative metrics, itâ€™s impossible to track accuracy degradation during prompt optimization or model changes. 
 
Established benchmarks like MMLU, HellaSwag, and HELM offer valuable standardized assessments across reasoning, knowledge retrieval, and factuality dimensions, efficiently helping narrow down candidate models without extensive internal resources. 
However, exclusive reliance on these benchmarks is problematic: they measure generalized rather than domain-specific performance, prioritize easily quantifiable metrics over business-critical capabilities, and canâ€™t account for your organizationâ€™s unique constraints around latency, costs, and safety requirements. A high-ranking model might excel at trivia while failing with your industry terminology or producing responses too verbose or costly for your specific implementation. 
A robust evaluation framework is vital for building trust, which is why no single metric can capture what makes an LLM response â€œgood.â€ Instead, you must evaluate across multiple dimensions: 
 
 Accuracy â€“ Does the model produce accurate information? Does it fully answer the question or cover required points? Is the response on-topic, contextually relevant, well-structured, and logically coherent? 
 Latency â€“ How fast does the model produce a response? For interactive applications, response time directly impacts user experience. 
 Cost-efficiency â€“ What is the monetary cost per API call or token? Different models have varying pricing structures and infrastructure costs. 
 
By evaluating along these facets, you can make informed decisions aligned with product requirements. For example, if robustness under adversarial inputs is crucial, a slightly slower but more aligned model might be preferable. For simple internal tasks, trading some accuracy for cost-efficiency might make sense. 
Although many metrics require qualitative judgment, you can structure and quantify these with careful evaluation methods. Industry best practices combine quantitative metrics with human or AI raters for subjective criteria, moving from â€œI like this answer moreâ€ to â€œModel A scored 4/5 on correctness and 5/5 on completeness.â€ This detail enables meaningful discussion and improvement, and technical managers should demand such accuracy measurements before deploying any model. 
Unique evaluation dimensions for LLM performance 
In this post, we make the case for structured, multi-metric assessment of foundation models (FMs) and discuss the importance of creating ground truth as a prerequisite to model evaluation. We use the open source 360-Eval framework as a practical, code-first tool to orchestrate rigorous evaluations across multiple models and cloud providers. 
We show the approach by comparing four LLMs within Amazon Bedrock, across a spectrum of correctness, completeness, relevance, format, coherence, and instruction following, to understand how each model responds matches our ground truth dataset. Our evaluation measures the accuracy, latency, and cost for each model, painting a 360Â° picture of their strengths and weaknesses. 
To evaluate FMs, itâ€™s highly recommended that you break up model performance into distinct dimensions. The following is a sample set of criteria and what each one measures: 
 
 Correctness (accuracy) â€“ The factual accuracy of the modelâ€™s output. For tasks with a known answer, you can measure this using exact match or cosine similarity; for open-ended responses, you might rely on human or LLM judgment of factual consistency. 
 Completeness â€“ The extent to which the modelâ€™s response addresses all parts of the query or problem. In human/LLM evaluations, completeness is often scored on a scale (did the answer partly address or fully address the query). 
 Relevance â€“ Measures if the content of the response is on-topic and pertinent to the userâ€™s request. Relevance scoring looks at how well the response stays within scope. High relevance means the model understood the query and stayed focused on it. 
 Coherence â€“ The logical flow and clarity of the response. Coherence can be judged by human or LLM evaluators, or approximated with metrics like coherence scores or by checking discourse structure. 
 Following instructions â€“ How well the model obeys explicit instructions in the prompt (formatting, style, length, and so on). For example, if asked â€œList three bullet-point advantages,â€ does the model produce a three-item bullet list? If the system or user prompt sets a role or tone, does the model adhere to it? Instruction-following can be evaluated by programmatically checking if the output meets the specified criteria (for example, contains the required sections) or using evaluator ratings. 
 
Performing such comprehensive evaluations manually can be extremely time-consuming. Each model needs to be run on many if not hundreds of prompts, and each output must be checked for across all metrics. Doing this by hand or writing one-off scripts is error-prone and doesnâ€™t scale. In practice, these can be evaluated automatically using LLM-as-a-judge or human feedback. This is where evaluation frameworks come into play. 
After youâ€™ve chosen an evaluation philosophy, itâ€™s wise to invest in tooling to support it. Instead of combining ad hoc evaluation scripts, you can use dedicated frameworks to streamline the process of testing LLMs across many metrics and models. 
Automating 360Â° model evaluation with 360-Eval 
360-Eval is a lightweight solution that captures the depth and breadth of model evaluation. You can use it as an evaluation orchestrator to define the following: 
 
 Your dataset of test prompts and respective golden answers (expected answers or reference outputs) 
 Models you want to evaluate 
 The metrics and tasks framework evaluating the models against 
 
The tool is designed to capture relevant and user-defined dimensions of model performance in one workflow, supporting multi-model comparisons out of the box. You can evaluate models hosted in Amazon Bedrock or Amazon SageMaker, or call external APIsâ€”the framework is flexible in integrating different model endpoints. This is ideal for a scenario where you might want to use the full power of Amazon Bedrock models without having to sacrifice performance. 
The framework consists of the following key components: 
 
 Data configuration â€“ You specify your evaluation dataset; for example, a JSONL file of prompts with optional expected outputs, the task, and a description. The framework can also work with a custom prompt CSV dataset you provide. 
 API gateway â€“ Using the versatile LiteLLM framework, it abstracts the API differences so the evaluation loop can treat all models uniformly. Inference metadata such as time-to-first-token (TTFT), time-to-last-token (TTLT), total token output, API errors count, and pricing is also captured. 
 Evaluation architecture â€“ 360-Eval uses LLM-as-a-judge to score and calculate the weight of model outputs on qualities like correctness or relevance. You can provide all the metrics you care about into one pipeline. Each evaluation algorithm will produce a score and verdict per test case per model. 
 
Choosing the right model: A real-world example 
For our example use case, AnyCompany is developing an innovative software as a service (SaaS) solution that streamlines database architecture for developers and businesses. Their platform accepts natural language requirements as input and uses LLMs to automatically generate PostgreSQL-specific data models. Users can describe their requirements in plain Englishâ€”for example, â€œI need a cloud-based order management platform designed to streamline operations for small to medium businessesâ€â€”and the tool intelligently extracts the entity and attribute information and creates an optimized table structure specifically for PostgreSQL. This solution avoids hours of manual entity and database design work, reduces the expertise barrier for database modeling, and supports PostgreSQL best practices even for teams without dedicated database specialists. 
In our example, we provide our model a set of requirements (as prompts) relevant to the task and ask it to extract the dominant entity and its attributes (a data extraction task) and also produce a relevant create table statement using PostgreSQL (a text-to-SQL task). 
Example prompt: 
 
 Given the following requirement, extract the data model and attributes that you will 
recommend. I need the output in a single line. You can provide the attributes separated 
by comma: "A global manufacturing company uses a web-based supply chain management 
system to track inventory across 50 locations, manage relationships with over 200 
suppliers, forecast material needs, and automatically trigger purchase orders when stock 
levels reach predefined thresholds......" 
 
The following table shows our task types, criteria, and golden answers for this example prompt. We have shortened the prompt for brevity. In a real-world use case, your requirements might span multiple paragraphs. 
 
  
   
   task_type 
   task_criteria 
   golden_answer 
   
   
   DATA EXTRACTION 
   Check if the extracted entity and attributes matches the requirements 
    
     
     Supply Chain Inventory: inventory_id, product_sku, 
location_id, quantity_on_hand, reorder_threshold, 
supplier_id, last_order_date, forecasted_demand, 
cost_per_unit, status, last_updated 
      
   
   
   TEXT-TO-SQL 
   Given the requirements check if the generated create table matches the requirements 
    
     
     CREATE TABLE supply_chain_inventory (
    inventory_id SERIAL PRIMARY KEY,
    product_sku VARCHAR(50) NOT NULL,
    location_id INTEGER NOT NULL,
    quantity_on_hand INTEGER NOT NULL,
    reorder_threshold INTEGER NOT NULL,
    supplier_id INTEGER,
    last_order_date TIMESTAMP,
    forecasted_demand NUMERIC(10,2),
    cost_per_unit NUMERIC(10,2),
    status VARCHAR(20),
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
); 
      
   
  
 
AnyCompany wants to find a model that will solve the task in the fastest and most cost-effective way, without compromising on quality. 
360-Eval UI 
To reduce the complexity of the process, we have built a UI on top of the evaluation engine. 
The UI_README.md file has instructions to launch and run the evaluation using the UI. You must also follow the instructions in the README.md to install the Python packages as prerequisites and enable Amazon Bedrock model access. 
Letâ€™s explore the different pages in the UI in more detail. 
Setup page 
As you launch the UI, you land on the initial Setup page, where you select your evaluation data, define your label, define your task as discreetly as possible, and set the temperature the models will have when being evaluated. Then you select the models you want to evaluate against your dataset, the judges that will evaluate the modelsâ€™ accuracy (using custom metrics and the standard quality and relevance metrics), configure pricing and AWS Region options, and finally configure how you want the evaluation to take place, such as concurrency, request per minute, and experiment counts (unique runs). 
 
This is where you specify the CSV file with sample prompts, task type, and task criteria according to your needs. 
Monitor page 
After the evaluation criteria and parameters are defined, they are displayed on the Monitor page, which you can navigate to by choosing Monitor in the Navigation section. On this page, you can monitor all your evaluations, including those currently running, those queued, and those not yet scheduled to run. You can choose the evaluation you want to run, and if any evaluation is no longer relevant, you can remove it here as well. 
The workflow is as follows: 
 
 Execute the prompts in the input file against the models selected. 
 Capture the metrics such as input token count, output token count, and TTFT. 
 Use the input and output tokens to calculate the cost of running each prompt against the models. 
 Use an LLM-as-a-judge to evaluate the accuracy against predefined metrics (correctness, completeness, relevance, format, coherence, following instructions) and any user-defined metrics. 
 
 
Evaluations page 
Detailed information of the evaluations, such as the evaluation configuration, the judge models used to evaluate, the Regions where the models are hosted, the input and output cost, and the task and its criteria the model was evaluated with, are displayed on the Evaluations page. 
 
Reports page 
Lastly, the Reports page is where you can select the completed evaluations to generate a report in HTML format. You can also delete old and irrelevant reports. 
 
Understanding the evaluation report 
The tool output is an HTML file that shows the results of the evaluation. It includes the following sections: 
 
 Executive Summary â€“ This section provides an overall summary of the results. It provides a quick summary of which model was most accurate, which model was the fastest overall, and which model provided the best success-to-cost ratio. 
 Recommendations â€“ This section contains more details and a breakdown of what you see in the executive summary, in a tabular format. 
 Latency Metrics â€“ In this section, you can review the performance aspect of your evaluation. We use the TTFT and output tokens per second as a measure for performance. 
 Cost Metrics â€“ This section shows the overall cost of running the evaluation, which indicates what you can expect in your AWS billing. 
 Task Analysis â€“ The tool further breaks down the performance and cost metrics by task type. In our case, there will be a section for the text-to-SQL task and one for data extraction. 
 Judge Scores Analysis â€“ In this section, you can review the quality of each model based on the various metrics. You can also explore prompt optimizations to improve your model. In our case, our prompts were more biased towards the Anthropic family, but if you use the Amazon Bedrock prompt optimization feature, you might be able to address this bias. 
 
Interpreting the evaluation results 
By using the 360-Eval UI, AnyCompany ran the evaluation with their own dataset and got the following results. They chose four different LLMs in Amazon Bedrock to conduct the evaluation. For this post, the exact models used arenâ€™t relevant. We call these models Model-A, Model-B, Model-C, and Model-D. 
These results will vary in your case depending on the dataset and prompts. The results here are a reflection of our own example within a test account. As shown in the following figures, Model-A was the fastest, followed by Model-B. Model-C was 3â€“4 times slower than Model-A. Model-D was the slowest. 
  
As shown in the following figure, Model B was the cheapest. Model A was three times more expensive than Model-B. Model-C and Model-D were both very expensive. 
 
The next focus was the quality of the evaluation. The two most important metrics to were the correctness and completeness of the response. In the following evaluation, only Model-D scored more than 3 for both task types. 
 
Model-C was the next closest contender. 
 
Model-B scored lowest in the correctness and completeness metrics. 
 
Model-A missed slightly on the completeness for the text-to-SQL use case. 
 
Evaluation summary 
Letâ€™s revisit AnyCompanyâ€™s criteria, which was to find a model that will solve the task in the fastest and most cost-effective way, without compromising on quality. There was no obvious winner. 
AnyCompany then considered providing a tiered pricing model to their customers. Premium-tier customers will receive the most accurate model at a premium price, and basic-tier customers will get the model with the best price-performance. 
Although for this use case, Model-D was the slowest and more expensive, it scored highest on the most crucial metrics: correctness and completeness of responses. For a database modeling tool, accuracy is far more important than speed or cost, because incorrect database schemas might lead to significant downstream issues in application development. AnyCompany chose Model-D for premium-tier customers. 
Cost is a major constraint for the basic-tier, so AnyCompany chose Model-A, because it scored reasonably well on correctness for both tasks and only slightly missed on completeness for one task type, while being faster and less expensive than the top performers. 
AnyCompany also considered Model-B as a viable option for free-tier customers. 
Conclusion 
As FMs become more reliant, they can also become more complex. Because their strengths and weaknesses more difficult to detect, evaluating them requires a systematic approach. By using a data-driven, multi-metric evaluation, technical leaders can make informed decisions rooted in the modelâ€™s actual performance, including factual accuracy, user experience, compliance, and cost. 
Adopting frameworks like 360-Eval can operationalize this approach. You can encode your evaluation philosophy into a standardized procedure, making sure every new model or version is judged the same, and enabling side-by-side comparisons. 
The framework handles the heavy lifting of running models on test cases and computing metrics, so your team can focus on interpreting results and making decisions. As the field of generative AI continues to evolve rapidly, having this evaluation infrastructure can help you find the right model for your use case. Furthermore, this approach can enable faster iteration on prompts and policies, and ultimately help you develop more reliable and effective AI systems in production. 
 
About the authors 
Claudio Mazzoni is a Sr Specialist Solutions Architect on the Amazon Bedrock GTM team. Claudio exceeds at guiding costumers through their Gen AI journey. Outside of work, Claudio enjoys spending time with family, working in his garden, and cooking Uruguayan food. 
Anubhav Sharma is a Principal Solutions Architect at AWS with over 2 decades of experience in coding and architecting business-critical applications. Known for his strong desire to learn and innovate, Anubhav has spent the past 6 years at AWS working closely with multiple independent software vendors (ISVs) and enterprises. He specializes in guiding these companies through their journey of building, deploying, and operating SaaS solutions on AWS.
â€¢ Splash Music transforms music generation using AWS Trainium and Amazon SageMaker HyperPod
  Generative AI is rapidly reshaping the music industry, empowering creatorsâ€”regardless of skillâ€”to create studio-quality tracks with foundation models (FMs) that personalize compositions in real time. As demand for unique, instantly generated content grows and creators seek smarter, faster tools, Splash Music collaborated with AWS to develop and scale music generation FMs, making professional music creation accessible to millions. 
In this post, we show how Splash Music is setting a new standard for AI-powered music creation by using its advanced HummingLM model with AWS Trainium on Amazon SageMaker HyperPod. As a selected startup in the 2024 AWS Generative AI Accelerator, Splash Music collaborated closely with AWS Startups and the AWS Generative AI Innovation Center (GenAIIC) to fast-track innovation and accelerate their music generation FM development lifecycle. 
Challenge: Scaling music generation 
Splash Music has empowered a new generation of creators to make music, and has already driven over 600 million streams worldwide. By giving users tools that adapt to their evolving tastes and styles, the service makes music production accessible, fun, and relevant to how fans actually want to create. However, building the technology to unlock this creative freedom, especially the models that power it, meant overcoming several key challenges: 
 
 Model complexity and scale â€“ Splash Music developed HummingLMâ€”a cutting-edge, multi-billion-parameter model tailored for generative music to deliver its mission of making music creation truly accessible. HummingLM is engineered to capture the subtlety of human humming, converting creative ideas into music tracks. Meeting these high standards of fidelity meant Splash had to scale up computing power and storage significantly, so the model could deliver studio-quality music. 
 Rapid pace of change â€“&nbsp;The pace of industry and technological change, driven by rapid AI advancement, means Splash Music must continually adapt, train, fine-tune, and deploy new models to meet user expectations for fresh, relevant features. 
 Infrastructure scaling â€“&nbsp;Managing and scaling large clusters in the generative AI model development lifecycle brought unpredictable costs, frequent interruptions, and time-consuming manual management. Prior to AWS, Splash Music relied on externally managed GPU clusters, which involved unpredictable latency, additional troubleshooting, and management complexity that hindered their ability to experiment and scale as quickly as needed. 
 
The service needed a scalable, automated, and cost-effective infrastructure. 
Overview of HummingLM: Splash Musicâ€™s foundation model 
HummingLM is Splash Musicâ€™s proprietary, multi-modal generative model, developed in close collaboration with the GenAIIC. It represents an improvement in how AI can interpret and generate music. The modelâ€™s architecture is built around a transformer-based large language model (LLM) coupled with a specialized music encoder upsampler: 
 
 HummingLM uses Descript-Audio-Codec (DAC) audio encoding to obtain compressed audio representations that capture both frequency and timbre characteristics 
 The system transforms hummed melodies into professional instrumental performances without explicit timbre representation learning 
 
The innovation lies in how HummingLM fuses these token streams. Using a transformer-based backbone, the model learns to blend the melodic intent from humming with the stylistic and structural cues from instrument sound (for example, to make the humming sound like a guitar, piano, flute, or different synthesized sound). Users can hum a tune, add an instrument control signal, and receive a fully arranged, high-fidelity track in return. HummingLMâ€™s architecture is designed for both efficiency and expressiveness. By using discrete token representations, the model achieves faster convergence and reduced computational overhead compared to traditional waveform-based approaches. This makes it possible to train on diverse, large-scale datasets and adapt quickly to new genres or user preferences. 
The following diagram illustrates how HummingLM is trained and the inference process to generate high-quality music: 
 
Solution overview: Accelerating model development with AWS Trainium on Amazon SageMaker HyperPod 
Splash Music collaborated with the GenAIIC to advance its HummingLM foundation model, using the combined capabilities of Amazon SageMaker HyperPod and AWS Trainium chips for model training. 
Splash Musicâ€™s architecture follows SageMaker HyperPod best-practices using Amazon Elastic Kubernetes Service (EKS) as the orchestrator, FSx for Lustre for storage to store over 2 PB of data, and AWS Trainium EC2 instances for acceleration. The following diagram illustrates the solution architecture. 
 
In the following sections, we walk through each step of the model development lifecycle, from dataset preparation to compilation for optimized inference. 
Dataset preparation 
Efficient preparation and processing of large-scale audio datasets is critical for developing controllable music generation models: 
 
 Feature extraction pipeline â€“ Splash Music built a feature extraction pipeline for efficient, scalable processing of large volumes of audio data, producing high-quality features for model training. It starts by retrieving audio in batches from a centralized database, minimizing I/O overhead and supporting large-scale operations. 
 Audio processing â€“ Each audio file is resampled from 44,100 Hz to 22,050 Hz to standardize inputs and reduce computational load. A mono reference signal is also created by averaging the stereo channels from a reference audio file, serving as a consistent benchmark for analysis. In parallel, a Basic Pitch Extractor generates a synthetic, MIDI-like version of the audio, providing a symbolic representation of pitch and rhythm that enhances the richness of extracted features. 
 Descript Audio Codec (DAC) extractor â€“&nbsp;The pipeline processes three audio streams: the stereo channels from the original audio, the mono reference, and the synthetic MIDI signal. This multi-stream approach captures diverse aspects of the audio signal, producing a robust set of features. Extracted data is organized into two main sets: audio-feature, which includes features from the original stereo channels, and sine-audio-feature, which contains features from the MIDI and mono reference audio. This structure streamlines downstream model training. 
 Parallel processing: To maximize performance, the pipeline uses parallel processing for concurrent feature extraction and data uploading. This significantly boosts efficiency, making sure the system handles large datasets with speed and consistency. 
 
In addition, the solution uses an advanced stem separation system that isolates songs into six distinct audio stems: drums, bass, vocals, lead, chordal, and other instruments: 
 
 Stem Preparation: Splash Music creates high-quality training data by preparing separate stems for each musical element. Lead and chordal stems are generated using a synthesizer tool and a diverse dataset of music tracks. This rich dataset covers multiple genres and styles. This provides a strong foundation for the model to learn precise component separation. 
 
By streamlining data handling from the outset, we make sure that the subsequent model training stages have access to clean, well-structured features. 
Model architecture and optimization 
HummingLM employs a dual-component architecture: 
 
 LLM for coarse token generation â€“ A 385 M parameter transformer-based language model (24 layers, 1024 embedding dimension, 16 attention heads) that generates foundational musical structure 
 Upsampling component â€“ A specialized component that expands the coarse representation into complete, high-fidelity audio. 
 
This division of labor is key to HummingLMâ€™s effectiveness: the LLM captures high-level musical intent, and the upsampling component handles acoustic details. Together with the GenAIIC, Splash collaborated on research to optimize the HummingLM model to facilitate optimal performance: 
 
 Flexible control signal design â€“&nbsp;The model accepts control signals of varying durations (1-5 seconds), a significant improvement over fixed-window approaches. 
 Zero-shot capability â€“&nbsp;Unlike systems requiring explicit timbre embedding learning, HummingLM can generalize to unseen instrument presets without additional training. 
 Non-autoregressive generation â€“&nbsp;The upsampling component uses parallel token prediction for significantly faster inference compared to traditional autoregressive approaches. 
 
Our evaluation demonstrated HummingLMâ€™s superior first codebook prediction capabilities â€“ a critical factor in residual quantization systems where the first codebook contains most acoustic information. The model consistently outperformed baseline approaches like VALL-E across multiple quality metrics. The evaluation revealed several important findings: 
 
 HummingLM demonstrates significant improvements over baseline approaches in signal fidelity (57.93% better SI-SDR) 
 The model maintains robust performance across diverse musical conditions, with particular strength in the Aeolian mode 
 Zero-shot performance on unseen instrument presets is comparable to seen presets, confirming strong generalization capabilities 
 Data augmentation strategies provide substantial benefits (27.70% improvement in SI-SDR) 
 
Overall, HummingLM achieves state-of-the-art controllable music generation by significantly improving signal fidelity, generalizing well to unseen instruments, and delivering strong performance across diverse musical styles, boosted further by effective data augmentation strategies. 
Efficient distributed training through parallelism, memory, and AWS Neuron optimization 
Splash Music compiled and optimized its model for AWS Neuron, accelerating its model development lifecycle and deployment on AWS Trainium chips. The team considered scalability, parallelization, and memory efficiency and designed a system for supporting models scaling from 2B to over 10B parameters. This includes: 
 
 Enable distributed training with sequence parallelism (SP), tensor parallelism (TP), and data parallelism (DP), scaling up to 64 trn1.32xlarge instances 
 Implement ZeRO-1 memory optimization with selective checkpoint re-computation 
 Integrate Neuron Kernel Interface (NKI) to deploy Flash Attention, accelerating dense attention layers and streamlining causal mask management 
 Decompose the model into core subcomponents (token processors, transformer layers, MLPs) and optimize each for Neuron execution 
 Implement mixed-precision training (bfloat16 and float32) 
 
When optimizations at the Neuron level were complete, optimizing the orchestration layer was important as well. Orchestrated by SageMaker HyperPod, Splash Music developed a robust, Slurm-integrated pipeline that streamlines multi-node training, balances parallelism, and uses activation checkpointing for superior memory efficiency. The pipeline processes data through several critical stages: 
 
 Tokenization â€“ Audio inputs are processed through a Descript Audio Codec (DAC) encoder to generate multiple codebook representations 
 Conditional generation â€“&nbsp;The model learns to predict codebooks given hummed melodies and timbre control signals 
 Loss functions â€“&nbsp; The solution uses a specialized cross-entropy loss function to optimize both token prediction and audio reconstruction quality 
 
Model Inference on AWS Inferentia on Amazon Elastic Container Service (ECS) 
After training, the model is deployed on an Amazon Elastic Container Service (Amazon ECS) cluster with AWS Inferentia instances. The audio is uploaded to Amazon Simple Storage Service (Amazon S3) to handle large volumes of user-submitted recordings, which often vary in quality. Each upload triggers an AWS Lambda function, which queues the file in Amazon Simple Queue Service (Amazon SQS) for delivery to the ECS cluster where inference runs. On the cluster, HummingLM performs two key steps: stem separation to isolate and clean vocals, and audio-to-melody conversion to extract musical structure. Finally, the pipeline recombines the cleaned vocals through a post-processing step with backing tracks, producing the fully processed remixed audio. 
Results and impact 
Splash Musicâ€™s research and development teams now rely on a unified infrastructure built on Amazon SageMaker HyperPod and AWS Trainium chips. The solution has yielded the following benefits: 
 
 Automated, resilient and scalable training â€“ SageMaker HyperPod provisions clusters of AWS Trainium EC2 instances at scale, managing orchestration, resource allocation, and fault recovery automatically. This removes weeks of manual setup and facilitates reliable, repeatable training runs. SageMaker HyperPod continuously monitors cluster health, automatically rerouting jobs and repairing failed nodes, minimizing downtime and maximizing resource utilization. With SageMaker HyperPod, Splash Music cut operational downtime to near zero, enabling weekly model refreshes and faster deployment of new features. 
 AWS Trainium reduced Splashâ€™s training costs by over 54% â€“&nbsp;Splash Music realized over twofold gains in training speed and cut training costs by 54% using AWS Trainium based instances over traditional GPU-based solutions used with their previous cloud provider. With this leap in efficiency, Splash Music can train larger models, release updates more frequently, and accelerate innovation across their generative music service. The acceleration also delivers faster model iteration, with 8% improvement in throughput, and increased its maximum batch size from 70 to 512 for a more efficient use of compute resources and higher throughput per training run. 
 
Splash achieved significant throughput improvements over conventional architectures, to process expansive datasets, supporting the modelâ€™s complex multimodal nature. The solution provides a robust foundation for future growth as data and models continue to scale. 

 â€œAWS Trainium and SageMaker HyperPod took the friction out of our workflow at Splash Music.â€&nbsp;says Daniel Hatadi, Software Engineer, Splash Music. â€œWe replaced brittle GPU clusters with automated, self-healing distributed training that scales seamlessly. Training times are nearly 50% faster, and training costs have dropped by 54%. By relying on AWS AI chips and SageMaker HyperPod and collaborating with the AWS Generative AI Innovation Center, we were able to focus on model design and music-specific research, instead of cluster maintenance. This collaboration has made it easier for us to iterate quickly, run more experiments, train larger models, and keep shipping improvements without needing a bigger team.â€
 
Splash Music also featured in the AWS Summit Sydney 2025 keynote: 

 
  
 
 
Conclusion and Next Steps 
Splash Music is redefining how creators bring their musical ideas to life, making it possible for anyone to generate fresh, personalized tracks that resonate with millions of listeners worldwide. To support this vision at scale, Splash built its HummingLM FM in close collaboration with AWS Startups and the GenAIIC, using services such as SageMaker HyperPod and AWS Trainium. These solutions provide the infrastructure and performance needed to keep pace, helping Splash to create even more intuitive and inspiring experiences for creators. 

 â€œWith SageMaker HyperPod and Trainium, our researchers experiment as fast as our community creates.â€ says Randeep Bhatia, Chief Technology Officer, Splash Music. â€œWeâ€™re not just keeping up with music trendsâ€”weâ€™re setting them.â€
 
Looking forward, Splash Music plans to expand its training datasets tenfold, explore multimodal audio/video generation, and additionally collaborate with the GenAIIC on additional R&amp;D and its next version of HummingLM FM. 
Try creating your own music using Splash Music, and learn more about Amazon SageMaker HyperPod and AWS Trainium. 
 
About the authors 
Sheldon Liu is an Senior Applied Scientist, ANZ Tech Lead at the AWS Generative AI Innovation Center. He partners with AWS customers across diverse industries to develop and implement innovative generative AI solutions, accelerating their AI adoption journey while driving significant business outcomes. 
Mahsa Paknezhad is a Deep Learning Architect and a key member of the AWS Generative AI Innovation Center. She works closely with enterprise clients to design, implement, and optimize cutting-edge generative AI solutions. With a focus on scalability and production readiness, Mahsa helps organizations across diverse industries harness advanced Generative AI models to achieve meaningful business outcomes. 
Xiaoning Wang is a machine learning engineer at the AWS Generative AI Innovation Center. He specializes in large language model training and optimization on AWS Trainium and Inferentia, with experience in distributed training, RAG, and low-latency inference. He works with enterprise customers to build scalable generative AI solutions that drive real business impact. 
Tianyu Liu is an applied scientist at the AWS Generative AI Innovation Center. He partners with enterprise customers to design, implement, and optimize cutting-edge generative AI models, advancing innovation and helping organizations achieve transformative results with scalable, production-ready AI solutions. 
Xuefeng Liu leads a science team at the AWS Generative AI Innovation Center in the Asia Pacific regions. His team partners with AWS customers on generative AI projects, with the goal of accelerating customersâ€™ adoption of generative AI. 
Daniel Wirjo is a Solutions Architect at AWS, focused on AI and SaaS startups. As a former startup CTO, he enjoys collaborating with founders and engineering leaders to drive growth and innovation on AWS. Outside of work, Daniel enjoys taking walks with a coffee in hand, appreciating nature, and learning new ideas.
â€¢ Iterative fine-tuning on Amazon Bedrock for strategic model improvement
  Organizations often face challenges when implementing single-shot fine-tuning approaches for their generative AI models. The single-shot fine-tuning method involves selecting training data, configuring hyperparameters, and hoping the results meet expectations without the ability to make incremental adjustments. Single-shot fine-tuning frequently leads to suboptimal results and requires starting the entire process from scratch when improvements are needed. 
Amazon Bedrock now supports iterative fine-tuning, enabling systematic model refinement through controlled, incremental training rounds. With this capability you can build upon previously customized models, whether they were created through fine-tuning or distillation, providing a foundation for continuous improvement without the risks associated with complete retraining. 
In this post, we will explore how to implement the iterative fine-tuning capability of Amazon Bedrock to systematically improve your AI models. Weâ€™ll cover the key advantages over single-shot approaches, walk through practical implementation using both the console and SDK, discuss deployment options, and share best practices for maximizing your iterative fine-tuning results. 
When to use iterative fine-tuning 
Iterative fine-tuning provides several advantages over single-shot approaches that make it valuable for production environments. Risk mitigation becomes possible through incremental improvements, so you can test and validate changes before committing to larger modifications. With this approach, you can make data-driven optimization based on real performance feedback rather than theoretical assumptions about what might work. The methodology also helps developers to apply different training techniques sequentially to refine model behavior. Most importantly, iterative fine-tuning accommodates evolving business requirements driven by continuous live data traffic. As user patterns change over time and new use cases emerge that werenâ€™t present in initial training, you can leverage this fresh data to refine your modelâ€™s performance without starting from scratch. 
How to implement iterative fine-tuning on Amazon Bedrock 
Setting up iterative fine-tuning involves preparing your environment and creating training jobs that build upon your existing custom models, whether through the console interface or programmatically using the SDK. 
Prerequisites 
Before beginning iterative fine-tuning, you need a previously customized model as your starting point. This base model can originate from either fine-tuning or distillation processes and supports customizable models and variants available on Amazon Bedrock. Youâ€™ll also need: 
 
 Standard IAM permissions for Amazon Bedrock model customization 
 Incremental training data focused on addressing specific performance gaps 
 S3 bucket for training data and job outputs 
 
Your incremental training data should target the specific areas where your current model needs improvement rather than attempting to retrain on all possible scenarios. 
Using the AWS Management Console 
The Amazon Bedrock console provides a straightforward interface for creating iterative fine-tuning jobs. 
Navigate to the Custom Models section and select Create fine-tuning job. The key difference in iterative fine-tuning lies in the base model selection, where you choose your previously customized model instead of a foundation model.  
During training, you can visit the Custom models page in the Amazon Bedrock console to track the job status.  
Once complete, you can monitor your jobs performance metrics on console through multiple metric charts, on the Training metrics and Validation metrics tabs.  
Using the SDK 
Programmatic implementation of iterative fine-tuning follows similar patterns to standard fine-tuning with one critical difference: specifying your previously customized model as the base model identifier. Hereâ€™s an example implementation: 
 
 import boto3
from datetime import datetime
import uuid

# Initialize Bedrock client
bedrock = boto3.client('bedrock')

# Define job parameters
job_name = f"iterative-finetuning-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"
custom_model_name = f"iterative-model-{str(uuid.uuid4())[:8]}"

# Key difference: Use your previously customized model ARN as base
# This could be from previous fine-tuning or distillation
base_model_id = "arn:aws:bedrock:&lt;Region&gt;:&lt;AccountID&gt;:custom-model/&lt;your-previous-custom-model-id&gt;"

# S3 paths for training data and outputs
training_data_uri = "s3://&lt;your-bucket&gt;/&lt;iterative-training-data&gt;"
output_path = "s3://&lt;your-bucket&gt;/&lt;iterative-output-folder&gt;/"

# Hyperparameters adjusted based on previous iteration learnings
hyperparameters = {
    "epochCount": "3" # Example
}

# Create the iterative fine-tuning job
response = bedrock.create_model_customization_job(
    customizationType="FINE_TUNING",
    jobName=job_name,
    customModelName=custom_model_name,
    roleArn=role_arn,
    baseModelIdentifier=base_model_id,  # Your previously customized model
    hyperParameters=hyperparameters,
    trainingDataConfig={
        "s3Uri": training_data_uri
    },
    outputDataConfig={
        "s3Uri": output_path
    }
)

job_arn = response.get('jobArn')
print(f"Iterative fine-tuning job created with ARN: {job_arn}")
 
 
Setting up inference for your iteratively fine-tuned model 
Once your iterative fine-tuning job completes, you have two primary options for deploying your model for inference, provisioned throughput and on-demand inference, each suited to different usage patterns and requirements. 
Provisioned Throughput 
Provisioned Throughput offers stable performance for predictable workloads where consistent throughput requirements exist. This option provides dedicated capacity so that the iteratively fine-tuned model maintains performance standards during peak usage periods. Setup involves purchasing model units based on expected traffic patterns and performance requirements. 
On-demand inference 
On-demand inference provides flexibility for variable workloads and experimentation scenarios. Amazon Bedrock now supports Amazon Nova Micro, Lite, and Pro models as well as Llama 3.3 models for on-demand inference with pay-per-token pricing. This option avoids the need for capacity planning so you can test your iteratively fine-tuned model without upfront commitments. The pricing model scales automatically with usage, making it cost-effective for applications with unpredictable or low-volume inference patterns. 
Best practices 
Successful iterative fine-tuning requires attention to several key areas. Most importantly, your data strategy should emphasize quality over quantity in incremental datasets. Rather than adding large volumes of new training examples, focus on high-quality data that addresses specific performance gaps identified in previous iterations. 
To track progress effectively, evaluation consistency across iterations allows meaningful comparison of improvements. Establish baseline metrics during your first iteration and maintain the same evaluation framework throughout the process. You can use Amazon Bedrock Evaluations to help you systematically identify where gaps exist in your model performance after each customization run. This consistency helps you understand whether changes are producing meaningful improvements. 
Finally, recognizing when to stop the iterative process helps to prevent diminishing returns on your investment. Monitor performance improvements between iterations and consider concluding the process when gains become marginal relative to the effort required. 
Conclusion 
Iterative fine-tuning on Amazon Bedrock provides a systematic approach to model improvement that reduces risks while enabling continuous refinement. With the iterative fine-tuning methodology organizations can build upon existing investments in custom models rather than starting from scratch when adjustments are needed. 
To get started with iterative fine-tuning, access the Amazon Bedrock console and navigate to the Custom models section. For detailed implementation guidance, refer to the Amazon Bedrock documentation. 
 
About the authors 
Yanyan Zhang is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Gautam Kumar is an Engineering Manager at AWS AI Bedrock, leading model customization initiatives across large-scale foundation models. He specializes in distributed training and fine-tuning. Outside work, he enjoys reading and traveling. 
Jesse Manders is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.

â¸»