‚úÖ Morning News Briefing ‚Äì September 03, 2025 10:42

üìÖ Date: 2025-09-03 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  11.4¬∞C
  Temperature: 11.4&deg;C Pressure / Tendency: 101.3 kPa falling Humidity: 97 % Dewpoint: 10.9&deg:C Wind:  calm km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Wednesday 3 September 2025 . Weather:   Wind: calm km-h .
‚Ä¢ Wednesday: A mix of sun and cloud. High 26.
  Wind becoming south 20 km/h gusting to 40 late this morning . High 26. Humidex 28. UV index 6 or high; UV index is 6 or very high . A mix of sun and cloud expected to be a mix of cloud and rain . High of 26.5 degrees in Canada this morning, with a high of 26 degrees in the mid-90s .
‚Ä¢ Wednesday night: Chance of showers. Low 17. POP 40%
  A few clouds. Increasing cloudiness late this evening then 40 percent chance of showers overnight . Wind south 30 km/h. Low 17.50¬∞C. Chance of rain in New York City, New York, New Jersey, Long Island, Connecticut, on September 3, 2025 . Forecast issued 5:00 AM EDT Wednesday 3 September 2025. Weather forecast: Showers, rain

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Hitler's bunker is now just a parking lot. But it's a 'dark tourism' attraction anyway
  A visit to Hitler's bunker, and a deep dive into the economics and ethical quandaries of "dark tourism" Why evil histories sell? "Dark tourism" is a form of tourism that focuses on evil history and evil history . The "Dark History" series explores the economics of dark tourism in the U.S. and the ethical implications of dark history and dark tourism . The
‚Ä¢ So um, why do we say 'um' so much?
  This week, we're breaking down the many uses of "um" and why the word is so controversial . Who knew two letters could spark so much conversation? This week's episode is a look at some of the most common uses of the word "um". The word is a word that has been used in the U.S. for more than a decade . Click here for more
‚Ä¢ With federal money in doubt, California's high-speed train seeks a new path forward
  California's high-speed rail project faces uncertain future with $4 billion in federal funding tied up in court . The project's leaders say they've learned from past mistakes, but federal funding is tied to court case . High-speed train project leaders say project has learned from previous mistakes and will be safe for the public, but it's unclear how long it will take to get funding .
‚Ä¢ 'Founders Museum' from White House and PragerU blurs history, AI-generated fiction
  Historians say it's good to highlight America's founders, but the project takes too narrow a view of history . Historians: The project takes a narrow view of America's founding history . The project is a good way to highlight the founders' achievements, but it's not a good idea, historians say . The White House has been criticized for its lack of transparency in its history .
‚Ä¢ Trump cannot use Alien Enemies Act to deport Venezuelan gang members, appeals court rules
  The administration deported people designated as Tren de Aragua members to a notorious prison in El Salvador where, it argued, U.S. courts could not order them freed . The administration argued that the courts were unable to order the deportees freed from the prison where they would be held . The deportees were sent to El Salvador, where they were held in a notorious El Salvador

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ 'Huge architectural change' to JetBrains ReSharper cuts Visual Studio freezes
  JetBrains has updated ReSharper, its .NET plugin for Visual Studio, with an out-of-process design that achieves a 61 percent reduction in UI freezes, the company claims . The new mode has reduced functionality, but some features do not work yet, and has the tool been overtaken by AI? The company claims the new mode is 61 percent faster than the previous
‚Ä¢ GNOME Foundation boss exits after just four months
  Board calls move a mutual decision but offers no details on what went wrong . Executive director Steven Deobald departed less than four months into the role . Board describes move as mutual, but offer no details of what happened to him . Board calls decision mutual but says it's a 'mutual decision' and offers no further details of how it ended up in the top job . Deob
‚Ä¢ UK Home Office hikes tech consultant spend to ¬£350M despite pledge to cut costs
  UK Home Office has upped its planned spending on external data and tech consultants by ¬£100 million to a maximum of ¬£350 million . Warnings of internal skills shortages fail to quell appetite for hand-holding . Home Office will spend ¬£100m on consultants and ¬£350m on data and data experts . Home office says it will spend more than ¬£1bn on consultants in the
‚Ä¢ Supermarket giant Tesco sues VMware, warns lack of support could disrupt food supply
  Tesco sues Broadcom for breach of contracts pertaining to its VMware licenses . Goes after Computacenter too, seeks ¬£100 million damages . Tesco has warned it may not be able to put food on the shelves if situation goes pear-shaped . It has also named Computacent as a co-defendant, and warned it could not sell food if situation went pear-
‚Ä¢ Internet mapping and research outfit Censys reveals state-based abuse, harassment
  ‚ÄòUniversities are being used to proxy offensive government operations, turning research access decisions political‚Äô Censys Inc, vendor of the popular internet-mapping tool, has revealed that state-based actors are trying to abuse its services by hiding behind academic researchers .‚Ä¶‚Ä¶‚Ä¶ State-backed actors are using academic researchers to hide behind their research decisions,‚Äô says C

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Ultra-processed foods ‚Äî it‚Äôs time for an improved definition
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Are ultra-processed foods really so unhealthy? What the science says
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Unravelling the rise in thyroid cancer incidence and addressing overdiagnosis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Detection of opioids and their metabolites in sweat by carbon nanotube FET sensor array
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ China‚Äôs chikungunya virus outbreak is a wake up call
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The connected customer
  As brands compete for increasingly price conscious consumers, customer experience (CX) has become a decisive differentiator . Yet many struggle to deliver, constrained by outdated systems, fragmented data, and organizational silos that limit both agility and consistency . This report explores how leading organizations are navigating that shift, and what it takes to move from AI potential to CX impact . The most effective organizations treat AI as a collaborative tool that enhances rather than replaces human connection and expertise .
‚Ä¢ Building the AI-enabled enterprise of the future
  The use of AI technologies is already driving changes across industries . 98% say they feel an increased sense of urgency in the last year . 85% believe they have less than 18 months to deploy an AI strategy or they will see negative business effects . Just 13% of companies globally say they are ready to leverage AI to its full potential . IT infrastructure is an increasing challenge as workloads grow ever larger . Good quality, well-managed enterprise-wide data is essential .
‚Ä¢ The Download: therapists secretly using AI, and Apple AirPods‚Äô hearing aid potential
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Therapists are secretly using ChatGPT. Clients are triggered.



Declan would never have found out his therapist was using ChatGPT had it not been for a technical mishap. The connection was patchy during one of their online sessions, so Declan suggested they turn off their video feeds. Instead, his therapist began inadvertently sharing his screen.For the rest of the session, Declan was privy to a real-time stream of ChatGPT analysis rippling across his therapist‚Äôs screen, who was taking what Declan was saying, putting it into ChatGPT, and then parroting its answers.



But Declan is not alone. In fact, a growing number of people are reporting receiving AI-generated communiqu√©s from their therapists. Clients‚Äô trust and privacy are being abandoned in the process. Read the full story.



‚ÄîLaurie Clarke







Apple AirPods: a gateway hearing aid



‚ÄîAshley Shew



When the US Food and Drug Administration approved hearing-aid software for Apple‚Äôs AirPods Pro in September 2024, with a device price point around $200, I was excited.I have hearing loss and tinnitus, and my everyday hearing aids cost just over $2,000. Ninety percent of the hearing-aid market is concentrated in the hands of a few companies, and there‚Äôs little competitive pricing. So I was thrilled that a major tech company has entered this field with the AirPods Pro 2. Here‚Äôs what I made of them.



This story is from our new print edition, which is all about the future of security. Subscribe here to catch future copies when they land.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 MAHA is in chaosRFK Jr‚Äôs movement is tearing itself apart over what it wants to achieve. (WSJ $)+ Trying to pressure food companies to alter their products is unlikely to work. (The Atlantic $)+ Ultra-processed food makes up a sizable proportion of the American diet. (Axios)+ RFK Jr‚Äôs plan to improve America‚Äôs diet is missing the point. (MIT Technology Review)2 DOGE is using AI to target SEC rules to ditchExperts fear its decisions won‚Äôt be checked by qualified humans. (The Information $)+ Can AI help DOGE slash government budgets? It‚Äôs complex. (MIT Technology Review)



3 Salesforce has replaced around 4,000 jobs with AI agentsIt‚Äôs slashed its support staff team nearly in half. (SF Chronicle $)+ Workers are trying to weather the AI-induced storm. (Vox)+ AI is coming for the job market, security, and prosperity. (MIT Technology Review)4 What‚Äôs up with China‚Äôs EV industry?Its cutthroat competitive practices are starting to grate on the government. (NYT $)+ The country‚Äôs robotmakers are on the rise. (FT $)+ China‚Äôs EV giants are betting big on humanoid robots. (MIT Technology Review)



5 A ‚Äúnearly naked‚Äù black hole has been spottedThe never-before-seen black hole may have been created moments after the big bang. (The Guardian)



6 How to make quantum computers usefulResearchers have turned their attention towards making software for the machines. (FT $)+ Why AI could eat quantum computing‚Äôs lunch. (MIT Technology Review)



7 OnlyFans has a piracy problemAdult creators‚Äô content isn‚Äôt staying behind the paywall. (404 Media)



8 These humans are paid to fix AI slopAnyone can prompt AI, but the results aren‚Äôt always good. (NBC News)



9 The hottest gadget for kids is a landline phone¬†And they‚Äôre learning phone etiquette for the first time. (Insider $)



10 Meet iTunes‚Äô diehard fansThey‚Äôre eschewing streaming platforms in favor of their digital libraries. (WP $)+ How to break free of Spotify‚Äôs algorithm. (MIT Technology Review)







Quote of the day



‚ÄúThe calculator doesn‚Äôt construct facts about world knowledge and give them to you.‚Äù



‚ÄîElisha Roberts, assistant director at the nonprofit Colorado Education Initiative, tells Bloomberg she doesn‚Äôt buy the idea that AI is comparable to other classroom tools like the calculator.







One more thing







Supershoes are reshaping distance runningSince 2016, when Nike introduced the Vaporfly, a paradigm-¬≠shifting shoe that helped athletes run more efficiently (and therefore faster), the elite running world has muddled through a period of soul-searching over the impact of high-tech footwear on the sport.‚ÄúSupershoes‚Äù ‚Äîwhich combine a lightweight, energy-¬≠returning foam with a carbon-fiber plate for stiffness‚Äîhave been behind every broken world record in distances from 5,000 meters to the marathon since 2020.To some, this is a sign of progress. In much of the world, elite running lacks a widespread following. Record-breaking adds a layer of excitement. And the shoes have benefits beyond the clock: most important, they help minimize wear on the body and enable faster recovery from hard workouts and races.Still, some argue that they‚Äôve changed the sport too quickly. Read the full story.&nbsp;



‚ÄîJonathan W. Rosen







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Happy birthday to Keanu Reeves, who turns 61 today! Here‚Äôs a compilation of his hilariously bad acting in Bram Stroker‚Äôs Dracula.+ Why do some cats hate water, yet others love it?+ If you fancy setting a Guinness World Record, there‚Äôs a few still up for grabs.+ To mark world coconut day (what do you mean, you forgot?), check out these delicious-looking recipes
‚Ä¢ What health care providers actually want from AI
  In a market flooded with AI promises, health care decision-makers are no longer dazzled by flashy demos or abstract potential. Today, they want pragmatic and pressure-tested products.¬†They want solutions that work for their clinicians, staff, patients, and their bottom line.



To gain traction in 2025 and beyond, health care providers are looking for real-world solutions in AI right now.







Solutions that fix real problems



Hospitals and health systems are looking at AI-enabled solutions that target their most urgent pain points: staffing shortages, clinician burnout, rising costs, and patient bottlenecks. These operational realities keep leadership up at night, and AI solutions¬† must directly address them.



For instance, hospitals and health systems are eager for AI tools that can reduce documentation burden for physicians and nurses. Natural language processing (NLP) solutions that auto-generate clinical notes or streamline coding to free up time for direct patient care are far more compelling pitches than generic efficiency gains. Similarly, predictive analytics that help optimize staffing levels or manage patient flows can directly address operational workflow and improve throughput.



Ultimately, if an AI solution doesn‚Äôt target these critical issues and deliver tangible benefits, it‚Äôs unlikely to capture serious buyer interest.



Demonstrate real-world results 



AI solutions need validation in environments that mirror actual care settings. The first step toward that is to leverage high-quality, well-curated real-world data to drive reliable insights and avoid misleading results when building and refining AI models.&nbsp;



Then, hospitals and health systems need evidence that the solution does what it claims to do, for instance through independent-third party validation, pilot projects, peer-reviewed publications, or documented case studies.



Mayo Clinic Platform offers a rigorous independent process where clinical, data science, and regulatory experts evaluate a solution for intended use, proposed value, and clinical and algorithmic performance, which gives innovators the credibility their solutions need to win the confidence of health-care leaders.&nbsp;&nbsp;&nbsp;&nbsp;



Integration with existing systems



With so many demands, health-care IT leaders have little patience for standalone AI tools that create additional complexity. They want solutions that integrate seamlessly into existing systems and workflows. Compatibility with major electronic health record (EHR) platforms, robust APIs, and smooth data ingestion processes are now baseline requirements.



Custom integrations that require significant IT resources‚Äîor worse, create duplicative work‚Äîare deal breakers for many organizations already stretched thin. The less disruption an AI solution introduces, the more likely it is to gain traction. This is the reason solution developers are turning to platforms like Mayo Clinic Platform Solutions Studio, a program that provides seamless integration, single implementation, expert guidance to reduce risk, and a simplified process to accelerate solution adoption among healthcare providers.&nbsp;



Explainability and transparency



The importance of trust cannot be overstated when it comes to health care, and transparency and explainability are critical to establishing trust in AI. As AI models grow more complex, health-care providers recognize that simply knowing what an algorithm predicts isn‚Äôt enough. They also need to understand how it arrived at that insight.



Health-care organizations are increasingly wary of black-box AI systems whose logic remains opaque. Instead, they‚Äôre demanding solutions that offer clear, understandable explanations clinicians can relay confidently to peers, patients, and regulators.



As McKinsey research shows, organizations that embed explainability into their AI strategy not only reduce risk but also see higher adoption, better performance outcomes, and stronger financial returns. Solution developers that can demystify their models, provide transparent performance metrics, and build trust at every level will have a significant edge in today‚Äôs health-care market.



Clear ROI and low implementation burden



Hospitals and health systems want to know precisely how quickly an AI solution will pay for itself, how much staff time it will save, and what costs it will help offset. The more specific and evidence-backed the answers, the better rate of adoption.



Solution developers that offer comprehensive training and responsive support are far more likely to win deals and keep customers satisfied over the long term.



Alignment with regulatory and compliance needs



As AI adoption grows, so does regulatory scrutiny. Health-care providers are increasingly focused on ensuring that any new solution complies with HIPAA, data privacy laws, and emerging guidelines around AI governance and bias mitigation.



Solution developers that can proactively demonstrate compliance provide significant peace of mind. Transparent data handling practices, rigorous security measures, and alignment with ethical AI principles are all becoming essential selling points as well.



A solution developer that understands health care



Finally, it‚Äôs not just about the technology. Health-care providers want partners that genuinely understand the complexities of clinical care and hospital operations. They‚Äôre looking for partners that speak the language of health care, grasp the nuances of change management, and appreciate the realities of delivering patient care under tight margins and high stakes.



Successful AI vendors recognize that even the best technology must fit into a highly human-centered and often unpredictable environment. Long-term partnerships, not short-term sales, are the goal.



Delivering true value with AI



To earn their trust and investment, AI developers must focus relentlessly on solving real problems, demonstrating proven results, integrating without friction, and maintaining transparency and compliance.



Those that deliver on these expectations will have the chance to help shape the future of health care.



This content was produced by Mayo Clinic Platform. It was not written by MIT Technology Review‚Äôs editorial staff.
‚Ä¢ How healthcare accelerator programs are changing care
  As healthcare faces mounting pressures, from rising costs and an aging population to widening disparities, forward thinking innovations are more essential than ever.



Accelerator programs have proven to be powerful launchpads for health tech companies, often combining resources, mentorship, and technology that startups otherwise would not have access to. By joining these fast-moving platforms, startups are better able to rapidly innovate, enhance, and scale their healthcare solutions, bringing transformative approaches to hospitals and patients faster.



So, why are healthcare accelerators becoming essential to the evolution of the industry? There are key reasons why these programs are reshaping health innovation and explanations how they are helping to make care more personalized, proactive, and accessible.







Empowering growth and scaling impact &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;



Healthcare accelerator programs offer a powerful combination of guidance, resources, and connections to help early-stage startups grow, scale, and succeed in a complex industry.¬†



Participants typically benefit from:&nbsp;




Expert mentorship from seasoned healthcare professionals, entrepreneurs, and industry leaders to navigate clinical, regulatory, and business challenges



Access to valuable resources such as clinical data, testing environments, and technical infrastructure to refine and validate health tech solutions



Strategic support for growth including investor introductions, partnership opportunities, and go-to-market guidance to expand reach and impact&nbsp;




Speeding up innovation&nbsp;



Accelerators help startups and early-stage companies bring their solutions to market faster by streamlining the path through one of the most complex industries: healthcare. Traditionally, innovation in this space is slowed by regulatory hurdles, extended sales cycles, clinical validation requirements, and fragmented data systems.&nbsp;&nbsp;



Through structured support, accelerators help companies refine their product market fit, navigate compliance and regulatory landscapes, integrate with healthcare systems, and gather the clinical evidence needed to build trust and credibility. They also open doors to early pilot opportunities, customer feedback, and strategic partnerships, compressing what could take years into just a few months.¬†



By removing barriers and accelerating critical early steps, these programs enable digital health innovators to reach the market more efficiently, with stronger solutions and a clearer path to impact.&nbsp;



Connecting startups with key stakeholders&nbsp;



Today, many accelerator programs are developed by large healthcare organizations that are driving change from within. These accelerator programs are especially beneficial to startups since they have strong partnerships with hospitals, pharma companies, insurance providers, and regulators. This gives startups a chance to validate their ideas in real-world settings, gather clinical feedback early, and scale more effectively.¬†¬†



Many accelerators also bring together people from different fields; doctors, engineers, data scientists, and designers, encouraging fresh perspectives on persistent problems like chronic disease management, preventative care, data interoperability, and patient engagement.&nbsp;



Breaking barriers to global expansion&nbsp;



Healthcare accelerator programs act as gateways for international digital health companies looking to enter the U.S. market, often considered one of the most complex and highly regulated healthcare landscapes in the world. These programs provide tailored support to navigate U.S. compliance standards, understand payer and provider dynamics, and tailor offerings to meet the needs of U.S. patients and care delivery models.¬†



Through market-specific mentorship, strategic introductions, and access to a robust health innovation ecosystem, accelerators help international startups overcome geographic and regulatory barriers, enabling global ideas to scale and make an impact where they‚Äôre needed most.&nbsp;



Building the future of healthcare



The role of healthcare accelerator programs extends far beyond startup support. They are helping to redefine how innovation happens, shifting it from isolated efforts to collaborative ecosystems of change. By bridging gaps between early-stage technology and real-world implementation, these programs play a critical role in making healthcare more personalized, preventative, and equitable.



As the digital transformation of healthcare continues, accelerator programs will remain indispensable in cultivating the next generation of breakthroughs, ensuring that bold ideas are not only born, but brought to life in meaningful, measurable ways.



Spotlight: Mayo Clinic Platform_Accelerate



One standout example of this innovation-forward approach is Mayo Clinic Platform_Accelerate, a 30-week accelerator program designed to help health tech startups reach market readiness. Participants gain access to de-identified clinical data, prototyping labs, and guidance from experts across clinical, regulatory, and business domains.



By combining Mayo Clinic‚Äôs legacy of clinical excellence with a forward-thinking innovation model, the Mayo Clinic Platform_Accelerate program helps promising startups to refine their solutions and prepare for meaningful scale, transforming how care is delivered across the continuum.



Finding value in accelerator programs



In a time when healthcare must evolve faster than ever, accelerator programs have become vital to the industry‚Äôs future. By supporting early-stage innovators with the tools, mentorship, and networks they need to succeed, these programs are paving the way for smarter, safer, and more connected care.



Whether tackling chronic disease, reimagining patient engagement, or unlocking the power of data, the startups nurtured in accelerator programs are helping to shape a more resilient and responsive health system, one innovation at a time.



This content was produced by Mayo Clinic Platform. It was not written by MIT Technology Review‚Äôs editorial staff.

üîí Cybersecurity & Privacy
‚Ä¢ The Ongoing Fallout from a Breach at AI Chatbot Maker Salesloft
  The recent mass-theft of authentication tokens from Salesloft, whose AI chatbot is used by a broad swath of corporate America to convert customer interaction into Salesforce leads, has left many companies racing to invalidate the stolen credentials before hackers can exploit them. Now Google warns the breach goes far beyond access to Salesforce data, noting the hackers responsible also stole valid authentication tokens for hundreds of online services that customers can integrate with Salesloft, including Slack, Google Workspace, Amazon S3, Microsoft Azure, and OpenAI.
Salesloft says its products are trusted by 5,000+ customers. Some of the bigger names are visible on the company&#8217;s homepage.
Salesloft disclosed on August 20 that, &#8220;Today, we detected a security issue in the Drift application,&#8221; referring to the technology that powers an AI chatbot used by so many corporate websites. The alert urged customers to re-authenticate the connection between the Drift and Salesforce apps to invalidate their existing authentication tokens, but it said nothing then to indicate those tokens had already been stolen.
On August 26, the Google Threat Intelligence Group (GTIG) warned that unidentified hackers tracked as UNC6395 used the access tokens stolen from Salesloft to siphon large amounts of data from numerous corporate Salesforce instances. Google said the data theft began as early as Aug. 8, 2025 and lasted through at least Aug. 18, 2025, and that the incident did not involve any vulnerability in the Salesforce platform.
Google said the attackers have been sifting through the massive data haul for credential materials such as AWS keys, VPN credentials, and credentials to the cloud storage provider Snowflake.
&#8220;If successful, the right credentials could allow them to further compromise victim and client environments, as well as pivot to the victim&#8217;s clients or partner environments,&#8221; the GTIG report stated.
The GTIG updated its advisory on August 28 to acknowledge the attackers used the stolen tokens to access email from &#8220;a very small number of Google Workspace accounts&#8221; that were specially configured to integrate with Salesloft. More importantly, it warned organizations to immediately invalidate all tokens stored in or connected to their Salesloft integrations &#8212; regardless of the third-party service in question.
&#8220;Given GTIG&#8217;s observations of data exfiltration associated with the campaign, organizations using Salesloft Drift to integrate with third-party platforms (including but not limited to Salesforce) should consider their data compromised and are urged to take immediate remediation steps,&#8221; Google advised.
On August 28, Salesforce blocked Drift from integrating with its platform, and with its productivity platforms Slack and Pardot.
The Salesloft incident comes on the heels of a broad social engineering campaign that used voice phishing to trick targets into connecting a malicious app to their organization&#8217;s Salesforce portal. That campaign led to data breaches and extortion attacks affecting a number of companies including Adidas, Allianz Life and Qantas.
On August 5, Google disclosed that one of its corporate Salesforce instances was compromised by the attackers, which the GTIG has dubbed UNC6040 (&#8220;UNC&#8221; stands for &#8220;uncategorized threat group&#8221;). Google said the extortionists consistently claimed to be the threat group ShinyHunters,¬†and that the group appeared to be preparing to escalate its extortion attacks by launching a data leak site.
ShinyHunters is an amorphous threat group known for using social engineering to break into cloud platforms and third-party IT providers, and for posting dozens of stolen databases to cybercrime communities like the now-defunct Breachforums.
The ShinyHunters brand dates back to 2020, and the group has been credited with or taken responsibility for dozens of data leaks that exposed hundreds of millions of breached records. The group&#8217;s member roster is thought to be somewhat fluid, drawing mainly from active denizens of the Com, a mostly English-language cybercrime community scattered across an ocean of Telegram and Discord servers.
Recorded Future&#8217;s Alan Liska told Bleeping Computer that the overlap in the &#8220;tools, techniques and procedures&#8221; used by ShinyHunters and the Scattered Spider extortion group likely indicate some crossover between the two groups.
To muddy the waters even further, on August 28 a Telegram channel that now has nearly 40,000 subscribers was launched under the intentionally confusing banner &#8220;Scattered LAPSUS$ Hunters 4.0,&#8221; wherein participants have repeatedly claimed responsibility for the Salesloft hack without actually sharing any details to prove their claims.
The Telegram group has been trying to attract media attention by threatening security researchers at Google and other firms. It also is using the channel&#8217;s sudden popularity to promote a new cybercrime forum called &#8220;Breachstars,&#8221; which they claim will soon host data stolen from victim companies who refuse to negotiate a ransom payment.
The &#8220;Scattered Lapsus$ Hunters 4.0&#8221; channel on Telegram now has roughly 40,000 subscribers.
But Austin Larsen, a principal threat analyst at Google&#8217;s threat intelligence group, said there is no compelling evidence to attribute the Salesloft activity to ShinyHunters or to other known groups at this time.
&#8220;Their understanding of the incident seems to come from public reporting alone,&#8221; Larsen told KrebsOnSecurity, referring to the most active participants in the Scattered LAPSUS$ Hunters 4.0 Telegram channel.
Joshua Wright, a senior technical director at Counter Hack,¬†is credited with coining the term &#8220;authorization sprawl&#8221; to describe one key reason that social engineering attacks from groups like Scattered Spider and ShinyHunters so often succeed: They abuse legitimate user access tokens to move seamlessly between on-premises and cloud systems.
Wright said this type of attack chain often goes undetected because the attacker sticks to the resources and access already allocated to the user.
&#8220;Instead of the conventional chain of initial access, privilege escalation and endpoint bypass, these threat actors are using centralized identity platforms that offer single sign-on (SSO) and integrated authentication and authorization schemes,&#8221; Wright wrote in a June 2025 column. &#8220;Rather than creating custom malware, attackers use the resources already available to them as authorized users.&#8221;
It remains unclear exactly how the attackers gained access to all Salesloft Drift authentication tokens. Salesloft announced on August 27 that it hired Mandiant, Google Cloud&#8217;s incident response division, to investigate the root cause(s).
&#8220;We are working with Salesloft Drift to investigate the root cause of what occurred and then it‚Äôll be up to them to publish that,&#8221; Mandiant Consulting CTO Charles Carmakal told Cyberscoop. &#8220;There will be a lot more tomorrow, and the next day, and the next day.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Train and deploy models on Amazon SageMaker HyperPod using the new HyperPod CLI and SDK
  Training and deploying large AI models requires advanced distributed computing capabilities, but managing these distributed systems shouldn‚Äôt be complex for data scientists and machine learning (ML) practitioners. The newly released command line interface (CLI) and software development kit (SDK) for Amazon SageMaker HyperPod simplify how you can use the service‚Äôs distributed training and inference capabilities. 
The SageMaker HyperPod CLI provides data scientists with an intuitive command-line experience, abstracting away the underlying complexity of distributed systems. Built on top of the SageMaker HyperPod SDK, the CLI offers straightforward commands for common workflows like launching training or fine-tuning jobs, deploying inference endpoints, and monitoring cluster performance. This makes it ideal for quick experimentation and iteration. 
For more advanced use cases requiring fine-grained control, the SageMaker HyperPod SDK enables programmatic access to customize your ML workflows. Developers can use the SDK‚Äôs Python interface to precisely configure training and deployment parameters while maintaining the simplicity of working with familiar Python objects. 
In this post, we demonstrate how to use both the CLI and SDK to train and deploy large language models (LLMs) on SageMaker HyperPod. We walk through practical examples of distributed training using Fully Sharded Data Parallel (FSDP) and model deployment for inference, showcasing how these tools streamline the development of production-ready generative AI applications. 
Prerequisites 
To follow the examples in this post, you must have the following prerequisites: 
 
 An AWS account with access to SageMaker HyperPod, Amazon Simple Storage Service (Amazon S3) and Amazon FSx for Lustre. 
 A local environment (either your local machine or a cloud-based compute environment) from which to run the SageMaker HyperPod CLI commands, configured as follows: 
   
   Operating system based on Linux or MacOS. 
   Python 3.8, 3.9, 3.10 or 3.11 installed. 
   The AWS Command Line Interface (AWS CLI) configured with the appropriate credentials to use the aforementioned services. 
    
 A SageMaker HyperPod cluster orchestrated through Amazon Elastic Kubernetes Service (Amazon EKS) running with an instance group configured with 8 ml.g5.8xlarge instances. For more information on how to create and configured a new SageMaker HyperPod cluster, refer to Creating a SageMaker HyperPod cluster with Amazon EKS orchestration. 
 An FSx for Lustre persistent volume claim (PVC) to store checkpoints. This can be created either at cluster creation time or separately. 
 
Because the use cases that we demonstrate are about training and deploying LLMs with the SageMaker HyperPod CLI and SDK, you must also install the following Kubernetes operators in the cluster: 
 
 HyperPod training operator ‚Äì For installation instructions, see Installing the training operator. 
 HyperPod inference operator ‚Äì For installation instructions, see Setting up your HyperPod clusters for model deployment and the corresponding notebook. 
 
Install the SageMaker HyperPod CLI 
First, you must install the latest version of the SageMaker HyperPod CLI and SDK (the examples in this post are based on version 3.1.0). From the local environment, run the following command (you can also install in a Python virtual environment): 
 
 # Install the HyperPod CLI and SDK
pip install sagemaker-hyperpod 
 
This command sets up the tools needed to interact with SageMaker HyperPod clusters. For an existing installation, make sure you have the latest version of the package installed (sagemaker-hyperpod&gt;=3.1.0) to be able to use the relevant set of features. To verify if the CLI is installed correctly, you can run the hyp command and check the outputs: 
 
 # Check if the HyperPod CLI is correctly installed
hyp 
 
The output will be similar to the following, and includes instructions on how to use the CLI: 
 
 Usage: hyp [OPTIONS] COMMAND [ARGS]...

Options:
&nbsp;&nbsp;--help &nbsp;Show this message and exit.

Commands:
&nbsp;&nbsp;create &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Create endpoints or pytorch jobs.
&nbsp;&nbsp;delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Delete endpoints or pytorch jobs.
&nbsp;&nbsp;describe &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Describe endpoints or pytorch jobs.
&nbsp;&nbsp;get-cluster-context &nbsp;Get context related to the current set cluster.
&nbsp;&nbsp;get-logs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Get pod logs for endpoints or pytorch jobs.
&nbsp;&nbsp;get-monitoring &nbsp; &nbsp; &nbsp; Get monitoring configurations for Hyperpod cluster.
&nbsp;&nbsp;get-operator-logs &nbsp; &nbsp;Get operator logs for endpoints.
&nbsp;&nbsp;invoke &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Invoke model endpoints.
&nbsp;&nbsp;list &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; List endpoints or pytorch jobs.
&nbsp;&nbsp;list-cluster &nbsp; &nbsp; &nbsp; &nbsp; List SageMaker Hyperpod Clusters with metadata.
&nbsp;&nbsp;list-pods &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;List pods for endpoints or pytorch jobs.
&nbsp;&nbsp;set-cluster-context &nbsp;Connect to a HyperPod EKS cluster. 
 
For more information on CLI usage and the available commands and respective parameters, refer to the CLI reference documentation. 
Set the cluster context 
The SageMaker HyperPod CLI and SDK use the Kubernetes API to interact with the cluster. Therefore, make sure the underlying Kubernetes Python client is configured to execute API calls against your cluster by setting the cluster context. 
Use the CLI to list the clusters available in your AWS account: 
 
 # List all HyperPod clusters in your AWS account
hyp list-cluster
[
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Cluster": "ml-cluster",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Instances": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.g5.8xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"TotalNodes": 8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"AcceleratorDevicesAvailable": 8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"NodeHealthStatus=Schedulable": 8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"DeepHealthCheckStatus=Passed": "N/A"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.m5.12xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"TotalNodes": 1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"AcceleratorDevicesAvailable": "N/A",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"NodeHealthStatus=Schedulable": 1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"DeepHealthCheckStatus=Passed": "N/A"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
] 
 
Set the cluster context specifying the cluster name as input (in our case, we use ml-cluster as &lt;cluster_name&gt;): 
 
 # Set the cluster context for subsequent commands
hyp set-cluster-context --cluster-name&nbsp;&lt;cluster_name&gt; 
 
Train models with the SageMaker HyperPod CLI and SDK 
The SageMaker HyperPod CLI provides a straightforward way to submit PyTorch model training and fine-tuning jobs to a SageMaker HyperPod cluster. In the following example, we schedule a Meta Llama 3.1 8B model training job with FSDP. 
The CLI executes training using the HyperPodPyTorchJob Kubernetes custom resource, which is implemented by the HyperPod training operator, that needs to be installed in the cluster as discussed in the prerequisites section. 
First, clone the awsome-distributed-training repository and create the Docker image that you will use for the training job: 
 
 cd ~
git clone https://github.com/aws-samples/awsome-distributed-training/
cd awsome-distributed-training/3.test_cases/pytorch/FSDP 
 
Then, log in to the Amazon Elastic Container Registry (Amazon ECR) to pull the base image and build the new container: 
 
 export&nbsp;AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')
export&nbsp;ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
export&nbsp;REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/
docker build -f Dockerfile -t ${REGISTRY}fsdp:pytorch2.7.1 . 
 
The Dockerfile in the awsome-distributed-training repository referenced in the preceding code already contains the HyperPod elastic agent, which orchestrates lifecycles of training workers on each container and communicates with the HyperPod training operator. If you‚Äôre using a different Dockerfile, install the HyperPod elastic agent following the instructions in HyperPod elastic agent. 
Next, create a new registry for your training image if needed and push the built image to it: 
 
 # Create registry if needed
REGISTRY_COUNT=$(aws ecr describe-repositories | grep "fsdp" | wc -l)
if [ "$REGISTRY_COUNT" -eq 0 ]; then
&nbsp;&nbsp; &nbsp;aws ecr create-repository --repository-name fsdp
fi

# Login to registry
echo "Logging in to $REGISTRY ..."
aws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY

# Push image to registry
docker image push ${REGISTRY}fsdp:pytorch2.7.1 
 
After you have successfully created the Docker image, you can submit the training job using the SageMaker HyperPod CLI. 
Internally, the SageMaker HyperPod CLI will use the Kubernetes Python client to build a HyperPodPyTorchJob custom resource and then create it on the Kubernetes the cluster. 
You can modify the CLI command for other Meta Llama configurations by exchanging the --args to the desired arguments and values; examples can be found in the Kubernetes manifests in the awsome-distributed-training repository. 
In the given configuration, the training job will write checkpoints to /fsx/checkpoints on the FSx for Lustre PVC. 
 
 hyp create hyp-pytorch-job \
&nbsp;&nbsp; &nbsp;--job-name fsdp-llama3-1-8b \
&nbsp;&nbsp; &nbsp;--image ${REGISTRY}fsdp:pytorch2.7.1 \
&nbsp;&nbsp; &nbsp;--command '[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;hyperpodrun,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--tee=3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--log_dir=/tmp/hyperpod,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--nproc_per_node=1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--nnodes=8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;/fsdp/train.py
&nbsp;&nbsp; &nbsp;]' \
&nbsp;&nbsp; &nbsp;--args '[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--max_context_width=8192,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--num_key_value_heads=8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--intermediate_size=14336,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--hidden_width=4096,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--num_layers=32,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--num_heads=32,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--model_type=llama_v3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--tokenizer=hf-internal-testing/llama-tokenizer,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--checkpoint_freq=50,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--validation_freq=25,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--max_steps=50,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--checkpoint_dir=/fsx/checkpoints,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--dataset=allenai/c4,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--dataset_config_name=en,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--resume_from_checkpoint=/fsx/checkpoints,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--train_batch_size=1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--val_batch_size=1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--sharding_strategy=full,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--offload_activations=1
&nbsp;&nbsp; &nbsp;]' \
&nbsp;&nbsp; &nbsp;--environment '{"PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:32"}' \
&nbsp;&nbsp; &nbsp;--pull-policy "IfNotPresent" \
&nbsp;&nbsp; &nbsp;--instance-type ml.g5.8xlarge \
&nbsp;&nbsp; &nbsp;--node-count 8 \
&nbsp;&nbsp; &nbsp;--tasks-per-node 1 \
&nbsp;&nbsp; &nbsp;--deep-health-check-passed-nodes-only false \
&nbsp;&nbsp; &nbsp;--max-retry 3 \
&nbsp;&nbsp; &nbsp;--volume name=shmem,type=hostPath,mount_path=/dev/shm,path=/dev/shm,read_only=false&nbsp;\
&nbsp; &nbsp;&nbsp;--volume name=fsx,type=pvc,mount_path=/fsx,claim_name=fsx-claim,read_only=false 
 
The hyp create hyp-pytorch-job command supports additional arguments, which can be discovered by running the following: 
 
 hyp create hyp-pytorch-job --help 
 
The preceding example code contains the following relevant arguments: 
 
 --command and --args offer flexibility in setting the command to be executed in the container. The command executed is hyperpodrun, implemented by the HyperPod elastic agent that is installed in the training container. The HyperPod elastic agent extends PyTorch‚Äôs ElasticAgent and manages the communication of the various workers with the HyperPod training operator. For more information, refer to HyperPod elastic agent. 
 --environment defines environment variables and customizes the training execution. 
 --max-retry indicates the maximum number of restarts at the process level that will be attempted by the HyperPod training operator. For more information, refer to Using the training operator to run jobs. 
 --volume is used to map persistent or ephemeral volumes to the container. 
 
If successful, the command will output the following: 
 
 Using version: 1.0
2025-08-12&nbsp;10:03:03,270 - sagemaker.hyperpod.training.hyperpod_pytorch_job - INFO - Successfully submitted HyperPodPytorchJob 'fsdp-llama3-1-8b'! 
 
You can observe the status of the training job through the CLI. Running hyp list hyp-pytorch-job will show the status first as Created and then as Running after the containers have been started: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NAMESPACE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; &nbsp; &nbsp; &nbsp; AGE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
--------------------------------------------------------------------------------
fsdp-llama3-1-8b &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Running &nbsp; &nbsp; &nbsp; &nbsp;6m &nbsp; &nbsp; &nbsp; &nbsp; 
 
To list the pods that are created by this training job, run the following command: 
 
 hyp list-pods hyp-pytorch-job --job-name fsdp-llama3-1-8b
Pods for job: fsdp-llama3-1-8b

POD NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NAMESPACE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
----------------------------------------------------------------------
fsdp-llama3-1-8b-pod-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
fsdp-llama3-1-8b-pod-1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-6&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;
fsdp-llama3-1-8b-pod-7&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
 
You can observe the logs of one of the training pods that get spawned by running the following command: 
 
 hyp get-logs hyp-pytorch-job --pod-name fsdp-llama3-1-8b-pod-0&nbsp;\
--job-name fsdp-llama3-1-8b
...
2025-08-12T14:59:25.069208138Z [HyperPodElasticAgent] 2025-08-12 14:59:25,069 [INFO] [rank0-restart0] /usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py:685: [default] Starting worker group 
2025-08-12T14:59:25.069301320Z [HyperPodElasticAgent] 2025-08-12 14:59:25,069 [INFO] [rank0-restart0] /usr/local/lib/python3.10/dist-packages/hyperpod_elastic_agent/hyperpod_elastic_agent.py:221: Starting workers with worker spec worker_group.spec=WorkerSpec(role='default', local_world_size=1, rdzv_handler=&lt;hyperpod_elastic_agent.rendezvous.hyperpod_rendezvous_backend.HyperPodRendezvousBackend object at 0x7f0970a4dc30&gt;, fn=None, entrypoint='/usr/bin/python3', args=('-u', '/fsdp/train.py', '--max_context_width=8192', '--num_key_value_heads=8', '--intermediate_size=14336', '--hidden_width=4096', '--num_layers=32', '--num_heads=32', '--model_type=llama_v3', '--tokenizer=hf-internal-testing/llama-tokenizer', '--checkpoint_freq=50', '--validation_freq=50', '--max_steps=100', '--checkpoint_dir=/fsx/checkpoints', '--dataset=allenai/c4', '--dataset_config_name=en', '--resume_from_checkpoint=/fsx/checkpoints', '--train_batch_size=1', '--val_batch_size=1', '--sharding_strategy=full', '--offload_activations=1'), max_restarts=3, monitor_interval=0.1, master_port=None, master_addr=None, local_addr=None)... 
2025-08-12T14:59:30.264195963Z [default0]:2025-08-12 14:59:29,968 [INFO] **main**: Creating Model 
2025-08-12T15:00:51.203541576Z [default0]:2025-08-12 15:00:50,781 [INFO] **main**: Created model with total parameters: 7392727040 (7.39 B) 
2025-08-12T15:01:18.139531830Z [default0]:2025-08-12 15:01:18 I [checkpoint.py:79] Loading checkpoint from /fsx/checkpoints/llama_v3-24steps ... 
2025-08-12T15:01:18.833252603Z [default0]:2025-08-12 15:01:18,081 [INFO] **main**: Wrapped model with FSDP 
2025-08-12T15:01:18.833290793Z [default0]:2025-08-12 15:01:18,093 [INFO] **main**: Created optimizer 
 
We elaborate on more advanced debugging and observability features at the end of this section. 
Alternatively, if you prefer a programmatic experience and more advanced customization options, you can submit the training job using the SageMaker HyperPod Python SDK. For more information, refer to the SDK reference documentation. The following code will yield the equivalent training job submission to the preceding CLI example: 
 
 import&nbsp;os
from&nbsp;sagemaker.hyperpod.training&nbsp;import&nbsp;HyperPodPytorchJob
from&nbsp;sagemaker.hyperpod.training&nbsp;import&nbsp;ReplicaSpec, Template, VolumeMounts, Spec, Containers, Resources, RunPolicy, Volumes, HostPath, PersistentVolumeClaim
from&nbsp;sagemaker.hyperpod.common.config&nbsp;import&nbsp;Metadata

REGISTRY&nbsp;=&nbsp;os.environ['REGISTRY']

# Define job specifications
nproc_per_node&nbsp;=&nbsp;"1"&nbsp;&nbsp;# Number of processes per node
replica_specs&nbsp;=&nbsp;[
&nbsp;&nbsp; &nbsp;ReplicaSpec(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name&nbsp;=&nbsp;"pod", &nbsp;# Replica name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;replicas&nbsp;=&nbsp;8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;template&nbsp;=&nbsp;Template(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;spec&nbsp;=&nbsp;Spec(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;containers&nbsp;=
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Containers(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Container name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="fsdp-training-container", &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Training image
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image=f"{REGISTRY}fsdp:pytorch2.7.1", &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Volume mounts
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;volume_mounts=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VolumeMounts(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="fsx",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mount_path="/fsx"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VolumeMounts(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="shmem", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mount_path="/dev/shm"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;env=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"name": "PYTORCH_CUDA_ALLOC_CONF", "value": "max_split_size_mb:32"},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Image pull policy
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_pull_policy="IfNotPresent",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;resources=Resources(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests={"nvidia.com/gpu": "1"}, &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;limits={"nvidia.com/gpu": "1"}, &nbsp; 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Command to run
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;command=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"hyperpodrun",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--tee=3",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--log_dir=/tmp/hyperpod",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--nproc_per_node=1",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--nnodes=8",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"/fsdp/train.py"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;], &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Script arguments
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;args&nbsp;=&nbsp;[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--max_context_width=8192',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--num_key_value_heads=8',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--intermediate_size=14336',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--hidden_width=4096',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--num_layers=32',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--num_heads=32',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--model_type=llama_v3',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--tokenizer=hf-internal-testing/llama-tokenizer',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--checkpoint_freq=2',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--validation_freq=25',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--max_steps=50',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--checkpoint_dir=/fsx/checkpoints',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--dataset=allenai/c4',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--dataset_config_name=en',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--resume_from_checkpoint=/fsx/checkpoints',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--train_batch_size=1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--val_batch_size=1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--sharding_strategy=full',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--offload_activations=1'
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;volumes&nbsp;=&nbsp;[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Volumes(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="fsx",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;persistent_volume_claim=PersistentVolumeClaim(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;claim_name="fsx-claim",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;read_only=False
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Volumes(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="shmem",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;host_path=HostPath(path="/dev/shm"),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;node_selector={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"node.kubernetes.io/instance-type": "ml.g5.8xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;)
]
run_policy&nbsp;=&nbsp;RunPolicy(clean_pod_policy="None", job_max_retry_count=3) &nbsp;
# Create and start the PyTorch job
pytorch_job&nbsp;=&nbsp;HyperPodPytorchJob(
&nbsp;&nbsp; &nbsp;# Job name
&nbsp;&nbsp; &nbsp;metadata&nbsp;=&nbsp;Metadata(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name="fsdp-llama3-1-8b", &nbsp; &nbsp; 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;namespace="default",
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;# Processes per node
&nbsp;&nbsp; &nbsp;nproc_per_node&nbsp;=&nbsp;nproc_per_node, &nbsp; 
&nbsp;&nbsp; &nbsp;# Replica specifications
&nbsp;&nbsp; &nbsp;replica_specs&nbsp;=&nbsp;replica_specs, &nbsp; &nbsp; &nbsp; &nbsp;
)
# Launch the job
pytorch_job.create() &nbsp; 
 
Debugging training jobs 
In addition to monitoring the training pod logs as described earlier, there are several other useful ways of debugging training jobs: 
 
 You can submit training jobs with an additional --debug True flag, which will print the Kubernetes YAML to the console when the job starts so it can be inspected by users. 
 You can view a list of current training jobs by running hyp list hyp-pytorch-job. 
 You can view the status and corresponding events of the job by running hyp describe hyp-pytorch-job ‚Äîjob-name fsdp-llama3-1-8b. 
 If the HyperPod observability stack is deployed to the cluster, run hyp get-monitoring --grafana and hyp get-monitoring --prometheus to get the Grafana dashboard and Prometheus workspace URLs, respectively, to view cluster and job metrics. 
 To monitor GPU utilization or view directory contents, it can be useful to execute commands or open an interactive shell into the pods. You can run commands in a pod by running, for example, kubectl exec -it&lt;pod-name&gt;-- nvtop to run nvtop for visibility into GPU utilization. You can open an interactive shell by running kubectl exec -it&lt;pod-name&gt;-- /bin/bash. 
 The logs of the HyperPod training operator controller pod can have valuable information about scheduling. To view them, run kubectl get pods -n aws-hyperpod | grep hp-training-controller-manager to find the controller pod name and run kubectl logs -n aws-hyperpod&lt;controller-pod-name&gt; to view the corresponding logs. 
 
Deploy models with the SageMaker HyperPod CLI and SDK 
The SageMaker HyperPod CLI provides commands to quickly deploy models to your SageMaker HyperPod cluster for inference. You can deploy both foundation models (FMs) available on Amazon SageMaker JumpStart as well as custom models with artifacts that are stored on Amazon S3 or FSx for Lustre file systems. 
This functionality will automatically deploy the chosen model to the SageMaker HyperPod cluster through Kubernetes custom resources, which are implemented by the HyperPod inference operator, that needs to be installed in the cluster as discussed in the prerequisites section. It is optionally possible to automatically create a SageMaker inference endpoint as well as an Application Load Balancer (ALB), which can be used directly using HTTPS calls with a generated TLS certificate to invoke the model. 
Deploy SageMaker JumpStart models 
You can deploy an FM that is available on SageMaker JumpStart with the following command: 
 
 hyp create hyp-jumpstart-endpoint \
&nbsp;&nbsp;--model-id deepseek-llm-r1-distill-qwen-1-5b \
&nbsp;&nbsp;--instance-type ml.g5.8xlarge \
&nbsp;&nbsp;--endpoint-name \
&nbsp;&nbsp;--tls-certificate-output-s3-uri s3://&lt;certificate-bucket&gt;/ \
&nbsp;&nbsp;--namespace&nbsp;default 
 
The preceding code includes the following parameters: 
 
 --model-id is the model ID in the SageMaker JumpStart model hub. In this example, we deploy a DeepSeek R1-distilled version of Qwen 1.5B, which is available on SageMaker JumpStart. 
 --instance-type is the target instance type in your SageMaker HyperPod cluster where you want to deploy the model. This instance type must be supported by the chosen model. 
 --endpoint-name is the name that the SageMaker inference endpoint will have. This name must be unique. SageMaker inference endpoint creation is optional. 
 --tls-certificate-output-s3-uri is the S3 bucket location where the TLS certificate for the ALB will be stored. This can be used to directly invoke the model through HTTPS. You can use S3 buckets that are accessible by the HyperPod inference operator IAM role. 
 --namespace is the Kubernetes namespace the model will be deployed to. The default value is set to default. 
 
The CLI supports more advanced deployment configurations, including auto scaling, through additional parameters, which can be viewed by running the following command: 
 
 hyp create hyp-jumpstart-endpoint --help 
 
If successful, the command will output the following: 
 
 Creating JumpStart model and sagemaker endpoint. Endpoint name: deepseek-distill-qwen-endpoint-cli.
&nbsp;The process may take a few minutes... 
 
After a few minutes, both the ALB and the SageMaker inference endpoint will be available, which can be observed through the CLI. Running hyp list hyp-jumpstart-endpoint will show the status first as DeploymentInProgress and then as DeploymentComplete when the endpoint is ready to be used: 
 
 | name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | namespace &nbsp; | labels &nbsp; | status &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
|------------------------------------|-------------|----------|--------------------|
| deepseek-distill-qwen-endpoint-cli | default &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| DeploymentComplete | 
 
To get additional visibility into the deployment pod, run the following commands to find the pod name and view the corresponding logs: 
 
 hyp list-pods hyp-jumpstart-endpoint&nbsp;--namespace &lt;namespace&gt;
hyp get-logs&nbsp;hyp-jumpstart-endpoint --namespace &lt;namespace&gt;&nbsp;--pod-name &lt;model-pod-name&gt; 
 
The output will look similar to the following: 
 
 2025-08-12T15:53:14.042031963Z WARN &nbsp;PyProcess W-195-model-stderr: Capturing CUDA graph shapes: 100%|??????????| 35/35 [00:18&lt;00:00, &nbsp;1.63it/s]
2025-08-12T15:53:14.042257357Z WARN &nbsp;PyProcess W-195-model-stderr: Capturing CUDA graph shapes: 100%|??????????| 35/35 [00:18&lt;00:00, &nbsp;1.94it/s]
2025-08-12T15:53:14.042297298Z INFO &nbsp;PyProcess W-195-model-stdout: INFO 08-12 15:53:14 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 26.18 seconds
2025-08-12T15:53:15.215357997Z INFO &nbsp;PyProcess Model [model] initialized.
2025-08-12T15:53:15.219205375Z INFO &nbsp;WorkerThread Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0)
2025-08-12T15:53:15.221591827Z INFO &nbsp;ModelServer Initialize BOTH server with: EpollServerSocketChannel.
2025-08-12T15:53:15.231404670Z INFO &nbsp;ModelServer BOTH API bind to: http://0.0.0.0:8080 
 
You can invoke the SageMaker inference endpoint you created through the CLI by running the following command: 
 
 hyp invoke hyp-jumpstart-endpoint \
&nbsp;&nbsp; &nbsp;--endpoint-name deepseek-distill-qwen-endpoint-cli \ &nbsp; &nbsp; &nbsp; 
&nbsp;&nbsp; &nbsp;--body '{"inputs":"What is the capital of USA?"}' 
 
You will get an output similar to the following: 
 
 {"generated_text": " What is the capital of France? What is the capital of Japan? What is the capital of China? What is the capital of Germany? What is"} 
 
Alternatively, if you prefer a programmatic experience and advanced customization options, you can use the SageMaker HyperPod Python SDK. The following code will yield the equivalent deployment to the preceding CLI example: 
 
 from&nbsp;sagemaker.hyperpod.inference.config.hp_jumpstart_endpoint_config&nbsp;import&nbsp;Model, Server, SageMakerEndpoint, TlsConfig
from&nbsp;sagemaker.hyperpod.inference.hp_jumpstart_endpoint&nbsp;import&nbsp;HPJumpStartEndpoint

model=Model(
&nbsp;&nbsp; &nbsp;model_id='deepseek-llm-r1-distill-qwen-1-5b',
)

server=Server(
&nbsp;&nbsp; &nbsp;instance_type='ml.g5.8xlarge',
)

endpoint_name=SageMakerEndpoint(name='deepseek-distill-qwen-endpoint-cli')

tls_config=TlsConfig(tls_certificate_output_s3_uri='s3://&lt;certificate-bucket&gt;')

js_endpoint=HPJumpStartEndpoint(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;server=server,
&nbsp;&nbsp; &nbsp;sage_maker_endpoint=endpoint_name,
&nbsp;&nbsp; &nbsp;tls_config=tls_config,
&nbsp;&nbsp; &nbsp;namespace="default"
)

js_endpoint.create()  
 
Deploy custom models 
You can also use the CLI to deploy custom models with model artifacts stored on either Amazon S3 or FSx for Lustre. This is useful for models that have been fine-tuned on custom data. You must provide the storage location of the model artifacts as well as a container image for inference that is compatible with the model artifacts and SageMaker inference endpoints. In the following example, we deploy a TinyLlama 1.1B model from Amazon S3 using the DJL Large Model Inference container image. 
In preparation, download the model artifacts locally and push them to an S3 bucket: 
 
 # Install huggingface-hub if not present on your machine
pip install huggingface-hub

# Download model
hf&nbsp;download TinyLlama/TinyLlama-1.1B-Chat-v1.0 --local-dir ./tinyllama-1.1b-chat

# Upload to S3
aws s3 cp ./tinyllama s3://&lt;model-bucket&gt;/models/tinyllama-1.1b-chat/ --recursive 
 
Now you can deploy the model with the following command: 
 
 hyp create hyp-custom-endpoint \
&nbsp; &nbsp; --endpoint-name my-custom-tinyllama-endpoint \
&nbsp; &nbsp; --model-name tinyllama \
&nbsp; &nbsp; --model-source-type s3 \
&nbsp; &nbsp; --model-location models/tinyllama-1.1b-chat/&nbsp;\
&nbsp; &nbsp; --s3-bucket-name &lt;model-bucket&gt; \
&nbsp; &nbsp; --s3-region &lt;model-bucket-region&gt;&nbsp;\
&nbsp; &nbsp; --instance-type ml.g5.8xlarge \
&nbsp; &nbsp; --image-uri 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128 \
&nbsp; &nbsp; --container-port 8080 \
&nbsp; &nbsp; --model-volume-mount-name modelmount \
&nbsp;&nbsp;&nbsp;&nbsp;--tls-certificate-output-s3-uri s3://&lt;certificate-bucket&gt;/ \
&nbsp;&nbsp;&nbsp;&nbsp;--namespace default 
 
The preceding code contains the following key parameters: 
 
 --model-name is the name of the model that will be created in SageMaker 
 --model-source-type specifies either fsx or s3 for the location of the model artifacts 
 --model-location specifies the prefix or folder where the model artifacts are located 
 --s3-bucket-name and ‚Äîs3-region specify the S3 bucket name and AWS Region, respectively 
 --instance-type, --endpoint-name, --namespace, and --tls-certificate behave the same as for the deployment of SageMaker JumpStart models 
 
Similar to SageMaker JumpStart model deployment, the CLI supports more advanced deployment configurations, including auto scaling, through additional parameters, which you can view by running the following command: 
 
 hyp create hyp-custom-endpoint --help 
 
If successful, the command will output the following: 
 
 Creating sagemaker model and endpoint. Endpoint name: my-custom-tinyllama-endpoint.
&nbsp;The process may take a few minutes... 
 
After a few minutes, both the ALB and the SageMaker inference endpoint will be available, which you can observe through the CLI. Running hyp list hyp-custom-endpoint will show the status first as DeploymentInProgress and as DeploymentComplete when the endpoint is ready to be used: 
 
 | name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | namespace &nbsp; | labels &nbsp; | status &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
|------------------------------|-------------|----------|----------------------|
| my-custom-tinyllama-endpoint | default &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| DeploymentComplete&nbsp; &nbsp;| 
 
To get additional visibility into the deployment pod, run the following commands to find the pod name and view the corresponding logs: 
 
 hyp list-pods hyp-custom-endpoint&nbsp;--namespace &lt;namespace&gt;
hyp get-logs&nbsp;hyp-custom-endpoint --namespace &lt;namespace&gt;&nbsp;--pod-name &lt;model-pod-name&gt; 
 
The output will look similar to the following: 
 
 ‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:36 [monitor.py:33] torch.compile takes 29.18 s in total &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:37 [kv_cache_utils.py:634] GPU KV cache size: 809,792 tokens &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:37 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 395.41x &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:59 [gpu_model_runner.py:1626] Graph capturing finished in 22 secs, took 0.37 GiB &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:59 [core.py:163] init engine (profile, create kv cache, warmup model) took 59.39 seconds &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:59 [core_client.py:435] Core engine process 0 ready. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess Model [model] initialized. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;WorkerThread Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;ModelServer Initialize BOTH server with: EpollServerSocketChannel. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;ModelServer BOTH API bind to: http://0.0.0.0:8080&nbsp; 
 
You can invoke the SageMaker inference endpoint you created through the CLI by running the following command: 
 
 hyp invoke hyp-custom-endpoint \
&nbsp;&nbsp; &nbsp;--endpoint-name my-custom-tinyllama-endpoint \ &nbsp; &nbsp; &nbsp; 
&nbsp;&nbsp; &nbsp;--body '{"inputs":"What is the capital of USA?"}' 
 
You will get an output similar to the following: 
 
 {"generated_text": " What is the capital of France? What is the capital of Japan? What is the capital of China? What is the capital of Germany? What is"} 
 
Alternatively, you can deploy using the SageMaker HyperPod Python SDK. The following code will yield the equivalent deployment to the preceding CLI example: 
 
 from sagemaker.hyperpod.inference.config.hp_endpoint_config import S3Storage, ModelSourceConfig, TlsConfig, EnvironmentVariables, ModelInvocationPort, ModelVolumeMount, Resources, Worker
from sagemaker.hyperpod.inference.hp_endpoint import HPEndpoint

model_source_config = ModelSourceConfig(
&nbsp;&nbsp; &nbsp;model_source_type='s3',
&nbsp;&nbsp; &nbsp;model_location="models/tinyllama-1.1b-chat/",
&nbsp;&nbsp; &nbsp;s3_storage=S3Storage(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;bucket_name='&lt;model-bucket&gt;',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;region='&lt;model-bucket-region&gt;',
&nbsp;&nbsp; &nbsp;),
)

worker = Worker(
&nbsp;&nbsp; &nbsp;image='763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128',
&nbsp;&nbsp; &nbsp;model_volume_mount=ModelVolumeMount(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name='modelmount',
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;model_invocation_port=ModelInvocationPort(container_port=8080),
&nbsp;&nbsp; &nbsp;resources=Resources(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests={"cpu": "30000m", "nvidia.com/gpu": 1, "memory": "100Gi"},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;limits={"nvidia.com/gpu": 1}
&nbsp;&nbsp; &nbsp;),
)

tls_config = TlsConfig(tls_certificate_output_s3_uri='s3://&lt;certificate-bucket&gt;/')

custom_endpoint = HPEndpoint(
&nbsp;&nbsp; &nbsp;endpoint_name='my-custom-tinyllama-endpoint',
&nbsp;&nbsp; &nbsp;instance_type='ml.g5.8xlarge',
&nbsp;&nbsp; &nbsp;model_name='tinyllama', &nbsp;
&nbsp;&nbsp; &nbsp;tls_config=tls_config,
&nbsp;&nbsp; &nbsp;model_source_config=model_source_config,
&nbsp;&nbsp; &nbsp;worker=worker,
)

custom_endpoint.create() 
 
Debugging inference deployments 
In addition to the monitoring of the inference pod logs, there are several other useful ways of debugging inference deployments: 
 
 You can access the HyperPod inference operator controller logs through the SageMaker HyperPod CLI. Run hyp get-operator-logs&lt;hyp-custom-endpoint/hyp-jumpstart-endpoint&gt;‚Äîsince-hours 0.5 to access the operator logs for custom and SageMaker JumpStart deployments, respectively. 
 You can view a list of inference deployments by running hyp list&lt;hyp-custom-endpoint/hyp-jumpstart-endpoint&gt;. 
 You can view the status and corresponding events of deployments by running hyp describe&lt;hyp-custom-endpoint/hyp-jumpstart-endpoint&gt;--name&lt;deployment-name&gt; to view the status and events for custom and SageMaker JumpStart deployments, respectively. 
 If the HyperPod observability stack is deployed to the cluster, run hyp get-monitoring --grafana and hyp get-monitoring --prometheus to get the Grafana dashboard and Prometheus workspace URLs, respectively, to view inference metrics as well. 
 To monitor GPU utilization or view directory contents, it can be useful to execute commands or open an interactive shell into the pods. You can run commands in a pod by running, for example, kubectl exec -it&lt;pod-name&gt;-- nvtop to run nvtop for visibility into GPU utilization. You can open an interactive shell by running kubectl exec -it&lt;pod-name&gt;-- /bin/bash. 
 
For more information on the inference deployment features in SageMaker HyperPod, see Amazon SageMaker HyperPod launches model deployments to accelerate the generative AI model development lifecycle and Deploying models on Amazon SageMaker HyperPod. 
Clean up 
To delete the training job from the corresponding example, use the following CLI command: 
 
 hyp delete hyp-pytorch-job --job-name fsdp-llama3-1-8b 
 
To delete the model deployments from the inference example, use the following CLI commands for SageMaker JumpStart and custom model deployments, respectively: 
 
 hyp delete hyp-jumpstart-endpoint --name deepseek-distill-qwen-endpoint-cli
hyp delete&nbsp;hyp-custom-endpoint --name&nbsp;my-custom-tinyllama-endpoint 
 
To avoid incurring ongoing costs for the instances running in your cluster, you can scale down the instances or delete instances. 
Conclusion 
The new SageMaker HyperPod CLI and SDK can significantly streamline the process of training and deploying large-scale AI models. Through the examples in this post, we‚Äôve demonstrated how these tools provide the following benefits: 
 
 Simplified workflows ‚Äì The CLI offers straightforward commands for common tasks like distributed training and model deployment, making powerful capabilities of SageMaker HyperPod accessible to data scientists without requiring deep infrastructure knowledge. 
 Flexible development options ‚Äì Although the CLI handles common scenarios, the SDK enables fine-grained control and customization for more complex requirements, so developers can programmatically configure every aspect of their distributed ML workloads. 
 Comprehensive observability ‚Äì Both interfaces provide robust monitoring and debugging capabilities through system logs and integration with the SageMaker HyperPod observability stack, helping quickly identify and resolve issues during development. 
 Production-ready deployment ‚Äì The tools support end-to-end workflows from experimentation to production, including features like automatic TLS certificate generation for secure model endpoints and integration with SageMaker inference endpoints. 
 
Getting started with these tools is as simple as installing the sagemaker-hyperpod package. The SageMaker HyperPod CLI and SDK provide the right level of abstraction for both data scientists looking to quickly experiment with distributed training and ML engineers building production systems. 
For more information about SageMaker HyperPod and these development tools, refer to the SageMaker HyperPod CLI and SDK documentation or explore the example notebooks. 
 
About the authors 
Giuseppe Angelo Porcelli&nbsp;is a Principal Machine Learning Specialist Solutions Architect for Amazon Web Services. With several years of software engineering and an ML background, he works with customers of any size to understand their business and technical needs and design AI and ML solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He has worked on projects in different domains, including MLOps, computer vision, and NLP, involving a broad set of AWS services. In his free time, Giuseppe enjoys playing football. 
Shweta Singh is a Senior Product Manager in the Amazon SageMaker Machine Learning platform team at AWS, leading the SageMaker Python SDK. She has worked in several product roles in Amazon for over 5 years. She has a Bachelor of Science degree in Computer Engineering and a Masters of Science in Financial Engineering, both from New York University. 
Nicolas Jourdan is a Specialist Solutions Architect at AWS, where he helps customers unlock the full potential of AI and ML in the cloud. He holds a PhD in Engineering from TU Darmstadt in Germany, where his research focused on the reliability, concept drift detection, and MLOps of industrial ML applications. Nicolas has extensive hands-on experience across industries, including autonomous driving, drones, and manufacturing, having worked in roles ranging from research scientist to engineering manager. He has contributed to award-winning research, holds patents in object detection and anomaly detection, and is passionate about applying cutting-edge AI to solve complex real-world problems.
‚Ä¢ Build a serverless Amazon Bedrock batch job orchestration workflow using AWS Step Functions
  As organizations increasingly adopt foundation models (FMs) for their artificial intelligence and machine learning (AI/ML) workloads, managing large-scale inference operations efficiently becomes crucial. Amazon Bedrock supports two general types of large-scale inference patterns: real-time inference and batch inference for use cases that involve processing massive datasets where immediate results aren‚Äôt necessary. 
Amazon Bedrock batch inference is a cost-effective solution that offers a 50% discount compared to on-demand processing, making it ideal for high-volume, time-insensitive workloads. However, implementing batch inference at scale comes with its own set of challenges, including managing input formatting and job quotas, orchestrating concurrent executions, and handling postprocessing tasks. Developers need a robust framework to streamline these operations. 
In this post, we introduce a flexible and scalable solution that simplifies the batch inference workflow. This solution provides a highly scalable approach to managing your FM batch inference needs, such as generating embeddings for millions of documents or running custom evaluation or completion tasks with large datasets. 
Solution overview 
The following diagram details a broad overview of the automated workflow, which includes three main phases: preprocessing of input datasets (for example, prompt formatting), execution of batch inference jobs in parallel, and postprocessing to parse the model outputs. 
 
This solution provides a flexible and scalable framework to simplify batch orchestration. Given a simple configuration input, the Step Functions state machine deployed in this AWS Cloud Development Kit (AWS CDK) stack handles preprocessing the dataset, launching parallel batch jobs, and postprocessing the output. 
In our specific use case, we use 2.2 million rows of data from the open source dataset SimpleCoT. The SimpleCoT dataset on Hugging Face is a collection of diverse task-oriented examples designed to demonstrate and train chain-of-thought (CoT) reasoning in language models. This dataset encompasses a wide range of problem types, including reading comprehension, mathematical reasoning, logical deduction, and natural language processing (NLP) tasks. The dataset is structured with each entry containing a task description, question, the correct answer, and a detailed explanation of the reasoning process. 
The following diagram illustrates the solution architecture. 
 
The Amazon Bedrock batch orchestration pattern uses scalable and serverless components to cover the key architectural considerations specific to batch processing workflows: 
 
 File format and storage ‚Äì Job inputs must be structured as JSONL files stored in an Amazon Simple Storage Service (Amazon S3) bucket, with each line representing a single input record that matches the API request structure for that FM or provider. For example, Anthropic‚Äôs Claude models have a different JSON structure compared to Amazon Titan Text Embeddings V2. There are also quotas to consider: at the time of writing, a minimum of 1,000 and maximum of 50,000 records per batch. You can request a quota increase using Service Quotas based on your use case requirements. 
 Step Functions state machine ‚Äì Orchestration of the asynchronous, long-running jobs requires a robust control flow system. Our architecture uses Step Functions to coordinate the overall process, with Amazon DynamoDB maintaining the inventory of individual jobs and their states. Again, there are important quota considerations: for example, the maximum sum of in-progress and submitted batch inference jobs using a base model for Amazon Titan Text Embeddings V2 is currently 20 per AWS Region. Using Map workflow states, Step Functions can help maximize throughput by controlling job submission and monitoring completion status. 
 Postprocessing ‚Äì Finally, you will likely want to perform some light postprocessing on the batch outputs (also JSONL files in Amazon S3) to parse the responses and join the output back to the original input. For example, when generating text embeddings, you must have a mechanism to map output vectors back to their source text. These configurable AWS Lambda functions are triggered as part of the Step Functions workflow after batch results arrive in Amazon S3. 
 
In the following sections, we walk through the steps to deploy the AWS CDK stack to your AWS environment. 
Prerequisites 
Complete the following prerequisite steps: 
 
 Install node and npm. 
 Install the AWS CDK: 
 
 
 npm install -g aws-cdk 
 
 
 Clone the GitHub repository into your local development environment: 
 
 
 git clone https://github.com/aws-samples/amazon-bedrock-samples
cd poc-to-prod/bedrock-batch-orchestrator 
 
Deploy the solution 
Install the required packages with the following code:npm i 
Check the prompt_templates.py file and add a new prompt template to prompt_id_to_template for your desired use case. 
prompt_id_to_template is a dict where the key is the prompt_id (allowing you to associate a given job with a particular prompt). Formatting keys in the prompt string template must also exist in your input file. For example, consider the following prompt template: 
 
 You are an AI assistant tasked with providing accurate and justified answers to users' questions.
    
You will be given a task, and you should respond with a chain-of-thought surrounded by &lt;thinking&gt; tags, then a final answer in &lt;answer&gt; tags.

For example, given the following task:

&lt;task&gt;
You are given an original reference as well as a system generated reference. Your task is to judge the naturaleness of the system generated reference. If the utterance could have been produced by a native speaker output 1, else output 0. System Reference: may i ask near where? Original Reference: where do you need a hotel near?.
&lt;/task&gt;

&lt;thinking&gt;
The utterance "may i ask near where?" is not natural. 
This utterance does not make sense grammatically.
Thus we output 0.
&lt;/thinking&gt;

&lt;answer&gt;0&lt;/answer&gt;

Your turn. Please respond to the following task:

&lt;task&gt;
{source}
&lt;/task&gt; 
 
You must make sure your input dataset has a column for each formatting key (for example, source in the preceding example code). 
Prompt templates are not used for embedding model-based jobs.Deploy the AWS CDK stack with the following code:npm run cdk deploy 
Take note of the AWS CloudFormation outputs denoting the names of the bucket and Step Functions workflow: 
 
 ‚úÖ BedrockBatchOrchestratorStack

‚ú® Deployment time: 23.16s

Outputs:
BedrockBatchOrchestratorStack.bucketName = batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;
BedrockBatchOrchestratorStack.stepFunctionName = bedrockBatchOrchestratorSfnE5E2B976-4yznxekguxxm
Stack ARN:
arn:aws:cloudformation:us-east-1:&lt;YOUR_ACCOUNT_ID&gt;:stack/BedrockBatchOrchestratorStack/0787ba80-b0cb-11ef-a481-0affd4b49c99

‚ú® Total time: 26.74s 
 
Job input structure 
As your input dataset, you can either use a Hugging Face dataset ID or point directly to a dataset in Amazon S3 (CSV or Parquet formats are supported at the time of writing). The source of the input dataset and the type of model (text generation or embedding) dictate the structure of the Step Functions input. 
Hugging Face dataset 
For a Hugging Face dataset, reference a dataset ID (for example, w601sxs/simpleCoT) and split (for example, train), and your dataset will be pulled directly from Hugging Face Hub. 
 
The question_answering prompt template in prompt_templates.py has a formatting key called source to match the name of the appropriate column in the referenced dataset (see the preceding example). We use this prompt to generate the rationale and answer for each of the 2.2 million rows in the dataset. See the following code: 
 
 {
  "job_name_prefix": "full-cot-job",
  "model_id": "us.anthropic.claude-3-5-haiku-20241022-v1:0",
  "prompt_id": "question_answering",
  "dataset_id": "w601sxs/simpleCoT",
  "split": "train",
  "max_records_per_job": 50000
} 
 
We also have optional keys for max_num_jobs (to limit the total number of jobs, which is useful for testing on a smaller scale) and max_records_per_batch. 
Amazon S3 dataset 
Upload an input CSV or parquet file to the S3 bucket and copy the S3 URI. For example:aws s3 cp topics.csv s3://batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;/inputs/jokes/topics.csv 
Open your Step Functions state machine on the Step Functions console and submit an input with the following structure. You must supply an s3_uri for S3 datasets. 
For example, for Anthropic models with an Amazon S3 input, use the following code: 
 
 {
"s3_uri": "s3://batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;/inputs/jokes/topics.csv",
"job_name_prefix": "test-joke-job1",
"model_id": "anthropic.claude-3-haiku-20240307-v1:0",
"prompt_id": "joke_about_topic"
} 
 
The prompt_id of joke_about_topic maps to a prompt template in prompt_templates.py, which has a formatting key for topic, which must be one of the columns in the input CSV file. 
Generate batch embeddings 
To generate embeddings with a model like Amazon Titan Text Embeddings V2, you don‚Äôt need to provide a prompt_id, but you do need to make sure your input CSV file has a column called input_text with the text you want to embed. For example: 
 
 {
"s3_uri": "s3://batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;/inputs/embeddings/embedding_input.csv",
"job_name_prefix": "test-embeddings-job1",
"model_id": "amazon.titan-embed-text-v2:0",
"prompt_id": null
} 
 
Step Functions workflow 
The following diagram shows an example of a successful Step Functions workflow execution. 
 
When a Step Functions state machine is initiated, it completes the following steps: 
 
 Preprocess input datasets to prepare batch job inputs for your particular model ID and prompt template. The BaseProcessor abstract class can quickly be extended for other model providers, such as Meta Llama 3 or Amazon Nova. 
 Orchestrate batch jobs in an event-driven fashion. We maintain an internal inventory of jobs in a DynamoDB table and keep it updated when Amazon Bedrock emits events related to job status changes. These updates are then transmitted back to the step function using the Wait for Task Token Callback integration pattern. Using a SFN Map, we make sure that the maximum capacity of concurrent jobs is maintained until the records have been processed. 
 Run concurrent postprocessing of batch outputs to perform some light parsing and merge model responses back to the original input data using the recordId field as a join key. The output data depends on the kind of model you use. For text-based models, the output string will be in a new column called response. 
 
Monitor your state machine as it runs the jobs. The maximum number of concurrent jobs is controlled by an AWS CDK context variable in cdk.json (key: maxConcurrentJobs). The paths to your resulting Parquet files will be aggregated in the outputs from the execution. 
The output Parquet files will contain the same columns as your input file alongside the generated responses. 
For text generation models, the output string will be in a new column called response, as shown in the following screenshot of a sample output. 
 
For embedding models, the output (list of floats) will be in a new column called embedding, as shown in the following screenshot. 
 
There are no guaranteed SLAs for the Batch Inference API. Runtimes will vary based on the demand of the desired model at the time of your request. For example, to process the 2.2 million records in the SimpleCoT dataset, execution was spread across 45 individual processing jobs, with a maximum of 20 concurrent jobs at a given time. In our experiment with Anthropic‚Äôs Claude Haiku 3.5 in the us-east-1 Region, each individual job execution took an average of 9 hours, for a total end-to-end processing time of about 27 hours. 
Clean up 
To avoid incurring additional costs, you can clean up the stack‚Äôs resources by running cdk destroy. 
Conclusion 
In this post, we outlined a serverless architecture for performing large-scale batch processing using Amazon Bedrock batch inference. We explored using the solution for various use cases, including large-scale data labeling and embedding generation. You can also generate a large amount synthetic data from a teacher model used to train a student model as part of model distillation process. 
The solution is publicly available in the GitHub repo. We can‚Äôt wait to see how you put this architecture to work for your use cases. 
 
About the authors 
Swagat Kulkarni is a Senior Solutions Architect at AWS and an active Generative AI practitioner. He is passionate about helping customers solve real-world challenges using cloud-native services and machine learning. With a strong background in driving digital transformation across diverse industries, Swagat has delivered impactful solutions that enable innovation and scale. Outside of work, he enjoys traveling, reading, and cooking. 
Evan Diewald is a Data &amp; Machine Learning Engineer with AWS Professional Services, where he helps AWS customers develop and deploy ML solutions in a variety of industry verticals. Prior to joining AWS, he received an M.S. from Carnegie Mellon University, where he conducted research at the intersection of advanced manufacturing and AI. Outside of work, he enjoys mountain biking and rock climbing. 
Shreyas Subramanian&nbsp;is a Principal Data Scientist and helps customers by using Generative AI and deep learning to solve their business challenges using AWS services like Amazon Bedrock and AgentCore. Dr. Subramanian contributes to cutting-edge research in deep learning, Agentic AI, foundation models and optimization techniques with several books, papers and patents to his name. In his current role at Amazon, Dr. Subramanian works with various science leaders and research teams within and outside Amazon, helping to guide customers to best leverage state-of-the-art algorithms and techniques to solve business critical problems. Outside AWS, Dr. Subramanian is a expert reviewer for AI papers and funding via organizations like Neurips, ICML, ICLR, NASA and NSF.
‚Ä¢ Natural language-based database analytics with Amazon Nova
  In this post, we explore how natural language database analytics can revolutionize the way organizations interact with their structured data through the power of large language model (LLM) agents. Natural language interfaces to databases have long been a goal in data management. Agents enhance database analytics by breaking down complex queries into explicit, verifiable reasoning steps and enabling self-correction through validation loops that can catch errors, analyze failures, and refine queries until they accurately match user intent and schema requirements. We demonstrate how this modern approach enables intuitive, conversation-like interactions with complex database systems while maintaining precision and reliability. 
To achieve optimal performance with minimal trade-offs, we use the Amazon Nova family of foundation models (FMs): Amazon Nova Pro, Amazon Nova Lite, and Amazon Nova Micro. These FMs encode vast amounts of world knowledge, facilitating nuanced reasoning and contextual understanding essential for complex data analysis. Our solution uses the ReAct (reasoning and acting) pattern, implemented through LangGraph‚Äôs flexible architecture. This approach combines the strengths of Amazon Nova LLMs for natural language understanding with explicit reasoning steps and actions. 
Challenges of natural language-based database analytics 
Many customers undergoing generative AI transformation share a common realization: their vast data stores hold untapped potential for automated analysis and natural language querying. This insight leads them to explore SQL-based solutions, where queries can range from simple SELECT and WHERE clauses to complex, multipage statements involving sophisticated aggregations and functions. 
At its core, successful analysis depends on identifying and retrieving the correct dataset. This foundational step enables all downstream activities, including visualization, further analysis, and exploration. Translating a user‚Äôs intent‚Äîwhether stated or implicit‚Äîinto a performant, precise, and valid SQL query is a significant challenge. 
Our solution excels in generating context and metadata-aware queries capable of retrieving precise datasets and performing intricate analyses. To fully harness the capabilities of Agents&nbsp;and Amazon Nova FMs, a user-friendly interface is crucial. We‚Äôve developed an intuitive interface where users are guided through their analysis journey with human-in-the-loop (HITL) capabilities, allowing for input, approvals, and modifications at critical decision points. 
Solution overview 
The solution architecture consists of three core components: UI, generative AI, and data. The agent in this solution serves as the central coordinator, combining critical capabilities such as question understanding, decision-making, workflow orchestration, intelligent routing, and generating comprehensive natural language responses. It enhances questions by improving text quality, standardizing terminology, and maintaining conversational context, helping users extend their analysis through a series of related queries while preserving precise analytical intent. The agent‚Äôs intelligent routing capabilities mean that correct tools are invoked for each user questions, enabling cohesive end-to-end query processing. Furthermore, it processes tabular and visual data and uses the complete context to generate comprehensive summaries that explain findings, highlight key insights, and suggest relevant follow-up questions. As an added benefit, the agent can suggest relevant follow-up questions and related topics, helping users explore their data more deeply and discover unexpected insights. The following tools are connected to the agent: 
 
 Text2SQL ‚Äì When the orchestrator determines data retrieval is needed, it uses the Text2SQL tool, which uses a comprehensive knowledge base that includes metadata, table schemas, example queries with their results, and detailed data dictionaries. Using this rich context, the tool transforms natural language questions into precise SQL queries. 
 SQLExecutor ‚Äì This tool directly connects to the structured data store and executes the SQL queries generated by the agent using the Text2SQL tool. The tool executes the generated SQL against a structured database endpoint such as Amazon Athena, Amazon Redshift, or Snowflake. 
 Text2Python ‚Äì When a visual representation of data is needed for the analysis, either by user request or orchestrator decision, the Text2Python tool transforms analytical results into compelling visualizations. This agent processes both the user‚Äôs query and the data table retrieved by the Text2SQL tool to generate appropriate Python scripts. Using industry-standard visualization libraries, these scripts execute locally to create diagrams, graphs, or charts that best represent the data. Like its SQL counterpart, this agent includes self-remediation capabilities. When execution errors occur, it uses the error feedback and context to regenerate the Python script, providing reliable visualization delivery. 
 PythonExecutor ‚Äì The PythonExecutor takes the generated Python scripts and executes them locally. This allows for the creation of high-quality data visualizations using industry-standard libraries. 
 
The agent then evaluates whether the returned dataset fully answers the user‚Äôs question. If the results are insufficient, it automatically regenerates and executes a more refined query to retrieve optimal data. A key feature is the agent‚Äôs self-remediation capability. When execution errors occur, the agent uses these errors and the full context to regenerate a corrected SQL query. This self-healing approach provides robust and reliable query processing, even in complex scenarios. The agent processes the inputs‚Äîthe rewritten question, analysis results, and context‚Äîto create a natural language summary and responds to the user, including tabular results with reasoning, visualizations with explanations, and a summary with key insights. 
The workflow is illustrated in the following diagram. 
 
The following shows an example of the conversation flow between the user and the agent: 
 
 User_A: what are the number of claims by staff name? 
 Chatbot: The following are the top 10‚Ä¶. 
 User_A: Visualize it 
 Chatbot: Visualize as a bar chart? [Rewritten Question] 
 User_A: Confirmed 
 Chatbot: &lt;IMAGE.PNG&gt; 
 
The agent maintains context through conversations, which means users only need to provide minimal follow-up inputs. It automatically reconstructs abbreviated questions using previous context for confirmation. Additionally, after each question-answer exchange, the agent suggests relevant follow-up exploratory questions for further exploration. The agent enforces consistent terminology, following industry standards, customer guidelines, and brand requirements. It standardizes abbreviations in both inputs and outputs by expanding them to their full forms. For example, ‚ÄúGDPR‚Äù is always expanded to General Data Protection Regulation in user input and agent responses. The agent improves text quality by maintaining clarity, correcting grammar, and refining sentence structure. All content is processed to provide professional, readable output.The solution uses the following AWS services and resources: 
 
 Amazon Athena ‚Äì Athena is used as the structured database for storing and querying the data 
 Amazon Bedrock ‚Äì The core of this solution is built on Amazon Bedrock, which enables the integration of generative AI agents and Amazon Nova 
 AWS Glue ‚Äì We use AWS Glue to prepare and load the dataset into Athena 
 Amazon Nova ‚Äì The state-of-the-art FM from Amazon is a key component, providing natural language understanding and generation capabilities 
 Amazon SageMaker ‚Äì We use SageMaker to create a notebook instance for running the code and experiments 
 
Prerequisites 
You need the following prerequisites to get started implementing the solution in this post: 
 
 An AWS account 
 Familiarity with FMs and Amazon Bedrock 
 Model access enabled in Amazon Bedrock for Amazon Nova models: Amazon Nova Pro, Amazon Nova Lite, and Amazon Nova Micro 
 
Set up a SageMaker notebook instance 
Follow these steps to create a SageMaker notebook instance: 
 
 On the SageMaker console, choose Notebook instances in the navigation pane. 
 Choose Create notebook instance. 
 For Notebook instance name, enter a name (for example, text2sql-nova-nb). 
 For Notebook Instance type, choose ml.t3.medium. 
 On the notebook, click on ‚ÄúIAM role ARN‚Äù, and add sagemaker.amazonaws.com and glue.amazonaws.com in the Trust relationships tab. 
 
 
 
 After the notebook instance has started, clone the GitHub repository. 
 
Download and prepare the database 
Follow these steps to download and prepare the database: 
 
 Download the Spider dataset. For this walkthrough, we use the insurance and claims database. 
 Unzip the dataset into the /data folder. 
 Create an AWS hosted Amazon Simple Storage Service (Amazon S3) bucket (for instructions, see Creating a general purpose bucket). 
 In the /src folder, use db_setup.ipynb to load the database into Athena. 
 
Run the Streamlit application 
To start the Streamlit application, use the following command:streamlit run app.pyThe following screenshot shows the interface. 
 
This demo uses Streamlit for illustration purposes only. For production deployments, review Streamlit‚Äôs security configuration and deployment architecture to make sure it aligns with your organization‚Äôs security requirements and best practices. 
Evaluations 
We evaluated the performance of Amazon Nova on the Spider text-to-SQL dataset, a widely used benchmark for complex cross-domain semantic parsing and text-to-SQL tasks. The evaluation provided insights into the Amazon Nova capabilities compared to other state-of-the-art approaches. The Spider dataset contains 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. The evaluation was conducted in a zero-shot setting, where the models weren‚Äôt fine-tuned on examples from the dataset, to assess their general text-to-SQL translation abilities. We used the following evaluation metrics: 
 
 Execution accuracy ‚Äì The execution accuracy metric evaluates whether the predicted SQL query produces the same result as the ground truth SQL query when executed against the database. The execution accuracy provides a practical assessment of the model‚Äôs performance because it measures the end-to-end capability to translate natural language questions into executable SQL queries. The top-performing models demonstrate strong execution accuracy on the Spider dataset, with the Amazon Nova model showing particularly competitive performance. Compared to other state-of-the-art models, Amazon Nova achieves similar or slightly higher accuracy across the various query complexity levels. For the most challenging queries, Amazon Nova outperforms the other leading models, showcasing its ability to handle complex natural language-to-SQL translations. 
 Latency ‚Äì In addition to accuracy, the speed and responsiveness of the text-to-SQL translation is an important consideration. Here, the Amazon Nova model stands out, demonstrating significantly faster processing times compared to other top-performing models. For a representative set of database queries, Amazon Nova was able to generate the SQL and retrieve the results notably quicker (a latency improvement of about 60%) than the competition. This latency improvement could translate to enhanced user experience and productivity, meaning that business users can use natural language interfaces to interact with databases more seamlessly. 
 
Overall, the evaluation results highlight the strengths of Amazon Nova in both accuracy and efficiency for text-to-SQL translation tasks. Its competitive performance, low latency, and advantages on the most complex queries make it a compelling option for organizations looking to democratize data access and analysis through natural language interfaces. 
Clean up 
To avoid continuing charges because of resources created as part of this walkthrough, perform the following cleanup steps: 
 
 Disable Amazon Bedrock model access for Amazon Nova Pro. 
 Delete project-specific AWS Glue Data Catalog databases and associated tables. 
 Remove the Amazon S3 content: 
   
   Empty the S3bucket. 
   Delete the S3 bucket. 
    
 Delete the SageMaker notebook instance. 
 
Conclusion 
The Generative AI Innovation Center (GenAIIC) at AWS has developed this natural language-based database analytics solution that uses the strengths of state-of-the-art Amazon Nova FMs, along with explicit reasoning steps and actions, as implemented through the ReAct pattern in LangGraph‚Äôs flexible architecture. This solution is built using Amazon Bedrock, which enables intuitive, conversation-like interactions with complex database systems. You can seamlessly translate your natural language queries into accurate SQL statements and generate insightful data visualizations. The evaluation results demonstrate the solution‚Äôs competitive performance, making it a compelling option for organizations looking to democratize data access and analysis. Furthermore, the GenAIIC provides you with access to a team of experts to help identify valuable use cases and implement practical generative AI solutions tailored to your specific needs, enhancing the potential of this technology. 
For more information, refer to Amazon Nova Foundation Models and Amazon Bedrock. If you‚Äôre interested in the collaboration with GenAIIC, you can find more information at AWS Generative AI Innovation Center. 
 
 Rahul Ghosh is an Applied Scientist at the AWS Generative AI Innovation Center, where he works with AWS customers across different verticals to expedite their use of Generative AI. Rahul holds a PhD in Computer Science from the University of Minnesota. 
 Gaurav Rele is a Senior Data Scientist at the AWS Generative AI Innovation Center, where he works with AWS customers across different verticals to accelerate their use of generative AI and AWS cloud services to solve their business challenges. 
 Amaran Asokkumar is a Deep Learning Architect at AWS, specializing in infrastructure, automation, and AI. He leads the design of generative AI-enabled solutions across industry segments. Amaran is passionate about all things AI and helping customers accelerate their generative AI exploration and transformation efforts. 
 Long Chen is a Sr. Applied Scientist at the AWS Generative AI Innovation Center. He holds a PhD in Applied Physics from the University of Michigan. With more than a decade of experience for research and development, he works on innovative solutions in various domains using generative AI and other machine learning techniques, facilitating the success of AWS customers. His interest includes generative models, multimodal systems, and graph learning. 
 Jae Oh Woo is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he specializes in developing custom solutions and model customization for a diverse range of use cases. He has a strong passion for interdisciplinary research that connects theoretical foundations with practical applications in the rapidly evolving field of generative AI. Prior to joining Amazon, Jae Oh was a Simons Postdoctoral Fellow at the University of Texas at Austin. He holds a Ph.D. in Applied Mathematics from Yale University. 
 Sungmin Hong is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he helps expedite the variety of use cases of AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds PhD in Computer Science from New York University. Outside of work, he prides himself on keeping his indoor plants alive for over 3 years. 
 Vidya Sagar Ravipati is a Science Manager at the AWS Generative AI Innovation Center, where he uses his vast experience in large-scale distributed systems and his passion for machine learning to help AWS customers across different industry verticals accelerate their AI and cloud adoption.
‚Ä¢ Deploy Amazon Bedrock Knowledge Bases using Terraform for RAG-based generative AI applications
  Retrieval Augmented Generation (RAG) is a powerful approach for building generative AI applications by providing foundation models (FMs) access to additional, relevant data. This approach improves response accuracy and transparency while avoiding the potential cost and complexity of FM training or fine-tuning. 
Many customers use Amazon Bedrock Knowledge Bases to help implement RAG workflows. You can deploy an Amazon Bedrock knowledge base for initial development and set up connections with data sources with a few clicks in the AWS Management console. When it comes to migrating your development setup to an infrastructure as code (IaC) template for production deployment, it‚Äôs helpful to start from an existing IaC project template because there are there are additional configuration details to specify that are abstracted away in the console. While CDK-based templates are already available for setting up Bedrock knowledge bases, many organizations use Terraform as their preferred infrastructure-as-code framework. 
In this post, we provide a Terraform IaC solution for deploying an Amazon Bedrock knowledge base and setting up a data source connection to help you quickly get started with deploying RAG workflows with Terraform. You can find the solution in our AWS Samples GitHub repository. 
Solution overview 
The solution automates the creation and configuration of the following AWS service components using Terraform: 
 
 AWS Identity and Access Management (IAM)&nbsp;role: Establishes secure access and execution policies across services. 
 Amazon OpenSearch Serverless: Configures an index collection to manage and query large datasets efficiently. 
 Amazon Bedrock Knowledge Bases: Provides FMs and agents contextual information from your data sources to deliver relevant, accurate, customized responses. 
 
The following diagram illustrates the solution architecture for how these services are integrated: 
 
The figure shows that there are several IAM policies that govern permissions for the services involved in the solution: 
 
 Amazon Bedrock policies 
   
   Amazon Bedrock invocation policy: Authorizes the system to invoke Amazon Bedrock services for knowledge base operations and data retrieval. 
   Amazon S3 access policy: Grants necessary read and write permissions to the system components ( like Amazon Bedrock, AWS OpenSearch, or other services) that require access to Amazon Simple Storage Service (Amazon S3) for secure data storage and retrieval. 
   OpenSearch data access policy: Defines access permissions to interact with AWS OpenSearch collections within the Amazon Bedrock environment. This policy controls who can query, update, or manage OpenSearch data, making sure only authorized users or services have access to the data. 
    
 OpenSearch policies 
   
   OpenSearch collection access policy: Defines access permissions for querying data in OpenSearch collections. 
   OpenSearch collection data encryption policy: Makes sure data stored within OpenSearch collections is encrypted for security. 
   OpenSearch collection network policy: Controls network-level access to OpenSearch collections to secure data transfer within the system. 
    
 
Deploying these resources through IaC enables programmable infrastructure management, reducing manual setup effort. The deployment process makes sure that you can start querying your data almost immediately after setup, with minimal configuration required. This automated approach streamlines maintenance of your RAG-based application. 
Prerequisites 
To successfully deploy this solution, make sure you have the following prerequisites in place: 
 
 AWS account: An active AWS account is required. If you don‚Äôt have one, you can create an account using the AWS Management Console. 
 IAM role and permissions: Make sure that you have an AWS Identity and Access Management (IAM) role and a user with the necessary permissions to create and manage AWS resources. This setup should include permissions to handle AWS services such as Amazon S3, Amazon OpenSearch Service, and Amazon Bedrock. 
 Terraform: Make sure that Terraform is installed on your local machine. Follow the installation guide provided in the AWS Modernization Workshop with Hashicorp Terraform Cloud for detailed instructions. 
 AWS CLI: Configure the AWS Command Line Interface (AWS CLI) with appropriate credentials to interact with your AWS resources. For setup instructions, see Configuring settings for the AWS CLI. 
 Document storage: Set up an Amazon S3 bucket and upload your documents in supported formats such as TXT, MD HTML, DOC, DOCX, CSL, XLS, XLSX, or PDF. Make sure your S3 bucket is correctly configured to work with other AWS services involved in the deployment. 
 
If you need a sample document for testing, we suggest using the AWS Well-Architected Framework guide which you can download as a PDF. 
Amazon Bedrock model access 
Make sure you have enabled access to a foundation model (FM) in Amazon Bedrock for generating embeddings. The solution uses the Titan Text Embeddings V2 model by default. You can complete the following steps to enable model access in Amazon Bedrock: 
 
 Navigate to the Amazon Bedrock console. 
 Choose Model access from the navigation pane under Bedrock Configurations. 
 Select Enable specific models. 
 
 
 
 Select the Titan Text Embeddings V2 model. 
 Choose Next and complete the access request form to enable the model. 
 
 
Set up the project 
 
 Clone the repository: 
   
   Decide where you want to clone the repository. This could be: 
     
      
       
       Your local machine: If you‚Äôre working directly on your laptop or desktop. 
       An EC2 instance: If you‚Äôre deploying the setup from a remote server. 
       AWS CloudShell: If you prefer using the AWS browser-based command line interface. 
        
      
   Configure your AWS credentials for Terraform using one of the supported methods. 
   Navigate to the appropriate directory on your chosen environment and run the following command to clone the repository: 
     
     git clone https://github.com/aws-samples/sample-bedrock-knowledge-base-terraform 
     Make sure that the environment has Git installed and that your SSH key is configured to access the repository. 
    
 
 
 Move into the cloned directory: 
   
   cd sample-bedrock-knowledge-base-terraform 
    
 Update the AWS Region: 
   
   Purpose: Set the AWS Region in the Terraform configuration to make sure the correct region is used. 
   Default: No default. This must be provided by the user. 
   Location to update: 
     
     File: main.tf 
     Line: Look for the "aws" provider block. 
       
       provider "aws" {
   region = "us-east-1" # Update this to your desired AWS region} 
        
      
    
 Provide S3 bucket name: 
   
   Purpose: Specifies the S3 bucket where knowledge base data will be stored. 
   Default: No default. This must be provided by the user. 
   Location to update: 
     
     File: main.tf 
     Line: Look for the "kb_s3_bucket_name_prefix" variable in the "knowledge_base" module block. 
       
       module "knowledge_base" {
  source = "./modules"
  kb_s3_bucket_name_prefix = "your-s3-bucket-name"  # Replace with your bucket name
} 
        
      
    
 Set additional configuration settings for the knowledge base module (optional):  The knowledge_base module accepts additional optional inputs to customize settings such as the chunking strategy, embedding model used, and the name of the created knowledge base resource. 
   
   module "knowledge_base" {
  ...
  chunking_strategy = "FIXED_SIZE"
  kb_model_id = ‚Äúamazon.titan-embed-text-v2:0‚Äù
  kb_name = ‚ÄúmyKnowledgeBase‚Äù
  ...
} 
   See the modules/variables.tf file for additional module input variables that can be used to control fine-grained settings like embedding size and other chunking behavior. 
 Validate and deploy the stack: Run the following command to prepare your working directory (Terraform folder): 
   
   terraform init 
   The following image shows the output of the terraform init command: Before applying changes, it‚Äôs crucial to understand what Terraform will modify in your environment. Use the following command to generate and review the execution plan: 
   
   terraform plan 
   The -out option is used to save the generated plan to a file, which can then be applied exactly with terraform apply. Without the -out option, Terraform will generate a new plan when you run terraform apply. This new plan might differ from the original one because of changes in the environment or resources between the time you created the plan and when you apply it.  This command displays the proposed changes, outlining what resources will be created, modified, or destroyed. The output includes detailed information about each resource: 
   
   + Create: Resources that will be newly created. 
   ~ Update in-place: Resources that will exist with modified attributes. 
   ‚Äì Destroy: Resources that will be removed. 
   By reviewing this plan, you can verify that the changes align with your expectations before moving forward with the deployment. This step makes sure that only the intended modifications are applied, helping to prevent potential disruptions in your environment.  
 Deploy the infrastructure: Apply the changes and deploy the resources to your AWS account: terraform apply When you run terraform apply, Terraform automatically creates a plan and prompts you to approve it before executing the configuration changes.The following image shows what the output prompt and changes to output variables will look like: 
 
Advanced customization options for your knowledge base deployment: 
The Terraform module offers flexibility for users to customize various parameters based on their specific use cases. These configurations can be quickly adjusted in the variables.tf file. In this section, we explain how you can tailor the chunking strategy and OpenSearch vector dimensions to meet your requirements. 
Customize the chunking strategy and token limits 
The chunking strategy determines how the knowledge base splits content into smaller, manageable chunks for efficient processing. This module supports the following strategies: 
 
 DEFAULT: Standard chunking with no additional customization. 
 FIXED_SIZE: Divides content into chunks of a fixed token size with optional overlaps. 
 HIERARCHICAL: Creates hierarchical chunks with parent-child structures. 
 SEMANTIC: Splits content based on semantic meaning for improved contextual grouping. 
 
Modify fixed-size chunking (optional) 
If you choose FIXED_SIZE as the chunking strategy, you can further customize: 
 
 Maximum tokens per chunk: Adjust the fixed_size_max_tokens variable. 
 Overlap between chunks: Configure the fixed_size_overlap_percentage variable. 
 
 
 variable "fixed_size_max_tokens" {
  type        = number
  description = "Maximum number of tokens for fixed-size chunking"
  default     = 512
}
variable "fixed_size_overlap_percentage" {
  type        = number
  description = "Percentage of overlap between chunks"
  default     = 20
} 
 
Modify hierarchical chunking (optional) 
For HIERARCHICAL chunking, you can adjust: 
 
 Parent and child token limits: Use hierarchical_parent_max_tokens and hierarchical_child_max_tokens. 
 Overlap tokens: Configure the hierarchical_overlap_tokens variable. 
 
 
 variable "hierarchical_parent_max_tokens" {
  type        = number
  description = "Maximum tokens for parent chunks"
  default     = 1000
}
variable "hierarchical_child_max_tokens" {
  type        = number
  description = "Maximum tokens for child chunks"
  default     = 500
}
variable "hierarchical_overlap_tokens" {
  type        = number
  description = "Number of tokens to overlap in hierarchical chunking"
  default     = 70
} 
 
Modify semantic chunking (optional) 
For SEMANTIC chunking, you can customize: 
 
 Maximum tokens: Use the semantic_max_tokens variable. 
 Buffer size: Adjust semantic_buffer_size. 
 Breakpoint percentile threshold: Modify semantic_breakpoint_percentile_threshold. 
 
 
 variable "semantic_max_tokens" {
  type        = number
  description = "Maximum tokens for semantic chunking"
  default     = 512
}
variable "semantic_buffer_size" {
  type        = number
  description = "Buffer size for semantic chunking"
  default     = 1
}
variable "semantic_breakpoint_percentile_threshold" {
  type        = number
  description = "Breakpoint percentile threshold for semantic chunking"
  default     = 75
} 
 
Customize the OpenSearch collection vector dimensions 
The vector dimension defines the size of embeddings generated by the selected model and impacts the precision of searches within the OpenSearch collection. You can adjust the vector_dimension variable in the variables.tf file: 
 
 variable "vector_dimension" {
  description = "The dimension of the vectors produced by the model."
  type        = number
  default     = 1024
} 
 
Depending on your use case and vector dimension values supported by your embedding model, you could consider increasing the vector dimension setting for increased retrieval precision. To optimize for storage and query performance, you could consider decreasing the vector dimension setting. 
Test the knowledge base in Amazon Bedrock: 
Follow these steps to test the knowledge base‚Äôs interaction with the chosen FM model to verify that it performs as expected. 
 
 Navigate to the Amazon Bedrock console and choose Knowledge Bases from the navigation pane. 
 Select the knowledge base you created. 
 Choose Sync to update the data stored in the S3 bucket. 
 Choose Select Model . 
 From the model selection window, select your desired FM. 
 Test the knowledge base by submitting your Q/A queries to see how the model responds. 
 
 
Clean up 
To avoid incurring additional costs, clean up your environment after testing or deploying with Terraform: 
Remove infrastructure resources 
 
 Open your terminal and navigate to the directory containing your Terraform files. 
 Execute the following command to remove the resources defined in your Terraform configuration: 
 
terraform destroy 
 
 The command will prompt you to confirm the destruction of the resources. Enter yes to proceed. 
 
 
 
 The following is a screenshot of the expected output confirming that the resources have been removed. 
 
 
Delete S3 bucket contents: 
 
 Make sure you delete all files within the S3 bucket to avoid future charges. 
 
Manual cleanup of state files (optional): 
 
 If you want to completely clean your project directory, remove the Terraform state files by executing the following: 
 
 
 rm -rf .terraform/ 
rm .terraform.lock.hcl
rm terraform.tfstate
rm terraform.tfstate.backup 
 
This procedure facilitates a thorough cleanup of your environment and avoids potential costs associated with unused resources. 
Conclusion 
In this post, we demonstrated how to automate the deployment of Amazon Knowledge Bases for RAG applications using Terraform. 
If you want to deepen understanding of AWS services and Terraform, see the following resources: 
 
 Learn more about Amazon Bedrock Knowledge Bases 
 Learn more about managing infrastructure with AWS Modernization Workshop with HashiCorp Terraform Cloud 
 
Do you have questions or insights about deploying RAG systems with Amazon Bedrock? Feel free to leave a comment below or share your experiences and challenges. 
 
About the authors 
Andrew Ang is a Senior ML Engineer with the AWS Generative AI Innovation Center, where he helps customers ideate and implement generative AI proof of concept projects. Outside of work, he enjoys playing squash and watching travel and food vlogs. 
Akhil Nooney is a Deep Learning Architect with the AWS Generative AI Innovation Center, where he collaborates with customers to understand their generative AI use case requirements and design scalable, production-ready solutions. He helps organizations tackle complex challenges using generative AI, driving efficiency and innovation. Akhil holds a Master‚Äôs degree in Data Science from the University of North Texas. His previous research focused on synthetic data generation for healthcare applications using GANs and VAEs.
‚Ä¢ Document intelligence evolved: Building and evaluating KIE solutions that scale
  Intelligent document processing (IDP) refers to the automated extraction, classification, and processing of data from various document formats‚Äîboth structured and unstructured. Within the IDP landscape, key information extraction (KIE) serves as a fundamental component, enabling systems to identify and extract critical data points from documents with minimal human intervention. Organizations across diverse sectors‚Äîincluding financial services, healthcare, legal, and supply chain management‚Äîare increasingly adopting IDP solutions to streamline operations, reduce manual data entry, and accelerate business processes.&nbsp;As document volumes grow exponentially, IDP solutions not only automate processing but also enable sophisticated agentic workflows‚Äîwhere AI systems can analyze extracted data and initiate appropriate actions with minimal human intervention. The ability to accurately process invoices, contracts, medical records, and regulatory documents has become not just a competitive advantage but a business necessity.&nbsp;Importantly, developing effective IDP solutions requires not only robust extraction capabilities but also tailored evaluation frameworks that align with specific industry needs and individual organizational use cases. 
In this blog post, we demonstrate an end-to-end approach for building and evaluating a KIE solution using Amazon Nova models available through Amazon Bedrock. This end-to-end approach encompasses three critical phases: data readiness (understanding and preparing your documents), solution development (implementing extraction logic with appropriate models), and performance measurement (evaluating accuracy, efficiency, and cost-effectiveness). We illustrate this comprehensive approach using the FATURA dataset‚Äîa collection of diverse invoice documents that serves as a representative proxy for real-world enterprise data. By working through this practical example, we show you how to select, implement, and evaluate foundation models for document processing tasks while taking into consideration critical factors such as extraction accuracy, processing speed, and operational costs. 
Whether you‚Äôre a data scientist exploring generative AI capabilities, a developer implementing document processing pipelines, or a business analyst seeking to understand automation possibilities, this guide provides valuable insights for your use case. By the end of this post, you‚Äôll have a practical understanding of how to use large language models for document extraction tasks, establish meaningful evaluation metrics for your specific use case, and make informed decisions about model selection based on both performance and business considerations. These skills can help your organization move beyond manual document handling toward more efficient, accurate, and scalable document processing solutions. 
Dataset 
Demonstrating our KIE solution and benchmarking its performance requires a dataset that provides realistic document processing scenarios while offering reliable ground truth for accurate performance measurement. One such dataset is FATURA, which contains 10,000 invoices with 50 distinct layouts (200 invoices per layout). The invoices are all one-page documents stored as JPEG images with annotations of 24 fields per document. High-quality labels are foundational to evaluation tasks, serving as the ground truth against which we measure extraction accuracy. Upon examining the FATURA dataset, we identified several variations in the ground truth labels that required standardization. These included structural inconsistencies (for example, nested versus flat field representations) and value format inconsistencies (for example, prefixed fields like INVOICE DATE: 01/15/2023 or numeric values stored as strings versus floats). 
To make sure of fair and accurate evaluation in our study, we normalized these variations by removing inconsistent prefixes and aligning the annotation format with our large language model (LLM) solution‚Äôs expected output structure. For this post, we sample 40 documents from 49 distinct layouts for a total of 1,960 samples and shown in the distribution of labels in the following figure, we omit one layout because of several inconsistencies in ground truth annotations. As shown in the figure, the distribution of fields across the samples is notably imbalanced, with occurrences ranging from approximately 250 to 1,800 instances across 18 different fields. This sparsity reflects the real-world nature of documents where not all fields are present in every document‚Äîa key challenge for information extraction systems, which must learn to handle missing fields rather than forcing predictions when data is absent. 
 
Additional data challenges that practitioners frequently encounter include handling multiple values for a single field (such as several phone numbers listed for contacts), inconsistent representation of missing information (empty strings, N/A, dashes, or other placeholders), dealing with fields that can contain either structured or unstructured text (addresses), and managing value hierarchies where one field might contextually depend on another (tax amounts based on subtotals). 
KIE in Amazon Bedrock 
Amazon Bedrock can streamline document processing by providing access to LLMs for extracting structured information without complex rule-based systems. 
Converse API approach 
The Amazon Bedrock Converse API offers a streamlined, unified interface for interacting with foundation models in Amazon Bedrock, significantly simplifying experimentation across different models for document processing tasks.&nbsp;This API removes the complexity of managing model-specific formatting requirements, enabling faster iteration and model comparison for document extraction workflows. 
To invoke language models through the Converse API, the required parameters include the model_id to specify which foundation model to use, and the messages containing your prompts and conversation context.&nbsp;The following example demonstrates how to structure this API call, with the next section detailing proper messages formatting techniques. 
 
 import&nbsp;boto3

bedrock_runtime&nbsp;=&nbsp;boto3.client("bedrock-runtime")


response = bedrock_runtime.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=model_id,&nbsp;# Here the model ID can be changed to any model on Amazon Bedrock
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;system=[{"text": system_prompt}],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig=inference_config
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) 
 
For comprehensive details on additional parameters, response handling, and best practices, see the Amazon Bedrock Converse API documentation. 
Standardized prompt engineering 
Effective information extraction requires consistent, model-agnostic prompting strategies that work across different LLMs. Using templating frameworks like Jinja2 enables maintaining a single prompt structure while incorporating rule-based logic. This approach provides flexibility while maintaining consistency across various extraction scenarios. 
When designing templates for KIE, consider these logical elements: input variations (textual context types and formats), instruction adjustments for different modalities (image, text, or combined), and field specifications including attributes and pseudonyms. Here‚Äôs an example Jinja2 template designed for KIE: 
 
 &lt;background&gt;
An invoice is a formal document issued by a seller to a buyer that records a 
transaction.
&lt;/background&gt;

{% if docs %}
&lt;document_ocr_data&gt;
Docs: {{ docs }}
&lt;/document_ocr_data&gt;
{% endif %}

&lt;task&gt;
Your task is to take the unstructured text provided and convert it into a well&nbsp;
organized format using JSON. Only extract these fields, and nothing else.&nbsp;

&lt;fields&gt;
{{ fields }}
&lt;/fields&gt;

{% if use_images %}
Carefully examine the entire image to identify all relevant invoice fields.
{% endif %}
{% if docs %}
Carefully examine the document text to identify all relevant invoice fields.
{% endif %}
&lt;/task&gt;
&nbsp; 
 
To implement this template in practice, you need to populate it with relevant data before sending it to the LLM. The following code demonstrates how to load your template, insert document-specific data, and generate the final prompt text. The LangChain PromptTemplate loads your Jinja2 file, then a dictionary of key-value pairs supplies the necessary variables, such as optical character recognition (OCR) text from the document and field descriptions. When the format method runs, Jinja2 processes conditional statements and variable substitutions to create the tailored instructions for your specific extraction task: 
 
 from langchain_core.prompts.prompt import PromptTemplate

# Load the template
prompt_template = PromptTemplate.from_file(template_file=&lt;path to .txt file&gt;, template_format="jinja2")

# Prepare arguments based on available data (here is an example)
prompt_kwargs = {
&nbsp;&nbsp; &nbsp;'docs': "Invoice #12345\nDate: 2023-10-15\nVendor: ABC Corp\nAmount: \$1,250.00",
&nbsp;&nbsp; &nbsp;'fields': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'invoice_number': 'The unique identifier for the invoice',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'date': 'The invoice issuance date',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'vendor_name': 'Name of the company issuing the invoice',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'total_amount': 'The total amount due on the invoice'
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;'use_images': True
}

# Generate the formatted prompt text
text_content = prompt_template.format(**prompt_kwargs) 
 
To handle multiple input modalities in a single request, construct a content array containing the available information sources. For each image, create a formatted entry with proper encoding and add it to the array. Similarly, add the text prompt as another entry. This unified approach accommodates various input combinations‚Äîtext-only, image-only, or multimodal‚Äîwithout requiring separate handling logic for each case. The following example demonstrates creating this composite input structure: 
 
 content = []

if images:
 &nbsp; &nbsp;for image in images:
 &nbsp; &nbsp; &nbsp; &nbsp;image_content = {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": "jpeg",
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {"bytes": image_to_bytes(image)},&nbsp;# This function resizes the image to the model constraints if applicable
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
 &nbsp; &nbsp; &nbsp; &nbsp;} 
 &nbsp; &nbsp; &nbsp; &nbsp;content.append(image_content) &nbsp;
content.append({"text": text_content})

messages = [{"role": "user", "content": content}] 
 
The image_to_bytes function converts the image provided into a format that the model can understand. 
 
 from PIL import image
from io import BytesIO 
def image_to_bytes(image):
        """
        Convert PIL Image to bytes.
        Parameters:
        image (PIL.Image.Image): PIL Image object
        Returns:
        bytes: bytes representation of the image
        """
        # Convert to RGB and save as JPEG in memory
        with BytesIO() as buffer:
            image.convert("RGB").save(buffer, format="JPEG", quality=85, optimize=True) 
 
 
             return buffer.getvalue() 
 
Note, further optimization of image resizing per model might be required for ideal performance. 
Measuring performance 
When building intelligent document processing solutions, establishing a robust evaluation framework is essential for meeting both technical requirements and business objectives. For KIE, evaluation must go beyond basic accuracy metrics to address the multidimensional nature of document processing. An effective evaluation strategy should include precision and recall measurements, and account for the varying importance of different fields&nbsp;correctly extracting a total amount might be more critical than capturing a memo field. Practical considerations such as processing latency and cost per document must also factor into your evaluation matrix. Using the FATURA dataset, we‚Äôll demonstrate how to construct metrics that balance technical performance with business value, so you too can quantify not only extraction accuracy, but also how effectively your solution addresses your organization‚Äôs specific document processing needs. 
F1-score 
For our KIE solution, we evaluate performance using the F1-score, which balances the system‚Äôs precision (correctness of extracted values) and recall (ability to find the relevant fields) to provide a comprehensive assessment of extraction accuracy.&nbsp;To calculate the F1-score, we need to accurately classify each extraction attempt as a true positive, false positive, or false negative. This classification hinges on a critical question: does the extracted value match the ground truth? For document processing, this seemingly simple comparison is complicated by the diverse nature of extracted information‚Äîdates might be formatted differently but represent the same day, or monetary values might include different currency symbols while being numerically identical. 
This challenge necessitates field-specific comparators that intelligently determine when an extraction counts as a match. Here are a few: 
 
 Numeric fields: Allow formatting variations while matching actual values 
 Text fields: Apply fuzzy matching for spacing or punctuation differences 
 Structured fields: Normalize formats for dates, addresses, and other structured data with variable representations before comparison 
 
With these comparators established, we classify each extraction attempt into one of four categories: 
 
 True positive (TP): The field exists in the ground truth and our system correctly extracted its value according to the field-specific comparator 
 False positive (FP): Our system extracted a value for a field, but either the field doesn‚Äôt exist in the ground truth or the extracted value doesn‚Äôt match the expected value 
 False negative (FN): The field exists in the ground truth, but our system failed to extract it 
 True negative (TN): The field doesn‚Äôt exist in the ground truth, and our system correctly didn‚Äôt extract it 
 
These values are used to calculate precision (TP/(TP+FP)) and recall (TP/(TP+FN)), combining them into the F1-score: 2 √ó (precision √ó recall)/(precision + recall). This approach provides a comprehensive evaluation that balances the system‚Äôs ability to find relevant information with its ability to avoid incorrect extractions. 
Latency and&nbsp;cost 
When implementing IDP solutions, latency and cost considerations are just as critical as extraction accuracy. These operational metrics directly impact both user experience and the economic viability of your document processing pipeline. Amazon Bedrock simplifies performance monitoring by including key metrics with every model response. Each time you invoke a foundation model, the response contains metadata on input tokens, output tokens, and processing latency in milliseconds. Using this built-in instrumentation, teams can track performance without implementing additional monitoring infrastructure. The following is a subset of the response syntax template (more information can be found in&nbsp;Converse): 
 
 {
&nbsp; &nbsp; "modelOutput": [ "string" ],
&nbsp;&nbsp;&nbsp;&nbsp;"metrics": {&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp;"latencyMs": number
   },
&nbsp;&nbsp;&nbsp; "usage": {&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp;"inputTokens": number,
&nbsp;&nbsp; &nbsp; &nbsp;"outputTokens": number,
&nbsp;&nbsp; &nbsp; &nbsp;"totalTokens": number
&nbsp;&nbsp; }&nbsp;&nbsp; 
&nbsp;} 
 
Latency in Amazon Bedrock represents the model‚Äôs response time‚Äîhow quickly it processes your document and generates extraction results. Smaller models within a model family typically process documents faster than larger variants, though this varies based on document complexity. This processing time directly impacts how quickly extracted information becomes available for downstream business processes. 
Cost in Amazon Bedrock is determined by token usage, following a straightforward formula:  
Each model uses its own tokenizer, so token counts vary between models even for identical documents. Pricing information for Amazon Bedrock models is available at Amazon Bedrock Pricing. 
Because document length varies significantly across business contexts‚Äîfrom single-page receipts to hundred-page contracts‚Äîcomparing performance metrics at the document level can be misleading. Normalizing latency and cost to a standard unit, such as per 1,000 pages, creates a consistent benchmark for evaluation. This standardized approach enables meaningful comparisons between different models, document types, and processing strategies. For instance, knowing that a solution processes documents at $15 per 1,000 pages with an average latency of 2.3 seconds per page provides a scalable understanding of operational costs and performance implications, regardless of individual document length. 
By systematically tracking these metrics across different document types, organizations can make informed decisions that balance extraction quality, processing speed, and operational costs based on their specific business requirements. 
Performance analysis: F1 score, latency, and cost trade-offs 
We conducted a comprehensive evaluation of intelligent document processing performance on the FATURA dataset across three dimensions: extraction accuracy (F1 score), processing speed (latency), and operational expenses (cost). Our analysis compared two models from the Nova family, representing both a lighter, smaller model and a larger, more capable model. We selected specific models from each family because of their multimodal capabilities, allowing us to evaluate performance across text, image, and combined modalities. 
Accuracy performance 
As expected, the larger model achieved higher extraction accuracy than the smaller counterpart. Amazon Nova Pro achieved the highest overall F1 score (0.9793) when using both text and image inputs.Across both models in this evaluation, text-only processing typically delivered the strongest extraction accuracy, with text and image combinations performing similarly or slightly lower. Image-only processing consistently achieved the lowest F1 scores in our tests.Breaking down accuracy by specific fields provides deeper insights into model performance. Field-level analysis often reveals that certain information types‚Äîsuch as dates, invoice numbers, or line items‚Äîmight have significantly different extraction success rates even within the same model. This granular evaluation helps identify which fields require prompt optimization or additional model tuning. For example, a model might excel at extracting total amounts but struggle with vendor addresses. Such field-specific error analysis enables targeted improvements to prompting strategies and extraction techniques, ultimately enhancing overall system performance. 
Latency considerations 
In line with expectations, the smaller model, Amazon Nova Lite, delivered faster processing. Image processing typically required additional processing time, particularly for larger models, reflecting the additional complexity of visual information extraction for Amazon Nova Pro. 
Cost efficiency 
Cost varied dramatically across these two models, with an over 20-fold difference between the most and least expensive options per 1,000 pages: 
 
 Nova Lite was the most economical, with costs well under $0.50 per 1,000 pages 
 Adding image processing generally increased costs due to higher input token counts 
 
Optimal configuration analysis 
Our evaluation highlights why organizations should weigh accuracy, speed, and cost metrics together when selecting foundation models for document processing. While these findings provide valuable insights, they are specific to the FATURA dataset‚Äîorganizations should conduct similar evaluations on their own document types to determine the optimal model and modality combinations for their specific business needs. 
The following table shows performance, latency, and cost comparisons from the Amazon Nova model family and across three input modalities. 
 
  
   
   Model 
   Modality 
   F1 score 
   Latency per page 
   Avg input/output token 
   Cost per 1,000 pages 
   
   
   Nova Lite 
   Image 
   0.8799 
   4.63 
   3773/305 
   $0.2996 
   
   
   Text 
   0.9222 
   3.07 
   2340/316 
   $0.2162 
   
   
   Both 
   0.9019 
   4.81 
   4090/311 
   $0.3200 
   
   
   Nova Pro 
   Image 
   0.9324 
   10.99 
   3773/305 
   $3.9944 
   
   
   Text 
   0.9784 
   5.19 
   2340/316 
   $2.8832 
   
   
   Both 
   0.9793 
   11.12 
   4090/311 
   $4.2672 
   
  
 
Conclusion 
Our exploration of intelligent document processing using foundation models available through Amazon Bedrock demonstrates the critical importance of a holistic approach‚Äîfrom data preparation through implementation to comprehensive evaluation. The three-dimensional framework‚Äîmeasuring accuracy, latency, and cost‚Äîenables organizations to assess IDP solutions comprehensively and develop document processing systems that align with their specific business objectives. The analysis reveals a key insight: while larger models generally achieve higher accuracy, smaller models can deliver impressive results at a fraction of the cost, highlighting the essential balance organizations must strike between performance and operational efficiency. As document volumes continue growing across industries, this end-to-end approach empowers data scientists, developers, and business analysts to implement IDP solutions that transform document handling from a manual burden into a strategic advantage‚Äîdelivering the right balance of accuracy and efficiency for your specific organizational needs. In upcoming work, we‚Äôll expand our benchmarking to more diverse and challenging document types across various domains and industries. We‚Äôll also explore how fine-tuning these foundation models on domain-specific data can enhance extraction accuracy and performance for specialized use cases‚Äîenabling organizations to build IDP solutions that more precisely address their unique business challenges and document processing requirements. 
Ready to transform your document processing workflow? Get started with Amazon Bedrock by visiting What is Amazon Bedrock. Follow the steps outlined in this post to evaluate how these foundation models perform on your own documents and begin building a tailored IDP solution that addresses your unique business challenges. 
 
About the authors 
Ayushi Haria is a Deep Learning Architect at Amazon Web Services (AWS), where she has worked for over two years following an earlier internship. For the past year, she has been a member of AWS‚Äôs Generative AI Innovation Center (GenAIIC), where she specializes in intelligent document processing and evaluation methods. 
Sujitha Martin is an Senior Applied Scientist in the Generative AI Innovation Center (GenAIIC). Her expertise is in building machine learning solutions involving computer vision and natural language processing for various industry verticals. Her research spans from developing human-centered situational awareness for highly autonomous systems to designing customizable intelligent document processing solutions. 
Spencer Romo is a Senior Data Scientist specializing in intelligent document processing, with deep expertise across computer vision, NLP, and signal processing. His innovative work in remote sensing has led to multiple patents. Based in Austin, Texas, he partners closely with customers to deliver impactful AI solutions. Outside of work, Spencer competes in the 24 Hours of Lemons racing series, combining his passion for engineering with budget-conscious motorsports. 
Jared Kramer is an Applied Science Manager at Amazon Web Services based in Seattle. Jared joined Amazon 11 years ago as an ML Science intern. He currently leads of team of Applied Scientists and Deep Learning Architects in the Generative AI Innovation Center, having previously spent 6 years in Customer Service Technologies and 4 years in Sustainability Science and Innovation.

‚∏ª