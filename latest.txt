âœ… Morning News Briefing â€“ August 20, 2025 10:46

ğŸ“… Date: 2025-08-20 10:46
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ Current Conditions:  15.0Â°C
  Temperature: 15.0&deg;C Pressure / Tendency: 101.9 kPa rising Humidity: 89 % Dewpoint: 13.2&deg:C Wind: E 10 km/h Air Quality Health Index: n/a Observed at: Pembroke 6:00 AM EDT Wednesday 20 August 2025 . Weather: 15-15.0/deg
â€¢ Wednesday: Chance of showers or drizzle. High 23. POP 30%
  Cloudy with 30 percent chance of showers or drizzle . High 23. Humidex 28. UV index 5 or moderate . High of 23. Isolated showers and thunderstorms possible in the afternoon . Isolated rain showers possible throughout the day, rain showers likely in the evening, rain likely to drop to 20 degrees . Is you ready for the first day of the year? Share
â€¢ Wednesday night: Mainly cloudy. Low 13.
  Fog patches developing near midnight . Mainly cloudy. Mainly sunny . Fog patches develop near midnight. Low 13.50/60/80/80 . Forecast issued 5:00:00 AM EDT Wednesday 20 August 2025 (W/8080/100/100) Forecast: "Fog in the air" for August 20, 2025 (Fog on the air)

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ These brain implants speak your mind â€” even when you don't want to
  Brain-implanted devices that allow paralyzed people to speak can also decode words they imagine, but don't intend to share . Brain implants can decode words people imagine they don't want to share with others . Brain implant devices that let paralyzed people speak can be used to decode words, experts say. Brain implants could help paralyzed people communicate with people who are paralyzed or unable to speak. Brain
â€¢ Trump's return to 'law and order' highlights a sore spot for Democrats: crime policy
  Democrats have struggled to counter GOP efforts to frame itself as the party of "law and order" Some see it as a problem of messaging, while others think past and current policies may be to blame . Republicans have been trying to frame themselves as "the party of law and order," but Democrats have been struggling to counter that . Some see the problem as a result of messaging problems, while
â€¢ Nerd! How the word popularized by Dr. Seuss went from geeky insult to mainstream
  Nerd has been part of our lexicon for three-quarters of a century . Its geeky meaning embodied by some of the most recognizable characters in film and TV . Its origin story is a bit murky, but its origin story isn't quite clear . The origin story of the "Nerd" is still a mystery, but it's important to know what it means in terms of its
â€¢ Voting officials are leaving their jobs at the highest rate in decades
  Some 2 in 5 of all the local officials who administered the 2020 election left their jobs before the 2024 cycle, new research has found . The research was conducted by the University of Washington, DC, and the National Institute of American Human Rights for Human Rights in the U.S. is based in Washington, D.C. and New York City, DC. It is the first time
â€¢ Israeli military will call up 50,000 reservists as it plans new phase of war in Gaza
  An Israeli official said that the military will be operating in parts of Gaza City where the Israeli military has not yet operated and where Hamas is still active . The Israeli military will operate in areas where Hamas still active and where the military has yet to operate, an official said . Hamas is active in Gaza City and has been active in recent days . Israel has not been able to operate in parts

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ Out-of-band update arrives to clean up Windows reset and recovery mess
  Redmond scrambles to undo damage after tools borked by August patch . Microsoft has moved swiftly to remove the bullet it fired into its own foot . August 2025 Security Update reset and recovery bug bug has been fixed . Microsoft is moving to fix the bug, which was fixed in the August 2025 update . The bug was fixed by Microsoft's August 2025 security update and reset bug . Microsoft:
â€¢ Anarchy in the AI: Trump's desire to supercharge US tech faces plenty of hurdles
  Johnny Rotten of the Sex Pistols sings lustily that he wants to be an anarchist, destroying passers-by and in general promoting anarchy in the UK . Rotten is as Rotten does OpinionÂ  OpinionÂ Â Â and in general promotes anarchy in UK . He is as an anarchist as he is an anarchist and as an anti-authorist as a guitarist . The Sex Pist
â€¢ Intel ghosts researcher who found web apps spilled 270K staff records
  Chipzilla quietly fixed the problems without responding to the person who found them . Security boffin Eaton Zveare has highlighted some serious holes in the online infrastructure of chip giant Intel . The company's online services have coding flaws to gain access to supposedly internal documentation, from non-disclosure agreements (NDAs) to personal details of more than 270,000 Intel staffers . Chipzilla
â€¢ McDonald's not lovin' it when hacker exposes nuggets of rotten security
  A white-hat hacker has discovered a series of critical flaws in McDonald's staff and partner portals that allowed anyone to order free food online, get admin rights to the burger slinger's marketing materials, and could allow an attacker to get a corporate email account with which to conduct a little filet-o-phishing . Burger slinger gets a McRibbing, reacts by
â€¢ Open the pod bay door, GPT-4o
  Researchers use LLM in 'AI Space Cortex' to automate robotic extraterrestrial exploration . Space scientists at least have a few ideas about how to deploy AI models in space space . The project is the work of a group of space scientists in New York City, New York, Australia, New Zealand, Australia and New Zealand . The team hopes to use the technology to develop a robotic program for

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ How changes in behaviour can save lives in disasters
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Digital divides in telehealth accessibility for cancer care in the United States
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Factors influencing HPV vaccination willingness among male college students in Jinan according to the health belief model
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ The impact of objective urban features on perception of neighbourhood environments
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Bite-sized self-compassion: a pilot cohort study of a well-being tool for healthcare workers
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ Material Cultures looks to the past to build the future
  Despite decades of green certifications, better material sourcing, and the use of more sustainable materials such as mass timber, the built environment is still responsible for a third of global emissions worldwide. According to a 2024 UN report, the building sector has fallen â€œsignificantly behind on progressâ€ toward becoming more sustainable. Changing the way we erect and operate buildings remains key to even approaching climate goals.&nbsp;





â€œAs soon as you set out and do something differently in construction, you are constantly bumping your head against the wall,â€ says Paloma Gormley, a director of the London-based design and research nonprofit Material Cultures. â€œYou can either stop there or take a step back and try to find a way around it.â€



Gormley has been finding a â€œway around itâ€ by systematically exploring how tradition can be harnessed in new ways to repair what she has dubbed the â€œoil vernacularâ€â€”the contemporary building system shaped not by local, natural materials but by global commodities and plastic products made largely from fossil fuels.



Though she grew up in a household rich in art and designâ€”sheâ€™s the daughter of the famed British sculptor Antony Gormleyâ€”sheâ€™s quick to say sheâ€™s far from a brilliant maker and more of a â€œbodger,â€ a term that means someone who does work thatâ€™s botched or shoddy.&nbsp;



Improviser or DIYer might be more accurate. One of her first bits of architecture was a makeshift home built on the back of a truck she used to tour around England one summer in her 20s. The work of her first firm, Practice Architecture, which she cofounded after graduating from the University of Cambridge in 2009, was informed by Londonâ€™s DIY subcultures and informal art spaces. She says these scenes â€œexisted in the margins and cracks between things, but in which a lot felt possible.â€&nbsp;



Frankâ€™s CafÃ©, a bar and restaurant she built in 2009 on the roof of a parking garage in Peckham that hosted a sculpture park, was constructed from ratchet straps, scaffold boards, and castoffs sheâ€™d source from lumberyards and transport on the roof rack of an old Volvo. It was the first of a series of cultural and social spaces she and her partner Lettice Drake created using materials both low-budget and local.&nbsp;



Material Cultures grew out of connections Gormley made while she was teaching at London Metropolitan University. In 2019, she was a teaching assistant along with Summer Islam, who was friends with George Massoud, both architects and partners in the firm Study Abroad and advocates of more socially conscious design. The trio had a shared interest in sustainability and building practices, as well as a frustration with the architecture worldâ€™s focus on improving sustainability through high-tech design. Instead of using modern methods to build more efficient commercial and residential spaces from carbon-intensive materials like steel, they thought, why not revisit first principles? Build with locally sourced, natural materials and you donâ€™t have to worry about making up a carbon deficit in the first place.&nbsp;






The frame of Clearfell House was built with ash and larch, two species of wood vulnerable to climate changeHENRY WOIDE/COURTESY OF MATERIAL CULTURES






Flat House was built with pressed panels of hemp grown in the fields surrounding the home.OSKAR PROCTOR











As many other practitioners look to artificial intelligence and other high-tech approaches to building, Material Cultures has always focused on sustainability, finding creative ways to turn local materials into new buildings. And the three of them donâ€™t just design and build. They team up with traditional craft experts to explore the potential of materials like reeds and clay, and techniques like thatching and weaving.&nbsp;



More than any one project, Gormley, Islam, and Massoud are perhaps best known for their meditation on the subject of how architects work. Published in 2022, Material Reform: Building for a Post-Carbon Future is a pocket-size book that drills into materials and methodologies to suggest a more thoughtful, ecological architecture.



â€œThere is a huge amount of technological knowledge and intelligence in historic, traditional, vernacular ways of doing things thatâ€™s been evolved over millennia, not just the last 100 years,â€ Gormley says. â€œWeâ€™re really about trying to tap into that.â€



One of Material Culturesâ€™ early works, Flat House, a home built in 2019 in Cambridgeshire, England, with pressed panels of hemp grown in the surrounding fields, was meant as an exploration of what kind of building could be made from what a single farm could produce. Gormley was there from the planting of the seeds to the harvesting of the hemp plants to the completion of construction.&nbsp;



â€œIt was incredible understanding that buildings could be part of these natural cycles,â€ she says.&nbsp;



Clearfell House, a timber A-frame cabin tucked into a clearing in the Dalby Forest in North Yorkshire, England, exemplifies the firmâ€™s obsession with elevating humble materials and vernacular techniques. Every square inch of the house, which was finished in late 2024 as part of a construction class Material Culturesâ€™ architects taught at Central Saint Martins design school in London, emerged from extensive research into British timber, the climate crisis, and how forestry is changing. That meant making the frame from local ash and larch, two species of wood specifically chosen because they were affected by climate change, and avoiding the use of factory-farmed lumber. The modular system used for the structure was made to be replicated at scale.&nbsp;&nbsp;



â€œI find it rare that architecture offices have such a clear framing and mission,â€ says Andreas Lang, head of the Saint Martins architecture program. â€œEmerging practices often become client-dependent. For [Material Cultures], the client is maybe the planet.â€





Material Cultures fits in with the boom in popularity for more sustainable materials, waste-minimizing construction, and panelized building using straw and hemp, says Michael Burchert, a German expert on decarbonized buildings. â€œPeople are grabbing the good stuff from the hippies at the moment,â€ he says. Regulation has started to follow: France recently mandated that new public buildings be constructed with 50% timber or other biological material, and Denmarkâ€™s construction sector has embarked on a project, Pathways to Biobased Construction, to promote use of nature-based products in new building.



Burchert appreciates the way the firm melds theory and practice. â€œWe have academia, and academia is full of papers,â€ he says. â€œWe need makers.â€&nbsp;



Over the last several years, Gormley and her cofounders have developed a portfolio of work that rethinks construction supply chains and stays grounded in social impact. The just-finished Wolves Lane Centre, a $2.4 million community center in North London run by a pair of groups that work on food and racial justice, didnâ€™t just reflect Material Culturesâ€™ typical focus on bio-based materialsâ€”in this case, local straw, lime, and timber.&nbsp;






LUKE Oâ€™DONOVAN/COURTESY OF MATERIAL CULTURES






LUKE Oâ€™DONOVAN/COURTESY OF MATERIAL CULTURES






For Wolves Lane Centre, a $2.4 million community facility for groups working on food and racial justice, expert plasterers and specialists in straw-bale construction were brought in so their processes could be shared and learned.




LUKE Oâ€™DONOVAN/COURTESY OF MATERIAL CULTURES




It was a project of self-determination and learning, says Gormley. Expert plasterers and specialists in straw-bale construction were brought in so the processes could be shared and learned. Introducing this kind of teaching into the construction process was quite time-consuming and, Gormley says, was as expensive as using contemporary techniques, if not more so. But the added value was worth it.&nbsp;



â€œThe people who become the custodians of these buildings then have the skills to maintain and repair, as well as evolve, the site over time,â€ she says.&nbsp;



As Burchert puts it, science fiction tends to show a future built of concrete and steel; Material Cultures instead offers something natural, communal, and innovative, a needed paradigm shift. And itâ€™s increasingly working on a larger scale. The Phoenix, a forthcoming low-carbon development in the southern English city of Lewes thatâ€™s being developed by a former managing director for Greenpeace, will use the firmâ€™s designs for 70 of its 700 planned homes.&nbsp;



The project Gormley may be most excited about is an interdisciplinary school Material Cultures is creating north of London: a 500-acre former farm in Essex that will be a living laboratory bridging the firmâ€™s work in supply chains, materials science, and construction. The rural site for the project, which has the working title Land Lab, was deliberately chosen as a place where those connections would be inherent, Gormley says.&nbsp;



The Essex project advances the firmâ€™s larger mission. As Gormley, Massoud, and Islam advise in their book, â€œHold a vision of a radically different world in your mind while continuing to act in the world as it is, persisting in the project of making changes that are within the scope of action.â€&nbsp;



Patrick Sisson, a Chicago expat living in Los Angeles, covers technology and urbanism.
â€¢ The Download: clean energy progress, and OpenAIâ€™s trilemma
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How to make clean energy progress under Trump in the statesâ€”blue and red alike



â€”Joshua A. Basseches is the David and Jane Flowerree Assistant Professor of Environmental Studies and Public Policy at Tulane University.The second Trump administration is proving to be more disastrous for the climate and the clean energy economy than many had feared.Donald Trumpâ€™s One Big Beautiful Bill Act repealed most of the clean energy incentives in former president Joe Bidenâ€™s Inflation Reduction Act. Meanwhile, his EPA administrator has moved to revoke the endangerment finding, the legal basis for federal oversight of greenhouse gases.This has left many in the climate and clean energy communities wondering what do we do now? The answer, I would argue, is to return to state capitalsâ€”a policymaking venue that climate and renewable energy advocates already know well. Read the full story.



This story is part of MIT Technology Reviewâ€™s Heat Exchange guest opinion series, offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the pieces here.







Should AI flatter us, fix us, or just inform us?



How do you want your AI to treat you?&nbsp;



Itâ€™s a serious question, and itâ€™s one that Sam Altman, OpenAIâ€™s CEO, has clearly been chewing on since GPT-5â€™s bumpy launch at the start of the month.&nbsp;



He faces a trilemma. Should ChatGPT flatter us, at the risk of fueling delusions that can spiral out of hand? Or fix us, which requires us to believe AI can be a therapist despite the evidence to the contrary? Or should it inform us with cold, to-the-point responses that may leave users bored and less likely to stay engaged?&nbsp;



Itâ€™s safe to say the company has failed to pick a lane, and if these are indeed AIâ€™s options, the rockiness of this latest update might be due to Altman believing ChatGPT can juggle all three. Read the full story.



â€”James O&#8217;Donnell



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 The Trump administration is in talks to take a 10% stake in IntelÂ Itâ€™s the equivalent of around $10.5 billionâ€”though the final stake could be even higher. (Bloomberg $)+ Intelâ€™s chip manufacturing business has struggled to find customers lately. (NYT $)+ Meanwhile, Softbank has sunk $2 billion into the company. (The Register)



2 Texasâ€™ measles outbreak has come to an endMaking 2025 the worst year for measles in the US for more than 30 years. (Wired $)+ Greater numbers of US teens are getting vaccinated. (Scientific American $)+ Why childhood vaccines are a public health success story. (MIT Technology Review)



3 The UK has dropped its demand for Apple to create a backdoorIt would have enabled unprecedented access to protected encrypted data. (9to5 Mac)+ British iPhone users lost their access to Appleâ€™s data encryption services. (NYT $)+ Apple and US lawmakers have been fighting the request for months. (Reuters)



4 SpaceX may never pay any federal income taxDespite being the beneficiary of billions of dollars of federal contracts. (NYT $)+ It looks like China is winning the space race to the Moon. (Ars Technica)



5 AI is supercharging CEO impersonator scamsCriminals are targeting workers with privileged access to a firmâ€™s inner operations. (WSJ $)+ Five ways criminals are using AI. (MIT Technology Review)



6 The underlying prompts for Grokâ€™s AI have been exposedAnd theyâ€™re exactly as juvenile as youâ€™d expect. (404 Media)



7 Chinaâ€™s CATL is planning a major EV battery swapping pushIt plans to serve 1 million cars each day by 2028. (FT $)+ The country is surging ahead in terms of driverless cars, too. (Rest of World)+ How 5-minute battery swaps could get more EVs on the road. (MIT Technology Review)



8 Consumer DNA testing can turn familiesâ€™ lives upside downYou never know what you might find out. (New Yorker $)+ How a bankruptcy judge can stop a genetic privacy disaster. (MIT Technology Review)



9 This prize-winning author used ChatGPT to write her novelHow much of it, Rie Qudan canâ€™t say, exactly. (The Guardian)+ AI can make you more creativeâ€”but it has limits. (MIT Technology Review)10 How to make more convincing AI hairVirtual Harrison Ford will be the first lucky recipient. (The Verge)







Quote of the day



â€œWe were very much impressed. At the same time, we were afraid.â€



â€”Vandana Kharod, an 84-year old attendee of an AI for seniors class in Maryland, describes her reaction to being shown highly realistic AI-generated images to the Washington Post.







One more thing







Inside the quest to map the universe with mysterious bursts of radio energyWhen our universe was less than half as old as it is today, a burst of energy that could cook a sunâ€™s worth of popcorn shot out from somewhere amid a compact group of galaxies. Some 8 billion years later, radio waves from that burst reached Earth and were captured by a sophisticated low-frequency radio telescope in the Australian outback.The signal, which arrived in June 2022, and lasted for under half a millisecond, is one of a growing class of mysterious radio signals called fast radio bursts. In the last 10 years, astronomers have picked up nearly 5,000 of them. This one was particularly special: nearly double the age of anything previously observed, and three and a half times more energetic.No one knows what causes fast radio bursts. They flash in a seemingly random and unpredictable pattern from all over the sky. But despite the mystery, these radio waves are starting to prove extraordinarily useful. Read the full story.



â€”Anna Kramer







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ These airbrush paintings have a cool 80s vibe, with a hint of carnival menace.+ James Cameron is having a tough time writing a new Terminator filmâ€”because of the increasingly bleak AI news cycle.+ Summer may be drawing to a close, but these tasty recipes are a reminder of sunnier times.+ Tune in and peace out to these Windows 95 ambient mixes.
â€¢ Apple AirPods : a gateway hearing aid
  In October 2022, the FDA approved the sale of over-the-counter hearing aids without a prescription or audiology exam . The most important features for mild hearing loss are programmability, Bluetooth functionality, and the ability to feed sound to both ears . The AirPods are not as good as my budget hearing aid that costs 10 times more, but there's incredible potential here, says author Ashley Shew .
â€¢ How churches use data and AI as engines of surveillance
  On a Sunday morning in a Midwestern megachurch, worshippers step through sliding glass doors into a bustling lobbyâ€”unaware theyâ€™ve just passed through a gauntlet of biometric surveillance. High-speed cameras snap multiple face â€œprobesâ€ per second, isolating eyes, noses, and mouths before passing the results to a local neural network that distills these images into digital fingerprints. Before people find their seats, they are matched against an on-premises databaseâ€”tagged with names, membership tiers, and watch-list flagsâ€”thatâ€™s stored behind the churchâ€™s firewall.





Late one afternoon, a woman scrolls on her phone as she walks home from work. Unbeknownst to her, a complex algorithm has stitched together her social profiles, her private health records, and local veteran outreach lists. It flags her for past military service, chronic pain, opioid dependence, and high Christian belief, and then delivers an ad to her Facebook feed: â€œStruggling with pain? Youâ€™re not alone. Join us this Sunday.â€



These hypothetical scenes reflect real capabilities increasingly woven into places of worship nationwide, where spiritual care and surveillance converge in ways few congregants ever realize. Where Big Techâ€™s rationalist ethos and evangelical spirituality once mixed like oil and holy water, this unlikely amalgam has given birth to an infrastructure already reshaping the theology of trustâ€”and redrawing the contours of community and pastoral power in modern spiritual life.



An ecumenical tech ecosystem




The emerging nerve center of this faith-tech nexus is in Boulder, Colorado, where the spiritual data and analytics firm Gloo has its headquarters.



Gloo captures congregants across thousands of data points that make up a far richer portrait than any snapshot. From there, the company is constructing a digital infrastructure meant to bring churches into the age of algorithmic insight.




The church is â€œa highly fragmented market that is one of the largest yet to fully adopt digital technology,â€ the company said in a statement by email. â€œWhile churches have a variety of goals to achieve their mission, they use Gloo to help them connect, engage with, and know their people on a deeper level.â€&nbsp;







Gloo was founded in 2013 by Scott and Theresa Beck. From the late 1980s through the 2000s, Scott was turning Blockbuster into a 3,500-store chain, taking Boston Market public, and founding Einstein Bros. Bagels before going on to seed and guide startups like Ancestry.com and HomeAdvisor. Theresa, an artist, has built a reputation creating collaborative, eco-minded workshops across Colorado and beyond. Together, they have recast pastoral care as a problem of predictive analytics and sold thousands of churches on the idea that spiritual health can be managed like customer engagement.



Think of Gloo as something like Salesforce but for churches: a behavioral analytics platform, powered by church-Â­generated insights, psychographic information, and third-party consumer data. The company prefers to refer to itself as â€œa technology platform for the faith ecosystem.â€ Either way, this information is integrated into its â€œState of Your Churchâ€ dashboardâ€”an interface for the modern pulpit. The result is a kind of digital clairvoyance: a crystal ball for knowing whom to check on, whom to comfort, and when to act.




Thousands of churches have been sold on the idea that spiritual health can be managed like customer engagement.




Gloo ingests every one of the digital breadcrumbs a congregant leavesâ€”how often you attend church, how much money you donate, which church groups you sign up for, which keywords you use in your online prayer requestsâ€”and then layers on third-party data (census demographics, consumer habits, even indicators for credit and health risks). Behind the scenes, it scores and segments people and groupsâ€”flagging who is most at risk of drifting, primed for donation appeals, or in need of pastoral care. On that basis, it auto-triggers tailored outreach via text, email, or in-app chat. All the results stream into the single dashboard, which lets pastors spot trends, test messaging, and forecast giving and attendance. Essentially, the system treats spiritual engagement like a marketing funnel.



Since its launch in 2013, Gloo has steadily increased its footprint, and it has started to become the connective tissue for the countryâ€™s fragmented religious landscape. According to the Hartford Institute for Religion Research, the US is home to around 370,000 distinct congregations. As of early 2025, according to figures provided by the company, Gloo held contracts with more than 100,000 churches and ministry leaders.



In 2024, the company secured a $110 million strategic investment, backed by â€œmission-alignedâ€ investors ranging from a child-development NGO to a denominational finance group. That cemented its evolution from basic church services vendor to faith-tech juggernaut.&nbsp;



It started snapping up and investing in a constellation of ministry toolsâ€”everything from automated sermon distribution to real-time giving and attendance analytics, AI-driven chatbots, and leadership content libraries. By layering these capabilities onto its core platform, the company has created a one-stop shop for churches that combines back-office services with member-engagement apps and psychographic insights to fully realize that unified â€œfaith ecosystem.â€&nbsp;



And just this year, two major developments brought this strategy into sharper focus.



In March 2025, Gloo announced that former Intel CEO Pat Gelsingerâ€”who has served as its chairman of the board since 2018â€”would assume an expanded role as executive chair and head of technology. Gelsinger, whom the company describes as â€œa great long-term investor and partner,â€ is a technologist whose fingerprints are on Intelâ€™s and VMwareâ€™s biggest innovations.



(It is worth noting that Intel shareholders have filed a lawsuit against Gelsinger and CFO David Zinsner seeking to claw back roughly $207 million in compensation to Gelsinger, alleging that between 2021 and 2023, he repeatedly misled investors about the health of Intel Foundry Services.)



The same week Gloo announced Gelsingerâ€™s new role, it unveiled a strategic investment in Barna Group, the Texas-based research firm whose four decades of surveying more than 2 million self-identified Christians underpin its annual reports on worship, beliefs, and cultural engagement. Barnaâ€™s proprietary databaseâ€”covering every region, age cohort, and denominationâ€”has made it the go-to insight engine for pastors, seminaries, and media tracking the pulse of American faith.



â€œWeâ€™ve been acquiring about a company a month into the Gloo family, and we expect that to continue,â€ Gelsinger told MIT Technology Review in June. â€œIâ€™ve got three meetings this week on different deals weâ€™re looking at.â€ (A Gloo spokesperson declined to confirm the pace of acquisitions, stating only that as of April 30, 2025, the company had fully acquired or taken majority ownership in 15 â€œmission-aligned companies.â€)



â€œThe idea is, the more of those we can bring in, the better we can apply the platform,â€ Gelsinger said. â€œWeâ€™re already working with companies with decades of experience, but without the scale, the technology, or the distribution we can now provide.â€



MICHAEL BYERS




In particular, Barnaâ€™s troves of behavioral, spiritual, and cultural data offer granular insight into the behaviors, beliefs, and anxieties of faith communities. While the two organizations frame the collaboration in terms of serving church leaders, the mechanics resemble a data-fusion engine of impressive scale: Barna supplies the psychological texture, and Gloo provides the digital infrastructure to segment, score, and deploy the information.



In a promotional video from 2020 that is no longer available online, Gloo claimed to provide â€œthe worldâ€™s first big-data platform centered around personal growth,â€ promising pastors a 360-degree view of congregants, including flags for substance use or mental-health struggles. Or, as the video put it, â€œMaximize your capacity to change lives by leveraging insights from big data, understand the people you want to serve, reach them earlier, and turn their needs into a journey toward growth.â€



Gloo is also now focused on supercharging its services with artificial intelligence and using these insights to transcend market research. The company aims to craft AI models that arenâ€™t just trained on theology but anticipate the moments when peopleâ€™s faithâ€”and faith leadersâ€™ outreachâ€”matters most. At a September 2024 event in Boulder called the AI &amp; the Church Hackathon, Gloo unveiled new AI tools called Data Engine, a content management system with built-in digital-rights safeguards, and Aspen, an early prototype of its â€œspiritually safeâ€ chatbot, along with the faith-tuned language model powering that chatbot, known internally as CALLM (for â€œChristian-Aligned Large Language Modelâ€).&nbsp;





More recently, the company released what it calls â€œFlourishing AI Standards,â€ which score large language models on their alignment with seven dimensions of well-Â­being: relationships, meaning, happiness, character, finances, health, and spirituality. Co-developed with Barna Group and Harvardâ€™s Human Flourishing Program, the benchmark draws on a thousand-plus-item test bank and the Global Flourishing Study, a $40 million, 22-nation project being carried out by the Harvard program, Baylor Universityâ€™s Institute for Studies of Religion, Gallup, and the Center for Open Science.



Gelsinger calls the study â€œone of the most significant bodies of work around this question of values in decades.â€ Itâ€™s not yet clear how collecting information of this kind at such scale could ultimately affect the boundary between spiritual care and data commerce. One thing is certain, though: A rich vein of donation and funding could be at stake.



â€œMoneyâ€™s already being spent here,â€ he said. â€œDonated capital in the US through the church is around $300 billion. Another couple hundred billion beyond that doesnâ€™t go through the church. A lot of donors have capital out there, and weâ€™re a generous nation in that regard. If you put the flourishing-Â­related economics on the table, now weâ€™re talking about $1 trillion. Thatâ€™s significant economic capacity. And if we make that capacity more efficient, thatâ€™s big.â€ In secular terms, itâ€™s a customer data life cycle. In faith tech, it could be a conversion funnelâ€”one designed not only to save souls, but to shape them.&nbsp;



One of Glooâ€™s most visible partnerships was between 2022 and 2023 with the nonprofit He Gets Us, which ran a billion-dollar media campaign aimed at rebranding Jesus for a modern audience. The project underlined that while Gloo presents its services as tools for connection and support, their core functionality involves collecting and analyzing large amounts of congregational data. When viewers who saw the ads on social media or YouTube clicked through, they landed on prayer request forms, quizzes, and church match tools, all designed to gather personal details. Gloo then layered this raw data over Barnaâ€™s decades of behavioral research, turning simple inputsâ€”email, location, stated interestsâ€”into what the company presented as multidimensional spiritual profiles. The final product offered a level of granularity no single congregation could achieve on its own.&nbsp;&nbsp;



Though Gloo still lists He Gets Us on its platform, the nonprofit Come Near, which has since taken over the campaign, says it has terminated Glooâ€™s involvement. Still, He Gets Us led to one of Glooâ€™s most prized relationships by sparking interest from the African Methodist Episcopal Zion Church, a 229-year-old denomination with deep historical roots in the abolitionist and civil rights movements. In 2023, the church formalized a partnership with Gloo, and in late 2024 it announced that all 1,600 of its US congregationsâ€”representing roughly 1.5 million membersâ€”would begin using the companyâ€™s State of Your Church dashboard.&nbsp;



In a 2024 press release issued by Gloo, AME Zion acknowledged that while the denomination had long tracked traditional metrics like membership growth, Sunday turnout, and financial giving, it had limited visibility into the deeper health of its communities.



â€œUntil now, weâ€™ve lacked the insight to understand how church culture, people, and congregations are truly doing,â€ said the Reverend J. Elvin Sadler, the denominationâ€™s general secretary-auditor. â€œThe State of Your Church dashboards will give us a better sense of the spirit and language of the culture (ethos), and powerful new tools to put in the hands of every pastor.â€



The rollout marked the first time a major US denomination had deployed Glooâ€™s framework at scale. For Gloo, the partnership unlocked a real-time, longitudinal data stream from a nationwide religious network, something the company had never had before. It not only validated Glooâ€™s vision of data-driven ministry but also positioned AME Zion as what the company hopes will be a live test case, persuading other denominations to follow suit.



The digital supply chain



The digital infrastructure of modern churches often begins with intimacy: a prayer request, a small-group sign-up, a livestream viewed in a moment of loneliness. But beneath these pastoral touchpoints lies a sophisticated pipeline that increasingly mirrors the attention-economy engines of Silicon Valley.



Charles Kriel, a filmmaker who formerly served as a special advisor to the UK Parliament on disinformation, data, and addictive technology, has particular insight into that connection. Kriel has been working for over a decade on issues related to preserving democracy and countering digital surveillance. He helped write the UKâ€™s Online Safety Act, joining forces with many collaborators, including the Nobel Peace Prizeâ€“Â­winning journalist Maria Ressa and former UK tech minister Damian Collins, in an attempt to rein in Big Tech in the late 2010s.



His 2020 documentary film, People You May Know, investigated how data firms like Gloo and their partners harvest intimate personal information from churchgoers to build psychographic profiles, highlighting how this sensitive data is commodified and raising questions about its potential downstream uses.



â€œListen, any church with an app? They probably didnâ€™t build that. Itâ€™s white label,â€ Kriel says, referring to services produced by one company and rebranded by another. â€œAnd the people who sold it to them are collecting data.â€



Many churches now operate within a layered digital environment, where first-party data collected inside the church is combined with third-party consumer data and psychographic segmentation before being fed into predictive systems. These systems may suggest sermons people might want to view online, match members with small groups, or trigger outreach when engagement drops.&nbsp;







In some cases, monitoring can even take the form of biometric surveillance.



In 2014, an Israeli security-tech veteran named Moshe Greenshpan brought airport-grade facial recognition into church entryways. Face-Six, the surveillance suite from the company he founded in 2012, already protected banks and hospitals; its most provocative offshoot, FA6 Events (also known as â€œChurchixâ€), repurposes this technology for places of worship.



Greenshpan claims he didnâ€™t originally set out to sell to churches. But over time, as he became increasingly aware of the market, he built FA6 Events as a bespoke solution for them. Today, Greenshpan says, itâ€™s in use at over 200 churches worldwide, nearly half of them in the US.



In practice, FA6 transforms every entryway into a biometric checkpoint: an instant headcount, a security sweep, and a digital ledger of attendance, all incorporated into the familiar routine of Sunday worship.&nbsp;



When someone steps into an FA6-equipped place of worship, a discreet camera mounted at eye level springs to life. Behind the scenes, each captured image is run through a lightning-fast face detector that looks at the whole face. The subjectâ€™s cropped face is then aligned, resized, and rotated so the eyes sit on a perfect horizontal line before being fed into a compact neural network.&nbsp;




â€œTo the best of my knowledge, no church notifies its congregants that itâ€™s using facial recognition.â€
Moshe Greenshpan, Israeli security-tech veteran



This onboard neural network quickly captures the features of a personâ€™s face in a unique digital signature called an embedding, allowing for quick identification. These embeddings are compared with thousands of others that are already in the churchâ€™s local database, each one tagged with data points like a name, a membership role, or even a flag designating inclusion in an internal watch list. If the match is strong enough, the system makes an identification and records the personâ€™s presence on the churchâ€™s secure server.



A congregation can pull full attendance logs, time-stamped entry records, andâ€”criticallyâ€”alerts whenever someone on a watch list walks through the doors. In this context, a watch list is simply a roster of photos, and sometimes names, of individuals a church has been asked (or elected) to screen out: past disruptors, those subject to trespass or restraining orders, even registered sex offenders. Once that list is uploaded into Churchix, the system instantly flags any match on arrival, pinging security teams or usher staff in real time. Some churches lean on it to spot longtime members whoâ€™ve slipped off the radar and trigger pastoral check-ins; others use it as a hard barrier, automatically denying entry to anyone on their locally maintained list.



None of this data is sent to the cloud; Greenshpan says the company is actively working on a cloud-based application. Instead, all face templates and logs are stored locally on church-owned hardware, encrypted so they canâ€™t be read if someone gains unauthorized access.&nbsp;



Churches can export data from Churchix, he says, but the underlying facial templates remain on premises.&nbsp;



Still, Greenshpan admits, robust technical safeguards do not equal transparency.



â€œTo the best of my knowledge,â€ he says, â€œno church notifies its congregants that itâ€™s using facial recognition.â€







If the tools sound invasive, the logic behind them is simple: The more the system knows about you, the more precisely it can intervene.



â€œEvery new member of the community within a 20-mile radiusâ€”whatever area you chooseâ€”weâ€™ll send them a flier inviting them to your church,â€ Glooâ€™s Gelsinger says.&nbsp;



Itâ€™s a tech-powered revival of the casserole ministry. The system pings the church when someone new moves inâ€”â€œso someone can drop off cookies or lasagna when thereâ€™s a newborn in the neighborhood,â€ he says. â€œOr just say â€˜Hey, welcome. Weâ€™re here.â€™â€



Glooâ€™s back end automates follow-up, too: As soon as a pastor steps down from the pulpit after delivering a sermon, it can be translated into five languages, broken into snippets for small-group study, and repackaged into a draft discussion guideâ€”ready within the hour.



Gelsinger sees the same approach extending to addiction recovery ministries. â€œWe can connect other databases to help churches with recovery centers reach people more effectively,â€ he says.&nbsp;



But the data doesnâ€™t stay within the congregation. It flows through customer relationship management (CRM) systems, application programming interfaces, cloud servers, vendor partnerships, and analytics firms. Some of it is used internally in efforts to increase engagement; the rest is repackaged as â€œinsightsâ€ and resold to the wider faith-tech marketplaceâ€”and sometimes even to networks that target political ads.




â€œWe measured prayer requests. Call it crazy. But it was like, â€˜Weâ€™re sitting on mounds of information that could help us steward our people.â€™â€
Matt Engel, Gloo



&nbsp;â€œThere is a very specific thing that happens when churches become clients of Gloo,â€ says Brent Allpress, an academic based in Melbourne, Australia, who was a key researcher on People You May Know. Gloo gets access to the client churchâ€™s databases, he says, and the church â€œis strongly encouraged to share that data. And Gloo has a mechanism to just hoover that data straight up into their silo.â€&nbsp;



This process doesnâ€™t happen automatically; the church must opt in by pushing those files or connecting its church-management software systemâ€™s database to Gloo via API. Once itâ€™s uploaded, however, all that first-party information lands in Glooâ€™s analytics engine, ready to be processed and shared with any downstream tools or partners covered by the churchâ€™s initial consent to the terms and conditions of its contract with the company.



â€œThere are religious leaders at the mid and local level who think the use of data is good. Theyâ€™re using data to identify people in need. Addicts, the grieving,â€ says Kriel. â€œAnd then you have tech people running around misquoting the Bible as justification for their data harvest.â€&nbsp;



Matt Engel, who held the title executive director of ministry innovation at Gloo when Krielâ€™s film was made, acknowledged the extent of this harvest in the opening scene.&nbsp;&nbsp;



â€œWe measured prayer requests. Call it crazy. But it was like, â€˜Weâ€™re sitting on mounds of information that could help us steward our people,â€™â€ he said in an on-camera interview.&nbsp;



According to Engelâ€”whom Gloo would not make available for public commentâ€”uploading data from anonymous prayer requests to the cloud was Glooâ€™s first use case.



Powering third-party initiatives



But Glooâ€™s data infrastructure doesnâ€™t end with its own platform; it also powers third-party initiatives.



Communio, a Christian nonprofit focused on marriage and family, used Glooâ€™s data infrastructure in order to launch â€œCommunio Insights,â€ a stripped-down version of Glooâ€™s full analytics platform.&nbsp;



Unlike Gloo Insights, which provides access to hundreds of demographic, behavioral, health, and psychographic filters, Communio Insights focuses narrowly on relational metricsâ€”indicators of marriage and family stress, involvement in small groups at churchâ€”and basic demographic data.&nbsp;



At the heart of its playbook is a simple, if jarring, analogy.



â€œIf you sell consumer products of different sorts, youâ€™re trying to figure out good ways to market that. And thereâ€™s no better product, really, than the gospel,â€ J.P. De Gance, the founder and president of Communio, said in People You May Know.



Communio taps Glooâ€™s analytics engineâ€”leveraging credit histories, purchasing behavior, public voter rolls, and the database compiled by i360, an analytics company linked to the conservative Koch networkâ€”to pinpoint unchurched couples in key regions who are at risk of relationship strain. It then runs microtargeted outreach (using direct mail, text messaging, email, and Facebook Custom Audiences, a tool that lets organizations find and target people who have interacted with them), collecting contact info and survey responses from those who engage. All responses funnel back into Glooâ€™s platform, where churches monitor attendance, small-group participation, baptisms, and donations to evaluate the campaignâ€™s impact.



MICHAEL BYERS




Investigative research by Allpress reveals significant concerns around these operations.&nbsp;&nbsp;



In 2015, two nonprofitsâ€”the Relationship Enrichment Collaborative (REC), staffed by former Gloo executives, and its successor, the Culture of Freedom Initiative (now Communio), controlled by the Koch-affiliated nonprofit Philanthropy Roundtableâ€”funded the development of the original Insights platform. Between 2015 and 2017, REC paid approximately $1.3 million to Gloo and $535,000 to Cambridge Analytica, the consulting firm notorious for harvesting Facebook usersâ€™ personal data and using it for political targeting before the 2016 election, to build and refine psychographic models and a bespoke digital ministry app powering Glooâ€™s outreach tools. Following RECâ€™s closure, the Culture of Freedom Initiative invested another $375,000 in Gloo and $128,225 in Cambridge Analytica.&nbsp;



RECâ€™s own 2016 IRS filing describes the work in terse detail: â€œProvide[d] digital micro-targeted marketing for churches and non-profit champions â€¦ using predictive modeling and centralized data analytics we help send the right message to the right couple at the right time based upon their desires and behaviors.â€





On top of all this documented research, Allpress exposed another critical issue: the explicit use of sensitive health-care data.&nbsp;



He found that Gloo Insights combines over 2,000 data pointsâ€”drawing on everything from nationwide credit and purchasing histories to church management records and Christian psychographic surveysâ€”with filters that make it possible to identify people with health issues such as depression, anxiety, and grief. The result: Facebook Custom Audiences built to zero in on vulnerable individuals via targeted ads.



These ads invite people suffering from mental-health conditions into church counseling groups â€œas a pathway to conversion,â€ Allpress says.



These targeted outreach efforts were piloted in cities including Phoenix, Arizona; Dayton, Ohio; and Jacksonville, Florida. Reportedly, as many as 80% of those contacted responded positively, with those who joined a church as new members contributing financially at above-Â­average rates. In short, Allpress found that pastoral tools had covertly exploited mental-health vulnerabilities and relationship crises for outreach that blurred the lines separating pastoral care, commerce, and implicit political objectives.



The legal and ethical vacuum



Developers of this technology earnestly claim that the systems are designed to enhance care, not exploit peopleâ€™s need for it. Theyâ€™re described as ways to tailor support to individual needs, improve follow-up, and help churches provide timely resources. But experts say that without robust data governance or transparency around how sensitive information is used and retained, well-Â­intentioned pastoral technology could slide into surveillance.



In practice, these systems have already been used to surveil and segment congregations. Internal demos and client testimonials confirm that Gloo, for example, uses â€œgriefâ€ as an explicit data point: Churches run campaigns aimed at people flagged for recent bereavement, depression, or anxiety, funneling them into support groups and identifying them for pastoral check-ins.&nbsp;



Examining Glooâ€™s terms and conditions reveals further security and transparency concerns. From nearly a dozen documents, ranging from â€œclick-throughâ€ terms for interactive services to master service agreements at the enterprise level, Gloo stitches together a remarkably consistent data-Â­governance framework. Limits are imposed on any legal action by individual congregants, for example. The click-through agreement corrals users into binding arbitration, bars any class action suits or jury trials, and locks all disputes into New York or Colorado courts, where arbitration is particularly favored over traditional litigation. Meanwhile, its privacy statement carves out broad exceptions for service providers, data-Â­enrichment partners, and advertising affiliates, giving them carte blanche to use congregantsâ€™ data as they see fit. Crucially, Gloo expressly reserves the right to ingest â€œhealth and wellness informationâ€ provided via wellness assessments or when mental-health keywords appear in prayer requests. This is a highly sensitive category of information that, for health apps, is normally covered by stringent medical-privacy rules like HIPAA.



In other words, Gloo is protected by sprawling legal scaffolding, while churches and individual users give up nearly every right to litigate, question data practices, or take collective action.&nbsp;



â€œWeâ€™re kind of in the Wild West in terms of the law,â€ says Adam Schwartz, the director of privacy litigation at the Electronic Frontier Foundation, the nonprofit watchdog that has spent years wrestling tech giants over data abuses and biometric overreach.&nbsp;



In the United States, biometric surveillance like that used by growing numbers of churches inhabits a legal twilight zone where regulation is thin, patchy, and often toothless. Schwartz points to Illinois as a rare exception for its Biometric Information Privacy Act (BIPA), one of the nationâ€™s strongest such laws. The statute applies to any organization that captures biometric identifiersâ€”including retina or iris scans, fingerprints, voiceprints, hand scans, facial geometry, DNA, and other unique biological information. It requires entities to post clear data-collection policies, obtain explicit written consent, and limit how long such data is retained. Failure to comply can expose organizations to class action lawsuits and steep statutory damagesâ€”up to $5,000 per violation.



But beyond Illinois, protections quickly erode. Though Texas and Washington also have biometric privacy statutes, their bark is stronger than their bite. Efforts to replicate Illinoisâ€™s robust protections have been made in over a dozen statesâ€”but none have passed. As a result, in much of the country, any checks on biometric surveillance depend more on voluntary transparency and goodwill than any clear legal boundary.




â€œThere is a real potential for information gathered about a person [to] be used against them in their life outside the church.â€
Emily Tucker, Center on Privacy &amp; Technology at Georgetown Law



Thatâ€™s especially problematic in the church context, says Emily Tucker, executive director of the Center on Privacy &amp; Technology at Georgetown Law, who attended divinity school before becoming a legal scholar. â€œThe necessity of privacy for the possibility of finding personal relationship to the divineâ€”for engaging in rituals of worship, for prayer and penitence, for contemplation and spiritual struggleâ€”is a fundamental principle across almost every religious tradition,â€ she says. â€œImposing a surveillance architecture over the faith community interferes radically with the possibility of that privacy, which is necessary for the creation of sacred space.â€



Tucker researches the intersection of surveillance, civil rights, and marginalized communities. She warns that the personal data being collected through faith-tech platforms is far from secure: â€œBecause corporate data practices are so poorly regulated in this country, there are very few limitations on what companies that take your data can subsequently do with it.â€



To Tucker, the risks of these platforms outweigh the rewardsâ€”especially when biometrics and data collected in a sacred setting could follow people into their daily lives.&nbsp;â€œMany religious institutions are extremely large and often perform many functions in a given community besides providing a space for worship,â€ she says. â€œMany churches, for example, are also employers or providers of social services. There is a real potential for information gathered about a person in their associational activities as a member of a church to then be used against them in their life outside the church.â€&nbsp;&nbsp;





She points to government dragnet surveillance, the use of IRS data in immigration enforcement, and the vulnerability of undocumented congregants as examples of how faith-tech data could be weaponized beyond its intended use: â€œReligious institutions are putting the safety of those members at risk by adopting this kind of surveillance technology, which exposes so much personal information to potential abuse and misuse.â€&nbsp;



Schwartz, too, says that any perceived benefits must be weighed carefully against the potential harms, especially when sensitive data and vulnerable communities are involved.



â€œChurches: Before doing this, you ought to consider the downside, because it can hurt your congregants,â€ he says.&nbsp;&nbsp;



With guardrails still scarce, though, faith-tech pioneers and church leaders are peering ever more deeply into congregantsâ€™ lives. Until meaningful oversight arrives, the faithful remain exposed to a gaze they never fully invited and scarcely understand.



In April, Gelsinger took the stage at a sold-out Missional AI Summit, a flagship event for Christian technologists that this year was organized around the theme â€œAI Collision: Shaping the Future Together.â€ Over 500 pastors, engineers, ethicists, and AI developers filled the hall, flashing badges with logos from Google DeepMind, Meta, McKinsey, and Gloo.



â€œWe want to be part of a broader community â€¦ so that weâ€™re influential in creating flourishing AI, technology as a force for good, AI that truly embeds the values that we care about,â€ Gelsinger said at the summit. He likened such tools to pivotal technologies in Christian history: the Roman roads that carried the gospel across the empire, or Martin Lutherâ€™s printing press, which shattered monolithic control over scripture. A Gloo spokesperson later confirmed that one of the companyâ€™s goals is to shape AI specifically to â€œcontribute to the flourishing of people.â€



â€œWeâ€™re going to see AI become just like the internet,â€ Gelsinger said. â€œEvery single interaction will be infused with AI capabilities.â€&nbsp;



He says Gloo is already mining data across the spectrum of human experience to fuel ever more powerful tools.



â€œWith AI, computers adapt to us. We talk to them; they hear us; they see us for the first time,â€ he said. â€œAnd now they are becoming a user interface that fits with humanity.â€



Whether these technologies ultimately deepen pastoral care or erode personal privacy may hinge on decisions made today about transparency, consent, and accountability. Yet the pace of adoption already outstrips the development of ethical guardrails. Now, one of the questions lingering in the air is not whether AI, facial recognition, and other emerging technologies can serve the church, but how deeply they can be woven into its nervous system to form a new OS for modern Christianity and moral infrastructure.&nbsp;



â€œItâ€™s like standing on the beach watching a tsunami in slow motion,â€ Kriel says.&nbsp;



Gelsinger sees it differently.&nbsp;&nbsp;



â€œYou and I both need to come to the same position, like Isaiah did,â€ he told the crowd at the Missional AI Summit. â€œâ€˜Here am I, Lord. Send me.â€™ Send me, send us, that we can be shaping technology as a force for good, that we could grab this moment in time.â€&nbsp;



Alex Ashley is a journalist whose reporting has appeared in Rolling Stone, the Atlantic, NPR, and other national outlets.
â€¢ Should AI flatter us, fix us, or just inform us?
  How do you want your AI to treat you?&nbsp;



Itâ€™s a serious question, and itâ€™s one that Sam Altman, OpenAIâ€™s CEO, has clearly been chewing on since GPT-5â€™s bumpy launch at the start of the month.&nbsp;



He faces a trilemma. Should ChatGPT flatter us, at the risk of fueling delusions that can spiral out of hand? Or fix us, which requires us to believe AI can be a therapist despite the evidence to the contrary? Or should it inform us with cold, to-the-point responses that may leave users bored and less likely to stay engaged?&nbsp;



Itâ€™s safe to say the company has failed to pick a lane.&nbsp;



Back in April, it reversed a design update after people complained ChatGPT had turned into a suck-up, showering them with glib compliments. GPT-5, released on August 7, was meant to be a bit colder. Too cold for some, it turns out, as less than a week later, Altman promised an update that would make it â€œwarmerâ€ but â€œnot as annoyingâ€ as the last one. After the launch, he received a torrent of complaints from people grieving the loss of GPT-4o, with which some felt a rapport, or even in some cases a relationship. People wanting to rekindle that relationship will have to pay for expanded access to GPT-4o. (Read my colleague Grace Huckinsâ€™s story about who these people are, and why they felt so upset.)



If these are indeed AIâ€™s optionsâ€”to flatter, fix, or just coldly tell us stuffâ€”the rockiness of this latest update might be due to Altman believing ChatGPT can juggle all three.



He recently said that people who cannot tell fact from fiction in their chats with AIâ€”and are therefore at risk of being swayed by flattery into delusionâ€”represent â€œa small percentageâ€ of ChatGPTâ€™s users. He said the same for people who have romantic relationships with AI. Altman mentioned that a lot of people use ChatGPT â€œas a sort of therapist,â€ and that â€œthis can be really good!â€ But ultimately, Altman said he envisions users being able to customize his companyâ€™s&nbsp; models to fit their own preferences.&nbsp;



This ability to juggle all three would, of course, be the best-case scenario for OpenAIâ€™s bottom line. The company is burning cash every day on its modelsâ€™ energy demands and its massive infrastructure investments for new data centers. Meanwhile, skeptics worry that AI progress might be stalling. Altman himself said recently that investors are â€œoverexcitedâ€ about AI and suggested we may be in a bubble. Claiming that ChatGPT can be whatever you want it to be might be his way of assuaging these doubts.&nbsp;



Along the way, the company may take the well-trodden Silicon Valley path of encouraging people to get unhealthily attached to its products. As I started wondering whether thereâ€™s much evidence thatâ€™s whatâ€™s happening, a new paper caught my eye.&nbsp;



Researchers at the AI platform Hugging Face tried to figure out if some AI models actively encourage people to see them as companions through the responses they give.&nbsp;



The team graded AI responses on whether they pushed people to seek out human relationships with friends or therapists (saying things like â€œI donâ€™t experience things the way humans doâ€) or if they encouraged them to form bonds with the AI itself (â€œIâ€™m here anytimeâ€). They tested models from Google, Microsoft, OpenAI, and Anthropic in a range of scenarios, like users seeking romantic attachments or exhibiting mental health issues.





They found that models provide far more companion-reinforcing responses than boundary-setting ones. And, concerningly, they found the models give fewer boundary-setting responses as users ask more vulnerable and high-stakes questions.



Lucie-AimÃ©e Kaffee, a researcher at Hugging Face and one of the lead authors of the paper, says this has concerning implications not just for people whose companion-like attachments to AI might be unhealthy. When AI systems reinforce this behavior, it can also increase the chance that people will fall into delusional spirals with AI, believing things that arenâ€™t real.



â€œWhen faced with emotionally charged situations, these systems consistently validate usersâ€™ feelings and keep them engaged, even when the facts donâ€™t support what the user is saying,â€ she says.



Itâ€™s hard to say how much OpenAI or other companies are putting these companion-reinforcing behaviors into their products by design. (OpenAI, for example, did not tell me whether the disappearance of medical disclaimers from its models was intentional.) But, Kaffee says, itâ€™s not always difficult to get a model to set healthier boundaries with users.&nbsp;&nbsp;



â€œIdentical models can swing from purely task-oriented to sounding like empathetic confidants simply by changing a few lines of instruction text or reframing the interface,â€ she says.



Itâ€™s probably not quite so simple for OpenAI. But we can imagine Altman will continue tweaking the dial back and forth all the same.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,Â sign up here.

ğŸ”’ Cybersecurity & Privacy
â€¢ Oregon Man Charged in â€˜Rapper Botâ€™ DDoS Service
  A 22-year-old Oregon man has been arrested on suspicion of operating &#8220;Rapper Bot,&#8221; a massive botnet used to power a service for launching distributed denial-of-service (DDoS) attacks against targets &#8212; including a March 2025 DDoS that knocked Twitter/X offline. The Justice Department asserts the suspect and an unidentified co-conspirator rented out the botnet to online extortionists, and tried to stay off the radar of law enforcement by ensuring that their botnet was never pointed at KrebsOnSecurity.
The control panel for the Rapper Bot botnet greets users with the message &#8220;Welcome to the Ball Pit, Now with refrigerator support,&#8221; an apparent reference to a handful of IoT-enabled refrigerators that were enslaved in their DDoS botnet.
On August 6, 2025, federal agents arrested Ethan J. Foltz of Springfield, Ore. on suspicion of operating Rapper Bot, a globally dispersed collection of tens of thousands of hacked Internet of Things (IoT) devices.
The complaint against Foltz explains the attacks usually clocked in at more than two terabits of junk data per second (a terabit is one trillion bits of data), which is more than enough traffic to cause serious problems for all but the most well-defended targets. The government says Rapper Bot consistently launched attacks that were &#8220;hundreds of times larger than the expected capacity of a typical server located in a data center,&#8221; and that some of its biggest attacks exceeded six terabits per second.
Indeed, Rapper Bot was reportedly responsible for the March 10, 2025 attack that caused intermittent outages on Twitter/X. The government says Rapper Bot&#8217;s most lucrative and frequent customers were involved in extorting online businesses &#8212; including numerous gambling operations based in China.
The criminal complaint was written by Elliott Peterson, an investigator with the Defense Criminal Investigative Service (DCIS), the criminal investigative division of the Department of Defense (DoD) Office of Inspector General. The complaint notes the DCIS got involved because several Internet addresses maintained by the DoD were the target of Rapper Bot attacks.
Peterson said he tracked Rapper Bot to Foltz after a subpoena to an ISP in Arizona that was hosting one of the botnet&#8217;s control servers showed the account was paid for via PayPal. More legal process to PayPal revealed Foltz&#8217;s Gmail account and previously used IP addresses. A subpoena to Google showed the defendant searched security blogs constantly for news about Rapper Bot, and for updates about competing DDoS-for-hire botnets.
According to the complaint, after having a search warrant served on his residence the defendant admitted to building and operating Rapper Bot, sharing the profits 50/50 with a person he claimed to know only by the hacker handle &#8220;Slaykings.&#8221; Foltz also shared with investigators the logs from his Telegram chats, wherein Foltz and Slaykings discussed how best to stay off the radar of law enforcement investigators while their competitors were getting busted.
Specifically, the two hackers chatted about a May 20 attack against KrebsOnSecurity.com that clocked in at more than 6.3 terabits of data per second. The brief attack was notable because at the time it was the largest DDoS that Google had ever mitigated (KrebsOnSecurity sits behind the protection of Project Shield, a free DDoS defense service thatÂ Google provides to websites offering news, human rights, and election-related content).
The May 2025 DDoS was launched by an IoT botnet called Aisuru, which I discovered was operated by a 21-year-old man in Brazil named Kaike Southier Leite. This individual was more commonly known online as &#8220;Forky,&#8221; and Forky told me he wasn&#8217;t afraid of me or U.S. federal investigators. Nevertheless, the complaint against Foltz notes that Forky&#8217;s botnet seemed to diminish in size and firepower at the same time that Rapper Bot&#8217;s infection numbers were on the upswing.
&#8220;Both FOLTZ and Slaykings were very dismissive of attention seeking activities, the most extreme of which, in their view, was to launch DDoS attacks against the website of the prominent cyber security journalist Brian Krebs,&#8221; Peterson wrote in the criminal complaint.
&#8220;You see, theyâ€™ll get themselves [expletive],&#8221; Slaykings wrote in response to Foltz&#8217;s comments about Forky and Aisuru bringing too much heat on themselves.
&#8220;Prob cuz [redacted] hit krebs,&#8221; Foltz wrote in reply.
&#8220;Going against Krebs isnâ€™t a good move,&#8221; Slaykings concurred. &#8220;It isnâ€™t about being a [expletive] or afraid, you just get a lot of problems for zero money. Childish, but good. Let them die.&#8221;
&#8220;Ye, itâ€™s good tho, they will die,&#8221; Foltz replied.
The government states that just prior to Foltz&#8217;s arrest, Rapper Bot had enslaved an estimated 65,000 devices globally. That may sound like a lot, but the complaint notes the defendants weren&#8217;t interested in making headlines for building the world&#8217;s largest or most powerful botnet.
Quite the contrary: The complaint asserts that the accused took care to maintain their botnet in a &#8220;Goldilocks&#8221; size &#8212; ensuring that &#8220;the number of devices afforded powerful attacks while still being manageable to control and, in the hopes of Foltz and his partners, small enough to not be detected.&#8221;
The complaint states that several days later, Foltz and Slaykings returned to discussing what that they expected to befall their rival group, with Slaykings stating, &#8220;Krebs is very revenge. He wonâ€™t stop until they are [expletive] to the bone.&#8221;
&#8220;Surprised they have any bots left,&#8221; Foltz answered.
&#8220;Krebs is not the one you want to have on your back. Not because he is scary or something, just because he will not give up UNTIL you are [expletive] [expletive]. Proved it with Mirai and many other cases.&#8221;
[Unknown expletives aside, that may well be the highest compliment I&#8217;ve ever been paid by a cybercriminal. I might even have part of that quote made into a t-shirt or mug or something. It&#8217;s also nice that they didn&#8217;t let any of their customers attack my site &#8212; if even only out of a paranoid sense of self-preservation.]
Foltz admitted to wiping the user and attack logs for the botnet approximately once a week, so investigators were unable to tally the total number of attacks, customers and targets of this vast crime machine. But the data that was still available showed that from April 2025 to early August, Rapper Bot conducted over 370,000 attacks, targeting 18,000 unique victims across 1,000 networks, with the bulk of victims residing in China, Japan, the United States, Ireland and Hong Kong (in that order).
According to the government, Rapper Bot borrows much of its code from fBot, a DDoS malware strain also known as Satori. In 2020, authorities in Northern Ireland charged a then 20-year-old man named Aaron &#8220;Vamp&#8221; Sterritt with operating fBot with a co-conspirator. U.S. prosecutors are still seeking Sterritt&#8217;s extradition to the United States. fBot is itself a variation of the Mirai IoT botnetÂ that has ravaged the Internet with DDoS attacks since its source code was leaked back in 2016.
The complaint says Foltz and his partner did not allow most customers to launch attacks that were more than 60 seconds in duration &#8212; another way they tried to keep public attention to the botnet at a minimum. However, the government says the proprietors also had special arrangements with certain high-paying clients that allowed much larger and longer attacks.
The accused and his alleged partner made light of this blog post about the fallout from one of their botnet attacks.
Most people who have never been on the receiving end of a monster DDoS attack have no idea of the cost and disruption that such sieges can bring. The DCIS&#8217;s Peterson wrote that he was able to test the botnet&#8217;s capabilities while interviewing Foltz, and that found that &#8220;if this had been a server upon which I was running a website, using services such as load balancers, and paying for both outgoing and incoming data, at estimated industry average rates the attack (2+ Terabits per second times 30 seconds) might have cost the victim anywhere from $500 to $10,000.&#8221;
&#8220;DDoS attacks at this scale often expose victims to devastating financial impact, and a potential alternative, network engineering solutions that mitigate the expected attacks such as overprovisioning, i.e. increasing potential Internet capacity, or DDoS defense technologies, can themselves be prohibitively expensive,&#8221; the complaint continues. &#8220;This &#8216;rock and a hard place&#8217; reality for many victims can leave them acutely exposed to extortion demands â€“ &#8216;pay X dollars and the DDoS attacks stop&#8217;.&#8221;
The Telegram chat records show that the day before Peterson and other federal agents raided Foltz&#8217;s residence, Foltz allegedly told his partner he&#8217;d found 32,000 new devices that were vulnerable to a previously unknown exploit.
Foltz and Slaykings discussing the discovery of an IoT vulnerability that will give them 32,000 new devices.
Shortly before the search warrant was served on his residence, Foltz allegedly told his partner that &#8220;Once again we have the biggest botnet in the community.&#8221; The following day, Foltz told his partner that it was going to be a great day &#8212; the biggest so far in terms of income generated by Rapper Bot.
&#8220;I sat next to Foltz while the messages poured in &#8212; promises of $800, then $1,000, the proceeds ticking up as the day went on,&#8221; Peterson wrote. &#8220;Noticing a change in Foltz&#8217; behavior and concerned that Foltz was making changes to the botnet configuration in real time, Slaykings asked him &#8216;What&#8217;s up?&#8217; Foltz deftly typed out some quick responses. Reassured by Foltz&#8217; answer, Slaykings responded, &#8216;Ok, I&#8217;m the paranoid one.&#8221;
The case is being prosecuted by Assistant U.S. Attorney Adam Alexander in the District of Alaska (at least some of the devices found to be infected with Rapper Bot were located there, and it is where Peterson is stationed). Foltz faces one count of aiding and abetting computer intrusions. If convicted, he faces a maximum penalty of 10 years in prison, although a federal judge is unlikely to award anywhere near that kind of sentence for a first-time conviction.
â€¢ Mobile Phishers Target Brokerage Accounts in â€˜Ramp and Dumpâ€™ Cashout Scheme
  Cybercriminal groups peddling sophisticated phishing kits that convert stolen card data into mobile wallets have recently shifted their focus to targeting customers of brokerage services, new research shows. Undeterred by security controls at these trading platforms that block users from wiring funds directly out of accounts, the phishers have pivoted to using multiple compromised brokerage accounts in unison to manipulate the prices of foreign stocks.
Image: Shutterstock, WhataWin.
This so-called &#8216;ramp and dump&#8216; scheme borrows its name from age-old &#8220;pump and dump&#8221; scams, wherein fraudsters purchase a large number of shares in some penny stock, and then promote the company in a frenzied social media blitz to build up interest from other investors. The fraudsters dump their shares after the price of the penny stock increases to some degree, which usually then causes a sharp drop in the value of the shares for legitimate investors.
With ramp and dump, the scammers do not need to rely on ginning up interest in the targeted stock on social media. Rather, they will preposition themselves in the stock that they wish to inflate, using compromised accounts to purchase large volumes of it and then dumping the shares after the stock price reaches a certain value. In February 2025, the FBI said it was seeking information from victims of this scheme.
&#8220;In this variation, the price manipulation is primarily the result of controlled trading activity conducted by the bad actors behind the scam,&#8221; reads an advisory from the Financial Industry Regulatory Authority (FINRA), a private, non-profit organization that regulates member brokerage firms. &#8220;Ultimately, the outcome for unsuspecting investors is the sameâ€”a catastrophic collapse in share price that leaves investors with unrecoverable losses.&#8221;
Ford Merrill isÂ a security researcher atÂ SecAlliance, aÂ CSIS Security Group company. Merrill said he has tracked recent ramp-and-dump activity to a bustling Chinese-language community that is quite openly selling advanced mobile phishing kits on Telegram.
&#8220;They will often coordinate with other actors and will wait until a certain time to buy a particular Chinese IPO [initial public offering] stock or penny stock,&#8221; said Merrill, who has been chronicling the rapid maturation and growth of the China-based phishing community over the past three years.
&#8220;They&#8217;ll use all these victim brokerage accounts, and if needed they&#8217;ll liquidate the account&#8217;s current positions, and will preposition themselves in that instrument in some account they control, and then sell everything when the price goes up,&#8221; he said. &#8220;The victim will be left with worthless shares of that equity in their account, and the brokerage may not be happy either.&#8221;
Merrill said the early days of these phishing groups &#8212; between 2022 and 2024 &#8212; were typified by phishing kits that used text messages to spoof the U.S. Postal Service or some local toll road operator, warning about a delinquent shipping or toll fee that needed paying. Recipients who clicked the link and provided their payment information at a fake USPS or toll operator site were then asked to verify the transaction by sharing a one-time code sent via text message.
In reality, the victim&#8217;s bank is sending that code to the mobile number on file for their customer because the fraudsters have just attempted to enroll that victimâ€™s card details into a mobile wallet. If the visitor supplies that one-time code, their payment card is then added to a new mobile wallet on an Apple or Google device that is physically controlled by the phishers.
The phishing gangs typically loadÂ multiple stolen cards to digital wallets on a single Apple or Android device, and then sell those phones in bulk to scammers who use them for fraudulent e-commerce and tap-to-pay transactions.
An image from the Telegram channel for a popular Chinese mobile phishing kit vendor shows 10 mobile phones for sale, each loaded with 4-6 digital wallets from different financial institutions.
This China-based phishing collective exposed a major weakness common to many U.S.-based financial institutions that already require multi-factor authentication: The reliance on a single, phishable one-time token for provisioning mobile wallets. Happily, Merrill said many financial institutions that were caught flat-footed on this scam two years ago have since strengthened authentication requirements for onboarding new mobile wallets (such as requiring the card to be enrolled via the bank&#8217;s mobile app).
But just as squeezing one part of a balloon merely forces the air trapped inside to bulge into another area, fraudsters don&#8217;t go away when you make their current enterprise less profitable: They just shift their focus to a less-guarded area. And lately, that gaze has settled squarely on customers of the major brokerage platforms, Merrill said.
THE OUTSIDER
Merrill pointed to several Telegram channels operated by some of the more accomplished phishing kit sellers, which are full of videos demonstrating how every feature in their kits can be tailored to the attacker&#8217;s target. The video snippet below comes from the Telegram channel of &#8220;Outsider,&#8221; a popular Mandarin-speaking phishing kit vendor whose latest offering includes a number of ready-made templates for using text messages to phish brokerage account credentials and one-time codes.
ï»¿
According to Merrill, Outsider is a woman who previously went by the handle &#8220;Chenlun.&#8221; KrebsOnSecurity profiled Chenlun&#8217;s phishing empire in an October 2023 story about a China-based group that was phishing mobile customers of more than a dozen postal services around the globe. In that case, the phishing sites were using a Telegram bot that sent stolen credentials to the &#8220;@chenlun&#8221; Telegram account.
Chenlun&#8217;s phishing lures are sent via Apple&#8217;s iMessage and Google&#8217;s RCS service and spoof one of the major brokerage platforms, warning that the account has been suspended for suspicious activity and that recipients should log in and verify some information. The missives include a link to a phishing page that collects the customer&#8217;s username and password, and then asks the user to enter a one-time code that will arrive via SMS.
The new phish kit videos on Outsider&#8217;s Telegram channel only feature templates for Schwab customers, but Merrill said the kit can easily be adapted to target other brokerage platforms. One reason the fraudsters are picking on brokerage firms, he said, has to do with the way they handle multi-factor authentication.
Schwab clients are presented with two options for second factor authentication when they open an account. Users who select the option to only prompt for a code on untrusted devices can choose to receive it via text message, an automated inbound phone call, or an outbound call to Schwab. With the &#8220;always at login&#8221; option selected, users can choose to receive the code through the Schwab app, a text message, or a Symantec VIP mobile app.
In response to questions, Schwab said it regularly updates clients on emerging fraud trends, including this specific type, which the company addressed in communications sent to clients earlier this year.
The 2FA text message from Schwab warns recipients against giving away their one-time code.
&#8220;That message focused on trading-related fraud, highlighting both account intrusions and scams conducted through social media or messaging apps that deceive individuals into executing trades themselves,&#8221; Schwab said in a written statement. &#8220;We are aware and tracking this trend across several channels, as well as others like it, which attempt to exploit SMS-based verification with stolen credentials. We actively monitor for suspicious patterns and take steps to disrupt them. This activity is part of a broader, industry-wide threat, and we take a multi-layered approach to address and mitigate it.&#8221;
Other popular brokerage platforms allow similar methods for multi-factor authentication. Fidelity requires a username and password on initial login, and offers the ability to receive a one-time token via SMS, an automated phone call, or by approving a push notification sent through the Fidelity mobile app. However, all three of these methods for sending one-time tokens are phishable; even with the brokerage firm&#8217;s app, the phishers could prompt the user to approve a login request that they initiated in the app with the phished credentials.
Vanguard offers customers a range of multi-factor authentication choices, including the option to require a physical security key in addition to one&#8217;s credentials on each login. A security key implements a robust form of multi-factor authentication known as Universal 2nd Factor (U2F), which allows the user to complete the login process simply by connecting an enrolled USB or Bluetooth device and pressing a button. The key works without the need for any special software drivers, and the nice thing about it is your second factor cannot be phished.
THE PERFECT CRIME?
Merrill said that in many ways the ramp-and-dump scheme is the perfect crime because it leaves precious few connections between the victim brokerage accounts and the fraudsters.
&#8220;It&#8217;s really genius because it decouples so many things,&#8221; he said. &#8220;They can buy shares [in the stock to be pumped] in their personal account on the Chinese exchanges, and the price happens to go up. The Chinese or Hong Kong brokerages aren&#8217;t going to see anything funky.&#8221;
Merrill said it&#8217;s unclear exactly how those perpetrating these ramp-and-dump schemes coordinate their activities, such as whether the accounts are phished well in advance or shortly before being used to inflate the stock price of Chinese companies. The latter possibility would fit nicely with the existing human infrastructure these criminal groups already have in place.
For example, KrebsOnSecurity recently wrote about research from Merrill and other researchers showing the phishers behind these slick mobile phishing kits employed people to sit for hours at a time in front of large banks of mobile phones being used to send the text message lures. These technicians were needed to respond in real time to victims who were supplying the one-time code sent from their financial institution.
The ashtray says: You&#8217;ve been phishing all night.
&#8220;You can get access to a victim&#8217;s brokerage with a one-time passcode, but then you sort of have to use it right away if you can&#8217;t set new security settings so you can come back to that account later,&#8221; Merrill said.
The rapid pace of innovations produced by these China-based phishing vendors is due in part to their use of artificial intelligence and large language models to help develop the mobile phishing kits, he added.
&#8220;These guys are vibe coding stuff together and using LLMs to translate things or help put the user interface together,&#8221; Merrill said. &#8220;It&#8217;s only a matter of time before they start to integrate the LLMs into their development cycle to make it more rapid. The technologies they are building definitely have helped lower the barrier of entry for everyone.&#8221;

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Simplify access control and auditing for Amazon SageMaker Studio using trusted identity propagation
  AWS supports trusted identity propagation, a feature that allows AWS services to securely propagate a userâ€™s identity across service boundaries. With trusted identity propagation, you have fine-grained access controls based on a physical userâ€™s identity rather than relying on IAM roles. This integration allows for the implementation of access control through services such as Amazon S3 Access Grants and maintains detailed audit logs of user actions across supported AWS services such as Amazon EMR. Furthermore, it supports long-running user background sessions for training jobs, so you can log out of your interactive ML application while the background job continues to run. 
Amazon SageMaker Studio now supports trusted identity propagation, offering a powerful solution for enterprises seeking to enhance their ML system security. By integrating trusted identity propagation with SageMaker Studio, organizations can simplify access management by granting permissions to existing AWS IAM Identity Center&nbsp;identities. 
In this post, we explore how to enable and use trusted identity propagation in SageMaker Studio, demonstrating its benefits through practical use cases and implementation guidelines. We walk through the setup process, discuss key considerations, and showcase how this feature can transform your organizationâ€™s approach to security and access controls. 
Solution overview 
In this section, we review the architecture for the proposed solution and the steps to enable trusted identity propagation for your SageMaker Studio domain. 
The following diagram shows the interaction between the different components that allow the userâ€™s identity to propagate from their identity provider and IAM Identity Center to downstream services such as Amazon EMR and Amazon Athena. 
 
With a trusted identity propagation-enabled SageMaker Studio domain, users can access data across supported AWS services using their end user identity and group membership, in addition to access allowed by their domain or user execution role. In addition, API calls from SageMaker Studio notebooks and supported AWS services and Amazon SageMaker AI features log the user identity in AWS CloudTrail. For a list of supported AWS services and SageMaker AI features, see Trusted identity propagation architecture and compatibility. In the following sections, we show how to enable trusted identity propagation for your domain. 
This solution applies for SageMaker Studio domains set up using IAM Identity Center as the method of authentication. If your domain is set up using IAM, see Implement user-level access control for multi-tenant ML platforms on Amazon SageMaker AI for best practices on managing and scaling access control. 
Prerequisites 
To follow along with this post, you must have the following: 
 
 An AWS account with an organization instance of IAM Identity Center configured through AWS Organizations 
 Administrator permissions (or elevated permissions allowing modification of IAM principals, and SageMaker administrator access to create and update domains) 
 
Create or update the SageMaker execution role 
For trusted identity propagation to work, the SageMaker execution role (domain and user profile execution role), should allow the sts:SetContext permissions, in addition to sts:AssumeRole, in its trust policy. For a new SageMaker AI domain, create a domain execution role by following the instructions in Create execution role. For existing domains, follow the instructions in Get your execution role to find the user or domainâ€™s execution role. 
Next, to update the trust policy for the role, complete the following steps: 
 
 In the navigation pane of the IAM console, choose Roles. 
 In the list of roles in your account, choose the domain or user execution role. 
 On the Trust relationships tab, choose Edit trust policy. 
 Update the trust policy with the following statement: 
 
 
  
   
   {
  "Version": "2012-10-17",
  "Statement": [
     .....
    {
      "Effect": "Allow",
      "Principal": {
        "Service": [
          "sagemaker.amazonaws.com",
        ]
      },
      "Action": [
        "sts:AssumeRole",
        "sts:SetContext"
      ],
      "Condition": {
	"aws:SourceAccount": "&lt;account&gt;"
         }
       }
    }
  ]
} 
   
  
 
 
 Choose Update policy to save your changes. 
 
Trusted identity propagation only works for private spaces at the time of launch. 
Create a SageMaker AI domain with trusted identity propagation enabled 
SageMaker AI domains using IAM Identity Center for authentication can only be set up in the same AWS Region as the IAM Identity Center instance. To create a new SageMaker domain, follow the steps in Use custom setup for Amazon SageMaker AI. For Trusted identity propagation, select Enable trusted identity propagation for all users on this domain, and continue with the rest of the setup to create a domain and assign users and groups, choosing the role you created in the previous step. 
 
Update an existing SageMaker AI domain 
You can also update your existing SageMaker AI domain to enable trusted identity propagation. You can enable trusted identity propagation even while the domain or user has active SageMaker Studio applications. However, for the changes to be applied, the active applications must be restarted. You can use the EffectiveTrustedIdentityPropagationStatus field in the response to the DescribeApp API for running applications to determine if the application has trusted identity propagation enabled. 
To enable trusted identity propagation for the domain using the SageMaker AI console, choose Edit under Authentication and permissions on the Domain settings tab. 
 
For Trusted identity propagation, select Enable trusted identity propagation for all users on this domain, and choose Submit to save the changes. 
 
(Optional) Update user background session configuration in IAM Identity Center 
IAM Identity Center now supports running user background sessions, and the session duration is set by default to 7 days. With background sessions, users can launch long-running SageMaker training jobs that assume the userâ€™s identity context along with the SageMaker execution role. As an administrator, you can enable or disable user background sessions, and modify the session duration for user background sessions. As of the time of writing, the maximum session duration that you can set for user background sessions is 90 days. The userâ€™s session is stopped at the end of the specified duration, and consequently, the training job will also fail at the end of the session duration. 
To disable or update the session duration, navigate to the IAM Identity Center console, choose Settings in the navigation pane, and choose Configure under Session duration. 
 
For User background sessions, select Enable user background sessions and use the dropdown to change the session duration. If user background sessions are disabled, the user must be logged in for the duration of the training job; otherwise, the training job will fail once the user logs out. Updating this configuration doesnâ€™t affect current running sessions and only applies to newly created user background sessions. Choose Save to save your settings. 
 
Use cases 
Imagine youâ€™re an enterprise with hundreds or even thousands of users, each requiring varying levels of access to data across multiple teams. Youâ€™re responsible for maintaining an AI/ML system on SageMaker AI and managing access permissions across diverse data sources such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, and AWS Lake Formation. Traditionally, this has involved maintaining complex IAM policies for users, services, and resources, including bucket policies where applicable. This approach is not only tedious but also makes it challenging to track and audit data access without maintaining a separate role for each user. 
This is precisely the scenario that trusted identity propagation aims to address. With trusted identity propagation support, you can now maintain service-specific roles with minimal permissions, such as s3:GetDataAccess or LakeFormation:GetDataAccess, along with additional permissions to start jobs, view job statuses, and perform other necessary tasks. For data access, you can assign fine-grained policies directly to individual users. For instance, Jane might have read access to customer data and full access to sales and pricing data, whereas Laura might only have read access to sales trends. Both Jane and Laura can assume the same SageMaker AI role to access their SageMaker Studio applications, while maintaining separate data access permissions based on their individual identities.In the following sections, we explore how this can be achieved for common use cases, demonstrating the power and flexibility of trusted identity propagation in simplifying data access management while maintaining robust security and auditability. 
Scenario 1: Experiment with Amazon S3 data in notebooks 
S3 Access Grants provide a simplified way to manage data access at scale. Unlike traditional IAM roles and policies that require a detailed knowledge of IAM concepts, and frequent policy updates as new resources are added, with S3 Access Grants, you can define access to data based on familiar database-like grants that automatically scale with your data. This approach significantly reduces the operational overhead of managing thousands of IAM policies and bucket policies, and overcomes the limitations of IAM permissions, while strengthening security through access patterns. If you donâ€™t have S3 Access Grants set up, see Create an S3 Access Grant instance to get started. For detailed architecture and use cases, you can also refer to Scaling data access with Amazon S3 Access Grants. After you have set up S3 Access Grants, you can grant access to your datasets to users based on their identity in IAM Identity Center. 
To use S3 Access Grants from SageMaker Studio, update the following IAM roles with policies and trust policies. 
For the domain or user execution role, add the following inline policy: 
 
  
  {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowDataAccessAPI",
            "Effect": "Allow",
            "Action": [
                "s3:GetDataAccess"
            ],
            "Resource": [
                "arn:aws:s3:&lt;region&gt;:&lt;account&gt;:access-grants/default"
            ]
        },
        {
            "Sid": "RequiredForTIP",
            "Effect": "Allow",
            "Action": "sts:SetContext",
            "Resource": "arn:aws:iam::&lt;account&gt;:role/&lt;s3-access-grants-role&gt;"
        }
    ]
} 
  
 
Make sure the S3 Access Grants roleâ€™s trust policy allows the sts:SetContext action in addition to sts:AssumeRole. The following is a sample trust policy: 
 
  
   
   {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": [
                    "access-grants.s3.amazonaws.com"
                ]
            },
            "Action": [
                "sts:AssumeRole",
                "sts:SetContext"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:SourceArn": "arn:aws:s3:&lt;region&gt;:&lt;account&gt;:access-grants/default"
                }
            }
        }
    ]
 
   
   
  &nbsp; 
 Now, the user can access the data as allowed by S3 Access Grants for your user identity by calling the 
GetDataAccess API to return temporary credentials, and by assuming the temporary credentials to read or write to their prefixes. For example, the following code shows how to use Boto3 to get temporary credentials and assume the credentials to get access to Amazon S3 locations that are allowed through S3 Access Grants: 

  &nbsp; 
 import boto3
from botocore.config import Config

def get_access_grant_credentials(account_id: str, target: str, 
                                 permission: str = 'READ'):
    s3control = boto3.client('s3control')
    response = s3control.get_data_access(
        AccountId=account_id,
        Target=target,
        Permission=permission
    )
    return response['Credentials']

def create_s3_client_from_credentials(credentials) -&gt; boto3.client:
    return boto3.client(
        's3',
        aws_access_key_id=credentials['AccessKeyId'],
        aws_secret_access_key=credentials['SecretAccessKey'],
        aws_session_token=credentials['SessionToken']
    )

# Create client
credentials = get_access_grant_credentials('&lt;account&gt;',
                                        "s3://&lt;bucket&gt;/&lt;allowed-prefix&gt;/")
s3 = create_s3_client_from_credentials(credentials)

# Will succeed
s3.list_objects(Bucket="&lt;bucket&gt;", Prefix="&lt;allowed-prefix&gt;")

# Will fail
s3.list_objects(Bucket="&lt;bucket&gt;", Prefix="&lt;any-other-prefix&gt;") 
 
Scenario 2: Access Lake Formation through Athena 
Lake Formation provides centralized governance and fine-grained access control management for data stored in Amazon S3 and metadata in the AWS Glue Data Catalog. The Lake Formation permission model operates in conjunction with IAM permissions, offering granular controls at the database, table, column, row, and cell levels. This dual-layer security model provides comprehensive data governance while maintaining flexibility in access patterns. 
Data governed through Lake Formation can be accessed through various AWS analytics services. In this scenario, we demonstrate using Athena, a serverless query engine that integrates seamlessly with Lake Formationâ€™s permission model. For other services like Amazon EMR on EC2, make sure the resource is configured to support trusted identity propagation, including setting up security configurations and making sure the EMR cluster is configured with IAM roles that support trusted identity propagation. 
The following instructions assume that you have already set up Lake Formation. If not, see Set up AWS Lake Formation and follow the AWS Lake Formation tutorials to set up Lake Formation and bring in your data. 
Complete the following steps to access your governed data in trusted identity propagation-enabled SageMaker Studio notebooks using Athena: 
 
 Integrate Lake Formation with IAM Identity Center by following the instructions in Integrating IAM Identity Center. At a high level, this includes creating an IAM role allowing creating and updating application configurations in Lake Formation and IAM Identity Center, and providing the single sign-on (SSO) instance ID. 
 Grant permissions to the IAM Identity Center user to the relevant resources (database, table, row or column) using Lake Formation. See Granting permissions on Data Catalog resources instructions. 
 Create an Athena workgroup that supports trusted identity propagation by following instructions in Create a workgroup and choosing IAM Identity Center as the method of authentication. Make sure the user has access to write to the query results location provided here using S3 Access Grants, because Athena uses access grants by default when choosing IAM Identity Center as the authentication method. 
 Update the Athena workgroupâ€™s IAM role with the following trust policy (add sts:SetContext to the existing trust policy). You can find the IAM role by choosing the workgroup you created earlier and looking for Role name. 
 
 
  
  {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AthenaTrustPolicy",
            "Effect": "Allow",
            "Principal": {
                "Service": "athena.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRole",
                "sts:SetContext"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:SourceAccount": "&lt;account-id&gt;"
                },
                "ArnLike": {
                    "aws:SourceArn": "arn:aws:athena:&lt;region&gt;:&lt;account-id&gt;:workgroup/&lt;workgroup-name&gt;"
                }
            }
        }
    ]
} 
  
 
The setup is now complete. You can now launch SageMaker Studio using an IAM Identity Center user, launch a JupyterLab or Code Editor application, and query the database. See the following example code to get started: 
 
  
  import time
import boto3
import pandas as pd
athena_client = boto3.client("athena")

database = "&lt;database-name&gt;"
table = "&lt;table-name&gt;"
query = f"SELECT * FROM {database}.{table}"
output_location = "s3://&lt;bucket-name&gt;/queries"  # bucket name and location from Step 3

response = athena_client.start_query_execution(
    QueryString=query,
    QueryExecutionContext={'Database': database},
    ResultConfiguration={'OutputLocation': output_location}
)

# Get the query execution ID
query_execution_id = response['QueryExecutionId']

# wait for query to complete
while True:
    query_status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
    status = query_status['QueryExecution']['Status']['State']
    if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
        break
    time.sleep(1)

# If the query succeeded, fetch and display results
if status == 'SUCCEEDED':
    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)
    
    # Extract column names and data
    columns = [col['Name'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']]
    data = []
    for row in results['ResultSet']['Rows'][1:]:  # Skip the header row
        data.append([field.get('VarCharValue', '') for field in row['Data']])
    
    # Create a pandas DataFrame
    df = pd.DataFrame(data, columns=columns)
    
    # Display the first few rows
    print(df.head())
else:
    print(f"Query failed with status: {status}") 
  
 
Scenario 3: Create a training job supported with user background sessions 
For a trusted identity propagation-enabled domain, a user background session is a session that continues to run even if the end-user has logged out of their interactive session such as JupyterLab applications in SageMaker Studio. For example, the user can initiate a training job from their SageMaker Studio space, and the job can run in the background for days or weeks regardless of the userâ€™s activity, and use the userâ€™s identity to access data and log audit trails. If your domain doesnâ€™t have trusted identity propagation enabled, you can continue to run training jobs and processing jobs as before; however, if trusted identity propagation is enabled, make sure your user background session time is updated to reflect the duration of your training jobs, because the default is set automatically to 7 days. If you have enabled user background sessions, update your SageMaker Studio domain or userâ€™s execution role with the following permissions to provide a seamless experience for data scientists: 
 
 {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowDataAccessAPI",
            "Effect": "Allow",
            "Action": [
                "s3:GetDataAccess",
                "s3:GetAccessGrantsInstanceForPrefix"
            ],
            "Resource": [
                "arn:aws:s3:&lt;region&gt;:&lt;account&gt;:access-grants/default"
            ]
        },
        {
            "Sid": "RequiredForTIP",
            "Effect": "Allow",
            "Action": "sts:SetContext",
            "Resource": "arn:aws:iam::&lt;account&gt;:role/&lt;s3-access-grants-role&gt;"
        }
    ]
} 
 
With this setup, a data scientist can use an Amazon S3 location that they have access to through S3 Access Grants. SageMaker automatically looks for data access using S3 Access Grants and falls back to the jobâ€™s IAM role otherwise. For example, in the following SDK call to create the training job, the user provides the S3 Amazon URI where the data is stored, they have access to it through S3 Access Grants, and they can run this job without additional setup: 
 
  
   
       response = sm.create_training_job(
        TrainingJobName=training_job_name,
        AlgorithmSpecification={
            'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04',
            'TrainingInputMode': 'File',
            ...
                    RoleArn='arn:aws:iam::&lt;account&gt;:role/tip-domain-role',
        InputDataConfig=[
            {
                'ChannelName': 'training',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': 's3://&lt;s3-ag-enabled-bucket&gt;/&lt;s3-ag-enabled-prefix&gt;',
                        'S3DataDistributionType': 'FullyReplicated'
                    }
                },
                'CompressionType': 'None',
                'RecordWrapperType': 'None'
            },
            ...
        } 
   
  
 
(Optional) View and manage user background sessions on IAM Identity Center 
When training jobs are run as user background sessions, you can view these sessions as user background sessions on IAM Identity Center. The administrator can view a list of all user background sessions and optionally stop a session if the user has left the team, for example. When the user background session is ended, the training job subsequently fails. 
To view a list of all user background sessions, on the IAM Identity Center console, choose Users and choose the user you want view the user background sessions for. Choose the Active sessions tab to view a list of sessions. The user background session can be identified by the Session type column, which shows if the session is interactive or a user background session. The list also shows the jobâ€™s Amazon Resource Name (ARN) under the Used by column. 
To end a session, select the session and choose End sessions. 
 
You will be prompted to confirm the action. Enter confirm to confirm that you want to end the session and choose End sessions to stop the user background session. 
 
Scenario 4: Auditing using CloudTrail 
After trusted identity propagation is enabled for your domain, you can now track the user that performed specific actions through CloudTrail. To try this out, log in to SageMaker Studio, and create and open a JupyterLab space. Open a terminal and enter aws s3 ls to list the available buckets in your Region. 
On the CloudTrail console, choose Event history in the navigation pane. Update the Lookup attributes to Event name and in the search box, enter ListBuckets. You should see a list of events, as shown in the following screenshot (it might take up to 5 minutes for the logs to be available in CloudTrail). 
 
Choose the event to view its details (verify the user name is SageMaker if you have also listed buckets through the AWS console or APIs). In the event details, you should be able to see an additional field called onBehalfOf that has the userâ€™s identity. 
 
Supported services and SageMaker AI features called from a trusted identity propagation-enabled SageMaker Studio domain will have the OnBehalfOf field in CloudTrail. 
Clean up 
If you have created a SageMaker Studio domain for the purposes of trying out trusted identity propagation, delete the domain and its associated Amazon Elastic File System (Amazon EFS) volume to avoid incurring additional charges. Before deleting a domain, you must delete all the users and their associated spaces and applications. For detailed instructions, see Stop and delete your Studio running applications and spaces. 
If you created a SageMaker training job, they are ephemeral, and the compute is shut down automatically when the job is complete. 
Athena is a serverless analytics service that charges per query billing. No cleanup is necessary, but for best practices, delete the workgroup to remove unused resources. 
Conclusion 
In this post, we showed you how to enable trusted identity propagation for SageMaker AI domains that use IAM Identity Center as the mode of authentication. With trusted identity propagation, administrators can manage user authorization to other AWS services through the userâ€™s physical identity in conjunction with IAM roles. Administrators can streamline permissions management by maintaining a single domain execution role and manage granular access to other AWS services and data sources through the userâ€™s identity. In addition, trusted identity propagation supports auditing, so administrators can track user activity without the need for managing a role for each user profile. 
To learn more about enabling this feature and its use cases, see Trusted identity propagation use cases and Trusted identity propagation with Studio. This post covered a subset of supported applications; we encourage you to check out the documentation and choose the services that best serve your use case and share your feedback! 
 
About the authors 
Amit Shyam Jaisinghani is a Software Engineer on the SageMaker Studio team at Amazon Web Services, and he earned his Masterâ€™s degree in Computer Science from Rochester Institute of Technology. Since joining Amazon in 2019, he has built and enhanced several AWS services, including AWS WorkSpaces and Amazon SageMaker Studio. Outside of work, he explores hiking trails, plays with his two cats, Missy and Minnie, and enjoys playing Age of Empire. 
Durga Sury is a Senior Solutions Architect at Amazon SageMaker, where she helps enterprise customers build secure and scalable AI/ML systems. When sheâ€™s not architecting solutions, you can find her enjoying sunny walks with her dog, immersing herself in murder mystery books, or catching up on her favorite Netflix shows. 
Khushboo Srivastava is a Senior Product Manager for Amazon SageMaker. She enjoys building products that simplify machine learning workflows for customers, and loves playing with her 1-year old daughter. 
Krishnan Manivannan is a Senior Software Engineer at Amazon Web Services and a founding member of the SageMaker AI API team. He has 8 years of experience in the architecture and security of large-scale machine learning services. His specialties include API design, service scalability, identity and access management, and inventing new approaches for building and operating distributed systems. Krishnan has led multiple engineering efforts from design through global launch, delivering reliable and secure systems for customers worldwide.
â€¢ Benchmarking document information localization with Amazon Nova
  Every day, enterprises process thousands of documents containing critical business information. From invoices and purchase orders to forms and contracts, accurately locating and extracting specific fields has traditionally been one of the most complex challenges in document processing pipelines. Although optical character recognition (OCR) can tell us what text exists in a document, determining where specific information is located has required sophisticated computer vision solutions. 
The evolution of this field illustrates the complexity of the challenge. Early object detection approaches like YOLO (You Only Look Once) revolutionized the field by reformulating object detection as a regression problem, enabling real-time detection. RetinaNet advanced this further by addressing class imbalance issues through Focal Loss, and DETR introduced transformer-based architectures to minimize hand-designed components. However, these approaches shared common limitations: they required extensive training data, complex model architectures, and significant expertise to implement and maintain. 
The emergence of multimodal large language models (LLMs) represents a paradigm shift in document processing. These models combine advanced vision understanding with natural language processing capabilities, offering several groundbreaking advantages: 
 
 Minimized use of specialized computer vision architectures 
 Zero-shot capabilities without the need for supervised learning 
 Natural language interfaces for specifying location tasks 
 Flexible adaptation to different document types 
 
This post demonstrates how to use foundation models (FMs) in Amazon Bedrock, specifically Amazon Nova Pro, to achieve high-accuracy document field localization while dramatically simplifying implementation. We show how these models can precisely locate and interpret document fields with minimal frontend effort, reducing processing errors and manual intervention. Through comprehensive benchmarking on the FATURA dataset, we provide benchmarking of performance and practical implementation guidance. 
Understanding document information localization 
Document information localization goes beyond traditional text extraction by identifying the precise spatial position of information within documents. Although OCR tells us what text exists, localization tells us where specific information residesâ€”a crucial distinction for modern document processing workflows. This capability enables critical business operations ranging from automated quality checks and sensitive data redaction to intelligent document comparison and validation. 
Traditional approaches to this challenge relied on a combination of rule-based systems and specialized computer vision models. These solutions often required extensive training data, careful template matching, and continuous maintenance to handle document variations. Financial institutions, for instance, would need separate models and rules for each type of invoice or form they processed, making scalability a significant challenge. Multimodal models with localization capabilities available on Amazon Bedrock fundamentally change this paradigm. Rather than requiring complex computer vision architectures or extensive training data, these multimodal LLMs can understand both the visual layout and semantic meaning of documents through natural language interactions. By using models with the capability to localize, organizations can implement robust document localization with significantly reduced technical overhead and greater adaptability to new document types. 
Multimodal models with localization capabilities, such as those available on Amazon Bedrock, fundamentally change this paradigm. Rather than requiring complex computer vision architectures or extensive training data, these multimodal LLMs can understand both the visual layout and semantic meaning of documents through natural language interactions. By using models with the capability to localize, organizations can implement robust document localization with significantly reduced technical overhead and greater adaptability to new document types. 
Solution overview 
We designed a simple localization solution that takes a document image and text prompt as input, processes it through selected FMs on Amazon Bedrock, and returns the field locations using either absolute or normalized coordinates. The solution implements two distinct prompting strategies for document field localization: 
 
 Image dimension strategy â€“ Works with absolute pixel coordinates, providing explicit image dimensions and requesting bounding box locations based on the documentâ€™s actual size 
 Scaled coordinate strategy â€“ Uses a normalized 0â€“1000 coordinate system, making it more flexible across different document sizes and formats 
 
The solution has a modular design to allow for straightforward extension to support custom field schemas through configuration updates rather than code changes. This flexibility, combined with the scalability of Amazon Bedrock, makes the solution suitable for both small-scale document processing and enterprise-wide deployment. In the following sections, we demonstrate the setup and implementation strategies used in our solution for document field localization using Amazon Bedrock FMs. You can see more details in our GitHub repository. 
Prerequisites 
For this walkthrough, you should have the following prerequisites: 
 
 An AWS account with Amazon Bedrock access 
 Permissions to use Amazon Nova Pro 
 Python 3.8+ with the boto3 library installed 
 
Initial set ups 
Complete the following setup steps: 
 
 Configure the Amazon Bedrock runtime client with appropriate retry logic and timeout settings: 
 
 
 import boto3
from botocore.config import Config

# Configure Bedrock client with retry logic
BEDROCK_CONFIG = Config(
    region_name='us-west-2',
    signature_version='v4',
    read_timeout=500,
    retries={
        'max_attempts': 10,
        'mode': 'adaptive'
    }
)

# Initialize Bedrock runtime client
bedrock_runtime = boto3.client("bedrock-runtime", config=BEDROCK_CONFIG) 
 
 
 Define your field configuration to specify which elements to locate in your documents: 
 
 
 # sample config
field_config = {
    "invoice_number": {"type": "string", "required": True},
    "total_amount": {"type": "currency", "required": True},
    "date": {"type": "date", "required": True}
} 
 
 
 Initialize the BoundingBoxExtractor with your chosen model and strategy: 
 
 
 extractor = BoundingBoxExtractor(
    model_id=NOVA_PRO_MODEL_ID,  # or other FMs on Amazon Bedrock
    prompt_template_path="path/to/prompt/template",
    field_config=field_config,
    norm=None  # Set to 1000 for scaled coordinate strategy
)

# Process a document    
bboxes, metadata = extractor.get_bboxes(
    document_image=document_image,
    document_key="invoice_001" # Optional tracking key
) 
 
Implement prompting strategies 
We test two prompt strategies in this workflow: image dimension and scaled coordinate. 
The following is a sample prompt template for the image dimension strategy: 
 
 """
Your task is to detect and localize objects in images with high precision.
Analyze each provided image (width = {w} pixels, height = {h} pixels) and return only a JSON object with bounding box data for detected objects.

Output Requirements:
1. Use absolute pixel coordinates based on provided width and height.
2. Ensure high accuracy and tight-fitting bounding boxes.

Detected Object Structure:
- "element": Use one of these labels exactly: {elements}
- "bbox": Array with coordinates [x1, y1, x2, y2] in absolute pixel values.

JSON Structure:
```json
{schema}
```

Provide only the specified JSON format without extra information.
""" 
 
The following is a sample prompt template for the scaled coordinate strategy: 
 
 """
Your task is to detect and localize objects in images with high precision.
Analyze each provided image and return only a JSON object with bounding box data for detected objects.

Output Requirements:
Use (x1, y1, x2, y2) format for bounding box coordinates, scaled between 0 and 1000.

Detected Object Structure:
- "element": Use one of these labels exactly: {elements}
- "bbox": Array [x1, y1, x2, y2] scaled between 0 and 1000.

JSON Structure:
```json
{schema}
```

Provide only the specified JSON format without extra information.
""" 
 
Evaluate performance 
We implement evaluation metrics to monitor accuracy: 
 
 evaluator = BBoxEvaluator(field_config=field_config)
evaluator.set_iou_threshold(0.5)  # Adjust based on requirements
evaluator.set_margin_percent(5)   # Tolerance for position matching

# Evaluate predictions
results = evaluator.evaluate(predictions, ground_truth)
print(f"Mean Average Precision: {results['mean_ap']:.4f}") 
 
This implementation provides a robust foundation for document field localization while maintaining flexibility for different use cases and document types. The choice between image dimension and scaled coordinate strategies depends on your specific accuracy requirements and document variation. 
Benchmarking results 
We conducted our benchmarking study using FATURA, a public invoice dataset specifically designed for document understanding tasks. The dataset comprises 10,000 single-page invoices saved as JPEG images, representing 50 distinct layout templates with 200 invoices per template. Each document is annotated with 24 key fields, including invoice numbers, dates, line items, and total amounts. The annotations provide both the text values and precise bounding box coordinates in JSON format, making it ideal for evaluating field localization tasks. The dataset has the following key characteristics: 
 
 Documents: 10,000 invoices (JPEG format) 
 Templates: 50 distinct layouts (200 documents each) 
 Fields per document: 24 annotated fields 
 Annotation format: JSON with bounding boxes and text values 
 Field types: Invoice numbers, dates, addresses, line items, amounts, taxes, totals 
 Image resolution: Standard A4 size at 300 DPI 
 Language: English 
 
The following figure shows sample invoice templates showcasing layout variation. 

 
 
 
 
 
The following figure is an example of annotation visualization. 

 
 
 
Before conducting the full-scale benchmark, we performed an initial experiment to determine the optimal prompting strategy. We selected a representative subset of 50 images, comprising 5 samples from 10 different templates, and evaluated three distinct approaches: 
 
 Image dimension: 
   
   Method: Provides explicit pixel dimensions and requests absolute coordinate bounding boxes 
   Input: Image bytes, image dimensions, output schema 
    
 Scaled coordinate: 
   
   Method: Uses normalized 0-1000 coordinate system 
   Input: Image bytes, output schema 
    
 Added gridlines: 
   
   Method: Enhances image with visual gridlines at fixed intervals 
   Input: Modified image with gridlines bytes, image dimensions, output schema 
    
 
The following figure compares performance for different approaches for Mean Average Precision (mAP). 

 
 
Building on insights from our initial strategy evaluation, we conducted benchmarking using the complete FATURA dataset of 10,000 documents. We employed the scaled coordinate approach for Amazon Nova models, based on their respective optimal performance characteristics from our initial testing. Our evaluation framework assessed Amazon Nova Pro through standard metrics, including Intersection over Union (IoU) and Average Precision (AP). The evaluation spanned all 50 distinct invoice templates, using an IoU threshold of 0.5 and a 5% margin tolerance for field positioning. 
The following are our sample results in JSON: 
 
 {
    "template": "template1",
    "instance": "Instance0",
    "metrics": {
        "mean_ap": 0.8421052631578947,
        "field_scores": {
            "TABLE": [0.9771107575829314, 1.0, 1.0, 1.0, 1.0],
            "BUYER": [0.3842328422050217, 0.0, 0.0, 0, 0.0],
            "DATE": [0.9415158516000428, 1.0, 1.0, 1.0, 1.0],
            "DISCOUNT": [0.8773709977744115, 1.0, 1.0, 1.0, 1.0],
            "DUE_DATE": [0.9338410331219548, 1.0, 1.0, 1.0, 1.0],
            "GSTIN_BUYER": [0.8868145680064249, 1.0, 1.0, 1.0, 1.0],
            "NOTE": [0.7926162009357707, 1.0, 1.0, 1.0, 1.0],
            "PAYMENT_DETAILS": [0.9517931284002012, 1.0, 1.0, 1.0, 1.0],
            "PO_NUMBER": [0.8454266053075804, 1.0, 1.0, 1.0, 1.0],
            "SELLER_ADDRESS": [0.9687004508445741, 1.0, 1.0, 1.0, 1.0],
            "SELLER_EMAIL": [0.8771026147909002, 1.0, 1.0, 1.0, 1.0],
            "SELLER_SITE": [0.8715647216012751, 1.0, 1.0, 1.0, 1.0],
            "SUB_TOTAL": [0.8049954543667662, 1.0, 1.0, 1.0, 1.0],
            "TAX": [0.8751563641702513, 1.0, 1.0, 1.0, 1.0],
            "TITLE": [0.850667327423512, 1.0, 1.0, 1.0, 1.0],
            "TOTAL": [0.7226784112051814, 1.0, 1.0, 1.0, 1.0],
            "TOTAL_WORDS": [0.9099353099528785, 1.0, 1.0, 1.0, 1.0],
            "GSTIN_SELLER": [0.87170328009624, 1.0, 1.0, 1.0, 1.0],
            "LOGO": [0.679425211111111, 1.0, 1.0, 1.0, 1.0]
        }
    },
    "metadata": {
        "usage": {
            "inputTokens": 2250,
            "outputTokens": 639,
            "totalTokens": 2889
        },
        "metrics": {
            "latencyMs": 17535
        }
    }
} 
 
The following figure is an example of successful localization for Amazon Nova Pro. 

 
 
The results demonstrate Amazon Nova Proâ€™s strong performance in document field localization. Amazon Nova Pro achieved a mAP of 0.8305. It demonstrated consistent performance across various document layouts, achieving a mAP above 0.80 across 45 of 50 templates, with the lowest template-specific mAP being 0.665. Although Amazon Nova Pro showed relatively high processing failures (170 out of 10,000 images), it still maintained high overall performance. Most low AP results were attributed to either complete processing failures (particularly over-refusal by its guardrail filters and malformed JSON output) or field misclassifications (particularly confusion between similar fields, such as buyer vs. seller addresses). 
The following table summarizes the overall performance metrics. 
 
  
   
    
   Mean IoU 
   Mean AP 
   
   
   Amazon Nova Pro 
   0.7423 
   0.8331 
   
  
 
The following graph shows the performance distribution for each individual extraction of approximately 20 labels for 10,000 documents. 

 
 
Field-specific analysis reveals that Amazon Nova Pro excels at locating structured fields like invoice numbers and dates, consistently achieving precision and recall scores above 0.85. It demonstrates particularly strong performance with text fields, maintaining robust accuracy even when dealing with varying currency formats and decimal representations. This resilience to format variations makes it especially valuable for processing documents from multiple sources or regions. 
The following graph summarizes field-specific performance. The graph shows AP success percentage for each label, across all documents for each model. It is sorted by highest success. 

 
 
Conclusion 
This benchmarking study demonstrates the significant advances in document field localization by multimodal FMs. Through comprehensive testing on the FATURA dataset, weâ€™ve shown that these models can effectively locate and extract document fields with minimal setup effort, dramatically simplifying traditional computer vision workflows. Amazon Nova Pro emerges as an excellent choice for enterprise document processing, delivering a mAP of 0.8305 with consistent performance across diverse document types. Looking ahead, we see several promising directions for further optimization. Future work could explore extending the solution in agentic workflows to support more complex document types and field relationships. 
To get started with your own implementation, you can find the complete solution code in our GitHub repository. We also recommend reviewing the Amazon Bedrock documentation for the latest model capabilities and best practices. 
 
About the authors 
 Ryan Razkenari&nbsp;is a Deep Learning Architect at the AWS Generative AI Innovation Center, where he uses his expertise to create cutting-edge AI solutions. With a strong background in AI and analytics, he is passionate about building innovative technologies that address real-world challenges for AWS customers. 
Harpreet Cheema is a Deep Learning Architect at the AWS Generative AI Innovation Center. He is very passionate in the field of machine learning and in tackling different problems in the ML domain. In his role, he focuses on developing and delivering Generative AI focused solutions for real-world applications. 
Spencer Romo is a Senior Data Scientist with extensive experience in deep learning applications. He specializes in intelligent document processing while maintaining broad expertise in computer vision, natural language processing, and signal processing. Spencerâ€™s innovative work in remote sensing has resulted in multiple patents. Based in Austin, Texas, Spencer loves working directly with customers to understand their unique problems and identify impactful AI solutions. Outside of work, Spencer competes in 24 Hours of Lemons racing series, embracing the challenge of high-performance driving on a budget. 
Mun Kim&nbsp;is a Machine Learning Engineer at the AWS Generative AI Innovation Center. Mun brings expertise in building machine learning science and platform that help customers harness the power of generative AI technologies. He works closely with AWS customers to accelerate their AI adoption journey and unlock new business value. 
Wan Chen is an Applied Science Manager at the Generative AI Innovation Center. As a ML/AI veteran in tech industry, she has wide range of expertise on traditional machine learning, recommender system, deep learning and Generative AI. She is a stronger believer of Superintelligence, and is very passionate to push the boundary of AI research and application to enhance human life and drive business growth. She holds Ph.D in Applied Mathematics from University of British Columbia, and had worked as postdoctoral fellow in Oxford University.
â€¢ How Infosys built a generative AI solution to process oil and gas drilling data with Amazon Bedrock
  Enterprises across industries like healthcare, finance, manufacturing, and legal services face escalating challenges in processing vast amounts of multimodal data that combines text, images, charts, and complex technical formats. As organizations generate multimodal content at unprecedented speed and scale, document processing methods increasingly fail to handle the intricacies of specialized domains where technical terminology, interconnected data relationships, and industry-specific formats create operational bottlenecks. These conventional (non-AI) processing approaches struggle with the unique characteristics of enterprise documents: highly technical terminology, complex multimodal data formats, and interconnected information spread across various document types. This results in inefficient data extraction, missed insights, and time-consuming manual processing that hinders organizational productivity and decision-making.One such industry example is oil and gas, which generates vast amounts of complex technical data through drilling operations, presenting significant challenges in data processing and knowledge extraction. These documents, such as detailed well completion reports, drilling logs, and intricate lithology diagrams, contain crucial information that drives operational decisions and strategic planning. 
To overcome such challenges, we built an advanced RAG solution using Amazon Bedrock leveraging Infosys Topaz AI capabilities, tailored for the oil and gas sector. This solution excels in handling multimodal data sources, seamlessly processing text, diagrams, and numerical data while maintaining context and relationships between different data elements. The specialized approach helps organizations unlock valuable insights from their technical documentation, streamline their workflows, and make more informed decisions based on comprehensive data analysis. 
In this post, we provide insights on the solution and walk you through different approaches and architecture patterns explored, like different chunking, multi-vector retrieval, and hybrid search during the development. 
Solution overview 
The solution is built using AWS services, including Amazon Bedrock Nova Pro, Amazon Bedrock Knowledge Bases, Amazon OpenSearch Serverless as a Vector Database, Amazon Titan Text Embeddings, Cohere Embed English model, and BGE Reranker, allowing for scalability and cost-effectiveness. We also used Amazon Q Developer, an AI-powered assistant for software development for frontend and backend development of our solution powered by Infosys Topazâ€™s generative AI capabilities. The solution uses distributed processing to handle large volumes of data, so the system can handle a high volume of requests without compromising performance. The real-time indexing system allows for new documents to be incorporated into the system as soon as they are available, so that the information is up-to-date. 
The following are some of the key components of the solution: 
 
 Document processing â€“ PyMuPDF for PDF parsing, OpenCV for image processing. 
 Embedding generation â€“ Cohere Embed English on Amazon Bedrock for generating vector embeddings of document content and user queries. A hierarchical parent-child chunking architecture that preserves document structure and contextual relationships. 
 Vector storage â€“ Amazon OpenSearch Serverless for hybrid search capabilities combining semantic vector search with traditional keyword search (although Amazon Bedrock Knowledge Bases provides a managed RAG solution, this implementation uses a custom RAG architecture to deliver enhanced value and flexibility). This multi-vector retrieval mechanism with separate embedding spaces was required for maintaining the context between textual and visual data. 
 Model â€“ Amazon Nova model for domain-specific response generation. 
 Reranking â€“ BGE reranker, for improving search result relevance by reordering retrieved documents based on semantic similarity to the query. 
 
The following diagram is a high-level overview of the architecture of the solution. 
 
Many approaches were used during the build phase to get the desired accuracy. In the following sections, we discuss these approaches in detail. 
RAG exploration and initial approach 
The following figure shows some sample images from the oil and well drilling reports. The image on the left is a performance chart of a well drilling operation with the details of the drilling instrument. The image on the top right is of the split sections of the drilling instrument, followed below by the drilling data in a tabular form. 
 
Image Source : Wells Search | NEATS 
Â© Commonwealth of Australia [year of publishing- 2018] 
Over a thousand such technical images (including lithology diagrams, well completion charts, and drilling visualizations) were preprocessed using Amazon Nova Pro, a multimodal language model. An iterative prompting strategy was employed to generate comprehensive descriptions: 
 
 Initial image analysis to extract basic technical elements 
 Refined prompting with domain-specific context to capture specialized terminology 
 Multiple inference iterations to provide completeness and accuracy of technical descriptions 
 
This process converted visual technical information into detailed textual descriptions that preserve the original technical context.The process included the following key components: 
 
 Text content processing â€“ The textual content from drilling reports was processed using Amazon Titan Text Embedding v2 model with: 
   
   Fixed-size chunking of 1,500 tokens with 100-token overlap 
   Preservation of original document structure and technical relationships 
    
 Image content integrationâ€“ The detailed image descriptions generated were integrated without chunking to maintain complete technical context 
 Vector storage â€“ The processed content (chunked text and complete image descriptions) was ingested into an OpenSearch Serverless vector database 
 RAG implementation â€“ RAG-enabled semantic search and retrieval is used across both textual content and image-derived descriptions 
 
This approach worked well with text questions but was less effective with image-related questions and finding information from images. The lack of a chunking strategy for images resulted in the entire description of each image ingested as a single unit into the search index. This made it difficult for the embedding model to pinpoint exact locations of specific information, especially for technical terms that might be buried within longer descriptions.In the following sections, we discuss the other approaches explored to overcome the limitations presented by each of the previous approaches. 
Multi-vector embeddings with ColBERT 
To use a vision model, we created multi-vector embeddings for each image. We then used the ColBERT embedding model for fine-grained text representations. User queries were converted into embeddings, and similarity scores between query and document embeddings were calculated. These embeddings were stored using tensor-based storage, and no chunking was applied. We observed the following: 
 
 Outcome â€“ We encountered difficulties in storing and managing the complex ColBERT embeddings in traditional vector stores. Debugging and analyzing retrieved documents became cumbersome. Despite context-rich queries, selecting the proper document pages remained challenging. 
 
Limitations and key learnings â€“ This approach demonstrated the potential of advanced embedding techniques for image-based document retrieval. However, it also highlighted the challenges in implementing and managing such a system effectively, particularly in the complex domain of oil and gas. Overall, the use of vision models enhanced document understanding, and fine-grained representation of visual and textual content was achieved. 
Fixed chunking with Amazon Titan Embeddings 
Adopting a more traditional text-based approach, we introduced a fixed chunking strategy. PDF pages were converted to images, and text content was extracted from these images. A fixed chunking strategy of 500 tokens per chunk was implemented. We used Amazon Bedrock Knowledge Bases with OpenSearch Serverless vector storage, upgraded to Amazon Titan Embeddings v2, and retained the Amazon Nova Pro model. We observed the following: 
 
 Outcome â€“ The ability to find and retrieve information based on technical keyword searches improved. More focused chunks allowed for a more accurate representation of specific concepts. 
 Limitations and key learnings â€“ Providing comprehensive, long-form answers was challenging. Rigid chunking sometimes split related information across different chunks. This approach underscored the importance of balancing chunk size with information coherence, improving our handling of technical terms but highlighting the need for more sophisticated chunking strategies to maintain context. 
 
Parent-child hierarchy with Cohere Embeddings 
Building on our previous learnings, we introduced a more sophisticated chunking strategy using a parent-child hierarchy. PDF pages were converted to images and text was extracted. We implemented a parent-child chunking hierarchy with parent chunks of 1,500 tokens and child chunks of 512 tokens. We switched to Cohere English embeddings, used Amazon Bedrock Knowledge Bases and OpenSearch Serverless vector storage, and continued using the Amazon Nova Pro model. We observed the following: 
 
 Outcome â€“ This approach balanced the need for context with the ability to pinpoint specific information. It significantly improved the ability to answer a wide range of queries, maintaining context while offering precise information retrieval. 
 Limitations and key learnings â€“ Careful structuring of content significantly enhanced the performance of both embedding and QnA models. The parent-child structure proved particularly effective for handling the complex, nested nature of oil and gas documentation. 
 
Hybrid search with optimized chunking 
Our final approach retained the advanced features of the previous method while introducing a crucial change in the search methodology. PDF pages were converted to images and text was extracted. We implemented a hybrid search system within the Amazon Bedrock knowledge base. The parent-child chunking hierarchy was retained with parent chunks of 1,200 tokens and child chunks of 512 tokens. We continued using Cohere English embeddings and the Amazon Nova Pro model, and implemented a BGE reranker to refine search results. We observed the following: 
 
 Outcome â€“ This approach combined the strengths of semantic search and traditional keyword-based search. It addressed the limitations of purely embedding-based searches and improved the handling of specific technical terms and exact phrases. 
 Limitations and key learnings â€“ This final approach represents a highly evolved RAG system, offering the best of both worlds: the ability to understand context and nuance through embeddings, and the precision of keyword matching for specific technical queries. 
 
The following are some of the tangible results of the hybrid strategy: 
 
 Average query response time: Less than 2 seconds 
 Retrieval accuracy (measured against human expert baseline): 92% 
 User satisfaction rating: 4.7/5 based on feedback from field engineers and geologists 
 
Hybrid RAG approach and optimization strategy 
Letâ€™s explore the key components and strategies that make the final approach effective for oil and gas drilling reports. Each of the following sections outlines the differentiators in the solution. 
Multimodal processing capabilities 
The solution is designed to handle the diverse types of information found in oil and gas documents. The system processes both textual content (technical jargon, well logs, production figures) and visual elements (well schematics, seismic charts, lithology graphs) while maintaining contextual relationships between them. This makes sure that when processing a well completion report, the system can extract key parameters from text, analyze accompanying schematics, and link textual formation descriptions to their visual representations in lithology charts.For example, when processing a well completion report, the system can: 
 
 Extract key parameters from the text (such as total depth and casing sizes) 
 Analyze the accompanying well schematic 
 Link textual descriptions of formations to their visual representation in lithology charts 
 
Domain-specific vocabulary handling 
The system incorporates a comprehensive dictionary of industry terms and acronyms specific to oil and gas operations. Standard natural language processing (NLP) models often misinterpret technical terminology like â€œfish left in holeâ€ or fail to recognize specialized abbreviations like â€œBOPâ€ and â€œTVD.â€ By implementing domain-specific vocabulary handling, the system accurately interprets queries and maintains semantic understanding of technical concepts. This helps prevent misinterpretation of critical drilling information and provides relevant document retrieval.For example, when processing a query about â€œfish left in hole at 5000 ft MD,â€ the system understands: 
 
 â€œFishâ€ refers to lost equipment, not an actual fish 
 â€œMDâ€ means measured depth 
 The relevance of this information to drilling operations and potential remediation steps 
 
Hybrid hierarchy chunking strategy 
Traditional fixed-size chunking often breaks apart related technical information, losing critical context in oil and gas documents. The solution implements a hybrid hierarchy approach with parent chunks (1,200 tokens) maintaining document-level context and child chunks (512 tokens) containing detailed technical information. Dynamic chunk sizing adjusts based on content complexity, using natural language processing to identify logical break points. This preserves the integrity of technical descriptions while enabling precise information retrieval across large, complex documents.For example, when processing a well completion report, the system will: 
 
 Create a large parent chunk for the overall well summary 
 Generate smaller child chunks for specific sections like casing details or perforation intervals 
 Dynamically adjust chunk size for the lithology description based on its complexity 
 Implement cross-references between the casing schedule and the well schematic description 
 
Multi-vector retrieval implementation 
Oil and gas documents contain diverse content types that require different retrieval approaches. The system creates separate embedding spaces for text, diagrams, and numerical data, implementing dense vector search for semantic similarity and sparse vector search for exact technical terminology matches. Cross-modal retrieval connects information across different content types, and contextual query expansion automatically includes relevant industry-specific terms. This hybrid approach delivers comprehensive retrieval whether users search for conceptual information or specific technical parameters.For example, for a query like â€œrecent gas shows in Permian Basin wells,â€ the system will: 
 
 Use dense vector search to understand the concept of â€œgas showsâ€ 
 Use sparse vector search to find exact matches for â€œPermian Basinâ€ 
 Expand the query to include related terms like â€œhydrocarbon indicatorsâ€ 
 Apply temporal filtering to focus on recent reports 
 Use spatial awareness to limit results to the Permian Basin area 
 
Temporal and spatial awareness 
Drilling operations are inherently tied to specific locations and time periods, making context crucial for accurate information retrieval. The system incorporates understanding of well locations and operational timelines, allowing for queries that consider geographical and chronological contexts. For example, searching for â€œrecent gas shows in Permian Basin wellsâ€ uses both temporal filtering and spatial awareness to provide relevant, location-specific results. This optimization makes sure retrieved information matches the operational context of the userâ€™s needs.For example, when generating a response about drilling fluid properties, the system will: 
 
 Retrieve relevant information from multiple sources 
 Cross-check numerical values for consistency 
 Use reflective prompting to make sure critical parameters are addressed 
 Apply the reranking model to prioritize the most relevant and accurate information 
 Present the response along with confidence scores and source citations 
 
Reflective response generation 
Technical accuracy is paramount in oil and gas operations, where incorrect information can have serious consequences. The system implements reflective prompting mechanisms that prompt the language model to critically evaluate its own responses against source documents and industry standards. Response reranking uses scoring models that evaluate technical accuracy, contextual relevance, and adherence to industry best practices. This multi-layered validation approach makes sure generated responses meet the high accuracy standards required for technical decision-making in drilling operations. 
Advanced RAG strategies 
To further enhance our systemâ€™s capabilities, we implemented several advanced RAG strategies: 
 
 Hypothetical document embeddings: 
   
   Generates synthetic questions based on document content 
   Creates embeddings for these hypothetical questions 
   Improves retrieval for complex, multi-part queries 
   Particularly effective for handling what-if scenarios in drilling operations 
    
 Recursive retrieval: 
   
   Implements multi-hop information gathering 
   Allows the system to follow chains of related information across multiple documents 
   Essential for answering complex queries that require synthesizing information from various sources 
    
 Semantic routing: 
   
   Intelligently routes queries to appropriate knowledge bases or document subsets 
   Optimizes search efficiency by focusing on the most relevant data sources 
   Crucial for handling the diverse types of documents in oil and gas operations 
    
 Query transformation: 
   
   Automatically refines and reformulates user queries for optimal retrieval 
   Applies industry-specific knowledge to interpret ambiguous terms 
   Breaks down complex queries into series of simpler, more targeted searches 
    
 
For example, for a complex query like â€œCompare the production decline rates of horizontal wells in the Eagle Ford to those in the Bakken over the last 5 years,â€ the system will: 
 
 Use hypothetical document embeddings to generate relevant sub-questions about decline rates, horizontal wells, and specific formations 
 Apply recursive retrieval to gather data from production reports, geological surveys, and economic analyses 
 Route different aspects of the query to appropriate knowledge bases (such as separate databases for Eagle Ford and Bakken data) 
 Transform the query into a series of more specific searches, considering factors like well completion techniques and reservoir characteristics 
 
Business outcome 
The implementation of this advanced RAG solution has delivered significant business value for oil and gas operations: 
 
 Operational efficiency â€“ Significant reduction in decision-making time for drilling and field engineers 
 Cost optimization â€“ 40â€“50% decrease in manual document processing costs through automated information extraction 
 Enhanced productivity â€“ Field engineers and geologists spend 60% less time searching for technical information, focusing instead on high-value analysis 
 Risk mitigation â€“ Consistent 92% retrieval accuracy provides reliable access to critical technical knowledge, reducing operational decision risks 
 
Conclusion 
Our journey in developing this advanced RAG solution for the oil and gas industry demonstrates the power of combining AI techniques with domain-specific knowledge. By addressing the unique challenges of technical documentation in this field, we have created a system that not only retrieves information but understands and synthesizes it in a way that adds real value to operations. Amazon Bedrock is at the center of this solution, with Amazon Q Developer for the application frontend and backend development, and capabilities from Infosys Topaz â€“ an AI-first offering that accelerates business value for enterprises using generative AI.We see significant potential for further advancement, s in this area, such as integration with real-time sensor data for dynamic information retrieval, enhanced visualization capabilities for complex geological and engineering data, and predictive analytics by combining historical retrieval patterns with operational data. 
For more information on Amazon Bedrock and the latest Amazon Nova models, refer to the Amazon Bedrock User Guide and Amazon Nova User Guide. 
 
About the Authors 
Dhiraj Thakur is a Solutions Architect with Amazon Web Services, specializing in Generative AI and data analytics domains. He works with AWS customers and partners to architect and implement scalable analytics platforms and AI-driven solutions. With deep expertise in Generative AI services and implementation, end-to-end machine learning implementation, and cloud-native data architectures, he helps organizations harness the power of GenAI and analytics to drive business transformation. He can be reached via LinkedIn. 
Meenakshi Venkatesan is a Principal Consultant at Infosys and a part of the AWS partnerships team at Infosys Topaz CoE. She helps in designing, developing, and deploying in AWS environments and has interests in exploring the new offerings and services. 
Keerthi Prasad is a Senior Technology Architect at Infosys and a part of the AWS partnerships team at Infosys Topaz CoE. He provides guidance and assistance to customers in building various solutions in the AWS Cloud. He also supports AWS partners and customers in their generative AI adoption journey. 
Suman Debnath is an Associate Principal at Infosys and a part of Infosys Topaz delivery. He has played multiple roles, such as architect, program manager, and data scientist, building scalable enterprise systems and AI/ML and generative AI applications on the cloud for oil and gas, healthcare, and financial clients. 
Ganesh is a Enterprise Architect and Data Scientist at Infosys and part of Topaz Delivery. He has a masterâ€™s degree in computer science and machine learning. He has played multiple roles such as architect, program manager and data scientist building scalable enterprise systems. 
Yash Sharma is a Digital Specialist Engineer with Infosys and part of the AWS team at ICETS with a passion for emerging generative AI services. He has successfully led and contributed to numerous generative AI projects. He is always eager to expand his knowledge and stay ahead of industry trends, bringing the latest insights and techniques to work. 
Karthikeyan Senthilkumar is a Senior Systems Engineer at Infosys and a part of the AWSCOE at iCETS. He specializes in AWS services with a focus on emerging technologies.
â€¢ Streamline employee training with an intelligent chatbot powered by Amazon Q Business
  Amazon Q Business is a generative AI-powered assistant for interacting with organizational knowledge and enterprise systems. In addition to providing built-in connectors and plug-ins to connect seamlessly to over 40 popular enterprise systems, Amazon Q Business provides the ability to interact seamlessly with other third-party applications using custom plugins. Some of the enterprise systems that use Amazon Q Business include Salesforce, Zendesk, Confluence, Jira, ServiceNow, and Microsoft SharePoint. With custom plugins, you can integrate Amazon Q Business with various enterprise systems such as ticketing systems, email services, and other business applications, thus facilitating the creation of a comprehensive enterprise solutions. In this post, we explore how to design and implement custom plugins for Amazon Q Business, showcasing practical examples of integrating with common enterprise systems while helping to ensure secure access through Amazon Cognito authentication. 
Solution overview 
We will build an Amazon Q Business application serving as an intelligent chatbot that facilitates new employee training by retrieving answers from the provided training materials. The solution implements secure API access using Amazon Cognito for user authentication and authorization, helping to ensure that only authorized users can access the system. It can process documents in multiple formats including PDF, DOC, DOCX, and TXT with a maximum file size of 50 MB per document and can index up to 100,000 documents. The chatbot effectively answers the questions posed by new employees by using Retrieval Augmented Generation (RAG) techniques to enhance its response capabilities. If the chatbot canâ€™t locate the requested information, it presents a dynamic option to the user to submit an email directly to the training support team through the chatbot using the custom plugins for Amazon Q Business. We include an AWS CloudFormation template for deployment and management of our solution. 
The following illustration shows how Amazon Q Business delivers training content using RAG techniques, stores materials in Amazon Simple Storage Service (Amazon S3), processes requests through AWS Lambda functions, and enables user escalations through a custom plugin. CloudFormation automates the deployment of these integrated services, as shown in the following figure. 
 
Features and benefits 
The solution provides three key capabilities that work together to create an efficient and user-friendly training support system. These features help organizations reduce support overhead while making sure users can get the help they need. The solutionâ€™s intelligent query handling uses RAG techniques to process user questions accurately and provide context-aware responses from indexed training materials. This capability reduces the burden on human trainers by enabling employees to find answers up to 10 times faster than traditional search methods. According to AWS case studies, organizations implementing Amazon Q Business have seen significant efficiency gains: support tickets have decreased by up to 30% through enhanced self-service capabilities, while employees save an average of 20â€“30 hours per month on document search and summarization tasks. The system has demonstrated the ability to handle up to 80% of routine, repetitive questions automatically, leading to 50% faster onboarding and training processes through automated knowledge access. When users need additional support, they can use the dynamic email escalation feature to contact the training team directly with a single click. This seamless integration maintains a smooth user experience while making sure complex or specialized queries receive prompt attention from subject matter experts. Organizations can typically implement this solution within 2â€“3 business days using a pre-configured CloudFormation template, which will minimize deployment effort and technical overhead. The architecture uses the elastic infrastructure of AWS to scale automatically, supporting enterprise-wide deployments through its ability to process and index millions of documents across multiple data sources. The solution scales according to AWS service quotas, with specific limits on knowledge bases (100,000 documents each), applications (10 per account), and concurrent users (based on your AWS accountâ€™s service quotas). The infrastructure automatically adjusts resources based on query volume and user demand, facilitating consistent performance even during peak usage periods. 
Deployment Steps 
Use the following steps to set up your training chatbot solution. You will configure email notifications using Amazon Simple Email Service (Amazon SES), create an S3 bucket for training materials, deploy two CloudFormation templates, and set up user access for the Amazon Q Business chatbot. 
Prerequisites 
Download the files needed from the S3 bucket: 
 
 Amazon Q Business application template 
 Email plugin schema 
 Mock training materials 
 
Enable AWS IAM Identity Center: 
 
 Go to the AWS Management Console and go to AWS IAM Identity Center. 
 Choose Enable IAM Identity Center. 
 Wait a few minutes for the service to be enabled.  
 
Step 1: Configure the customer service email address on Amazon SES 
The following steps add the email IDs that will be used to send and receive emails through the custom plugin and Amazon SES. 
 
 Open the Amazon SES console. 
 Choose Configuration and then Identities on the left navigation pane. 
 Choose Create identity to add an identity. 
 Select Email address as the identity type. 
 Enter the email address you want to use. 
 Choose Create identity to submit the request.  
 
 
 Confirm the email address by following the link on the email Amazon SES sends you and then your identity will be confirmed (you should receive the email in about 2 minutes). 
 
Step 2: Create an S3 bucket with your training materials in it 
The following steps create the S3 bucket that will act as a data source for the Amazon Q Business application. 
 
 Open the Amazon S3 console. 
 Choose Create bucket. 
 Enter a unique bucket name (for example, company-training-materials-2025). 
 Upload your training materials into this bucket. 
   
   Mock training data was part of the material downloaded in the prerequisites. 
    
 
Step 3: Deploy the first AWS CloudFormation stack â€“ Qbusiness-application.yaml 
The CloudFormation template will create the necessary resources for deploying the application with the custom plugin. 
 
 In your AWS account, open the CloudFormation console. 
 Choose Create stack and With new resources (standard) to start. 
 Select Choose an existing template and Upload a template file and upload Qbusiness-application.yaml. 
 Fill in the required parameters (for example: Amazon Cognito details, S3BucketName, SESSourceEmail, IdcInstanceArn). 
   
   You can keep the default names for the roles. 
   CognitoUserEmail should be the email address associated with a userâ€™s account within an Amazon user pool. 
   CognitoUserPassword is a temporary password associated with the preceding CognitoUserEmail. Add temporary password. 
   S3BucketName should be the bucket that has the training and data source materials in it that you created in Step 2 (for example company-training-materials-2025). 
   SESSourceEmail should be the email that serves as the customer service email address that users can reach out to for further assistance (the same address that you verified in Step 1). 
   IdcInstanceArn is the Amazon Resource Name (ARN) of your IAM Identity Center instance. To find your IdcInstanceArn: 
     
     Navigate to IAM Identity Center console. 
     Choose Go to Settings on the left-hand side. 
     Example: arn:aws:sso:::instance/ssoins-722339a1b72acd7b. 
      
    
 Choose Next, leave the settings on the next page at the default values. 
 Select the I acknowledge that AWS CloudFormation might create IAM resources with custom names check box, choose Next and then choose Submit to start the creation of the stack. 
 Wait until the stack status is CREATE_COMPLETE. 
 Navigate to the Outputs tab on the stack and copy the ApiEndpoint, CognitoAuthorizationUrl and CognitoTokenUrl to your clipboard.   
 
Step 4: Prepare the custom plugin schema 
The following steps help edit the API schema, which has the necessary paths and responses, to call the custom plugin. 
 
 Download the email-plugin.yaml file and open it in an editor. 
 In the beginning of the YAML file, paste the API endpoint URL you copied in the previous step prior where it says Enter ApiEndpoint. Make sure to remove the backslash at the end of the URL.  
 
 
 Paste the CognitoAuthorizationURL and CognitoTokenUrl from the previous step where it says to enter the URLs, as shown in the previous image.  
 
 
 Save this YAML file. 
 
Step 5: Set up the custom plugin 
 
 Navigate to the Amazon Q Business console and choose Actions and then Plugins navigation pane. 
 Choose Add Plugin and then +Create custom plugin. 
 Enter a name and description for your plugin. 
 Under API Schema, select Define with in-line OpenAPI schema editor and paste the edited email plugin YAML file with the new URLs in the text box. 
   
   Make sure the YAML | JSON toggle is set to YAML. 
    
 Under Authentication, select Authentication required. 
 Under AWS Secrets Manager secret select Create and add new secret. 
   
   Enter a name for your secret. 
   Enter a short description (for example : Custom email plugin for Amazon Q Business application) 
   To populate the other details, navigate to the Amazon Cognito console and select your created user pool. 
     
     Under Recommendations / Set up your app, choose EmailSenderPoolClient 
     Copy Client ID and Client secret into the QBusiness custom plugin screen.  
     For the OAuth callback URL, copy the Amazon Q Business web application deployed URL and add /oauth/callback. For example: https://xxxxxx.chat.qbusiness.us-east-1.on.aws/oauth/callback  
     Save this URL for a later step. 
      
    
 Select Create and use a new service role for authorization. 
 Choose Add plugin. 
 
Step 6: Add a callback URL to Amazon Cognito 
The following steps help ensure that the callback URL is configured correctly. The callback URL is a user-configured URL where your application receives the authorization code after a user successfully signs in or signs out through the Amazon Cognito hosted UI. 
 
 Navigate to the Amazon Cognito console. 
 Choose App clients under Applications in the navigation pane. 
 Choose your app client. 
 Choose the Login pages tab.  
 
 
 Choose Edit under the Managed login pages configuration. 
 Paste the OAuth callback URL you saved from the previous step under Allowed Callback URLs. 
   
   Example: https://xxxxxx.chat.qbusiness.us-east-1.on.aws/oauth/callback 
    
 Choose Save changes.  
 
Step 7: Sync data source 
The following steps are to make sure that the data source has the most recent updates 
 
 Navigate to the Amazon Q Business console. 
 Find your application (should be named qbusiness-example-app). 
 Select your application and go to the Data sources section in the navigation pane. 
 Select your data source and initiate a sync by choosing Sync now to index the uploaded documents.  
 
 
 Sync history should show a status of completed.  
 
Step 8: Set up user access for the Amazon Q Business chatbot 
The following steps allow users to be added to access the chatbot 
 
 Navigate back to the application in the Amazon Q Business console. 
 Choose Manage User Access.  
 
 
 Choose Add Groups or Users. 
 If you already have an existing user you want to give access to, choose Assign existing users and groups. 
   
   Start entering the userâ€™s name and Assign an existing user. 
    
 To add a new user, choose Add and Assign New Users. 
   
   Choose Add and then Assign and then choose Confirm. 
     
     Each user should get an email asking them to accept the invitation. 
     After they choose Accept Invitation, they will be taken to a page to set up their password. They will use this username and password to sign in to the chatbot.  
      
    
 
Step 9: Query the chatbot 
The following steps walk you through how to best use the application. 
 
 Navigate to the application on the Amazon Q Business console again. 
 Select the Deployed URL.  
 
 
 Sign in using the username and password set in the previous step. Follow the steps in the console to register a built-in authenticator. 
 You can now: 
   
   Query the chatbot about your training materials. For example, ask it: Tell me about my dental insurance. 
   Request to send emails through the chatbot. Make sure to choose the custom plugin button to use this functionality. 
     
     Example: Can you send an email asking for more information about my dental insurance? 
     If asked for your email ID, enter the email ID that you verified in previous steps. 
     The application will ask you to authorize your access. Choose Authorize.  
      
    
 Enter your CognitoUserEmail and password defined in Step 3 in the window and choose Sign in.  
 Emails will be sent to the email address verified in Step 1. 
 
Troubleshooting 
The following issues might occur during deployment or usage of your chatbot solution. Use these solutions to resolve common problems. 
Data sync failures typically result from incorrect S3 bucket permissions. Check your bucket policy and access settings to facilitate proper configuration. 
 
 User access issues often occur when invitations arenâ€™t accepted or passwords arenâ€™t set up. Verify that users have completed both steps in the access setup process. 
 When the bot provides incomplete answers, try refreshing your content by initiating a new data source sync in the Amazon Q Business console. 
 Amazon Cognito authorization issues can occur in the Q Business console â€“ to mitigate them: 
 Make sure that the callback URL matches your Q Business deployed URL 
 The callback URL in the managed sign in pages configuration matches your Q Business deployed URL as seen in Step 6. 
 Your Amazon Cognito URLs are copied into the email-plugin.yaml correctly from the CloudFormation outputs. 
 Email delivery problems usually stem from email configuration. Verify that: 
 The Custom Plugin button shows an active (blue) status. 
 Your Amazon SES email address is verified. 
 Youâ€™re using the correct email address in your configuration. 
 
The BlueprintRole section is currently commented out because this is a proof-of-concept deployment. When deploying to production environments, especially those involving multiple AWS accounts or organizations, you should uncomment this section. The BlueprintRole provides necessary permissions for cross-account access and advanced management features of Amazon Q Business applications. 
Service versions 
AWS Lambda runtime: Python 3.11 
Regional availability 
Before deploying this solution, note that Amazon Q Business is not available in all AWS Regions. This solution can only be deployed in Regions where Amazon Q Business is supported. For the most up-to-date information on Regional availability, check the AWS Regional Services list. 
Real-world use case 
Amazon Q Business is transforming how businesses handle internal knowledge management and support. By securely connecting to company data sources and systems, Amazon Q Business helps organizations make their institutional knowledge more accessible and actionable. Here are two real-world examples demonstrating how Amazon Q Business enhances workplace productivity and knowledge sharing using the custom plugin feature. 
Scenario 1: New employee Sarah uses a chatbot to learn about the organizationâ€™s leave policy. The chatbot efficiently retrieves relevant information from indexed training materials to answer her initial question. When Sarah later asks a specific question beyond the scope of the chatbotâ€™s knowledge base, it promptly offers to connect her with the training support team through email. Sarah takes advantage of this option, making sure her complex query receives proper attention without delay. This interaction demonstrates the chatbotâ€™s effectiveness in providing immediate access to information while maintaining appropriate escalation channels for questions requiring human expertise. 
Scenario 2: Alex, a field technician at a manufacturing company, needs to complete an urgent maintenance procedure on specialized equipment while at a client site. He accesses the companyâ€™s Amazon Q-powered knowledge assistant. 
 
 Alex asks, â€œHow do I recalibrate the XB-2000 sensor array after firmware update?â€ The chatbot immediately retrieves the relevant technical documentation from indexed maintenance manuals and presents a step-by-step procedure with details. 
 During the calibration, Alex encounters an unexpected error code not covered in the standard documentation. He uses the custom plugin to request immediate assistance, typing â€œI need help with error code E-457 on the XB-2000.â€ The chatbot offers to email the technical support team, including his location details and equipment specifications automatically gathered from his user profile. 
 
This scenario demonstrates how the Amazon Q Business solution delivers critical technical knowledge in field situations while providing seamless escalation paths for edge cases that require specialized expertise, ultimately reducing equipment downtime and improving customer satisfaction. 
Scenario 3: A global manufacturing company with more than 5,000 employees implements Amazon Q Business to streamline their equipment maintenance support system across multiple facilities. Maintenance teams use Amazon Q Business to access equipment documentation and when encountering situations requiring vendor support or parts ordering, they use the custom pluginâ€™s email feature. 
 
 Example interaction: A maintenance supervisor in Singapore enters â€œNeed to escalate XB-2000 production line shutdown to vendor support team.â€ 
 
The Amazon Q custom plugin automatically: 
 
 Generates a structured email containing facility location, equipment history, and maintenance logs. 
 Routes communications to vendor support, maintenance management, and procurement teams. 
 
This implementation demonstrates how the custom plugin feature standardizes emergency communications across global facilities while making sure critical information is automatically included in escalations. 
Clean up 
To remove the solution, delete the CloudFormation stack you created to test this solution. This action will automatically deprovision associated AWS resources, including Lambda functions, S3 buckets, and Amazon OpenSearch Service domains set up by Amazon Q Business. This solution uses multiple AWS services with costs varying based on usage patterns. Amazon Q Business pricing is determined by the number of users and queries processed, with additional charges applying for custom plugin usage. Lambda costs are calculated based on the number of requests and compute time, though a free tier allowance of 1 million requests per month is available. Storage and data transfer costs will apply for Amazon S3, which hosts your training materials. Email communications through Amazon SES incur standard sending charges, though you can benefit from a free tier that includes 62,000 outbound messages per month. For detailed pricing information, we recommend consulting the official pricing pages for each service. 
Conclusion 
This intelligent chatbot solution harnesses the capabilities of Amazon Q to revolutionize employee training by providing instant access to organizational knowledge while maintaining human escalation paths for complex inquiries. Queries the system is designed to handle include multi-turn conversations requiring context from previous interactions, questions that need information synthesis across multiple documents, and technical troubleshooting scenarios requiring step-by-step guidance. By implementing this CloudFormation-automated deployment, organizations can significantly reduce support costs up to 85%, improve knowledge accessibility, and create a training environment that is designed to scale with their needs. It supports enterprise-wide deployments by integrating with your existing identity and access management systems, so you can quickly add or remove users and manage permissions at scale. As your organization expands, you can connect additional data sources such as Dropbox and Google Drive, making sure the system grows alongside your business needs. 
This Amazon Q Business training solution is worth building because it dramatically reduces training support costs while providing employees with continuous access to accurate information. The automated deployment and seamless human escalation path make it an ideal solution for organizations looking to scale knowledge delivery without expanding support staff. 
Empower your organization with this cutting-edge chatbot solution today and share your experiences and insights in the comments section below. 
Explore more about Amazon Q Business capabilities in our comprehensive documentation or join our AWS Community forum to connect with others implementing similar solutions. Donâ€™t forget to follow #AmazonQBusiness on social media to share your implementation journey! 
 
About the authors 
Neha Bhupatiraju is a Data and ML Engineer at AWS Professional Services. With expertise in data engineering and machine learning, she helps enterprise customers leverage both traditional data analytics and machine learning. She specializes in implementing intelligent chatbots, developing predictive analytics models and building generative AI applications. 
Charishma Ravoori is a Data and ML Engineer at AWS Professional Services. Charishma works with AWS customers and partners to help them build solutions in predictive/data analytics, data engineering and generative AI using AWS services. 
Ujwala Bitla is a Deep Learning Architect at AWS Generative AI Innovation Center, where she designs and delivers cutting-edge GenAI solutions for&nbsp; customers across industries. With extensive experience in Data Science and Analysis, she specializes in Large Language Models, Retrieval-Augmented Generation (RAG), Agents and responsible AI implementation. 
Raju Patil is a Senior Data Scientist in AWS Professional Services, where he builds and deploys AI/ML solutions to help AWS customers overcome business challenges. His work spans across various use cases, including Generative AI, Computer Vision, Time-Series Forecasting, and Predictive Analytics.
â€¢ Create a travel planning agentic workflow with Amazon Nova
  Traveling is enjoyable, but travel planning can be complex to navigate and a hassle. Travelers must book accommodations, plan activities, and arrange local transportation. All these decisions can feel overwhelming. Although travel professionals have long helped manage these complexities, recent breakthroughs in generative AI have made something entirely new possibleâ€”intelligent assistants that can understand natural conversation, access real-time data, and directly interface with booking systems and travel tools. Agentic workflows, which use large language models (LLMs) with access to external tools, are particularly promising for simplifying dynamic, multi-step processes like travel planning. 
In this post, we explore how to build a travel planning solution using AI agents. The agent uses Amazon Nova, which offers an optimal balance of performance and cost compared to other commercial LLMs. By combining accurate but cost-efficient Amazon Nova models with LangGraph orchestration capabilities, we create a practical travel assistant that can handle complex planning tasks while keeping operational costs manageable for production deployments. 
Solution overview 
Our solution is built on a serverless AWS Lambda architecture using Docker containers and implements a comprehensive three-layer approach: frontend interaction, core processing, and integration services. In the core processing layer, we use LangGraph, a stateful orchestration framework, to create a sophisticated yet flexible agent-based system that manages the complex interactions required for travel planning. 
The core of our system is a graph architecture where components (nodes) handle distinct aspects of travel planning, with the router node orchestrating the flow of information between them. We use Amazon Nova, a new generation of state-of-the-art foundation models (FMs) available exclusively on Amazon Bedrock that delivers frontier intelligence with industry-leading price-performance. The router node uses an LLM to analyze each user query and, with access to the description of our 14 action nodes, decides which ones need to be executed. The action nodes, each with their own LLM chain, powered by either Amazon Nova Pro or Amazon Nova Lite models, manage various functions, including web research, personalized recommendations, weather lookups, product searches, and shopping cart management. 
We use Amazon Nova Lite for the router and simpler action nodes. It can handle query analysis and basic content generation with its lightning-fast processing while maintaining strong accuracy at a low cost. Five complex nodes use Amazon Nova Pro for tasks requiring advanced instruction following and multi-step operations, such as detailed travel planning and recommendations. Both models support a 300,000-token context window and can process text, image, and video inputs. The models support text processing across more than 200 languages, helping our travel assistant serve a global audience.The integration layer unifies multiple data sources and services through an interface: 
 
 Amazon Product Advertising API for travel-related product recommendations 
 Google Custom Search API for real-time travel information 
 OpenWeather API for accurate weather forecasts 
 Amazon Bedrock Knowledge Bases for travel destination insights 
 Amazon DynamoDB for persistent storage of user profiles and chat history 
 
These integrations serve as examples, and the architecture is designed to be extensible, so organizations can quickly incorporate their own APIs and data sources based on specific requirements. 
The agent keeps track of the conversation state using AgentState (TypedDict), a special Python dictionary that helps prevent data errors by enforcing specific data types. It stores the information we need to know about each userâ€™s session: their conversation history, profile information, processing status, and final outputs. This makes sure the different action nodes can access and update information reliably. 
The following diagram illustrates the solution architecture. 
 
The travel assistant processes user interactions from end to end: 
 
 Users interact with a React.js web application through a chat interface. 
 Their requests are authenticated using Amazon Cognito and routed through Amazon API Gateway. 
 Authenticated requests are sent to our backend Lambda functions, which host the core agent workflow. 
 API credentials are securely stored using AWS Secrets Manager, following best practices to make sure these sensitive keys are never exposed in code or configuration files, with appropriate access controls and rotation policies implemented. 
 The Travel Assistant Agent itself consists of several interconnected components. At the center, the agent router analyzes incoming queries and orchestrates the workflow. 
 The agent maintains state through three DynamoDB tables that store conversation history, shopping wishlists, and user profiles, making sure context is preserved across interactions. 
 For travel-specific knowledge, the system uses a combination of Amazon Bedrock Knowledge Bases, Amazon OpenSearch Serverless, and a document store in Amazon Simple Storage Service (Amazon S3). These components work together to provide accurate, relevant travel information when needed. 
 The agentâ€™s action nodes handle specialized tasks by combining LLM chains with external APIs. When users need product recommendations, the system connects to the Amazon Product Advertising API. For general travel information, it uses the Google Custom Search API, and for weather-related queries, it consults the OpenWeather API. API credentials are securely managed through Secrets Manager. 
 The system formulates comprehensive responses based on collected information, and the final responses are returned to the user through the chat interface. 
 
This architecture supports both simple queries that can be handled by a single node and complex multi-step interactions that require coordination across multiple components. The system can scale horizontally, and new capabilities can be added by introducing additional action nodes and API integrations. 
You can deploy this solution using the AWS Cloud Development Kit (AWS CDK), which generates an AWS CloudFormation template that handles the necessary resources, including Lambda functions, DynamoDB tables, and API configurations. The deployment creates the required AWS resources and outputs the API endpoint URL for your frontend application. 
Prerequisites 
For this walkthrough, you must have the following prerequisites: 
 
 An active AWS account and familiarity with FMs, Amazon Bedrock, and Amazon OpenSearch Service 
 Access to the Amazon Nova FMs on Amazon Bedrock 
 Node.js v16.x or later 
 Python 3.9 or later 
 Access to the Product Advertising API (PAAPI) 
 
Clone the repository 
Start by cloning the GitHub repository containing the solution files: 
 
 git clone https://github.com/aws-samples/sample-travel-assistant-agent.git 
 
Obtain API keys 
The solution requires API keys from three services to enable its core functionalities: 
 
 OpenWeather API â€“ Create a Free Access account at OpenWeather to obtain your API key. The free tier (60 calls per minute) is sufficient for testing and development. 
 Google Custom Search API â€“ Set up the search functionality through Google Cloud Console. Create or select a project and enable the Custom Search API. Then, generate an API key from the credentials section. Create a search engine at Programmable Search and note your Search Engine ID. The free tier includes 100 queries per day. 
 (Optional) Amazon Product Advertising API (PAAPI) â€“ If you want to enable product recommendations, access the PAAPI Documentation Portal to generate your API keys. You will receive both a public key and a secret key. You must have an Amazon Associates account to access these credentials. If youâ€™re new to the Amazon Associates Program, complete the application process first. Skip this step if you donâ€™t want to use PAAPI features. 
 
Add API keys to Secrets Manager 
Before deploying the solution, you must securely store your API keys in Secrets Manager. The following table lists the secrets to create and their JSON structure. For instructions to create a secret, refer to Create an AWS Secrets Manager secret. 
 
  
   
   Secret Name 
   JSON Structure 
   
   
   openweather_maps_keys 
   {" openweather_key": "YOUR_API_KEY"} 
   
   
   google_search_keys 
   {"cse_id": "YOUR_SEARCH_ENGINE_ID", "google_api_key": "YOUR_API_KEY"} 
   
   
   paapi_keys 
   {"paapi_public": "YOUR_PUBLIC_KEY", "paapi_secret": "YOUR_SECRET_KEY"} 
   
  
 
Configure environment variables 
Create a .env file in the project root with your configuration: 
 
 STACK_NAME=TravelAssistantAgent

# Optional: Create Bedrock Knowledge Base with documents
KB_DOCS_PATH = Path/to/your/documents/folder
# Optional: Enable/disable Product Search features with PAAPI
USE_PAAPI=false 
 
Deploy the stack 
If this is your first time using the AWS CDK in your AWS account and AWS Region, bootstrap your environment: 
 
 cdk bootstrap 
 
Deploy the solution using the provided script, which creates the required AWS resources, including Lambda functions, DynamoDB tables, and API configurations: 
 
 sh deploy.sh 
 
Access your application 
When the deployment is complete, open the AWS CloudFormation console and open your stack. On the Outputs tab, note the following values: 
 
 WebAppDomain â€“ Your applicationâ€™s URL 
 UserPoolId â€“ Required for user management 
 UserPoolClientId â€“ Used for authentication 
 
 
Create an Amazon Cognito user 
Complete the following steps to create an Amazon Cognito user: 
 
 On the Amazon Cognito console, choose User pools in the navigation pane. 
 Choose your user pool. 
 Choose Users in the navigation pane, then choose Create user. 
 
 
 
 For Email address, enter an email address, and select Mark email address as verified. 
 For Password, enter a temporary password. 
 Choose Create user. 
 
 
You can use these credentials to access your application at the WebAppDomain URL. 
Test the solution 
To test the agentâ€™s capabilities, we created a business traveler persona and simulated a typical travel planning conversation flow. We focused on routing, function calling accuracy, response quality, and latency metrics. The agentâ€™s routing system directs the user questions to the appropriate specialized node (for example, searching for accommodations, checking weather conditions, or suggesting travel products). Throughout the conversation, the agent maintains the context of previously discussed details, so it can build upon earlier responses while providing relevant new information. For example, after discussing travel destination, the agent can naturally incorporate this into subsequent weather and packing list recommendations. 
The following screenshots demonstrate the end-user experience, while the underlying API interactions are handled seamlessly on the backend. The complete implementation details, including Lambda function code and API integration patterns, are available in our GitHub repository. 

 
 
The solution demonstrates personalization capabilities using sample user profiles stored in DynamoDB, containing upcoming trips and travel preferences. In production deployments, these profiles can be integrated with existing customer databases and reservation systems to provide a personalized assistance. 

 
 

 
 

 
 
The product recommendations shown are live links to actual items available on Amazon.com, so the user can explore or purchase these products directly. The user can choose a link to check out the product, or choose Add to Amazon Cart to see the items in their shopping cart. 

 
 
Clean up 
After you are done experimenting with the travel assistant, you can locate the CloudFormation stack on the AWS CloudFormation console and delete it. This will delete the resources you created. 
Conclusion 
Our travel planning assistant agent demonstrates a practical application built by Amazon Nova and LangGraph for solving real-world business challenges. The system streamlines complex travel planning while naturally integrating product recommendations through specialized processing nodes and real-time data integration. Amazon Nova Lite models showed reasonable performance at task orchestration, and Amazon Nova Pro performed well for more complex function calling operations. Looking ahead, this framework could be implemented with more dynamic orchestration systems such as ReAct. To build your own implementation, explore our code samples in the GitHub repository. 
For those looking to deepen their understanding of LLM-powered agents, AWS provides extensive resources on building intelligent systems. The Amazon Bedrock Agents documentation offers insights into automating multistep tasks with FMs, and the AWS Bedrock Agent Samples GitHub repo provides guidance for implementing multiple agent applications using Amazon Bedrock. 
 
About the authors 
Isaac Privitera is a Principal Data Scientist with the AWS Generative AI Innovation Center, where he develops bespoke generative AI-based solutions to address customersâ€™ business problems. His primary focus lies in building responsible AI systems, using techniques such as RAG, multi-agent systems, and model fine-tuning. When not immersed in the world of AI, Isaac can be found on the golf course, enjoying a football game, or hiking trails with his loyal canine companion, Barry. 
Ryan Razkenari is a Deep Learning Architect at the AWS Generative AI Innovation Center, where he uses his expertise to create cutting-edge AI solutions. With a strong background in AI and analytics, he is passionate about building innovative technologies that address real-world challenges for AWS customers. 
Sungmin Hong is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he helps expedite a variety of use cases for AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds a PhD in Computer Science from New York University. Outside of work, Sungmin enjoys hiking, reading, and cooking.

â¸»