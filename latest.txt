‚úÖ Morning News Briefing ‚Äì September 17, 2025 10:43

üìÖ Date: 2025-09-17 10:43
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions: Fog Patches, 6.7¬∞C
  Temperature: 6.7&deg;C Pressure: 102.2 kPa Visibility: 11 km Visibility : 11 km Humidity: 96 % Dewpoint:  6.1&deg:C Wind: SW 4 km/h Air Quality Health Index: n/a . The weather is currently at Garrison Petawawa 6:00 AM EDT Wednesday 17 September 2025 .
‚Ä¢ Wednesday: Mainly sunny. High 26.
  Fog patches dissipating this morning . Mainly sunny. High 26. Humidex 29. UV index 6 or high . Forecast issued 5:00 AM EDT Wednesday 17 September 2025. Forecast: "Fog patches dissipated this morning. Mainly . sunny. Fog patches . dissipate this morning and will be mostly sunny today . High 26, with a high of 26
‚Ä¢ Wednesday night: Clear. Low 13.
  Forecast issued 5:00 AM EDT Wednesday 17 September 2025 . Clear. Clear. Low 13.70s . Clear skies . Clear seasideide weather. Clear skies. Clear seasides. Low seaside seaside weather forecast. Clear seaide weather conditions. Forecast: 20-year-old will be the first to see the sun on 17 September 25th, 2025 .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Suspect in UK toddler disappearance is released after serving time in unrelated case
  A German national under investigation in the disappearance of British toddler Madeleine McCann 18 years ago was freed from prison Wednesday after serving a sentence in an unrelated case, police said . Police said a German national was freed after serving an unrelated prison sentence . The case is the focus of an ongoing investigation into the disappearance and disappearance of the missing British toddler, Madelein McCann, who was found
‚Ä¢ Beyond 'draining the swamp': How Trump is knocking down checks on presidential power
  President Trump wants to be able to fire far more executive branch employees at will . The move would upend checks on presidential power that have existed for more than a century . The president wants to change the checks on his executive branch power . The White House is weighing up on the idea of ending checks on the president's authority to fire his staff at will, according to the White House .
‚Ä¢ What to know about zarfs, the fanciest way to drink coffee
  During the Ottoman Empire, people used devices called "zarfs" to hold their coffee cups . Here's what to know about this word's history . Use the word of the week to help you understand today's featured news stories in Washington, D.C., Europe and beyond . Back to Mail Online's weekly Newsquiz. Back to the page you came from .Back to the
‚Ä¢ Facing Trump's pressure, the Fed is likely to cut rates for the first time this year
  The Fed is likely to lower interest rates by a quarter percentage point Wednesday in an effort to cushion the sagging job market . The move comes as policymakers face growing pressure from Trump . The Fed will lower interest rate by a half percentage point in a bid to keep the job market going strong in the U.S., which has seen a sharp drop in jobless numbers since the recession began
‚Ä¢ When her car ran out of gas, help came from a homeless encampment
  Juli Cobb's car ran out of gas in the middle of the road . Three men from a nearby homeless encampment rushed over to push her car to safety . The men pushed the car to the safety of the car after it was pushed to safety by the men from the encampment . The homeless men are now living in an encampment in a homeless home in New York City .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Why Microsoft has the name of an old mouse hidden in its Bluetooth drivers
  Microsoft hardcoded reference to the Microsoft Wireless Notebook Presenter Mouse 8000 . Is this nostalgic favoritism from Microsoft? Or is it just somebody, somewhere, making a mistake that an engineer had to work around? Is it just a screw-up or conspiracy? Or just a mistake made by an engineer? We ask: Is this nostalgia favoritism or just a coincidence of a mistake?
‚Ä¢ Whitehall lobs ¬£40M at 'critical' phase of police DB reboot
  Home Office is flinging nearly ¬£40 million in taxpayer cash at PA Consulting . Officials say there's no time to switch suppliers if they want the PNC off life support before March 2026 . PNC replacement will be replaced by the Police National Computer (PNC) in March 2023 . The PNC is the UK's biggest-ever police computer system, replacing it with a
‚Ä¢ AI, Arm, and Copilot: Living with Microsoft's Surface Laptop 7
  Arm-based Surface Laptop 7 was introduced in 2024, followed by an Intel-powered version a few months later . As with much of the Surface line, it's a well-engineered piece of hardware . Nice hardware, shame about the OS, but it's not as good as the Windows 8th-gen laptop . I needed something that could run off the battery for a
‚Ä¢ UEFI Secure Boot for Linux Arm64 ‚Äì where do we stand?
  Arm devices are everywhere today and many of them run Linux . The operating system also powers cloud computing and IT environments all over the world . x86 is still the dominant architecture of global computer hardware, where the Unified Extensible Firmware Interface (UEFI) with Secure Boot incorporated is a standard . But what does UEFI look like from an Arm perspective?‚Ä¶‚Ä¶ What does it look
‚Ä¢ UK Cabinet Office hands stalled Microsoft migration to another department
  Cabinet Office has handed project to migrate from Google Workspace to Microsoft 365 to another department . Project to get off Google remains a red risk, according to government assessment . M365 project is a much-delayed project that has been handed over to a new department . The Cabinet Office is the strategic center of the UK government's strategic center, the Cabinet Office . Microsoft 365 is Microsoft's

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ How billions of hacked mosquitoes and a vaccine could beat the deadly dengue virus
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Quality of life improvements associated with weight loss using a novel shape-shifting hydrogel capsule: RESET study results
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Realizing the full potential of Our Future Health through data linkage and trans-biobank efforts
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Current applications and future directions in natural language processing for news media and mental health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ How to measure the returns on R&D spending
  MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.



Given the draconian cuts to US federal funding for science, including the administration‚Äôs proposal to reduce the 2026 budgets of the National Institutes of Health by 40% and the National Science Foundation by 57%, it‚Äôs worth asking some hard-nosed money questions: How much should we be spending on R&amp;D? How much value do we get out of such investments, anyway? To answer that, it‚Äôs important to look at both successful returns and at investments that went nowhere.





Sure, it‚Äôs easy to argue for the importance of spending on science by pointing out that many of today‚Äôs most useful technologies had their origins in government-funded R&amp;D. The internet, CRISPR, GPS‚Äîthe list goes on and on. All true. But this argument ignores all the technologies that received millions in government funding and haven‚Äôt gone anywhere‚Äîat least not yet. We still don‚Äôt have DNA computers or molecular electronics. Never mind the favorite examples cited by contrarian politicians of seemingly silly or frivolous science projects (think shrimp on treadmills).



While cherry-picking success stories help illustrate the glories of innovation and the role of science in creating technologies that have changed our lives, it provides little guidance for how much we should spend in the future‚Äîand where the money should go.



A far more useful approach to quantifying the value of R&amp;D is to look at its return on investment (ROI). A favorite metric for stock pickers and PowerPoint-wielding venture capitalists, ROI weighs benefits versus costs. If applied broadly to the nation‚Äôs R&amp;D funding, the same kind of thinking could help account for both the big wins and all the money spent on research that never got out of the lab.



The problem is that it‚Äôs notoriously difficult to calculate returns for science funding‚Äîthe payoffs can take years to appear and often take a circuitous route, so the eventual rewards are distant from the original funding. (Who could have predicted Uber as an outcome of GPS? For that matter, who could have predicted that the invention of ultra-precise atomic clocks in the late 1940s and 1950s would eventually make GPS possible?) And forget trying to track the costs of countless failures or apparent dead ends.



But in several recent papers, economists have approached the problem in clever new ways, and though they ask slightly different questions, their conclusions share a bottom line: R&amp;D is, in fact, one of the better long-term investments that the government can make.



This story is part of MIT Technology Review‚Äôs &#8220;America Undone‚Äù series, examining how the foundations of US success in science and innovation are currently under threat.¬†You can read the rest here.



That might not seem very surprising. We‚Äôve long thought that innovation and scientific advances are key to our prosperity. But the new studies provide much-needed details, supplying systematic and rigorous evidence for the impact that R&amp;D funding, including public investment in basic science, has on overall economic growth.



And the magnitude of the benefits is surprising.



Bang for your buck



In ‚ÄúA Calculation of the Social Returns to Innovation,‚Äù Benjamin Jones, an economist at Northwestern University, and Lawrence Summers, a Harvard economist and former US Treasury secretary, calculate the effects of the nation‚Äôs total R&amp;D spending on gross domestic product and our overall standard of living. They‚Äôre taking on the big picture, and it‚Äôs ambitious because there are so many variables. But they are able to come up with a convincing range of estimates for the returns, all of them impressive.



On the conservative end of their estimates, says Jones, investing $1 in R&amp;D yields about $5 in returns‚Äîdefined in this case as additional GDP per person (basically, how much richer we become). Change some of the assumptions‚Äîfor example, by attempting to account for the value of better medicines and improved health care, which aren‚Äôt fully captured in GDP‚Äîand you get even larger payoffs.



While the $5 return is at the low end of their estimates, it‚Äôs still ‚Äúa remarkably good investment,‚Äù Jones says. ‚ÄúThere aren‚Äôt many where you put in $1 and get $5 back.&#8221;



That‚Äôs the return for the nation‚Äôs overall R&amp;D funding. But what do we get for government-funded R&amp;D in particular? Andrew Fieldhouse, an economist at Texas A&amp;M, and Karel Mertens at the Federal Reserve Bank of Dallas looked specifically at how changes in public R&amp;D spending affect the total factor productivity (TFP) of businesses. A favorite metric of economists, TFP is driven by new technologies and innovative business know-how‚Äînot by adding more workers or machines‚Äîand is the main driver of the nation‚Äôs prosperity over the long term.



The economists tracked changes in R&amp;D spending at five major US science funding agencies over many decades to see how the shifts eventually affected private-sector productivity. They found that the government was getting a huge bang for its nondefense R&amp;D buck.





The benefits begin kicking in after around five to 10 years and often have a long-lasting impact on the economy. Nondefense public R&amp;D funding has been responsible for 20% to 25% of all private-sector productivity growth in the country since World War II, according to the economists. It‚Äôs an astonishing number, given that the government invests relatively little in nondefense R&amp;D. For example, its spending on infrastructure, another contributor to productivity growth, has been far greater over those years.



The large impact of public R&amp;D investments also provides insight into one of America‚Äôs most troubling economic mysteries: the slowdown in productivity growth that began in the 1970s, which has roiled the country‚Äôs politics as many people face stunted living standards and limited financial prospects. Their research, says Fieldhouse, suggests that as much as a quarter of that slowdown was caused by a decline in public R&amp;D funding that happened roughly over the same time.



After reaching a high of 1.86% of GDP in 1964, federal R&amp;D spending began dropping. Starting in the early 1970s, TFP growth also began to decline, from above 2% a year in the late 1960s to somewhere around 1% since the 1970s (with the exception of a rise during the late 1990s), roughly tracking the spending declines with a lag of a few years.



If in fact the productivity slowdown was at least partially caused by a drop in public R&amp;D spending, it‚Äôs evidence that we would be far richer today if we had kept up a higher level of science investment. And it also flags the dangers of today‚Äôs proposed cuts. ‚ÄúBased on our research,‚Äù says Fieldhouse, ‚ÄúI think it‚Äôs unambiguously clear that if you actually slash the budget of the NIH by 40%, if you slash the NSF budget by 50%, there‚Äôs going to be a deceleration in US productivity growth over the next seven to 10 years that will be measurable.‚Äù



Out of whack



Though the Trump administration‚Äôs proposed 2026 budget would slash science budgets to an unusual degree, public funding of R&amp;D has actually been in slow decline for decades. Federal funding of science is at its lowest rate in the last 70 years, accounting for only around 0.6% of GDP.







Even as public funding has dropped, business R&amp;D investments have steadily risen. Today businesses spend far more than the government; in 2023, companies invested about $700 billion in R&amp;D while the US government spent $172 billion, according to data from the NSF‚Äôs statistical agency. You might think, Good‚Äîlet companies do research. It‚Äôs more efficient. It‚Äôs more focused. Keep the government out of it.



But there is a big problem with that argument. Publicly funded research, it turns out, tends to lead to relatively more productivity growth over time because it skews more toward fundamental science than the applied work typically done by companies.



In a new working paper called ‚ÄúPublic R&amp;D Spillovers and Productivity Growth,‚Äù Arnaud Dy√®vre, an assistant professor at of economics at HEC Paris, documents the broad and often large impacts of so-called knowledge spillovers‚Äîthe benefits that flow to others from work done by the original research group. Dy√®vre found that the spillovers of public-funded R&amp;D have three times more impact on productivity growth across businesses and industries than those from private R&amp;D funding.



The findings are preliminary, and Dy√®vre is still updating the research‚Äîmuch of which he did as a postdoc at MIT‚Äîbut he says it does suggest that the US ‚Äúis underinvesting in fundamental R&amp;D,‚Äù which is heavily funded by the government. ‚ÄúI wouldn‚Äôt be able to tell you exactly which percentage of R&amp;D in the US needs to be funded by the government or what percent needs to be funded by the private sector. We need both,‚Äù he says. But, he adds, ‚Äúthe empirical evidence‚Äù suggests that ‚Äúwe‚Äôre out of balance.‚Äù



The big question



Getting the balance of funding for fundamental science and applied research right is just one of the big questions that remain around R&amp;D funding. In mid-July, Open Philanthropy and the Alfred P. Sloan Foundation, both nonprofit organizations, jointly announced that they planned to fund a five-year ‚Äúpop-up journal‚Äù that would attempt to answer many of the questions still swirling around how to define and optimize the ROI of research funding.



‚ÄúThere is a lot of evidence consistent with a really high return to R&amp;D, which suggests we should do more of it,‚Äù says Matt Clancy, a senior program officer at Open Philanthropy. ‚ÄúBut when you ask me how much more, I don‚Äôt have a good answer. And when you ask me what types of R&amp;D should get more funding, we don‚Äôt have a good answer.‚Äù



Pondering such questions should keep innovation economists busy for the next several years. But there is another mystifying piece of the puzzle, says Northwestern‚Äôs Jones. If the returns on R&amp;D investments are so high‚Äîthe kind that most venture capitalists or investors would gladly take‚Äîwhy isn‚Äôt the government spending more?





&#8220;I think it‚Äôs unambiguously clear that if you actually slash the budget of the NIH by 40%, if you slash the NSF budget by 50%, there‚Äôs going to be a deceleration in US productivity growth over the next seven to 10 years that will be measurable.&#8221;





Jones, who served as a senior economic advisor in the Obama administration, says discussions over R&amp;D budgets in Washington are often ‚Äúa war of anecdotes.‚Äù Science advocates cite the great breakthroughs that resulted from earlier government funding, while budget hawks point to seemingly ludicrous projects or spectacular failures. Both have plenty of ammunition. ‚ÄúPeople go back and forth,‚Äù says Jones, ‚Äúand it doesn‚Äôt really lead to anywhere.‚Äù



The policy gridlock is rooted in in the very nature of fundamental research. Today‚Äôs science will lead to great advances. And there will be countless failures; a lot of money will be wasted on fruitless experiments. The problem, of course, is that when you‚Äôre deciding to fund new projects, it‚Äôs impossible to predict which the outcome will be, even in the case of odd, seemingly silly science. Guessing just what research will or will not lead to the next great breakthrough is a fool‚Äôs errand.



Take the cuts in the administration‚Äôs proposed fiscal 2026 budget for the NSF, a leading funder of basic science. The administration‚Äôs summary begins with the assertion that its NSF budget ‚Äúis prioritizing investments that complement private-sector R&amp;D and offer strong potential to drive economic growth and strengthen U.S. technological leadership.‚Äù So far, so good. It cites the government‚Äôs commitment to AI and quantum information science. But dig deeper and you will see the contradictions in the numbers.



Not only is NSF&#8217;s overall budget cut by 57%, but funding for physical sciences like chemistry and materials research‚Äîfields critical to advancing AI and quantum computers‚Äîhas also been blown apart. Funding for the NSF‚Äôs mathematical and physical sciences program was reduced by 67%. The directorate for computer and information science and engineering fared little better; its research funding was cut by 66%.



There is a great deal of hope among many in the science community that Congress, when it passes the actual 2026 budget, will at least partially reverse these cuts. We‚Äôll see. But even if it does, why attack R&amp;D funding in the first place? It‚Äôs impossible to answer that without plunging into the messy depths of today‚Äôs chaotic politics. And it is equally hard to know whether the recent evidence gathered by academic economists on the strong returns to R&amp;D investments will matter when it comes to partisan policymaking.



But at least those defending the value of public funding now have a far more productive way to make their argument, rather than simply touting past breakthroughs. Even for fiscal hawks and those pronouncing concerns about budget deficits, the recent work provides a compelling and simple conclusion: More public funding for basic science is a sound investment that makes us more prosperous.
‚Ä¢ De-risking investment in AI agents
  Automation has become a defining force in the customer experience . The future belongs to organizations that focus on outcome-oriented design . The move from scripted, deterministic flows to non-deterministic, generative systems brings new challenges . How can you balance safety and flexibility when giving an AI system access to core infrastructure? And how can you manage cost, transparency, and ethical risk while still pursuing meaningful returns?
‚Ä¢ The Download: regulators are coming for AI companions, and meet our Innovator of 2025
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The looming crackdown on AI companionship



As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin. But another threat entirely‚Äîthat of kids forming unhealthy bonds with AI‚Äîis pulling AI safety out of the academic fringe and into regulators‚Äô crosshairs.



This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that their models contributed to the suicides of two teenagers. A study published in July, found that 72% of teenagers have used AI for companionship. And stories about ‚ÄúAI psychosis‚Äù have highlighted how endless conversations with chatbots can lead people down delusional spirals.



It‚Äôs hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but harmful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind.



‚ÄîJames O‚ÄôDonnell



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.



If you‚Äôre interested in reading more about AI companionship, why not check out:+ AI companions are the final stage of digital addiction‚Äîand lawmakers are taking aim. Read the full story.+ Chatbots are rapidly changing how we connect to each other‚Äîand ourselves. We‚Äôre never going back. Read the full story.



+ Why GPT-4o‚Äôs sudden shutdown last month left people grieving. Read the full story.+ An AI chatbot told a user how to kill himself‚Äîbut the company doesn‚Äôt want to ‚Äúcensor‚Äù it.+ OpenAI has released its first research into how using ChatGPT affects people‚Äôs emotional well-being. But there‚Äôs still a lot we don‚Äôt know.







Meet the designer of the world‚Äôs fastest whole-genome sequencing method



Every year, MIT Technology Review selects one individual whose work we admire to recognize as Innovator of the Year. For 2025, we chose Sneha Goenka, who designed the computations behind the world‚Äôs fastest whole-genome sequencing method. Thanks to her work, physicians can now sequence a patient‚Äôs genome and diagnose a genetic condition in less than eight hours‚Äîan achievement that could transform medical care.



Register here to join an exclusive subscriber-only Roundtable conversation with Goenka, Leilani Battle, assistant professor at the University of Washington, and our editor in chief Mat Honan at 1pm ET on Tuesday September 23.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Childhood vaccination rates are falling across the USMuch of the country no longer has the means to stop the spread of deadly disease. (NBC News)+ Take a look at the factors driving vaccine hesitancy. (WP $)+ RFK Jr is appointing more vaccine skeptics to the CDC advisory panel. (Ars Technica)+ Why US federal health agencies are abandoning mRNA vaccines. (MIT Technology Review)



2 The US and China have reached a TikTok deal¬†Beijing says the spin-off version sold to US investors will still use ByteDance‚Äôs algorithm. (FT $)+ But further details are still pretty scarce. (WP $)+ The deal may have been fueled by China‚Äôs desire for Trump to visit the country. (WSJ $)3 OpenAI is releasing a version of GPT-5 optimized for agentic codingIt‚Äôs a direct rival to Anthropic‚Äôs Claude Code and Microsoft‚Äôs GitHub Copilot. (TechCrunch)+ OpenAI says it‚Äôs been trained on real-world engineering tasks. (VentureBeat)+ The second wave of AI coding is here. (MIT Technology Review)



4 The FTC is investigating Ticketmaster‚Äôs bot-fighting measures¬†It‚Äôs probing whether the platform is doing enough to prevent illegal automated reselling. (Bloomberg $)



5 Google has created a new privacy-preserving LLMVaultGemma uses a technique called differential privacy to reduce the amount of data AI holds onto. (Ars Technica)



6 Space tech firms are fighting it out for NATO contractsMilitaries are willing to branch out and strike deals with commercial vendors. (FT $)+ Why Trump‚Äôs ‚Äúgolden dome‚Äù missile defense idea is another ripped straight from the movies. (MIT Technology Review)



7 Facebook users are receiving their Cambridge Analytica payoutsDon‚Äôt spend it all at once! (The Verge)



8 The future of supercomputing could hinge on moon mining missionsCompanies are rushing to buy the moon‚Äôs resources before mining has even begun. (WP $)



9 What it‚Äôs like living with an AI toyFeaturing unsettling conversations galore. (The Guardian)10 Anthropic‚Äôs staff are obsessed with an albino alligator As luck would have it, he just happens to be called Claude. (WSJ $)







Quote of the day



‚ÄúIt‚Äôs going to mean more infections, more hospitalizations, more disability and more death.‚Äù



‚ÄîDemetre Daskalakis, former director of the CDC&#8217;s National Center for Immunization and Respiratory Diseases, explains the probable outcomes of America‚Äôs current vaccine policy jumble, the BBC reports.







One more thing







Robots are bringing new life to extinct speciesIn the last few years, paleontologists have developed a new trick for turning back time and studying prehistoric animals: building experimental robotic models of them.In the absence of a living specimen, scientists say, an ambling, flying, swimming, or slithering automaton is the next best thing for studying the behavior of extinct organisms. Here are four examples of robots that are shedding light on creatures of yore.



‚ÄîShi En Kim







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ New York City is full of natural life, if you know where to look.+ This photo of Jim Morrison enjoying a beer for breakfast is the epitome of rock ‚Äòn‚Äô roll.+ How to age like a champion athlete.+ Would you dare drive the world‚Äôs most narrow car?
‚Ä¢ The looming crackdown on AI companionship
  As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin from data center sprawl. But this week showed that another threat entirely‚Äîthat of kids forming unhealthy bonds with AI‚Äîis the one pulling AI safety out of the academic fringe and into regulators‚Äô crosshairs.



This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that companion-like behavior in their models contributed to the suicides of two teenagers. A study by US nonprofit Common Sense Media, published in July, found that 72% of teenagers have used AI for companionship. Stories in reputable outlets about ‚ÄúAI psychosis‚Äù have highlighted how endless conversations with chatbots can lead people down delusional spirals.



It‚Äôs hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but a technology that‚Äôs more harmful than helpful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind.



A California law passes the legislature



On Thursday, the California state legislature passed a first-of-its-kind bill. It would require AI companies to include reminders for users they know to be minors that responses are AI generated. Companies would also need to have a protocol for addressing suicide and self-harm and provide annual reports on instances of suicidal ideation in users‚Äô conversations with their chatbots. It was led by Democratic state senator Steve Padilla, passed with heavy bipartisan support, and now awaits Governor Gavin Newsom‚Äôs signature.&nbsp;



There are reasons to be skeptical of the bill‚Äôs impact. It doesn‚Äôt specify efforts companies should take to identify which users are minors, and lots of AI companies already include referrals to crisis providers when someone is talking about suicide. (In the case of Adam Raine, one of the teenagers whose survivors are suing, his conversations with ChatGPT before his death included this type of information, but the chatbot allegedly went on to give advice related to suicide anyway.)



Still, it is undoubtedly the most significant of the efforts to rein in companion-like behaviors in AI models, which are in the works in other states too. If the bill becomes law, it would strike a blow to the position OpenAI has taken, which is that ‚ÄúAmerica leads best with clear, nationwide rules, not a patchwork of state or local regulations,‚Äù as the company‚Äôs chief global affairs officer, Chris Lehane, wrote on LinkedIn last week.



The Federal Trade Commission takes aim



The very same day, the Federal Trade Commission announced an inquiry into seven companies, seeking information about how they develop companion-like characters, monetize engagement, measure and test the impact of their chatbots, and more. The companies are Google, Instagram, Meta, OpenAI, Snap, X, and Character Technologies, the maker of Character.AI.



The White House now wields immense, and potentially illegal, political influence over the agency. In March, President Trump fired its lone Democratic commissioner, Rebecca Slaughter. In July, a federal judge ruled that firing illegal, but last week the US Supreme Court temporarily permitted the firing.



‚ÄúProtecting kids online is a top priority for the Trump-Vance FTC, and so is fostering innovation in critical sectors of our economy,‚Äù said FTC chairman Andrew Ferguson in a press release about the inquiry.&nbsp;



Right now, it‚Äôs just that‚Äîan inquiry‚Äîbut the process might (depending on how public the FTC makes its findings) reveal the inner workings of how the companies build their AI companions to keep users coming back again and again.&nbsp;



Sam Altman on suicide cases



Also on the same day (a busy day for AI news), Tucker Carlson published an hour-long interview with OpenAI‚Äôs CEO, Sam Altman. It covers a lot of ground‚ÄîAltman‚Äôs battle with Elon Musk, OpenAI‚Äôs military customers, conspiracy theories about the death of a former employee‚Äîbut it also includes the most candid comments Altman‚Äôs made so far about the cases of suicide following conversations with AI.&nbsp;



Altman talked about ‚Äúthe tension between user freedom and privacy and protecting vulnerable users‚Äù in cases like these. But then he offered up something I hadn‚Äôt heard before.



‚ÄúI think it‚Äôd be very reasonable for us to say that in cases of young people talking about suicide seriously, where we cannot get in touch with parents, we do call the authorities,‚Äù he said. ‚ÄúThat would be a change.‚Äù



So where does all this go next? For now, it‚Äôs clear that‚Äîat least in the case of children harmed by AI companionship‚Äîcompanies‚Äô familiar playbook won‚Äôt hold. They can no longer deflect responsibility by leaning on privacy, personalization, or ‚Äúuser choice.‚Äù Pressure to take a harder line is mounting from state laws, regulators, and an outraged public.



But what will that look like? Politically, the left and right are now paying attention to AI‚Äôs harm to children, but their solutions differ. On the right, the proposed solution aligns with the wave of internet age-verification laws that have now been passed in over 20 states. These are meant to shield kids from adult content while defending ‚Äúfamily values.‚Äù On the left, it‚Äôs the revival of stalled ambitions to hold Big Tech accountable through antitrust and consumer-protection powers.&nbsp;



Consensus on the problem is easier than agreement on the cure. As it stands, it looks likely we‚Äôll end up with exactly the patchwork of state and local regulations that OpenAI (and plenty of others) have lobbied against.&nbsp;



For now, it‚Äôs down to companies to decide where to draw the lines. They‚Äôre having to decide things like: Should chatbots cut off conversations when users spiral toward self-harm, or would that leave some people worse off? Should they be licensed and regulated like therapists, or treated as entertainment products with warnings? The uncertainty stems from a basic contradiction: Companies have built chatbots to act like caring humans, but they‚Äôve postponed developing the standards and accountability we demand of real caregivers. The clock is now running out.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.
‚Ä¢ The Download: computing‚Äôs bright young minds, and cleaning up satellite streaks
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Meet tomorrow‚Äôs rising stars of computing



Each year, MIT Technology Review honors 35 outstanding people under the age of 35 who are driving scientific progress and solving tough problems in their fields.Today we want to introduce you to the computing innovators on the list who are coming up with new AI chips and specialized datasets‚Äîalong with smart ideas about how to assess advanced systems for safety.Check out the full list of honorees‚Äîincluding our innovator of the year‚Äîhere.¬†







Job titles of the future: Satellite streak astronomer



Earlier this year, the $800 million Vera Rubin Observatory commenced its decade-long quest to create an extremely detailed time-lapse movie of the universe.Rubin is capable of capturing many more stars than any other astronomical observatory ever built; it also sees many more satellites. Up to 40% of images captured by the observatory within its first 10 years of operation will be marred by their sunlight-reflecting streaks.Meredith Rawls, a research scientist at the telescope‚Äôs flagship observation project, Vera Rubin‚Äôs Legacy Survey of Space and Time, is one of the experts tasked with protecting Rubin‚Äôs science mission from the satellite blight. Read the full story.



‚ÄîTereza Pultarova



This story is from our new print edition, which is all about the future of security. Subscribe here to catch future copies when they land.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 China has accused Nvidia of violating anti-monopoly lawsAs US and Chinese officials head into a second day of tariff negotiations. (Bloomberg $)+ The investigation dug into Nvidia‚Äôs 2020 acquisition of computing firm Mellanox. (CNBC)+ But China&#8217;s antitrust regulator hasn‚Äôt confirmed if it will punish it. (WSJ $)



2 The US is getting closer to making a TikTok dealBut it‚Äôs still prepared to go ahead with a ban if an agreement can‚Äôt be reached. (Reuters)



3 Grok spread misinformation about a far-right rally in LondonIt falsely claimed that police misrepresented old footage as being from the protest. (The Guardian)+ Elon Musk called for a new UK government during a video speech. (Politico)



4 Here‚Äôs what people are really using ChatGPT forUsers are more likely to use it for personal, rather than work-related queries. (WP $)+ Anthropic says businesses are using AI to automate, not collaborate. (Bloomberg $)+ Therapists are secretly using ChatGPT. Clients are triggered. (MIT Technology Review)



5 How China‚Äôs Hangzhou became a global AI hubSpawning not just Alibaba, but DeepSeek too. (WSJ $)+ China and the US are completely dominating the global AI race. (Rest of World)+ How DeepSeek ripped up the AI playbook. (MIT Technology Review)



6 Driverless car fleets could plunge US cities into traffic chaosAre we really prepared? (Vox $)



7 The shipping industry is harnessing AI to fight cargo firesThe risk of deadly fires is rising due to shipments of batteries and other flammable goods. (FT $)



8 Sales of used EVs are sky-rocketingBuyers are snapping up previously-owned bargains. (NYT $)+ EV owners won‚Äôt be able to drive in carpool lanes any more. (Wired $)



9 A table-top fusion reactor isn‚Äôt as crazy as it soundsThis startup is trying to make compact reactors a reality. (Economist $)+ Inside a fusion energy facility. (MIT Technology Review)



10 How a magnetic field could help clean up spaceIf we don‚Äôt, we could soon lose access to Earth‚Äôs low orbit altogether. (IEEE Spectrum)+ The world‚Äôs next big environmental problem could come from space. (MIT Technology Review)







Quote of the day



‚ÄúIf we‚Äôre going on a journey, they‚Äôre absolutely taking travel sickness tablets immediately. They‚Äôre not even considering coming in the car without them.‚Äù



‚ÄîPhil Bellamy, an electric car owner, describes the extreme nausea his daughters experience while riding in his vehicle to the Guardian.







One more thing







Google, Amazon and the problem with Big Tech‚Äôs climate claimsLast year, Amazon trumpeted that it had purchased enough clean electricity to cover the energy demands of all its global operations, seven years ahead of its sustainability target.That news closely followed Google‚Äôs acknowledgment that the soaring energy demands of its AI operations helped ratchet up its corporate emissions by 13% last year‚Äîand that it had backed away from claims that it was already carbon neutral.If you were to take the announcements at face value, you‚Äôd be forgiven for believing that Google is stumbling while Amazon is speeding ahead in the race to clean up climate pollution.But while both companies are coming up short in their own ways, Google‚Äôs approach to driving down greenhouse-gas emissions is now arguably more defensible. To learn why, read our story.‚ÄîJames Temple







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Steven Spielberg was just 26 when he made Jaws? The more you know.+ This tiny car‚Äôs huge racing track journey is completely hypnotic.+ Easy dinner recipes? Yes please.+ This archive of thousands of historical children‚Äôs books is a real treasure trove‚Äîand completely free to read.

üîí Cybersecurity & Privacy
‚Ä¢ Self-Replicating Worm Hits 180+ Software Packages
  At least 187 code packages made available through the JavaScript repository NPM have been infected with a self-replicating worm that steals credentials from developers and publishes those secrets on GitHub, experts warn.¬†The malware, which briefly infected multiple code packages from the security vendor CrowdStrike, steals and publishes even more credentials every time an infected package is installed.
Image: https://en.wikipedia.org/wiki/Sandworm_(Dune)
The novel malware strain is being dubbed Shai-Hulud &#8212; after the name for the giant sandworms in Frank Herbert&#8217;s Dune novel series &#8212; because it publishes any stolen credentials in a new public GitHub repository that includes the name &#8220;Shai-Hulud.&#8221;
&#8220;When a developer installs a compromised package, the malware will look for a npm token in the environment,&#8221; said Charlie Eriksen, a researcher for the Belgian security firm Aikido. &#8220;If it finds it, it will modify the 20 most popular packages that the npm token has access to, copying itself into the package, and publishing a new version.&#8221;
At the center of this developing maelstrom are code libraries available on NPM (short for ‚ÄúNode Package Manager‚Äù), which acts as a central hub for JavaScript development and provides the latest updates to widely-used JavaScript components.
The Shai-Hulud worm emerged just days after unknown attackers launched a broad phishing campaign that spoofed NPM and asked developers to &#8220;update&#8221; their multi-factor authentication login options. That attack led to malware being inserted into at least two-dozen NPM code packages, but the outbreak was quickly contained and was narrowly focused on siphoning cryptocurrency payments.
Image: aikido.dev
In late August, another compromise of an NPM developer resulted in malware being added to &#8220;nx,&#8221; an open-source code development toolkit with as many as six million weekly downloads. In the nx compromise, the attackers introduced code that scoured the user‚Äôs device for authentication tokens from programmer destinations like GitHub and NPM, as well as SSH and API keys. But instead of sending those stolen credentials to a central server controlled by the attackers, the malicious nx code created a new public repository in the victim‚Äôs GitHub account, and published the stolen data there for all the world to see and download.
Last month&#8217;s attack on nx did not self-propagate like a worm, but this Shai-Hulud malware does and bundles reconnaissance tools to assist in its spread. Namely, it uses the open-source tool TruffleHog to search for exposed credentials and access tokens on the developer&#8217;s machine. It then attempts to create new GitHub actions and publish any stolen secrets.
&#8220;Once the first person got compromised, there was no stopping it,&#8221; Aikido&#8217;s Eriksen told KrebsOnSecurity. He said the first NPM package compromised by this worm appears to have been altered on Sept. 14, around 17:58 UTC.
The security-focused code development platform socket.dev reports the Shai-Halud attack briefly compromised at least 25 NPM code packages managed by CrowdStrike. Socket.dev said the affected packages were quickly removed by the NPM registry.
In a written statement shared with KrebsOnSecurity, CrowdStrike said that after detecting several malicious packages in the public NPM registry, the company swiftly removed them and rotated its keys in public registries.
&#8220;These packages are not used in the Falcon sensor, the platform is not impacted and customers remain protected,&#8221; the statement reads, referring to the company&#8217;s widely-used endpoint threat detection service. &#8220;We are working with NPM and conducting a thorough investigation.&#8221;
A writeup on the attack from StepSecurity found that for cloud-specific operations, the malware enumerates AWS, Azure and Google Cloud Platform secrets. It also found the entire attack design assumes the victim is working in a Linux or macOS environment, and that it deliberately skips Windows systems.
StepSecurity said Shai-Hulud spreads by using stolen NPM authentication tokens, adding its code to the top 20 packages in the victim&#8217;s account.
&#8220;This creates a cascading effect where an infected package leads to compromised maintainer credentials, which in turn infects all other packages maintained by that user,&#8221; StepSecurity&#8217;s Ashish Kurmi wrote.
Eriksen said Shai-Hulud is still propagating, although its spread seems to have waned in recent hours.
&#8220;I still see package versions popping up once in a while, but no new packages have been compromised in the last ~6 hours,&#8221; Eriksen said. &#8220;But that could change now as the east coast starts working. I would think of this attack as a &#8216;living&#8217; thing almost, like a virus. Because it can lay dormant for a while, and if just one person is suddenly infected by accident, they could restart the spread. Especially if there&#8217;s a super-spreader attack.&#8221;
For now, it appears that the web address the attackers were using to exfiltrate collected data was disabled due to rate limits, Eriksen said.
Nicholas Weaver is a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif. Weaver called the Shai-Hulud worm &#8220;a supply chain attack that conducts a supply chain attack.&#8221; Weaver said NPM (and all other similar package repositories) need to immediately switch to a publication model that requires explicit human consent for every publication request using a phish-proof 2FA method.
&#8220;Anything less means attacks like this are going to continue and become far more common, but switching to a 2FA method would effectively throttle these attacks before they can spread,&#8221; Weaver said. &#8220;Allowing purely automated processes to update the published packages is now a proven recipe for disaster.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Streamline access to ISO-rating content changes with Verisk rating insights and Amazon Bedrock
  This post is co-written with Samit Verma, Eusha Rizvi, Manmeet Singh, Troy Smith, and Corey Finley from Verisk. 
Verisk Rating Insights as a feature of ISO Electronic Rating Content (ERC) is a powerful tool designed to provide summaries of ISO Rating changes between two releases. Traditionally, extracting specific filing information or identifying differences across multiple releases required manual downloads of full packages, which was time-consuming and prone to inefficiencies. This challenge, coupled with the need for accurate and timely customer support, prompted Verisk to explore innovative ways to enhance user accessibility and automate repetitive processes. Using generative AI and Amazon Web Services (AWS) services, Verisk has made significant strides in creating a conversational user interface for users to easily retrieve specific information, identify content differences, and improve overall operational efficiency. 
In this post, we dive into how Verisk Rating Insights, powered by Amazon Bedrock, large language models (LLM), and Retrieval Augmented Generation (RAG), is transforming the way customers interact with and access ISO ERC changes. 
The challenge 
Rating Insights provides valuable content, but there were significant challenges with user accessibility and the time it took to extract actionable insights: 
 
 Manual downloading ‚Äì Customers had to download entire packages to get even a small piece of relevant information. This was inefficient, especially when only a part of the filing needed to be reviewed. 
 Inefficient data retrieval ‚Äì Users couldn‚Äôt quickly identify the differences between two content packages without downloading and manually comparing them, which could take hours and sometimes days of analysis. 
 Time-consuming customer support ‚Äì Verisk‚Äôs ERC Customer Support team spent 15% of their time weekly addressing queries from customers who were impacted by these inefficiencies. Furthermore, onboarding new customers required half a day of repetitive training to ensure they understood how to access and interpret the data. 
 Manual analysis time ‚Äì Customers often spent 3‚Äì4 hours per test case analyzing the differences between filings. With multiple test cases to address, this led to significant delays in critical decision-making. 
 
Solution overview 
To solve these challenges, Verisk embarked on a journey to enhance Rating Insights with generative AI technologies. By integrating Anthropic‚Äôs Claude, available in Amazon Bedrock, and Amazon OpenSearch Service, Verisk created a sophisticated conversational platform where users can effortlessly access and analyze rating content changes. 
The following diagram illustrates the high-level architecture of the solution, with distinct sections showing the data ingestion process and inference loop. The architecture uses multiple AWS services to add generative AI capabilities to the Ratings Insight system. This system‚Äôs components work together seamlessly, coordinating multiple LLM calls to generate user responses. 
 
The following diagram shows the architectural components and the high-level steps involved in the Data Ingestion process. 
 
  
   
    
   
  
 
The steps in the data ingestion process proceed as follows: 
 
 This process is triggered when a new file is dropped. It is responsible for chunking the document using a custom chunking strategy. This strategy recursively checks each section and keeps them intact without overlap. The process then embeds the chunks and stores them in OpenSearch Service as vector embeddings. 
 The embedding model used in Amazon Bedrock is amazon titan-embed-g1-text-02. 
 Amazon OpenSearch Serverless is utilized as a vector embedding store with metadata filtering capability. 
 
The following diagram shows the architectural components and the high-level steps involved in the inference loop to generate user responses. 
 
The steps in the inference loop proceed as follows: 
 
 This component is responsible for multiple tasks: it supplements user questions with recent chat history, embeds the questions, retrieves relevant chunks from the vector database, and finally calls the generation model to synthesize a response. 
 Amazon ElastiCache is used for storing recent chat history. 
 The embedding model utilized in Amazon Bedrock is amazon titan-embed-g1-text-02. 
 OpenSearch Serverless is implemented for RAG (Retrieval-Augmented Generation). 
 For generating responses to user queries, the system uses Anthropic‚Äôs Claude Sonnet 3.5 (model ID: anthropic.claude-3-5-sonnet-20240620-v1:0), which is available through Amazon Bedrock. 
 
Key technologies and frameworks used 
We used Anthropic‚Äôs Claude Sonnet 3.5 (model ID: anthropic.claude-3-5-sonnet-20240620-v1:0) to understand user input and provide detailed, contextually relevant responses. Anthropic‚Äôs Claude Sonnet 3.5 enhances the platform‚Äôs ability to interpret user queries and deliver accurate insights from complex content changes. LlamaIndex, which is an open source framework, served as the chain framework for efficiently connecting and managing different data sources to enable dynamic retrieval of content and insights. 
We implemented RAG, which allows the model to pull specific, relevant data from the OpenSearch Serverless vector database. This means the system generates precise, up-to-date responses based on a user‚Äôs query without needing to sift through massive content downloads. The vector database enables intelligent search and retrieval, organizing content changes in a way that makes them quickly and easily accessible. This eliminates the need for manual searching or downloading of entire content packages. Verisk applied guardrails in Amazon Bedrock Guardrails along with custom guardrails around the generative model so the output adheres to specific compliance and quality standards, safeguarding the integrity of responses. 
Verisk‚Äôs generative AI solution is a comprehensive, secure, and flexible service for building generative AI applications and agents. Amazon Bedrock connects you to leading FMs, services to deploy and operate agents, and tools for fine-tuning, safeguarding, and optimizing models along with knowledge bases to connect applications to your latest data so that you have everything you need to quickly move from experimentation to real-world deployment. 
Given the novelty of generative AI, Verisk has established a governance council to oversee its solutions, ensuring they meet security, compliance, and data usage standards. Verisk implemented strict controls within the RAG pipeline to ensure data is only accessible to authorized users. This helps maintain the integrity and privacy of sensitive information. Legal reviews ensure IP protection and contract compliance. 
How it works 
The integration of these advanced technologies enables a seamless, user-friendly experience. Here‚Äôs how Verisk Rating Insights now works for customers: 
 
 Conversational user interface ‚Äì Users can interact with the platform by using a conversational interface. Instead of manually reviewing content packages, users enter a natural language query (for example, ‚ÄúWhat are the changes in coverage scope between the two recent filings?‚Äù). The system uses Anthropic‚Äôs Claude Sonnet 3.5 to understand the intent and provides an instant summary of the relevant changes. 
 Dynamic content retrieval ‚Äì Thanks to RAG and OpenSearch Service, the platform doesn‚Äôt require downloading entire files. Instead, it dynamically retrieves and presents the specific changes a user is seeking, enabling quicker analysis and decision-making. 
 Automated difference analysis ‚Äì The system can automatically compare two content packages, highlighting the differences without requiring manual intervention. Users can query for precise comparisons (for example, ‚ÄúShow me the differences in rating criteria between Release 1 and Release 2‚Äù). 
 Customized insights ‚Äì The guardrails in place mean that responses are accurate, compliant, and actionable. Additionally, if needed, the system can help users understand the impact of changes and assist them in navigating the complexities of filings, providing clear, concise insights. 
 
The following diagram shows the architectural components and the high-level steps involved in the evaluation loop to generate relevant and grounded responses. 
 
The steps in the evaluation loop proceed as follows: 
 
 This component is responsible for calling Anthropic‚Äôs Claude Sonnet 3.5 model and subsequently invoking the custom-built evaluation APIs to ensure response accuracy. 
 The generation model employed is Anthropic‚Äôs Claude Sonnet 3.5, which handles the creation of responses. 
 The Evaluation API ensures that responses remain relevant to user queries and stay grounded within the provided context. 
 
The following diagram shows the process of capturing the chat history as contextual memory and storage for analysis. 
 
Quality benchmarks 
The Verisk Rating Insights team has implemented a comprehensive evaluation framework and feedback loop mechanism respectively, shown in the above figures, to support continuous improvement and address the issues that might arise. 
Ensuring high accuracy and consistency in responses is essential for Verisk‚Äôs generative AI solutions. However, LLMs can sometimes produce hallucinations or provide irrelevant details, affecting reliability. To address this, Verisk implemented: 
 
 Evaluation framework ‚Äì Integrated into the query pipeline, it validates responses for precision and relevance before delivery. 
 Extensive testing ‚Äì Product subject matter experts (SMEs) and quality experts rigorously tested the solution to ensure accuracy and reliability. Verisk collaborated with in-house insurance domain experts to develop SME evaluation metrics for accuracy and consistency. Multiple rounds of SME evaluations were conducted, where experts graded these metrics on a 1‚Äì10 scale. Latency was also tracked to assess speed. Feedback from each round was incorporated into subsequent tests to drive improvements. 
 Continual model improvement ‚Äì Using customer feedback serves as a crucial component in driving the continuous evolution and refinement of the generative models, improving both accuracy and relevance. By seamlessly integrating user interactions and feedback with chat history, a robust data pipeline is created that streams the user interactions to an Amazon Simple Storage Service (Amazon S3) bucket, which acts as a data hub. The interactions then go into Snowflake, which is a cloud-based data platform and data warehouse as a service that offers capabilities such as data warehousing, data lakes, data sharing, and data exchange. Through this integration, we built comprehensive analytics dashboards that provide valuable insights into user experience patterns and pain points. 
 
Although the initial results were promising, they didn‚Äôt meet the desired accuracy and consistency levels. The development process involved several iterative improvements, such as redesigning the system and making multiple calls to the LLM. The primary metric for success was a manual grading system where business experts compared the results and provided continuous feedback to improve overall benchmarks. 
Business impact and opportunity 
By integrating generative AI into Verisk Rating Insights, the business has seen a remarkable transformation. Customers enjoyed significant time savings. By eliminating the need to download entire packages and manually search for differences, the time spent on analysis has been drastically reduced. Customers no longer spend 3‚Äì4 hours per test case. What at one time took days now takes minutes. 
This time savings brought increased productivity. With an automated solution that instantly provides relevant insights, customers can focus more on decision-making rather than spending time on manual data retrieval. And by automating difference analysis and providing a centralized, effortless platform, customers can be more confident in the accuracy of their results and avoid missing critical changes. 
For Verisk, the benefit was a reduced customer support burden because the ERC customer support team now spends less time addressing queries. With the AI-powered conversational interface, users can self-serve and get answers in real time, freeing up support resources for more complex inquiries. 
The automation of repetitive training tasks meant quicker and more efficient customer onboarding. This reduces the need for lengthy training sessions, and new customers become proficient faster. The integration of generative AI has reduced redundant workflows and the need for manual intervention. This streamlines operations across multiple departments, leading to a more agile and responsive business. 
Conclusion 
Looking ahead, Verisk plans to continue enhancing the Rating Insights platform twofold. First, we‚Äôll expand the scope of queries, enabling more sophisticated queries related to different filing types and more nuanced coverage areas. Second, we‚Äôll scale the platform. With Amazon Bedrock providing the infrastructure, Verisk aims to scale this solution further to support more users and additional content sets across various product lines. 
Verisk Rating Insights, now powered by generative AI and AWS technologies, has transformed the way customers interact with and access rating content changes. Through a conversational user interface, RAG, and vector databases, Verisk intends to eliminate inefficiencies and save customers valuable time and resources while enhancing overall accessibility. For Verisk, this solution has improved operational efficiency and provided a strong foundation for continued innovation. 
With Amazon Bedrock and a focus on automation, Verisk is driving the future of intelligent customer support and content management, empowering both their customers and their internal teams to make smarter, faster decisions. 
For more information, refer to the following resources: 
 
 Explore generative AI on AWS 
 Learn about unlocking the business value of generative AI 
 Learn more about Anthropic‚Äôs Claude 3 models on Amazon Bedrock 
 Learn about Amazon Bedrock and how to build and scale generative AI applications with FMs 
 Explore generative AI quick start proofs of concept 
 
 
 
About the authors 
Samit Verma serves as the Director of Software Engineering at Verisk, overseeing the Rating and Coverage development teams. In this role, he plays a key part in architectural design and provides strategic direction to multiple development teams, enhancing efficiency and ensuring long-term solution maintainability. He holds a master‚Äôs degree in information technology. 
Eusha Rizvi serves as a Software Development Manager at Verisk, leading several technology teams within the Ratings Products division. Possessing strong expertise in system design, architecture, and engineering, Eusha offers essential guidance that advances the development of innovative solutions. He holds a bachelor‚Äôs degree in information systems from Stony Brook University. 
Manmeet Singh is a Software Engineering Lead at Verisk and AWS Certified Generative AI Specialist. He leads the development of an agentic RAG-based generative AI system on Amazon Bedrock, with expertise in LLM orchestration, prompt engineering, vector databases, microservices, and high-availability architecture. Manmeet is passionate about applying advanced AI and cloud technologies to deliver resilient, scalable, and business-critical systems. 
Troy Smith is a Vice President of Rating Solutions at Verisk. Troy is a seasoned insurance technology leader with more than 25 years of experience in rating, pricing, and product strategy. At Verisk, he leads the team behind ISO Electronic Rating Content, a widely used resource across the insurance industry. Troy has held leadership roles at Earnix and Capgemini and was the cofounder and original creator of the Oracle Insbridge Rating Engine. 
Corey Finley is a Product Manager at Verisk. Corey has over 22 years of experience across personal and commercial lines of insurance. He has worked in both implementation and product support roles and has led efforts for major carriers including Allianz, CNA, Citizens, and others. At Verisk, he serves as Product Manager for VRI, RaaS, and ERC. 
Arun Pradeep Selvaraj is a Senior Solutions Architect at Amazon Web Services (AWS). Arun is passionate about working with his customers and stakeholders on digital transformations and innovation in the cloud while continuing to learn, build, and reinvent. He is creative, energetic, deeply customer-obsessed, and uses the working backward process to build modern architectures to help customers solve their unique challenges. Connect with him on LinkedIn. 
Ryan Doty is a Solutions Architect Manager at Amazon Web Services (AWS), based out of New York. He helps financial services customers accelerate their adoption of the AWS Cloud by providing architectural guidelines to design innovative and scalable solutions. Coming from a software development and sales engineering background, the possibilities that the cloud can bring to the world excite him.
‚Ä¢ Unified multimodal access layer for Quora‚Äôs Poe using Amazon Bedrock
  Organizations gain competitive advantage by deploying and integrating new generative AI models quickly through Generative AI Gateway architectures. This unified interface approach simplifies access to multiple foundation models (FMs), addressing a critical challenge: the proliferation of specialized AI models, each with unique capabilities, API specifications, and operational requirements. Rather than building and maintaining separate integration points for each model, the smart move is to build an abstraction layer that normalizes these differences behind a single, consistent API. 
The AWS Generative AI Innovation Center and Quora recently collaborated on an innovative solution to address this challenge. Together, they developed a unified wrapper API framework that streamlines the deployment of Amazon Bedrock FMs on Quora‚Äôs Poe system. This architecture delivers a ‚Äúbuild once, deploy multiple models‚Äù capability that significantly reduces deployment time and engineering effort, with real protocol bridging code visible throughout the codebase. 
For technology leaders and developers working on AI multi-model deployment at scale, this framework demonstrates how thoughtful abstraction and protocol translation can accelerate innovation cycles while maintaining operational control. 
In this post, we explore how the AWS Generative AI Innovation Center and Quora collaborated to build a unified wrapper API framework that dramatically accelerates the deployment of Amazon Bedrock FMs on Quora‚Äôs Poe system. We detail the technical architecture that bridges Poe‚Äôs event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, demonstrate how a template-based configuration system reduced deployment time from days to 15 minutes, and share implementation patterns for protocol translation, error handling, and multi-modal capabilities. We show how this ‚Äúbuild once, deploy multiple models‚Äù approach helped Poe integrate over 30 Amazon Bedrock models across text, image, and video modalities while reducing code changes by up to 95%. 
Quora and Amazon Bedrock 
Poe.com is an AI system developed by Quora that users and developers can use to interact with a wide range of advanced AI models and assistants powered by multiple providers. The system offers multi-model access, enabling side-by-side conversations with various AI chatbots for tasks such as natural language understanding, content generation, image creation, and more. 
This screenshot below showcases the user interface of Poe, the AI platform created by Quora. The image displays Poe‚Äôs extensive library of AI models, which are presented as individual ‚Äúchatbots‚Äù that users can interact with. 
 
The following screenshot provides a view of the Model Catalog within Amazon Bedrock, a fully managed service from Amazon Web Services (AWS) that offers access to a diverse range of foundation models (FMs). This catalog acts as a central hub for developers to discover, evaluate, and access state-of-the-art AI from various providers. 
 
Initially, integrating the diverse FMs available through Amazon Bedrock presented significant technical challenges for the Poe.com team. The process required substantial engineering resources to establish connections with each model while maintaining consistent performance and reliability standards. Maintainability emerged as an extremely important consideration, as was the ability to efficiently onboard new models as they became available‚Äîboth factors adding further complexity to the integration challenges. 
Technical challenge: Bridging different systems 
The integration between Poe and Amazon Bedrock presented fundamental architectural challenges that required innovative solutions. These systems were built with different design philosophies and communication patterns, creating a significant technical divide that the wrapper API needed to bridge. 
Architectural divide 
The core challenge stems from the fundamentally different architectural approaches of the two systems. Understanding these differences is essential to appreciating the complexity of the integration solution. Poe operates on a modern, reactive, ServerSentEvents-based architecture through the Fast API library (fastapi_poe). This architecture is stream-optimized for real-time interactions and uses an event-driven response model designed for continuous, conversational AI. Amazon Bedrock, on the other hand, functions as an enterprise cloud service. It offers REST-based with AWS SDK access patterns, SigV4 authentication requirements, AWS Region-specific model availability, and a traditional request-response pattern with streaming options. This fundamental API mismatch creates several technical challenges that the Poe wrapper API solves, as detailed in the following table. 
 
  
   
   Challenge Category 
   Technical Issue 
   Source Protocol 
   Target Protocol 
   Integration Complexity 
   
  
  
   
   Protocol Translation 
   Converting between WebSocket-based protocol and REST APIs 
   WebSocket (bidirectional, persistent) 
   REST (request/response, stateless) 
   High: Requires protocol bridging 
   
   
   Authentication Bridging 
   Connecting JWT validation with AWS SigV4 signing 
   JWT token validation 
   AWS SigV4 authentication 
   Medium: Credential transformation needed 
   
   
   Response Format Transformation 
   Adapting JSON responses into expected format 
   Standard JSON structure 
   Custom format requirements 
   Medium: Data structure mapping 
   
   
   Streaming Reconciliation 
   Mapping chunked responses to ServerSentEvents 
   Chunked HTTP responses 
   ServerSentEvents stream 
   High: Real-time data flow conversion 
   
   
   Parameter Standardization 
   Creating unified parameter space across models 
   Model-specific parameters 
   Standardized parameter interface 
   Medium: Parameter normalization 
   
  
 
API evolution and the Converse API 
In May 2024, Amazon Bedrock introduced the Converse API, which offered standardization benefits that significantly simplified the integration architecture: 
 
 Unified interface across diverse model providers (such as Anthropic, Meta, and Mistral) 
 Conversation memory with consistent handling of chat history 
 Streaming and non-streaming modes through a single API pattern 
 Multimodal support for text, images, and structured data 
 Parameter normalization that reduces model-specific implementation quirks 
 Built-in content moderation capabilities 
 
The solution presented in this post uses the Converse API where appropriate, while also maintaining compatibility with model-specific APIs for specialized capabilities. This hybrid approach provides flexibility while taking advantage of the Converse API‚Äôs standardization benefits. 
Solution overview 
The wrapper API framework provides a unified interface between Poe and Amazon Bedrock models. It serves as a translation layer that normalizes the differences between models and protocols while maintaining the unique capabilities of each model. 
The solution architecture follows a modular design that separates concerns and enables flexible scaling, as illustrated in the following diagram. 
 
The wrapper API consists of several key components working together to provide a seamless integration experience: 
 
 Client ‚Äì The entry point where users interact with AI capabilities through various interfaces. 
 Poe layer ‚Äì Consists of the following: 
   
   Poe UI ‚Äì Handles user experience, request formation, parameters controls, file uploads, and response visualization. 
   Poe FastAPI ‚Äì Standardizes user interactions and manages the communication protocol between clients and underlying systems. 
    
 Bot Factory ‚Äì Dynamically creates appropriate model handlers (bots) based on the requested model type (chat, image, or video). This factory pattern provides extensibility for new model types and variations. See the following code: 
 
 
 # From core/bot_factory.py - Actual implementation
class BotFactory:
    """
    Factory for creating different types of bots.
    Handles bot creation based on the bot type and configuration.
    """
    @staticmethod
    def create_bot(bot_config: BotConfig) -&gt; PoeBot:
        # Check if a custom bot class is specified
        if hasattr(bot_config, 'bot_class') and bot_config.bot_class:
            # Use the custom bot class directly
            bot = bot_config.bot_class(bot_config)
            
            # Explicitly ensure we're returning a PoeBot
            if not isinstance(bot, PoeBot):
                raise TypeError(f"Custom bot class must return a PoeBot instance, got {type(bot)}")
            return bot

        # Determine bot type based on configuration
        if hasattr(bot_config, 'enable_video_generation') and bot_config.enable_video_generation:
            # Video generation bot
            if 'luma' in bot_config.bot_name:
                from core.refactored_luma_bot import LumaVideoBot
                return LumaVideoBot(bot_config)
            else:
                from core.refactored_nova_reel_bot import NovaReelVideoBot
                return NovaReelVideoBot(bot_config)
                
        elif hasattr(bot_config, 'enable_image_generation') and bot_config.enable_image_generation:
            # Image generation bot
            if hasattr(bot_config, 'model_id') and "stability" in bot_config.model_id.lower():
                # Stability AI image generation bot
                from core.refactored_image_stability_ai import AmazonBedrockImageStabilityAIBot
                return AmazonBedrockImageStabilityAIBot(bot_config)
            else:
                # Other image generation bot (Titan, Canvas, etc.)
                from core.refactored_image_bot_amazon import RefactoredAmazonImageGenerationBot
                return RefactoredAmazonImageGenerationBot(bot_config)
                
        else:
            # Check if this is a Claude 3.7 model
            if hasattr(bot_config, 'model_id') and "claude-3-7" in bot_config.model_id.lower():
                return ClaudePlusBot(bot_config)
            else:
                # Default to standard chat bot
                return RefactoredAmazonBedrockPoeBot(bot_config) 
 
 
 Service manager: Orchestrates the services needed to process requests effectively. It coordinates between different specialized services, including: 
   
   Token services ‚Äì Managing token limits and counting. 
   Streaming services ‚Äì Handling real-time responses. 
   Error services ‚Äì Normalizing and handling errors. 
   AWS service integration ‚Äì Managing API calls to Amazon Bedrock. 
    
 AWS services component ‚Äì Converts responses from Amazon Bedrock format to Poe‚Äôs expected format and vice-versa, handling streaming chunks, image data, and video outputs. 
 Amazon Bedrock layer ‚Äì Amazon‚Äôs FM service that provides the actual AI processing capabilities and model hosting, including: 
   
   Model diversity ‚Äì Provides access to over 30 text models (such as Amazon Titan, Amazon Nova, Anthropic‚Äôs Claude, Meta‚Äôs Llama, Mistral, and more), image models, and video models. 
   API structure ‚Äì Exposes both model-specific APIs and the unified Converse API. 
   Authentication ‚Äì Requires AWS SigV4 signing for secure access to model endpoints. 
   Response management ‚Äì Returns model outputs with standardized metadata and usage statistics. 
    
 
The request processing flow in this unified wrapper API shows the orchestration required when bridging Poe‚Äôs event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, showcasing how multiple specialized services work together to deliver a seamless user experience. 
The flow begins when a client sends a request through Poe‚Äôs interface, which then forwards it to the Bot Factory component. This factory pattern dynamically creates the appropriate model handler based on the requested model type, whether for chat, image, or video generation. The service manager component then orchestrates the various specialized services needed to process the request effectively, including token services, streaming services, and error handling services. 
The following sequence diagram illustrates the complete request processing flow. 
 
Configuration template for rapid multi-bot deployment 
The most powerful aspect of the wrapper API is its unified configuration template system, which supports rapid deployment and management of multiple bots with minimal code changes. This approach is central to the solution‚Äôs success in reducing deployment time. 
The system uses a template-based configuration approach with shared defaults and model-specific overrides: 
 
 # Bot configurations using the template pattern

CHAT_BOTS = {
    'poe-nova-micro': BotConfig(
        # Identity
        bot_name='poe-nova-micro',
        model_id='amazon.nova-micro-v1:0',
        aws_region=aws_config['region'],
        poe_access_key='XXXXXXXXXXXXXXXXXXXXXX',

        # Model-specific parameters
        supports_system_messages=True,
        enable_image_comprehension=True,
        expand_text_attachments=True,
        streaming=True,
        max_tokens=1300,
        temperature=0.7,
        top_p=0.9,

        # Model-specific pricing
        enable_monetization=True,
        pricing_type="variable",
        input_token_cost_milli_cents=2,
        output_token_cost_milli_cents=4,
        image_analysis_cost_milli_cents=25,

        # Generate rate card with model-specific values
        custom_rate_card=create_rate_card(2, 4, 25),

        # Include common parameters
        **DEFAULT_CHAT_CONFIG
    ),

    'poe-mistral-pixtral': BotConfig(
        # Identity
        bot_name='poe-mistral-pixtral',
        model_id='us.mistral.pixtral-large-2502-v1:0',
        aws_region=aws_config['region'],
        poe_access_key='XXXXXXXXXXXXXXXXXXXXXX',

        # Model-specific parameters
        supports_system_messages=False,
        enable_image_comprehension=False,
        # ...
        # Include common parameters
        **DEFAULT_CHAT_CONFIG
    )
}
 
 
This configuration-driven architecture offers several significant advantages: 
 
 Rapid deployment ‚Äì Adding new models requires only creating a new configuration entry rather than writing integration code. This is a key factor in the significant improvement in deployment time. 
 Consistent parameter management ‚Äì Common parameters are defined one time in DEFAULT_CHAT_CONFIG and inherited by bots, maintaining consistency and reducing duplication. 
 Model-specific customization ‚Äì Each model can have its own unique settings while still benefiting from the shared infrastructure. 
 Operational flexibility ‚Äì Parameters can be adjusted without code changes, allowing for quick experimentation and optimization. 
 Centralized credential management ‚Äì AWS credentials are managed in one place, improving security and simplifying updates. 
 Region-specific deployment ‚Äì Models can be deployed to different Regions as needed, with Region settings controlled at the configuration level. 
 
The BotConfig class provides a structured way to define bot configurations with type validation: 
 
 # From config/bot_config.py - Actual implementation (partial)
class BotConfig(BaseModel):
    # Core Bot Identity
    bot_name: str = Field(..., description="Name of the bot")
    model_id: str = Field(..., description="Identifier for the AI model")

    # AWS Configuration
    aws_region: Optional[str] = Field(default="us-east-1", description="AWS region for deployment")
    aws_access_key: Optional[str] = Field(default=None, description="AWS access key")
    aws_secret_key: Optional[str] = Field(default=None, description="AWS secret key")
    aws_security_token: Optional[str] = None

    # Poe Configuration
    poe_access_key: str = Field(..., description="Poe access key")
    modal_app_name: str = Field(..., description="Modal app name")

    # Capability Flags
    allow_attachments: bool = Field(default=True, description="Whether to allow file attachments in Poe")
    supports_system_messages: bool = Field(default=False)
    enable_image_comprehension: bool = Field(default=False)
    expand_text_attachments: bool = Field(default=False)
    streaming: bool = Field(default=False)
    enable_image_generation: bool = Field(default=False)
    enable_video_generation: bool = Field(default=False)

    # Inference Configuration
    max_tokens: Optional[int] = Field(default=None, description="Maximum number of tokens to generate")
    temperature: Optional[float] = Field(default=None, description="Temperature for sampling")
    top_p: Optional[float] = Field(default=None, description="Top-p sampling parameter")
    optimize_latency: bool = Field(default=False, description="Enable latency optimization with performanceConfig")

    # Reasoning Configuration (Claude 3.7+)
    enable_reasoning: bool = Field(default=False, description="Enable Claude's reasoning capability")
    reasoning_budget: Optional[int] = Field(default=1024, description="Token budget for reasoning (1024-4000 recommended)")

    # Monetization Configuration
    enable_monetization: bool = Field(default=False, description="Enable variable pricing monetization")
    custom_rate_card: Optional[str] = Field(
        default=None,
        description="Custom rate card for variable pricing in markdown format"
    )
    input_token_cost_milli_cents: Optional[int] = Field(
        default=None,
        description="Cost per input token in thousandths of a cent"
    )
    output_token_cost_milli_cents: Optional[int] = Field(
        default=None,
        description="Cost per output token in thousandths of a cent"
    )
    image_analysis_cost_milli_cents: Optional[int] = Field(
        default=None,
        description="Cost per image analysis in thousandths of a cent"
    )
 
 
Advanced multimodal capabilities 
One of the most powerful aspects of the framework is how it handles multimodal capabilities through simple configuration flags: 
 
 enable_image_comprehension ‚Äì When set to True for text-only models like Amazon Nova Micro, Poe itself uses vision capabilities to analyze images and convert them into text descriptions that are sent to the Amazon Bedrock model. This enables even text-only models to classify images without having built-in vision capabilities. 
 expand_text_attachments ‚Äì When set to True, Poe parses uploaded text files and includes their content in the conversation, enabling models to work with document content without requiring special file handling capabilities. 
 supports_system_messages ‚Äì This parameter controls whether the model can accept system prompts, allowing for consistent behavior across models with different capabilities. 
 
These configuration flags create a powerful abstraction layer that offers the following benefits: 
 
 Extends model capabilities ‚Äì Text-only models gain pseudo-multimodal capabilities through Poe‚Äôs preprocessing 
 Optimizes built-in features ‚Äì True multimodal models can use their built-in capabilities for optimal results 
 Simplifies integration ‚Äì It‚Äôs controlled through simple configuration switches rather than code changes 
 Maintains consistency ‚Äì It provides a uniform user experience regardless of the underlying model‚Äôs native capabilities 
 
Next, we explore the technical implementation of the solution in more detail. 
Protocol translation layer 
The most technically challenging aspect of the solution was bridging between Poe‚Äôs API protocols and the diverse model interfaces available through Amazon Bedrock. The team accomplished this through a sophisticated protocol translation layer: 
 
 # From services/streaming_service.py - Actual implementation
def _extract_content_from_event(self, event: Dict[str, Any]) -&gt; Optional[str]:
    """Extract content from a streaming event based on model provider."""
    try:
        # Handle Anthropic Claude models
        if "message" in event:
            message = event.get("message", {})
            if "content" in message and isinstance(message["content"], list):
                for content_item in message["content"]:
                    if content_item.get("type") == "text":
                        return content_item.get("text", "")
            elif "content" in message:
                return str(message.get("content", ""))

        # Handle Amazon Titan models
        if "delta" in event:
            delta = event.get("delta", {})
            if "text" in delta:
                return delta.get("text", "")

        # Handle other model formats
        if "chunk" in event:
            chunk_data = event.get("chunk", {})
            if "bytes" in chunk_data:
                # Process binary data if present
                try:
                    text = chunk_data["bytes"].decode("utf-8")
                    return json.loads(text).get("completion", "")
                except Exception:
                    self.logger.warning("Failed to decode bytes in chunk")

        # No matching format found
        return None
 
 
This translation layer handles subtle differences between models and makes sure that regardless of which Amazon Bedrock model is being used, the response to Poe is consistent and follows Poe‚Äôs expected format. 
Error handling and normalization 
A critical aspect of the implementation is comprehensive error handling and normalization. The ErrorService provides consistent error handling across different models: 
 
 # Simplified example of error handling (not actual code)
class ErrorService:
    def normalize_Amazon_Bedrock_error(self, error: Exception) -&gt; str:
        """Normalize Amazon Bedrock errors into a consistent format."""
        if isinstance(error, ClientError):
            if "ThrottlingException" in str(error):
                return "The model is currently experiencing high demand. Please try again in a moment."
            elif "ValidationException" in str(error):
                return "There was an issue with the request parameters. Please try again with different settings."
            elif "AccessDeniedException" in str(error):
                return "Access to this model is restricted. Please check your permissions."
            else:
                return f"An error occurred while communicating with the model: {str(error)}"
        elif isinstance(error, ConnectionError):
            return "Connection error. Please check your network and try again."
        else:
            return f"An unexpected error occurred: {str(error)}"
 
 
This approach makes sure users receive meaningful error messages regardless of the underlying model or error condition. 
Token counting and optimization 
The system implements sophisticated token counting and optimization to maximize effective use of models: 
 
 # From services/streaming_service.py - Actual implementation (partial)
# Calculate approximate JSON overhead
user_message_tokens = 0
for msg in conversation['messages']:
    for content_block in msg.get('content', []):
        if 'text' in content_block:
            # Simple word-based estimation of actual text content
            user_message_tokens += len(content_block['text'].split())

# Estimate JSON structure overhead (difference between total and content)
json_overhead = int((input_tokens - system_tokens) - user_message_tokens)

# Ensure we're working with integers for calculations
input_tokens_for_pct = int(input_tokens)
system_tokens_for_pct = int(system_tokens)
json_overhead_for_pct = int(json_overhead)

# Calculate percentage with float arithmetic and proper integer division
json_overhead_percent = (float(json_overhead_for_pct) / max(1, input_tokens_for_pct - system_tokens_for_pct)) * 100
... 
 
This detailed token tracking enables accurate cost estimation and optimization, facilitating efficient use of model resources. 
AWS authentication and security 
The AwsClientService handles authentication and security for Amazon Bedrock API calls.This implementation provides secure authentication with AWS services while providing proper error handling and connection management. 
Comparative analysis 
The implementation of the wrapper API dramatically improved the efficiency and capabilities of deploying Amazon Bedrock models on Poe, as detailed in the following table. 
 
  
   
   Feature 
   Before (Direct API) 
   After (Wrapper API) 
   
  
  
   
   Deployment Time 
   Days per model 
   Minutes per model 
   
   
   Developer Focus 
   Configuration and plumbing 
   Innovation and features 
   
   
   Model Diversity 
   Limited by integration capacity 
   Extensive (across Amazon Bedrock models) 
   
   
   Maintenance Overhead 
   High (separate code for each model) 
   Low (configuration-based) 
   
   
   Error Handling 
   Custom per model 
   Standardized across models 
   
   
   Cost Tracking 
   Complex (multiple integrations) 
   Simplified (centralized) 
   
   
   Multimodal Support 
   Fragmented 
   Unified 
   
   
   Security 
   Varied implementations 
   Consistent best practices 
   
  
 
This comparison highlights the significant improvements achieved through the wrapper API approach, demonstrating the value of investing in a robust abstraction layer. 
Performance metrics and business impact 
The wrapper API framework delivered significant and measurable business impact across multiple dimensions, including increased model diversity, deployment efficiency, and developer productivity. 
Poe can rapidly expand its model offerings, integrating tens of Amazon Bedrock models across text, image, and video modalities. This expansion occurred over a period of weeks rather than the months it would have taken with the previous approach. 
The following table summarizes the deployment efficiency metrics. 
 
  
   
   Metric 
   Before 
   After 
   Improvement 
   
  
  
   
   New Model Deployment 
   2 ‚Äì3 days 
   15 minutes 
   96x faster 
   
   
   Code Changes Required 
   500+ lines 
   20‚Äì30 lines 
   95% reduction 
   
   
   Testing Time 
   8‚Äì12 hours 
   30‚Äì60 minutes 
   87% reduction 
   
   
   Deployment Steps 
   10‚Äì15 steps 
   3‚Äì5 steps 
   75% reduction 
   
  
 
These metrics were measured through direct comparison of engineering hours required before and after implementation, tracking actual deployments of new models. 
The engineering team saw a dramatic shift in focus from integration work to feature development, as detailed in the following table. 
 
  
   
   Activity 
   Before (% of time) 
   After (% of time) 
   Change 
   
  
  
   
   API Integration 
   65% 
   15% 
   -50% 
   
   
   Feature Development 
   20% 
   60% 
   +40% 
   
   
   Testing 
   10% 
   15% 
   +5% 
   
   
   Documentation 
   5% 
   10% 
   +5% 
   
  
 
Scaling and performance considerations 
The wrapper API is designed to handle high-volume production workloads with robust scaling capabilities. 
Connection pooling 
To handle multiple concurrent requests efficiently, the wrapper implements connection pooling using aiobotocore. This allows it to maintain a pool of connections to Amazon Bedrock, reducing the overhead of establishing new connections for each request: 
 
 # From services/aws_service.py - Connection management
async def setup_client(self) -&gt; None:
    """Initialize AWS client with proper configuration."""
    async with self._client_lock:
        try:
            # Always clean up existing clients first to avoid stale connections
            if self.Amazon_Bedrock_client:
                await self.cleanup()

            # Increase timeout for image generation
            config = Config(
                read_timeout=300,  # 5 minutes timeout
                retries={'max_attempts': 3, 'mode': 'adaptive'},
                connect_timeout=30  # 30 second connection timeout
            )

            # Create the Amazon Bedrock client with proper error handling
            self.Amazon_Bedrock_client = await self.session.create_client(
                service_name="Amazon_Bedrock-runtime",
                region_name=self.bot_config.aws_region,
                aws_access_key_id=self.bot_config.aws_access_key,
                aws_secret_access_key=self.bot_config.aws_secret_key,
                aws_session_token=self.bot_config.aws_security_token,
                config=config
            ).__aenter__()
        except Exception as e:
            self.Amazon_Bedrock_client = None
            raise
 
 
Asynchronous processing 
The entire framework uses asynchronous processing to handle concurrent requests efficiently: 
 
 # From core/refactored_chat_bot.py - Asynchronous request handling
async def get_response(self, query: QueryRequest) -&gt; AsyncIterable[PartialResponse]:
    try:
        # Ensure AWS client is set up
        await aws_service.setup_client()

        # Validate and format the conversation
        conversation = await conversation_service.validate_conversation(query)

        # Process the request with streaming
        if self.bot_config.streaming:
            async for chunk in streaming_service.stream_Amazon_Bedrock_response(conversation, request_id):
                yield chunk
        else:
            # Non-streaming mode
            response_text, input_tokens, output_tokens = await streaming_service.non_stream_Amazon_Bedrock_response(conversation, request_id)
            if response_text:
                yield PartialResponse(text=response_text)
            else:
                yield PartialResponse(text=self.bot_config.fallback_response)
            # Send done event for non-streaming mode
            yield self.done_event()

    except Exception as e:
        # Error handling
        error_message = error_service.log_error(e, request_id, "Error during request processing")
        yield PartialResponse(text=error_message)
        yield self.done_event()
 
 
Error recovery and retry logic 
The system implements sophisticated error recovery and retry logic to handle transient issues: 
 
 # From services/streaming_service.py - Retry logic
max_retries = 3
base_delay = 1  # Start with 1 second delay

for attempt in range(max_retries):
    try:
        if not self.aws_service.Amazon_Bedrock_client:
            yield PartialResponse(text="Error: Amazon Bedrock client is not initialized")
            break

        response = await self.aws_service.Amazon_Bedrock_client.converse_stream(**stream_config)
        # Process response...
        break  # Success, exit retry loop

    except ClientError as e:
        if "ThrottlingException" in str(e):
            if attempt &lt; max_retries - 1:
                delay = base_delay * (2 ** attempt)  # Exponential backoff
                await asyncio.sleep(delay)
                continue
        error_message = f"Amazon Bedrock API Error: {str(e)}"
        yield PartialResponse(text=f"Error: {error_message}")
        break
 
 
Performance metrics 
The system collects detailed performance metrics to help identify bottlenecks and optimize performance: 
 
 # From services/streaming_service.py - Performance metrics
# Log token usage and latency
latency = time.perf_counter() - start_time

self.logger.info(
f"[{request_id}] Streaming Response Metrics:\n"
f" Time to First Token: {first_token_time:.4f} seconds\n"
f" Input Tokens: {input_tokens} (includes system prompt)\n"
f" Input Tokens for Billing: {input_tokens - system_tokens} (excludes system prompt)\n"
f" Output Tokens: {output_tokens}\n"
f" Total Tokens: {total_tokens}\n"
f" Amazon Bedrock Latency: {latency:.4f} seconds\n"
f" Latency Optimization: {'enabled' if hasattr(self.bot_config, 'optimize_latency') and self.bot_config.optimize_latency else 'disabled'}"
) 
 
Security considerations 
Security is a critical aspect of the wrapper implementation, with several key features to support secure operation. 
JWT validation with AWS SigV4 signing 
The system integrates JWT validation for Poe‚Äôs authentication with AWS SigV4 signing for Amazon Bedrock API calls: 
 
 JWT validation ‚Äì Makes sure only authorized Poe requests can access the wrapper API 
 SigV4 signing ‚Äì Makes sure the wrapper API can securely authenticate with Amazon Bedrock 
 Credential management ‚Äì AWS credentials are securely managed and not exposed to clients 
 
Secrets management 
The system integrates with AWS Secrets Manager to securely store and retrieve sensitive credentials: 
 
 # From services/aws_service.py - Secrets management
@staticmethod
def get_secret(secret_name: str, region_name: str = "us-east-1") -&gt; Dict[str, Any]:
    """
    Retrieve a secret from AWS Secrets Manager.

    Args:
        secret_name: Name of the secret to retrieve
        region_name: AWS region where the secret is stored

    Returns:
        Dict[str, Any]: The secret value as a dictionary
    """
    # Create a Secrets Manager client
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )

    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except Exception as e:
        logging.error(f"Error retrieving secret {secret_name}: {str(e)}")
        raise

    # Depending on whether the secret is a string or binary, one of these fields will be populated.
    if 'SecretString' in get_secret_value_response:
        import json
        try:
            # Explicitly annotate the return type for mypy
            result: Dict[str, Any] = json.loads(get_secret_value_response['SecretString'])
            return result
        except json.JSONDecodeError:
            # If not a JSON, return as a single-key dictionary
            return {"SecretString": get_secret_value_response['SecretString']}
    else:
        import base64
        decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])
        return {"SecretBinary": decoded_binary_secret}
 
 
Secure connection management 
The system implements secure connection management to help prevent credential leakage and facilitate proper cleanup: 
 
 # From services/aws_service.py - Secure connection cleanup
async def cleanup(self) -&gt; None:
    """Clean up AWS client resources."""
    try:
        if self.Amazon_Bedrock_client:
            try:
                await self.Amazon_Bedrock_client.__aexit__(None, None, None)
            except Exception as e:
                self.logger.error(f"Error closing Amazon Bedrock client: {str(e)}")
            finally:
                self.Amazon_Bedrock_client = None

        self.logger.info("Successfully cleaned up AWS client resources")
    except Exception as e:
        # Even if cleanup fails, reset the references to avoid stale connections
        self.Amazon_Bedrock_client = None
 
 
Troubleshooting and debugging 
The wrapper API includes comprehensive logging and debugging capabilities to help identify and resolve issues. The system implements detailed logging throughout the request processing flow. Each request is assigned a unique ID that is used throughout the processing flow to enable tracing: 
 
 # From core/refactored_chat_bot.py - Request tracing
request_id = str(id(query))
start_time = time.perf_counter()

# Used in all log messages
self.logger.info(f"[{request_id}] Incoming request received") 
 
Lessons learned and best practices 
Through this collaboration, several important technical insights emerged that might benefit others undertaking similar projects: 
 
 Configuration-driven architecture ‚Äì Using configuration files rather than code for model-specific behaviors proved enormously beneficial for maintenance and extensibility. This approach allowed new models to be added without code changes, significantly reducing the risk of introducing bugs. 
 Protocol translation challenges ‚Äì The most complex aspect was handling the subtle differences in streaming protocols between different models. Building a robust abstraction required careful consideration of edge cases and comprehensive error handling. 
 Error normalization ‚Äì Creating a consistent error experience across diverse models required sophisticated error handling that could translate model-specific errors into user-friendly, actionable messages. This improved both developer and end-user experiences. 
 Type safety ‚Äì Strong typing (using Python‚Äôs type hints extensively) was crucial for maintaining code quality across a complex codebase with multiple contributors. This practice reduced bugs and improved code maintainability. 
 Security first ‚Äì Integrating Secrets Manager from the start made sure credentials were handled securely throughout the system‚Äôs lifecycle, helping prevent potential security vulnerabilities. 
 
Conclusion 
The collaboration between the AWS Generative AI Innovation Center and Quora demonstrates how thoughtful architectural design can dramatically accelerate AI deployment and innovation. By creating a unified wrapper API for Amazon Bedrock models, the teams were able to reduce deployment time from days to minutes while expanding model diversity and improving user experience. 
This approach‚Äîfocusing on abstraction, configuration-driven development, and robust error handling‚Äîoffers valuable lessons for organizations looking to integrate multiple AI models efficiently. The patterns and techniques demonstrated in this solution can be applied to similar challenges across a wide range of AI integration scenarios. 
For technology leaders and developers working on similar challenges, this case study highlights the value of investing in flexible integration frameworks rather than point-to-point integrations. The initial investment in building a robust abstraction layer pays dividends in long-term maintenance and capability expansion. 
To learn more about implementing similar solutions, explore the following resources: 
 
 The AWS Well-Architected Framework for best practices in building secure, high-performing, resilient, and efficient infrastructure 
 The Amazon Bedrock Developer Guide for detailed information on working with FMs 
 The AWS Generative AI Innovation Center for assistance with your generative AI projects 
 AWS Prescriptive Guidance for LLM Deployment for best practices in deploying large language models 
 
The AWS Generative AI Innovation Center and Quora teams continue to collaborate on enhancements to this framework, making sure Poe users have access to the latest and most capable AI models with minimal deployment delay. 
 
About the authors 
Dr. Gilbert V Lepadatu is a Senior Deep Learning Architect at the AWS Generative AI Innovation Center, where he helps enterprise customers design and deploy scalable, cutting-edge GenAI solutions. With a PhD in Philosophy and dual Master‚Äôs degrees, he brings a holistic and interdisciplinary approach to data science and AI. 
Nick Huber is the AI Ecosystem Lead for Poe (by Quora), where he is responsible for ensuring high-quality &amp; timely integrations of the leading AI models onto the Poe platform.
‚Ä¢ Schedule topology-aware workloads using Amazon SageMaker HyperPod task governance
  Today, we are excited to announce a new capability of Amazon SageMaker HyperPod task governance to help you optimize training efficiency and network latency of your AI workloads. SageMaker HyperPod task governance streamlines resource allocation and facilitates efficient compute resource utilization across teams and projects on Amazon Elastic Kubernetes Service (Amazon EKS) clusters. Administrators can govern accelerated compute allocation and enforce task priority policies, improving resource utilization. This helps organizations focus on accelerating generative AI innovation and reducing time to market, rather than coordinating resource allocation and replanning tasks. Refer to Best practices for Amazon SageMaker HyperPod task governance for more information. 
Generative AI workloads typically demand extensive network communication across Amazon Elastic Compute Cloud (Amazon EC2) instances, where network bandwidth impacts both workload runtime and processing latency. The network latency of these communications depends on the physical placement of instances within a data center‚Äôs hierarchical infrastructure. Data centers can be organized into nested organizational units such as network nodes and node sets, with multiple instances per network node and multiple network nodes per node set. For example, instances within the same organizational unit experience faster processing time compared to those across different units. This means fewer network hops between instances results in lower communication. 
To optimize the placement of your generative AI workloads in your SageMaker HyperPod clusters by considering the physical and logical arrangement of resources, you can use EC2 network topology information during your job submissions. An EC2 instance‚Äôs topology is described by a set of nodes, with one node in each layer of the network. Refer to How Amazon EC2 instance topology works for details on how EC2 topology is arranged. Network topology labels offer the following key benefits: 
 
 Reduced latency by minimizing network hops and routing traffic to nearby instances 
 Improved training efficiency by optimizing workload placement across network resources 
 
With topology-aware scheduling for SageMaker HyperPod task governance, you can use topology network labels to schedule your jobs with optimized network communication, thereby improving task efficiency and resource utilization for your AI workloads. 
In this post, we introduce topology-aware scheduling with SageMaker HyperPod task governance by submitting jobs that represent hierarchical network information. We provide details about how to use SageMaker HyperPod task governance to optimize your job efficiency. 
Solution overview 
Data scientists interact with SageMaker HyperPod clusters. Data scientists are responsible for the training, fine-tuning, and deployment of models on accelerated compute instances. It‚Äôs important to make sure data scientists have the necessary capacity and permissions when interacting with clusters of GPUs. 
To implement topology-aware scheduling, you first confirm the topology information for all nodes in your cluster, then run a script that tells you which instances are on the same network nodes, and finally schedule a topology-aware training task on your cluster. This workflow facilitates higher visibility and control over the placement of your training instances. 
In this post, we walk through viewing node topology information and submitting topology-aware tasks to your cluster. For reference, NetworkNodes describes the network node set of an instance. In each network node set, three layers comprise the hierarchical view of the topology for each instance. Instances that are closest to each other will share the same layer 3 network node. If there are no common network nodes in the bottom layer (layer 3), then see if there is commonality at layer 2. 
Prerequisites 
To get started with topology-aware scheduling, you must have the following prerequisites: 
 
 An EKS cluster 
 A SageMaker HyperPod cluster with instances enabled for topology information 
 The SageMaker HyperPod task governance add-on installed (version 1.2.2 or later) 
 Kubectl installed 
 (Optional) The SageMaker HyperPod CLI installed 
 
Get node topology information 
Run the following command to show node labels in your cluster. This command provides network topology information for each instance. 
 
 kubectl get nodes -L topology.k8s.aws/network-node-layer-1
kubectl get nodes -L topology.k8s.aws/network-node-layer-2
kubectl get nodes -L topology.k8s.aws/network-node-layer-3 
 
Instances with the same network node layer 3 are as close as possible, following EC2 topology hierarchy. You should see a list of node labels that look like the following:topology.k8s.aws/network-node-layer-3: nn-33333exampleRun the following script to show the nodes in your cluster that are on the same layers 1, 2, and 3 network nodes: 
 
 git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/1.architectures/7.sagemaker-hyperpod-eks/task-governance 
chmod +x visualize_topology.sh
bash visualize_topology.sh 
 
The output of this script will print a flow chart that you can use in a flow diagram editor such as Mermaid.js.org to visualize the node topology of your cluster. The following figure is an example of the cluster topology for a seven-instance cluster. 
 
Submit tasks 
SageMaker HyperPod task governance offers two ways to submit tasks using topology awareness. In this section, we discuss these two options and a third alternative option to task governance. 
Modify your Kubernetes manifest file 
First, you can modify your existing Kubernetes manifest file to include one of two annotation options: 
 
 kueue.x-k8s.io/podset-required-topology ‚Äì Use this option if you must have all pods scheduled on nodes on the same network node layer in order to begin the job 
 kueue.x-k8s.io/podset-preferred-topology ‚Äì Use this option if you ideally want all pods scheduled on nodes in the same network node layer, but you have flexibility 
 
The following code is an example of a sample job that uses the kueue.x-k8s.io/podset-required-topology setting to schedule pods that share the same layer 3 network node: 
 
 apiVersion: batch/v1
kind: Job
metadata:
&nbsp;&nbsp;name: test-tas-job
&nbsp;&nbsp;namespace: hyperpod-ns-team-a
&nbsp;&nbsp;labels:
&nbsp;&nbsp; &nbsp;kueue.x-k8s.io/queue-name: hyperpod-ns-team-a-localqueue
&nbsp;&nbsp; &nbsp;kueue.x-k8s.io/priority-class: inference-priority
spec:
&nbsp;&nbsp;parallelism: 10
&nbsp;&nbsp;completions: 10
&nbsp;&nbsp;suspend: true
&nbsp;&nbsp;template:
&nbsp;&nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp;labels:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;kueue.x-k8s.io/queue-name: hyperpod-ns-team-a-localqueue
&nbsp;&nbsp; &nbsp; &nbsp;annotations:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;kueue.x-k8s.io/podset-required-topology: "topology.k8s.aws/network-node-layer-3"
&nbsp;&nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp;containers:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: dummy-job
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image: public.ecr.aws/docker/library/alpine:latest
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;command: ["sleep", "3600s"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;resources:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;cpu: "1"
&nbsp;&nbsp; &nbsp; &nbsp;restartPolicy: Never 
 
To verify which nodes your pods are running on, use the following command to view node IDs per pod:kubectl get pods -n hyperpod-ns-team-a -o wide 
Use the SageMaker HyperPod CLI 
The second way to submit a job is through the SageMaker HyperPod CLI. Be sure to install the latest version (version pending) to use topology-aware scheduling. To use topology-aware scheduling with the SageMaker HyperPod CLI, you can include either the --preferred-topology parameter or the --required-topology parameter in your create job command. 
The following code is an example command to start a topology-aware mnist training job using the SageMaker HyperPod CLI, replace XXXXXXXXXXXX with your AWS account ID: 
 
 hyp create hyp-pytorch-job \
--job-name test-pytorch-job-cli \
--image XXXXXXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/ptjob:mnist \
--pull-policy "Always" \
--tasks-per-node 1 \
--max-retry 1 \
--preferred-topology topology.k8s.aws/network-node-layer-3 
 
Clean up 
If you deployed new resources while following this post, refer to the Clean Up section in the SageMaker HyperPod EKS workshop to make sure you don‚Äôt accrue unwanted charges. 
Conclusion 
During large language model (LLM) training, pod-to-pod communication distributes the model across multiple instances, requiring frequent data exchange between these instances. In this post, we discussed how SageMaker HyperPod task governance helps schedule workloads to enable job efficiency by optimizing throughput and latency. We also walked through how to schedule jobs using SageMaker HyperPod topology network information to optimize network communication latency for your AI tasks. 
We encourage you to try out this solution and share your feedback in the comments section. 
 
About the authors 
Nisha Nadkarni is a Senior GenAI Specialist Solutions Architect at AWS, where she guides companies through best practices when deploying large scale distributed training and inference on AWS. Prior to her current role, she spent several years at AWS focused on helping emerging GenAI startups develop models from ideation to production. 
Siamak Nariman&nbsp;is a Senior Product Manager at AWS. He is focused on AI/ML technology, ML model management, and ML governance to improve overall organizational efficiency and productivity. He has extensive experience automating processes and deploying various technologies. 
Zican Li is a Senior Software Engineer at Amazon Web Services (AWS), where he leads software development for Task Governance on SageMaker HyperPod. In his role, he focuses on empowering customers with advanced AI capabilities while fostering an environment that maximizes engineering team efficiency and productivity.  
Anoop Saha is a Sr GTM Specialist at Amazon Web Services (AWS) focusing on generative AI model training and inference. He partners with top frontier model builders, strategic customers, and AWS service teams to enable distributed training and inference at scale on AWS and lead joint GTM motions. Before AWS, Anoop held several leadership roles at startups and large corporations, primarily focusing on silicon and system architecture of AI infrastructure.
‚Ä¢ How msg enhanced HR workforce transformation with Amazon Bedrock and msg.ProfileMap
  This post is co-written with Stefan Walter from msg. 
With more than 10,000 experts in 34 countries, msg is both an independent software vendor and a system integrator operating in highly regulated industries, with over 40 years of domain-specific expertise. msg.ProfileMap is a software as a service (SaaS) solution for skill and competency management. It‚Äôs an AWS Partner qualified software available on AWS Marketplace, currently serving more than 7,500 users. HR and strategy departments use msg.ProfileMap for project staffing and workforce transformation initiatives. By offering a centralized view of skills and competencies, msg.ProfileMap helps organizations map their workforce‚Äôs capabilities, identify skill gaps, and implement targeted development strategies. This supports more effective project execution, better alignment of talent to roles, and long-term workforce planning. 
In this post, we share how msg automated data harmonization for msg.ProfileMap, using Amazon Bedrock to power its large language model (LLM)-driven data enrichment workflows, resulting in higher accuracy in HR concept matching, reduced manual workload, and improved alignment with compliance requirements under the EU AI Act and GDPR. 
The importance of AI-based data harmonization 
HR departments face increasing pressure to operate as data-driven organizations, but are often constrained by the inconsistent, fragmented nature of their data. Critical HR documents are unstructured, and legacy systems use mismatched formats and data models. This not only impairs data quality but also leads to inefficiencies and decision-making blind spots.Accurate and harmonized HR data is foundational for key activities such as matching candidates to roles, identifying internal mobility opportunities, conducting skills gap analysis, and planning workforce development. msg identified that without automated, scalable methods to process and unify this data, organizations would continue to struggle with manual overhead and inconsistent results. 
Solution overview 
HR data is typically scattered across diverse sources and formats, ranging from relational databases to Excel files, Word documents, and PDFs. Additionally, entities such as personnel numbers or competencies have different unique identifiers as well as different text descriptions, although with the same semantics. msg addressed this challenge with a modular architecture, tailored for IT workforce scenarios. As illustrated in the following diagram, at the core of msg.ProfileMap is a robust text extraction layer, which transforms heterogeneous inputs into structured data. This is then passed to an AI-powered harmonization engine that provides consistency across data sources by avoiding duplication and aligning disparate concepts. 
 
The harmonization process uses a hybrid retrieval approach that combines vector-based semantic similarity and string-based matching techniques. These methods align incoming data with existing entities in the system. Amazon Bedrock is used to semantically enrich data, improving cross-source compatibility and matching precision. Extracted and enriched data is indexed and stored using Amazon OpenSearch Service and Amazon DynamoDB, facilitating fast and accurate retrieval, as shown in the following diagram. 
 
The framework is designed to be unsupervised and domain independent. Although it‚Äôs optimized for IT workforce use cases, it has demonstrated strong generalization capabilities in other domains as well. 
msg.ProfileMap is a cloud-based application that uses several AWS services, notably Amazon Neptune, Amazon DynamoDB, and Amazon Bedrock. The following diagram illustrates the full solution architecture. 
 
Results and technical validation 
msg evaluated the effectiveness of the data harmonization framework through internal testing on IT workforce concepts and external benchmarking in the Bio-ML Track of the Ontology Alignment Evaluation Initiative (OAEI), an international and EU-funded research initiative that evaluates ontology matching technologies since 2004. 
During internal testing, the system processed 2,248 concepts across multiple suggestion types. High-probability merge recommendations reached 95.5% accuracy, covering nearly 60% of all inputs. This helped msg reduce manual validation workload by over 70%, significantly improving time-to-value for HR teams. 
During OAEI 2024, msg.ProfileMap ranked at the top of the 2024 Bio-ML benchmark, outperforming other systems across multiple biomedical datasets. On NCIT-DOID, it achieved a 0.918 F1 score, with Hits@1 exceeding 92%, validating the engine‚Äôs generalizability beyond the HR domain. Additional details are available in the official test results. 
Why Amazon Bedrock 
msg relies on LLMs to semantically enrich data in near real time. These workloads require low-latency inference, flexible scaling, and operational simplicity. Amazon Bedrock met these needs by providing a fully managed, serverless interface to leading foundation models‚Äîwithout the need to manage infrastructure or deploy custom machine learning stacks. 
Unlike hosting models on Amazon Elastic Compute Cloud (Amazon EC2) or Amazon SageMaker, Amazon Bedrock abstracts away provisioning, versioning, scaling, and model selection. Its consumption-based pricing aligns directly with msg‚Äôs SaaS delivery model‚Äîresources are used (and billed) only when needed. This simplified integration reduced overhead and helped msg scale elastically as customer demand grew. 
Amazon Bedrock also helped msg meet compliance goals under the EU AI Act and GDPR by enabling tightly scoped, auditable interactions with model APIs‚Äîcritical for HR use cases that handle sensitive workforce data. 
Conclusion 
msg‚Äôs successful integration of Amazon Bedrock into msg.ProfileMap demonstrates that large-scale AI adoption doesn‚Äôt require complex infrastructure or specialized model training. By combining modular design, ontology-based harmonization, and the fully managed LLM capabilities of Amazon Bedrock, msg delivered an AI-powered workforce intelligence platform that is accurate, scalable, and compliant.This solution improved concept match precision and achieved top marks in international AI benchmarks, demonstrating what‚Äôs possible when generative AI is paired with the right cloud-based service. With Amazon Bedrock, msg has built a platform that‚Äôs ready for today‚Äôs HR challenges‚Äîand tomorrow‚Äôs. 
msg.ProfileMap is available as a SaaS offering on AWS Marketplace. If you are interested in knowing more, you can reach out to msg.hcm.backoffice@msg.group. 
The content and opinions in this blog post are those of the third-party author and AWS is not responsible for the content or accuracy of this post. 
 
About the authors 
Stefan Walter&nbsp;is Senior Vice President of AI SaaS Solutions at msg. With over 25 years of experience in IT software development, architecture, and consulting, Stefan Walter leads with a vision for scalable SaaS innovation and operational excellence. As a BU lead at msg, Stefan has spearheaded transformative initiatives that bridge business strategy with technology execution, especially in complex, multi-entity environments. 
Gianluca Vegetti is a Senior Enterprise Architect in the AWS Partner Organization, aligned to Strategic Partnership Collaboration and Governance (SPCG) engagements. In his role, he supports the definition and execution of Strategic Collaboration Agreements with selected AWS partners. 
Yuriy Bezsonov is a Senior Partner Solution Architect at AWS. With over 25 years in the tech, Yuriy has progressed from a software developer to an engineering manager and Solutions Architect. Now, as a Senior Solutions Architect at AWS, he assists partners and customers in developing cloud solutions, focusing on container technologies, Kubernetes, Java, application modernization, SaaS, developer experience, and GenAI. Yuriy holds AWS and Kubernetes certifications, and he is a recipient of the AWS Golden Jacket and the CNCF Kubestronaut Blue Jacket.
‚Ä¢ Automate advanced agentic RAG pipeline with Amazon SageMaker AI
  Retrieval Augmented Generation (RAG) is a fundamental approach for building advanced generative AI applications that connect large language models (LLMs) to enterprise knowledge. However, crafting a reliable RAG pipeline is rarely a one-shot process. Teams often need to test dozens of configurations (varying chunking strategies, embedding models, retrieval techniques, and prompt designs) before arriving at a solution that works for their use case. Furthermore, management of high-performing RAG pipeline involves complex deployment, with teams often using manual RAG pipeline management, leading to inconsistent results, time-consuming troubleshooting, and difficulty in reproducing successful configurations. Teams struggle with scattered documentation of parameter choices, limited visibility into component performance, and the inability to systematically compare different approaches. Additionally, the lack of automation creates bottlenecks in scaling the RAG solutions, increases operational overhead, and makes it challenging to maintain quality across multiple deployments and environments from development to production. 
In this post, we walk through how to streamline your RAG development lifecycle from experimentation to automation, helping you operationalize your RAG solution for production deployments with Amazon SageMaker AI, helping your team experiment efficiently, collaborate effectively, and drive continuous improvement. By combining experimentation and automation with SageMaker AI, you can verify that the entire pipeline is versioned, tested, and promoted as a cohesive unit. This approach provides comprehensive guidance for traceability, reproducibility, and risk mitigation as the RAG system advances from development to production, supporting continuous improvement and reliable operation in real-world scenarios. 
Solution overview 
By streamlining both experimentation and operational workflows, teams can use SageMaker AI to rapidly prototype, deploy, and monitor RAG applications at scale. Its integration with SageMaker managed MLflow provides a unified platform for tracking experiments, logging configurations, and comparing results, supporting reproducibility and robust governance throughout the pipeline lifecycle. Automation also minimizes manual intervention, reduces errors, and streamlines the process of promoting the finalized RAG pipeline from the experimentation phase directly into production. With this approach, every stage from data ingestion to output generation operates efficiently and securely, while making it straightforward to transition validated solutions from development to production deployment. 
For automation, Amazon SageMaker Pipelines orchestrates end-to-end RAG workflows from data preparation and vector embedding generation to model inference and evaluation all with repeatable and version-controlled code. Integrating continuous integration and delivery (CI/CD) practices further enhances reproducibility and governance, enabling automated promotion of validated RAG pipelines from development to staging or production environments. Promoting an entire RAG pipeline (not just an individual subsystem of the RAG solution like a chunking layer or orchestration layer) to higher environments is essential because data, configurations, and infrastructure can vary significantly across staging and production. In production, you often work with live, sensitive, or much larger datasets, and the way data is chunked, embedded, retrieved, and generated can impact system performance and output quality in ways that are not always apparent in lower environments. Each stage of the pipeline (chunking, embedding, retrieval, and generation) must be thoroughly evaluated with production-like data for accuracy, relevance, and robustness. Metrics at every stage (such as chunk quality, retrieval relevance, answer correctness, and LLM evaluation scores) must be monitored and validated before the pipeline is trusted to serve real users. 
The following diagram illustrates the architecture of a scalable RAG pipeline built on SageMaker AI, with MLflow experiment tracking seamlessly integrated at every stage and the RAG pipeline automated using SageMaker Pipelines. SageMaker managed MLflow provides a unified platform for centralized RAG experiment tracking across all pipeline stages. Every MLflow execution run whether for RAG chunking, ingestion, retrieval, or evaluation sends execution logs, parameters, metrics, and artifacts to SageMaker managed MLflow. The architecture uses SageMaker Pipelines to orchestrate the entire RAG workflow through versioned, repeatable automation. These RAG pipelines manage dependencies between critical stages, from data ingestion and chunking to embedding generation, retrieval, and final text generation, supporting consistent execution across environments. Integrated with CI/CD practices, SageMaker Pipelines enable seamless promotion of validated RAG configurations from development to staging and production environments while maintaining infrastructure as code (IaC) traceability. 
 
For the operational workflow, the solution follows a structured lifecycle: During experimentation, data scientists iterate on pipeline components within Amazon SageMaker Studio notebooks while SageMaker managed MLflow captures parameters, metrics, and artifacts at every stage. Validated workflows are then codified into SageMaker Pipelines and versioned in Git. The automated promotion phase uses CI/CD to trigger pipeline execution in target environments, rigorously validating stage-specific metrics (chunk quality, retrieval relevance, answer correctness) against production data before deployment. The other core components include: 
 
 Amazon SageMaker JumpStart for accessing the latest LLM models and hosting them on SageMaker endpoints for model inference with the embedding model huggingface-textembedding-all-MiniLM-L6-v2 and text generation model deepseek-llm-r1-distill-qwen-7b. 
 Amazon OpenSearch Service as a vector database to store document embeddings with the OpenSearch index configured for k-nearest neighbors (k-NN) search. 
 The Amazon Bedrock model anthropic.claude-3-haiku-20240307-v1:0 as an LLM-as-a-judge component for all the MLflow LLM evaluation metrics. 
 A SageMaker Studio notebook for a development environment to experiment and automate the RAG pipelines with SageMaker managed MLflow and SageMaker Pipelines. 
 
You can implement this agentic RAG solution code from the GitHub repository. In the following sections, we use snippets from this code in the repository to illustrate RAG pipeline experiment evolution and automation. 
Prerequisites 
You must have the following prerequisites: 
 
 An AWS account with billing enabled. 
 A SageMaker AI domain. For more information, see Use quick setup for Amazon SageMaker AI. 
 Access to a running SageMaker managed MLflow tracking server in SageMaker Studio. For more information, see the instructions for setting up a new MLflow tracking server. 
 Access to SageMaker JumpStart to host LLM embedding and text generation models. 
 Access to the Amazon Bedrock foundation models (FMs) for RAG evaluation tasks. For more details, see Subscribe to a model. 
 
SageMaker MLFlow RAG experiment 
SageMaker managed MLflow provides a powerful framework for organizing RAG experiments, so teams can manage complex, multi-stage processes with clarity and precision. The following diagram illustrates the RAG experiment stages with SageMaker managed MLflow experiment tracking at every stage. This centralized tracking offers the following benefits: 
 
 Reproducibility: Every experiment is fully documented, so teams can replay and compare runs at any time 
 Collaboration: Shared experiment tracking fosters knowledge sharing and accelerates troubleshooting 
 Actionable insights: Visual dashboards and comparative analytics help teams identify the impact of pipeline changes and drive continuous improvement 
 
The following diagram illustrates the solution workflow. 
 
Each RAG experiment in MLflow is structured as a top-level run under a specific experiment name. Within this top-level run, nested runs are created for each major pipeline stage, such as data preparation, data chunking, data ingestion, RAG retrieval, and RAG evaluation. This hierarchical approach allows for granular tracking of parameters, metrics, and artifacts at every step, while maintaining a clear lineage from raw data to final evaluation results. 
The following screenshot shows an example of the experiment details in MLflow. 
 
The various RAG pipeline steps defined are: 
 
 Data preparation: Logs dataset version, preprocessing steps, and initial statistics 
 Data chunking: Records chunking strategy, chunk size, overlap, and resulting chunk counts 
 Data ingestion: Tracks embedding model, vector database details, and document ingestion metrics 
 RAG retrieval: Captures retrieval model, context size, and retrieval performance metrics 
 RAG evaluation: Logs evaluation metrics (such as answer similarity, correctness, and relevance) and sample results 
 
This visualization provides a clear, end-to-end view of the RAG pipeline‚Äôs execution, so you can trace the impact of changes at any stage and achieve full reproducibility. The architecture supports scaling to multiple experiments, each representing a distinct configuration or hypothesis (for example, different chunking strategies, embedding models, or retrieval parameters). MLflow‚Äôs experiment UI visualizes these experiments side by side, enabling side-by-side comparison and analysis across runs. This structure is especially valuable in enterprise settings, where dozens or even hundreds of experiments might be conducted to optimize RAG performance. 
We use MLflow experimentation throughout the RAG pipeline to log metrics and parameters, and the different experiment runs are initialized as shown in the following code snippet: 
 
 with mlflow.start_run() as run:
&nbsp;&nbsp; &nbsp;main_run_id = run.info.run_id
&nbsp;&nbsp; &nbsp;print("mlflow_run", run_id)
&nbsp;&nbsp; &nbsp;with mlflow.start_run(run_name="DataPreparation", nested=True): 
 
RAG pipeline experimentation 
The key components of the RAG workflow are ingestion, chunking, retrieval, and evaluation, which we explain in this section. The MLflow dashboard makes it straightforward to visualize and analyze these parameters and metrics, supporting data-driven refinement of the chunking stage within the RAG pipeline. 
 
Data ingestion and preparation 
In the RAG workflow, rigorous data preparation is foundational to downstream performance and reliability. Tracking detailed metrics on data quality, such as the total number of question-answer pairs, the count of unique questions, average context length, and initial evaluation predictions, provides essential visibility into the dataset‚Äôs structure and suitability for RAG tasks. These metrics help validate the dataset is comprehensive, diverse, and contextually rich, which directly impacts the relevance and accuracy of the RAG system‚Äôs responses. Additionally, logging critical RAG parameters like the data source, detected personally identifiable information (PII) types, and data lineage information is vital for maintaining compliance, reproducibility, and trust in enterprise environments. Capturing this metadata in SageMaker managed MLflow supports robust experiment tracking, auditability, efficient comparison, and root cause analysis across multiple data preparation runs, as visualized in the MLflow dashboard. This disciplined approach to data preparation lays the groundwork for effective experimentation, governance, and continuous improvement throughout the RAG pipeline. The following screenshot shows an example of the experiment run details in MLflow. 
 
Data chunking 
After data preparation, the next step is to split documents into manageable chunks for efficient embedding and retrieval. This process is pivotal, because the quality and granularity of chunks directly affect the relevance and completeness of answers returned by the RAG system. The RAG workflow in this post supports experimentation and RAG pipeline automation with both fixed-size and recursive chunking strategies for comparison and validations. However, this RAG solution can be expanded to many other chucking techniques. 
 
 FixedSizeChunker divides text into uniform chunks with configurable overlap 
 RecursiveChunker splits text along logical boundaries such as paragraphs or sentences 
 
Tracking detailed chunking metrics such as total_source_contexts_entries, total_contexts_chunked, and total_unique_chunks_final is crucial for understanding how much of the source data is represented, how effectively it is segmented, and whether the chunking approach is yielding the desired coverage and uniqueness. These metrics help diagnose issues like excessive duplication or under-segmentation, which can impact retrieval accuracy and model performance. 
Additionally, logging parameters such as chunking_strategy_type (for example, FixedSizeChunker), chunking_strategy_chunk_size (for example, 500 characters), and chunking_strategy_chunk_overlap provide transparency and reproducibility for each experiment. Capturing these details in SageMaker managed MLflow helps teams systematically compare the impact of different chunking configurations, optimize for efficiency and contextual relevance, and maintain a clear audit trail of how chunking decisions evolve over time. The MLflow dashboard makes it straightforward to visualize and analyze these parameters and metrics, supporting data-driven refinement of the chunking stage within the RAG pipeline. The following screenshot shows an example of the experiment run details in MLflow. 
 
After the documents are chunked, the next step is to convert these chunks into vector embeddings using a SageMaker embedding endpoint, after which the embeddings are ingested into a vector database such as OpenSearch Service for fast semantic search. This ingestion phase is crucial because the quality, completeness, and traceability of what enters the vector store directly determine the effectiveness and reliability of downstream retrieval and generation stages. 
Tracking ingestion metrics such as the number of documents and chunks ingested provides visibility into pipeline throughput and helps identify bottlenecks or data loss early in the process. Logging detailed parameters, including the embedding model ID, endpoint used, and vector database index, is essential for reproducibility and auditability. This metadata helps teams trace exactly which model and infrastructure were used for each ingestion run, supporting root cause analysis and compliance, especially when working with evolving datasets or sensitive information. 
Retrieval and generation 
For a given query, we generate an embedding and retrieve the top-k relevant chunks from OpenSearch Service. For answer generation, we use a SageMaker LLM endpoint. The retrieved context and the query are combined into a prompt, and the LLM generates an answer. Finally, we orchestrate retrieval and generation using LangGraph, enabling stateful workflows and advanced tracing: 
 
 graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_with_context = graph_builder.compile() 
 
With the GenerativeAI agent defined with LangGraph framework, the agentic layers are evaluated for each iteration of RAG development, verifying the efficacy of the RAG solution for agentic applications. Each retrieval and generation run is logged to SageMaker managed MLflow, capturing the prompt, generated response, and key metrics and parameters such as retrieval performance, top-k values, and the specific model endpoints used. Tracking these details in MLflow is essential for evaluating the effectiveness of the retrieval stage, making sure the returned documents are relevant and that the generated answers are accurate and complete. It is equally important to track the performance of the vector database during retrieval, including metrics like query latency, throughput, and scalability. Monitoring these system-level metrics alongside retrieval relevance and accuracy makes sure the RAG pipeline delivers correct and relevant answers and meets production requirements for responsiveness and scalability. The following screenshot shows an example of the Langraph RAG retrieval tracing in MLflow. 
 
RAG Evaluation 
Evaluation is conducted on a curated test set, and results are logged to MLflow for quick comparison and analysis. This helps teams identify the best-performing configurations and iterate toward production-grade solutions. With MLflow you can evaluate the RAG solution with heuristics metrics, content similarity metrics and LLM-as-a-judge. In this post, we evaluate the RAG pipeline using advanced LLM-as-a-judge MLflow metrics (answer similarity, correctness, relevance, faithfulness): 
 
 metrics_genai_only = [answer_correctness_aws, answer_similarity_aws, answer_relevance_aws, answer_faithfulness_aws] 
 
The following screenshot shows an RAG evaluation stage experiment run details in MLflow. 
 
You can use MLflow to log all metrics and parameters, enabling quick comparison of different experiment runs. See the following code for reference: 
 
 with mlflow.start_run(run_id=main_run_id) as run:
&nbsp;&nbsp; &nbsp;with mlflow.start_run(run_name="RAGEvaluation", nested=True):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;results = mlflow.evaluate(
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Other parameters
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;extra_metrics=metrics_genai_only,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;evaluator_config={
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...&nbsp;# Config parameters
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;) 
 
By using MLflow‚Äôs evaluation capabilities (such as mlflow.evaluate()), teams can systematically assess retrieval quality, identify potential gaps or misalignments in chunking or embedding strategies, and compare the performance of different retrieval and generation configurations. MLflow‚Äôs flexibility allows for seamless integration with external libraries and evaluation libraries such as RAGAS for comprehensive RAG pipeline assessment. RAGAS is an open source library that provide tools specifically for evaluation of LLM applications and generative AI agents. RAGAS includes the method ragas.evaluate() to run evaluations for LLM agents with the choice of LLM models (evaluators) for scoring the evaluation, and an extensive list of default metrics. To incorporate RAGAS metrics into your MLflow experiments, refer to the following GitHub repository. 
Comparing experiments 
In the MLflow UI, you can compare runs side by side. For example, comparing FixedSizeChunker and RecursiveChunker as shown in the following screenshot reveals differences in metrics such as answer_similarity (a difference of 1 point), providing actionable insights for pipeline optimization. 
 
Automation with Amazon SageMaker pipelines 
After systematically experimenting with and optimizing each component of the RAG workflow through SageMaker managed MLflow, the next step is transforming these validated configurations into production-ready automated pipelines. Although MLflow experiments help identify the optimal combination of chunking strategies, embedding models, and retrieval parameters, manually reproducing these configurations across environments can be error-prone and inefficient. 
To produce the automated RAG pipeline, we use SageMaker Pipelines, which helps teams codify their experimentally validated RAG workflows into automated, repeatable pipelines that maintain consistency from development through production. By converting the successful MLflow experiments into pipeline definitions, teams can make sure the exact same chunking, embedding, retrieval, and evaluation steps that performed well in testing are reliably reproduced in production environments. 
SageMaker Pipelines offers a serverless workflow orchestration for converting experimental notebook code into a production-grade pipeline, versioning and tracking pipeline configurations alongside MLflow experiments, and automating the end-to-end RAG workflow. The automated Sagemaker pipeline-based RAG workflow offers dependency management, comprehensive custom testing and validation before production deployment, and CI/CD integration for automated pipeline promotion. 
With SageMaker Pipelines, you can automate your entire RAG workflow, from data preparation to evaluation, as reusable, parameterized pipeline definitions. This provides the following benefits: 
 
 Reproducibility ‚Äì Pipeline definitions capture all dependencies, configurations, and executions logic in version-controlled code 
 Parameterization ‚Äì Key RAG parameters (chunk sizes, model endpoints, retrieval settings) can be quickly modified between runs 
 Monitoring ‚Äì Pipeline executions provide detailed logs and metrics for each step 
 Governance ‚Äì Built-in lineage tracking supports full audibility of data and model artifacts 
 Customization ‚Äì Serverless workflow orchestration is customizable to your unique enterprise landscape, with scalable infrastructure and flexibility with instances optimized for CPU, GPU, or memory-intensive tasks, memory configuration, and concurrency optimization 
 
To implement a RAG workflow in SageMaker pipelines, each major component of the RAG process (data preparation, chunking, ingestion, retrieval and generation, and evaluation) is included in a SageMaker processing job. These jobs are then orchestrated as steps within a pipeline, with data flowing between them, as shown in the following screenshot. This structure allows for modular development, quick debugging, and the ability to reuse components across different pipeline configurations. 
 
The key RAG configurations are exposed as pipeline parameters, enabling flexible experimentation with minimal code changes. For example, the following code snippets showcase the modifiable parameters for RAG configurations, which can be used as pipeline configurations: 
 
 processor&nbsp;&nbsp;PyTorchProcessor(
&nbsp; &nbsp; ...
&nbsp; &nbsp; arguments[
&nbsp; &nbsp; "--experiment-name",&nbsp;experiment_name,
&nbsp; &nbsp; "--mlflow-tracking-uri",&nbsp;mlflow_tracking_uri,
&nbsp; &nbsp; "--embedding-endpoint-name",&nbsp;embedding_endpoint_name,
&nbsp; &nbsp; "--text-endpoint-name",&nbsp;text_endpoint_name,
&nbsp; &nbsp; "--domain-name",&nbsp;domain_name,
&nbsp; &nbsp; "--index-name",&nbsp;index_name,
&nbsp; &nbsp; "--chunking-strategy",&nbsp;chunking_strategy,
&nbsp; &nbsp; "--chunk-size",&nbsp;chunk_size,
&nbsp; &nbsp; "--chunk-overlap",&nbsp;chunk_overlap,
&nbsp; &nbsp; "--context-retrieval-size",&nbsp;context_retrieval_size,
&nbsp; &nbsp; "--embedding-model-id",&nbsp;embedding_model_id,
&nbsp; &nbsp; "--text-model-id",&nbsp;text_model_id,
&nbsp; &nbsp; "--output-data-path",&nbsp;"/opt/ml/processing/output",
&nbsp; &nbsp; "--role-arn",&nbsp;role
&nbsp; &nbsp; ],
) 
 
In this post, we provide two agentic RAG pipeline automation approaches to building the SageMaker pipeline, each with own benefits: single-step SageMaker pipelines and multi-step pipelines. 
The single-step pipeline approach is designed for simplicity, running the entire RAG workflow as one unified process. This setup is ideal for straightforward or less complex use cases, because it minimizes pipeline management overhead. With fewer steps, the pipeline can start quickly, benefitting from reduced execution times and streamlined development. This makes it a practical option when rapid iteration and ease of use are the primary concerns. 
The multi-step pipeline approach is preferred for enterprise scenarios where flexibility and modularity are essential. By breaking down the RAG process into distinct, manageable stages, organizations gain the ability to customize, swap, or extend individual components as needs evolve. This design enables plug-and-play adaptability, making it straightforward to reuse or reconfigure pipeline steps for various workflows. Additionally, the multi-step format allows for granular monitoring and troubleshooting at each stage, providing detailed insights into performance and facilitating robust enterprise management. For enterprises seeking maximum flexibility and the ability to tailor automation to unique requirements, the multi-step pipeline approach is the superior choice. 
CI/CD for an agentic RAG pipeline 
Now we integrate the SageMaker RAG pipeline with CI/CD. CI/CD is important for making a RAG solution enterprise-ready because it provides faster, more reliable, and scalable delivery of AI-powered workflows. Specifically for enterprises, CI/CD pipelines automate the integration, testing, deployment, and monitoring of changes in the RAG system, which brings several key benefits, such as faster and more reliable updates, version control and traceability, consistency across environments, modularity and flexibility for customization, enhanced collaboration and monitoring, risk mitigation, and cost savings. This aligns with general CI/CD benefits in software and AI systems, emphasizing automation, quality assurance, collaboration, and continuous feedback essential to enterprise AI readiness. 
When your SageMaker RAG pipeline definition is in place, you can implement robust CI/CD practices by integrating your development workflow and toolsets already enabled at your enterprise. This setup makes it possible to automate code promotion, pipeline deployment, and model experimentation through simple Git triggers, so changes are versioned, tested, and systematically promoted across environments. For demonstration, in this post, we show the CI/CD integration using GitHub Actions and by using GitHub Actions as the CI/CD orchestrator. Each code change, such as refining chunking strategies or updating pipeline steps, triggers an end-to-end automation workflow, as shown in the following screenshot. You can use the same CI/CD pattern with your choice of CI/CD tool instead of GitHub Actions, if needed. 
 
Each GitHub Actions CI/CD execution automatically triggers the SageMaker pipeline (shown in the following screenshot), allowing for seamless scaling of serverless compute infrastructure. 
 
Throughout this cycle, SageMaker managed MLflow records every executed pipeline (shown in the following screenshot), so you can seamlessly review results, compare performance across different pipeline runs, and manage the RAG lifecycle. 
 
After an optimal RAG pipeline configuration is determined, the new desired configuration (Git version tracking captured in MLflow as shown in the following screenshot) can be promoted to higher stages or environments directly through an automated workflow, minimizing manual intervention and reducing risk. 
 
Clean up 
To avoid unnecessary costs, delete resources such as the SageMaker managed MLflow tracking server, SageMaker pipelines, and SageMaker endpoints when your RAG experimentation is complete. You can visit the SageMaker Studio console to destroy resources that aren‚Äôt needed anymore or call appropriate AWS APIs actions. 
Conclusion 
By integrating SageMaker AI, SageMaker managed MLflow, and Amazon OpenSearch Service, you can build, evaluate, and deploy RAG pipelines at scale. This approach provides the following benefits: 
 
 Automated and reproducible workflows with SageMaker Pipelines and MLflow, minimizing manual steps and reducing the risk of human error 
 Advanced experiment tracking and comparison for different chunking strategies, embedding models, and LLMs, so every configuration is logged, analyzed, and reproducible 
 Actionable insights from both traditional and LLM-based evaluation metrics, helping teams make data-driven improvements at every stage 
 Seamless deployment to production environments, with automated promotion of validated pipelines and robust governance throughout the workflow 
 
Automating your RAG pipeline with SageMaker Pipelines brings additional benefits: it enables consistent, version-controlled deployments across environments, supports collaboration through modular, parameterized workflows, and supports full traceability and auditability of data, models, and results. With built-in CI/CD capabilities, you can confidently promote your entire RAG solution from experimentation to production, knowing that each stage meets quality and compliance standards. 
Now it‚Äôs your turn to operationalize RAG workflows and accelerate your AI initiatives. Explore SageMaker Pipelines and managed MLflow using the solution from the GitHub repository to unlock scalable, automated, and enterprise-grade RAG solutions. 
 
About the authors 
Sandeep Raveesh is a GenAI Specialist Solutions Architect at AWS. He works with customers through their AIOps journey across model training, generative AI applications like agents, and scaling generative AI use cases. He also focuses on Go-To-Market strategies, helping AWS build and align products to solve industry challenges in the generative AI space. You can find Sandeep on LinkedIn. 
Blake Shin is an Associate Specialist Solutions Architect at AWS who enjoys learning about and working with new AI/ML technologies. In his free time, Blake enjoys exploring the city and playing music.

‚∏ª