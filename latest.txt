âœ… Morning News Briefing â€“ October 14, 2025 10:45

ğŸ“… Date: 2025-10-14 10:45
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ Current Conditions:  8.6Â°C
  Temperature: 8.6&deg;C Pressure / Tendency: 102.3 kPa falling Humidity: 96 % Humidity is 96 % Dewpoint: 8 .0&deg:C Wind: W 4 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 14 October 2025 . Weather forecast: 10/11
â€¢ Tuesday: A mix of sun and cloud. High 19.
  Fog patches dissipating this morning . Wind becoming northwest 30 km/h this afternoon . A mix of sun and cloud with a mix of rain and wind . High 19. UV index 4 or moderate . Forecast issued 5:00 AM EDT Tuesday 14 October 2025 . Weather forecast: Sunny and sunny with a high of 19.5 degrees in the morning of Tuesday 14.10am .
â€¢ Tuesday night: Chance of showers. Low plus 2. POP 30%
  Clear this evening then partly cloudy with 30 percent chance of showers before morning . Wind north 30 km/h gusting to 50 mph . Low plus 2.50Â°F is expected to be the coldest night of the year . Forecast issued 5:00 AM EDT Tuesday 14 October 2025 . Weather forecasters predict temperatures will drop to minus 2Â°F in the early hours of Tuesday

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ Data centers are booming. But there are big energy and environmental risks
  How tech companies and government officials handle local impacts will shape the industry's future in the U.S. The tech industry will continue to grow in the United States as a result of local impacts on the economy . Tech companies and officials will be looking to shape the future of the tech industry in the country, especially in the state of New York and Washington, D.C., according to
â€¢ Opinion: Why I'm handing in my Pentagon press pass
  Tom Bowman has held his Pentagon press pass for 28 years . Bowman says the Pentagon's new media policy makes it impossible to be a journalist . Bowman: Find out what's really going on behind the scenes and not accepting wholesale what anyâ€¯government or administration says . He says it's important to not accept wholesale what the government or administration has said. Bowman: It's important not to
â€¢ In reading, the nation's students are still stuck in a pandemic slump
  New 2025 testing data shows third- through eighth-graders scored far below 2019 levels in reading . In math, some grades have made gains, but all are lagging compared to before the pandemic . New 2025 test data shows students are still lagging in reading, math, reading and math tests in the U.S. Third through eighth graders are far behind 2019 levels .
â€¢ Death toll from torrential rains in Mexico rises to 64 as search expands
  Helicopters have ferried food and water to 200 some communities that remained cut off by ground . Mexico has deployed some 10,000 troops in addition to civilian rescue teams . Helicopter has ferried water and food to the 200 communities that remain cut off from each other by ground. Helicopter helicopters have also ferried supplies to the remaining areas of Mexico's worst-hit communities .
â€¢ SpaceX launches 11th test flight of its mega Starship rocket with another win
  Starship, the biggest and most powerful rocket ever built, thundered into the evening sky from the southern tip of Texas . Starship is the biggest rocket ever made and the most powerful ever built . Starship was launched from Texas's southern tip, the state's largest rocket ever launched . The rocket was the largest and largest ever built rocket in the history of the rocket launch industry, launching from

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ Ofcom refuses to bite over Openreach's fiber freebies
  Watchdog says it sees no case to investigate discounted FTTP upgrade offer â€“ but will keep an eye on it . Ofcom declined to intervene after smaller network providers complained that a special upgrade offer from Openreach could threaten competition in the broadband market . Openreach has been accused of threatening competition in broadband market with a discounted upgrade offer . Watchdog said it saw no case for investigation but will
â€¢ Brit AI boffins making bank with Â£560K average pay packet at Anthropic
  UK units of some US tech companies are paying average salaries of well into six figures . Google, DeepMind, Microsoft also shower UK staff with six-figure salaries . Some more than matching that with share-based payments, according to Companies House . Companies House annual accounts published by Companies House have been published by companies House . Google and DeepMind are among the companies that have paid UK staff
â€¢ Ubuntu 25.10 lands: Rustier and Wayland-ier, but Flatpak is broken
  Canonical's Questing Quokka waddles in at 5.7 GB with AppArmor woes . This isn't a long-term release, yet many of its differences will be in 26.04 next year . The latest interim release of Ubuntu is here, showcasing some significant changes . Many of the changes will be made in the next version of the operating system in 2015 .
â€¢ Unwary SAP private cloud users face 10% renewal hikes, warns Gartner
  SAP customers opting for private cloud have seen price increases of 10 percent or more on renewal proposals if they fail to negotiate a renewal price cap in the original deal . On-prem discounts drying up as ERP giant sends 'mixed signals' on pricing . Gartner has reported that SAP customers opted to opt out of SAP's cloud-based ERP cloud service for a price increase
â€¢ EU biometric border system launches, suffers teeting problems
  European Union's new biometric Exit/Entry System (EES) got off to a chaotic start at Prague's international airport . Malfunctioning equipment and manual processing cause 90-minute waits . The EES is the first biometric exit/entry system to be implemented in the EU . The new system is being trialled in Prague's Prague International International Airport, Prague Airport Airport .

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ The probiotic home: where microbes are welcome guests
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Exposure to ambient air pollution and onset of Parkinsonâ€™s disease in a large cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Maternal sedentary behavior and physical activity levels in early to mid-pregnancy and obstetric outcomes: a cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Multiple myeloma incidence, transplant utilization, and mortality- impact of social vulnerability
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Sustainability practices made easy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ How aging clocks can help us understand why we ageâ€”and if we can reverse it
  Be honest: Have you ever looked up someone from your childhood on social media with the sole intention of seeing how theyâ€™ve aged?&nbsp;



One of my colleagues, who shall remain nameless, certainly has. He recently shared a photo of a former classmate. â€œCan you believe weâ€™re the same age?â€ he asked, with a hint of glee in his voice. A relative also delights in this pastime. â€œWow, she looks like an old woman,â€ sheâ€™ll say when looking at a picture of someone she has known since childhood. The years certainly are kinder to some of us than others.



But wrinkles and gray hairs aside, it can be difficult to know how wellâ€”or poorlyâ€”someoneâ€™s body is truly aging, under the hood. A person who develops age-related diseases earlier in life, or has other biological changes associated with aging (such as elevated cholesterol or markers of inflammation), might be considered â€œbiologically olderâ€ than a similar-age person who doesnâ€™t have those changes. Some 80-year-olds will be weak and frail, while others are fit and active.&nbsp;





Doctors have long used functional tests that measure their patientsâ€™ strength or the distance they can walk, for example, or simply â€œeyeballâ€ them to guess whether they look fit enough to survive some treatment regimen, says Tamir Chandra, who studies aging at the Mayo Clinic.&nbsp;



But over the past decade, scientists have been uncovering new methods of looking at the hidden ways our bodies are aging. What theyâ€™ve found is changing our understanding of aging itself.&nbsp;



â€œAging clocksâ€ are new scientific tools that can measure how our organs are wearing out, giving us insight into our mortality and health. They hint at our biological age. While chronological age is simply how many birthdays weâ€™ve had, biological age is meant to reflect something deeper. It measures how our bodies are handling the passing of time andâ€”perhapsâ€”lets us know how much more of it we have left. And while you canâ€™t change your chronological age, you just might be able to influence your biological age.



Itâ€™s not just scientists who are using these clocks. Longevity influencers like Bryan Johnson often use them to make the case that they are aging backwards. â€œMy telomeres say Iâ€™m 10 years old,â€ Johnson posted on X in April. The Kardashians have tried them too (KhloÃ© was told on TV that her biological age was 12 years below her chronological age). Even my local health-food store offers biological age testing. Some are pushing the use of clocks even further, using them to sell unproven â€œanti-agingâ€ supplements.



The science is still new, and few experts in the fieldâ€”some of whom affectionately refer to it as â€œclock worldâ€â€”would argue that an aging clock can definitively reveal an individualâ€™s biological age.&nbsp;



But their work is revealing that aging clocks can offer so much more than an insta-brag, a snake-oil pitchâ€”or even just an eye-catching number. In fact, they are helping scientists unravel some of the deepest mysteries in biology: Why do we age? How do we age? When does aging begin? What does it even mean to age?



Ultimately, and most importantly, they might soon tell us whether we can reverse the whole process.



Clocks kick off



The way your genes work can change. Molecules called methyl groups can attach to DNA, controlling the way genes make proteins. This process is called methylation, and it can potentially occur at millions of points along the genome. These epigenetic markers, as they are known, can switch genes on or off, or increase or decrease how much protein they make. Theyâ€™re not part of our DNA, but they influence how it works.



In 2011, Steve Horvath, then a biostatistician at the University of California, Los Angeles, took part in a study that was looking for links between sexual orientation and these epigenetic markers. Steve is straight; he says his twin brother, Markus, who also volunteered, is gay.



That study didnâ€™t find a link between DNA methylÂ­ation and sexual orientation. But when Horvath looked at the data, he noticed a different trendâ€”a very strong link between age and methylation at around 88 points on the genome. He once told me he fell off his chair when he saw it.Â 



Many of the affected genes had already been linked to age-related brain and cardiovascular diseases, but it wasnâ€™t clear how methylation might be related to those diseases.&nbsp;




If a model could work out what average aging looks like, it could potentially estimate whether someone was aging unusually fast or slowly. It could transform medicine and fast-track the search for an anti-aging drug. It could help us understand what aging is, and why it happens at all.




In 2013, Horvath collected methylation data from 8,000 tissue and cell samples to create what he called the Horvath clockâ€”essentially a mathematical model that could estimate age on the basis of DNA methylation at 353 points on the genome. From a tissue sample, it was able to detect a personâ€™s age within a range of 2.9 years.



That clock changed everything. Its publication in 2013 marked the birth of â€œclock world.â€ To some, the possibilities were almost endless. If a model could work out what average aging looks like, it could potentially estimate whether someone was aging unusually fast or slowly. It could transform medicine and fast-track the search for an anti-aging drug. It could help us understand what aging is, and why it happens at all.



The epigenetic clock was a success story in â€œa field that, frankly, doesnâ€™t have a lot of success stories,â€ says JoÃ£o Pedro de MagalhÃ£es, who researches aging at the University of Birmingham, UK.



It took a few years, but as more aging researchers heard about the clock, they began incorporating it into their research and even developing their own clocks. Horvath became a bit of a celebrity. Scientists started asking for selfies with him at conferences, he says. Some researchers even made T-shirts bearing the front page of his 2013 paper.





Some of the many other aging clocks developed since have become notable in their own right. Examples include the PhenoAge clock, which incorporates health data such as blood cell counts and signs of inflammation along with methylÂ­ation, and the Dunedin Pace of Aging clock, which tells you how quickly or slowly a person is aging rather than pointing to a specific age. Many of the clocks measure methylation, but some look at other variables, such as proteins in blood or certain carbohydrate molecules that attach to such proteins.



Today, there are hundreds or even thousands of clocks out there, says Chiara Herzog, who researches aging at Kingâ€™s College London and is a member of the Biomarkers of Aging Consortium. Everyone has a favorite. Horvath himself favors his GrimAge clock, which was named after the Grim Reaper because it is designed to predict time to death.



That clock was trained on data collected from people who were monitored for decades, many of whom died in that period. Horvath wonâ€™t use it to tell people when they might die of old age, he stresses, saying that it wouldnâ€™t be ethical. Instead, it can be used to deliver a biological age that hints at how long a person might expect to live. Someone who is 50 but has a GrimAge of 60 can assume that, compared with the average 50-year-old, they might be a bit closer to the end.



GrimAge is not perfect. While it can strongly predict time to death given the health trajectory someone is on, no aging clock can predict if someone will start smoking or get a divorce (which generally speeds aging) or suddenly take up running (which can generally slow it). â€œPeople are complicated,â€ Horvath tells MIT Technology Review. â€œThereâ€™s a huge error bar.â€



On the whole, the clocks are pretty good at making predictions about health and lifespan. Theyâ€™ve been able to predict that people over the age of 105 have lower biological ages, which tracks given how rare it is for people to make it past that age. A higher epigenetic age has been linked to declining cognitive function and signs of Alzheimerâ€™s disease, while better physical and cognitive fitness has been linked to a lower epigenetic age.



Black-box clocks



But accuracy is a challenge for all aging clocks. Part of the problem lies in how they were designed. Most of the clocks were trained to link age with methylation. The best clocks will deliver an estimate that reflects how far a personâ€™s biology deviates from the average. Aging clocks are still judged on how well they can predict a personâ€™s chronological age, but you donâ€™t want them to be too close, says Lucas Paulo de Lima Camillo, head of machine learning at Shift Bioscience, who was awarded $10,000 by the Biomarkers of Aging Consortium for developing a clock that could estimate age within a range of 2.55 years.



None of the clocks are precise enough to predict the biological age of a single person. Putting the same biological sample through five different clocks will give you five wildly different results.LEON EDLER




â€œThereâ€™s this paradox,â€ says Camillo. If a clock is really good at predicting chronological age, thatâ€™s all it will tell youâ€”and it probably wonâ€™t reveal much about your biological age. No one needs an aging clock to tell them how many birthdays theyâ€™ve had. Camillo says heâ€™s noticed that when the clocks get too close to â€œperfectâ€ age prediction, they actually become less accurate at predicting mortality.



Therein lies the other central issue for scientists who develop and use aging clocks: What is the thing they are really measuring? It is a difficult question for a field whose members notoriously fail to agree on the basics. (Everything from the definition of aging to how it occurs and why is up for debate among the experts.)



They do agree that aging is incredibly complex. A methylation-based aging clock might tell you about how that collection of chemical markers compares across individuals, but at best, itâ€™s only giving you an idea of their â€œepigenetic age,â€ says Chandra. There are probably plenty of other biological markers that might reveal other aspects of aging, he says: â€œNone of the clocks measure everything.â€&nbsp;



We donâ€™t know why some methyl groups appear or disappear with age, either. Are these changes causing damage? Or are they a by-product of it? Are the epigenetic patterns seen in a 90-year-old a sign of deterioration? Or have they been responsible for keeping that person alive into very old age?



To make matters even more complicated, two different clocks can give similar answers by measuring methylation at entirely different regions of the genome. No one knows why, or which regions might be the best ones to focus on.



â€œThe biomarkers have this black-box quality,â€ says Jesse Poganik at Brigham and Womenâ€™s Hospital in Boston. â€œSome of them are probably causal, some of them may be adaptive â€¦ and some of them may just be neutralâ€: either â€œthereâ€™s no reason for them not to happenâ€ or â€œthey just happen by random chance.â€



What we know is that, as things stand, none of the clocks are precise enough to predict the biological age of a single person (sorry, KhloÃ©). Putting the same biological sample through five different clocks will give you five wildly different results.



Even the same clock can give you different answers if you put a sample through it more than once. â€œTheyâ€™re not yet individually predictive,â€ says Herzog. â€œWe donâ€™t know what [a clock result] means for a person, [or if] theyâ€™re more or less likely to develop disease.â€



And itâ€™s why plenty of aging researchersâ€”even those who regularly use the clocks in their workâ€”havenâ€™t bothered to measure their own epigenetic age. â€œLetâ€™s say I do a clock and it says that my biological age â€¦ is five years older than it should be,â€ says MagalhÃ£es. â€œSo what?â€ He shrugs. â€œI donâ€™t see much point in it.â€



You might think this lack of clarity would make aging clocks pretty useless in a clinical setting. But plenty of clinics are offering them anyway. Some longevity clinics are more careful, and will regularly test their patients with a range of clocks, noting their results and tracking them over time. Others will simply offer an estimate of biological age as part of a longevity treatment package.



And then there are the people who use aging clocks to sell supplements. While no drug or supplement has been definitively shown to make people live longer, that hasnâ€™t stopped the lightly regulated wellness industry from pushing a range of â€œtreatmentsâ€ that range from lotions to herbal pills all the way through to stem-cell injections.



Some of these people come to aging meetings. I was in the audience at an event when one CEO took to the stage to claim he had reversed his own biological age by 18 yearsâ€”thanks to the supplement he was selling. Tom Weldon of Ponce de Leon Health told us his gray hair was turning brown. His biological age was supposedly reversing so rapidly that he had reached â€œlongevity escape velocity.â€





But if the people who buy his supplements expect some kind of Benjamin Button effect, they might be disappointed. His company hasnâ€™t yet conducted a randomized controlled trial to demonstrate any anti-aging effects of that supplement, called Rejuvant. Weldon says that such a trial would take years and cost millions of dollars, and that heâ€™d â€œhave to increase the price of our product more than four timesâ€ to pay for one. (The company has so far tested the active ingredient in mice and carried out a provisional trial in people.)



More generally, Horvath says he â€œgets a bad taste in [his] mouthâ€ when people use the clocks to sell products and â€œmake a quick buck.â€ But he thinks that most of those sellers have genuine faith in both the clocks and their products. â€œPeople truly believe their own nonsense,â€ he says. â€œThey are so passionate about what they discovered, they fall into this trap of believing [their] own prejudices.â€&nbsp;



The accuracy of the clocks is at a level that makes them useful for research, but not for individual predictions. Even if a clock did tell someone they were five years younger than their chronological age, that wouldnâ€™t necessarily mean the person could expect to live five years longer, says MagalhÃ£es. â€œThe field of aging has long been a rich ground for snake-oil salesmen and hype,â€ he says. â€œIt comes with the territory.â€ (Weldon, for his part, says Rejuvant is the only product that has â€œclinically meaningfulâ€ claims.)&nbsp;



In any case, MagalhÃ£es adds that he thinks any publicity is better than no publicity.



And thereâ€™s the rub. Most people in the longevity field seem to have mixed feelings about the trendiness of aging clocks and how they are being used. Theyâ€™ll agree that the clocks arenâ€™t ready for consumer prime time, but they tend to appreciate the attention. Longevity research is expensive, after all. With a surge in funding and an explosion in the number of biotech companies working on longevity, aging scientists are hopeful that innovation and progress will follow.&nbsp;



So they want to be sure that the reputation of aging clocks doesnâ€™t end up being tarnished by association. Because while influencers and supplement sellers are using their â€œbiological agesâ€ to garner attention, scientists are now using these clocks to make some remarkable discoveries. Discoveries that are changing the way we think about aging.



How to be young again



Two little mice lie side by side, anesthetized and unconscious, as Jim White prepares his scalpel. The animals are of the same breed but look decidedly different. One is a youthful three-month-old, its fur thick, black, and glossy. By comparison, the second mouse, a 20-month-old, looks a little the worse for wear. Its fur is graying and patchy. Its whiskers are short, and it generally looks kind of frail.



But the two mice are about to have a lot more in common. White, with some help from a colleague, makes incisions along the side of each mouseâ€™s body and into the upper part of an arm and leg on the same side. He then carefully stitches the two animals togetherâ€”membranes, fascia, and skin.&nbsp;



The procedure takes around an hour, and the mice are then roused from their anesthesia. At first, the two still-groggy animals pull away from each other. But within a few days, they seem to have accepted that they now share their bodies. Soon their circulatory systems will fuse, and the animals will share a blood flow too.



â€œPeople are complicated. Thereâ€™s a huge error bar.â€ â€” Steve Horvath, former biostatistician at the University of California, Los AngelesLEON EDLER




White, who studies aging at Duke University, has been stitching mice together for years; he has performed this strange procedure, known as heterochronic parabiosis, more than a hundred times. And heâ€™s seen a curious phenomenon occur. The older mice appear to benefit from the arrangement. They seem to get younger.



Experiments with heterochronic parabiosis have been performed for decades, but typically scientists keep the mice attached to each other for only a few weeks, says White. In their experiment, he and his colleagues left the mice attached for three monthsâ€”equivalent to around 10 human years. The team then carefully separated the animals to assess how each of them had fared. â€œYouâ€™d think that theyâ€™d want to separate immediately,â€ says White. â€œBut when you detach them â€¦ they kind of follow each other around.â€



The most striking result of that experiment was that the older mice who had been attached to a younger mouse ended up living longer than other mice of a similar age. â€œ[They lived] around 10% longer, but [they] also maintained a lot of [their] function,â€ says White. They were more active and maintained their strength for longer, he adds.



When his colleagues, including Poganik, applied aging clocks to the mice, they found that their epigenetic ages were lower than expected. â€œThe young circulation slowed aging in the old mice,â€ says White. The effect seemed to last, tooâ€”at least for a little while. â€œIt preserved that youthful state for longer than we expected,â€ he says.



The young mice went the other way and appeared biologically older, both while they were attached to the old mice and shortly after they were detached. But in their case, the effect seemed to be short-lived, says White: â€œThe young mice went back to being young again.â€&nbsp;



To White, this suggests that something about the â€œyouthful stateâ€ might be programmed in some way. That perhaps it is written into our DNA. Maybe we donâ€™t have to go through the biological process of aging.&nbsp;



This gets at a central debate in the aging field: What is aging, and why does it happen? Some believe itâ€™s simply a result of accumulated damage. Some believe that the aging process is programmed; just as we grow limbs, develop a brain, reach puberty, and experience menopause, we are destined to deteriorate. Others think programs that play an important role in our early development just turn out to be harmful later in life by chance. And there are some scientists who agree with all of the above.



Whiteâ€™s theory is that being old is just â€œa loss of youth,â€ he says. If thatâ€™s the case, thereâ€™s a silver lining: Knowing how youth is lost might point toward a way to somehow regain it, perhaps by restoring those youthful programs in some way.&nbsp;



Dogs and dolphins



Horvathâ€™s eponymous clock was developed by measuring methylation in DNA samples taken from tissues around the body. It seems to represent aging in all these tissues, which is why Horvath calls it a pan-tissue clock. Given that our organs are thought to age differently, it was remarkable that a single clock could measure aging in so many of them.



But Horvath had ambitious plans for an even more universal clock: a pan-species model that could measure aging in all mammals. He started out, in 2017, with an email campaign that involved asking hundreds of scientists around the world to share samples of tissues from animals they had worked with. He tried zoos, too.&nbsp; &nbsp;




The pan-mammalian clock suggests that there is something universal about agingâ€”not just that all mammals experience it in a similar way, but that a similar set of genetic or epigenetic factors might be responsible for it.




â€œI learned that people had spent careers collecting [animal] tissues,â€ he says. â€œThey had freezers full of [them].â€ Amenable scientists would ship those frozen tissues, or just DNA, to Horvathâ€™s lab in California, where he would use them to train a new model.



Horvath says he initially set out to profile 30 different species. But he ended up receiving around 15,000 samples from 200 scientists, representing 348 speciesâ€”including everything from dogs to dolphins. Could a single clock really predict age in all of them?



â€œI truly felt it would fail,â€ says Horvath. â€œBut it turned out that I was completely wrong.â€ He and his colleagues developed a clock that assessed methylation at 36,000 locations on the genome. The result, which was published in 2023 as the pan-mammalian clock, can estimate the age of any mammal and even the maximum lifespan of the species. The data set is open to anyone who wants to download it, he adds: â€œI hope people will mine the data to find the secret of how to extend a healthy lifespan.â€



The pan-mammalian clock suggests that there is something universal about agingâ€”not just that all mammals experience it in a similar way, but that a similar set of genetic or epigenetic factors might be responsible for it.



Comparisons between mammals also support the idea that the slower methylation changes occur, the longer the lifespan of the animal, says Nelly Olova, an epigeneticist who researches aging at the University of Edinburgh in the UK. â€œDNA methylation slowly erodes with age,â€ she says. â€œWe still have the instructions in place, but they become a little messier.â€ The research in different mammals suggests that cells can take only so much change before they stop functioning.



â€œThereâ€™s a finite amount of change that the cell can tolerate,â€ she says. â€œIf the instructions become too messy and noisy â€¦ it cannot support life.â€



Olova has been investigating exactly when aging clocks first begin to tickâ€”in other words, the point at which aging starts. Clocks can be trained on data from volunteers, and by matching the patterns of methylation on their DNA to their chronological age. The trained clocks are then typically used to estimate the biological age of adults. But they can also be used on samples from children. Or babies. They can be used to work out the biological age of cells that make up embryos.&nbsp;



In her research, Olova used adult skin cells, whichâ€”thanks to Nobel Prizeâ€“winning research in the 2000sâ€”can be â€œreprogrammedâ€ back to a state resembling that of the pluripotent stem cells found in embryos. When Olova and her colleagues used a â€œpartial reprogrammingâ€ approach to take cells close to that state, they found that the closer they got to the entirely reprogrammed state, the â€œyoungerâ€ the cells were.&nbsp;





It was around 20 days after the cells had been reprogrammed into stem cells that they reached the biological age of zero according to the clock used, says Olova. â€œIt was a bit surreal,â€ she says. â€œThe pluripotent cells measure as minus 0.5; theyâ€™re slightly below zero.â€



Vadim Gladyshev, a prominent aging researcher at Harvard University, has since proposed that the same negative level of aging might apply to embryos. After all, some kind of rejuvenation happens during the early stages of embryo formationâ€”an aged egg cell and an aged sperm cell somehow create a brand-new cell. The slate is wiped clean.



Gladyshev calls this point â€œground zero.â€ He posits that itâ€™s reached sometime during the â€œmid-embryonic state.â€ At this point, aging begins. And so does â€œorganismal life,â€ he argues. â€œItâ€™s interesting how this coincides with philosophical questions about when life starts,â€ says Olova.Â 



Some have argued that life begins when sperm meets egg, while others have suggested that the point when embryonic cells start to form some kind of unified structure is what counts. The ground zero point is when the body plan is set out and cells begin to organize accordingly, she says. â€œBefore that, itâ€™s just a bunch of cells.â€



This doesnâ€™t mean that life begins at the embryonic state, but it does suggest that this is when aging beginsâ€”perhaps as the result of â€œa generational clearance of damage,â€ says Poganik.



It is early daysâ€”no pun intendedâ€”for this research, and the science is far from settled. But knowing when aging begins could help inform attempts to rewind the clock. If scientists can pinpoint an ideal biological age for cells, perhaps they can find ways to get old cells back to that state. There might be a way to slow aging once cells reach a certain biological age, too.&nbsp;



â€œPresumably, there may be opportunities for targeting aging before â€¦ youâ€™re full of gray hair,â€ says Poganik. â€œIt could mean that there is an ideal window for intervention which is much earlier than our current geriatrics-based approach.â€



When young meets old



When White first started stitching mice together, he would sit and watch them for hours. â€œI was like, look at them go! Theyâ€™re together, and they donâ€™t even care!â€ he says. Since then, heâ€™s learned a few tricks. He tends to work with female mice, for instanceâ€”the males tend to bicker and nip at each other, he says. The females, on the other hand, seem to get on well.&nbsp;



The effect their partnership appears to have on their biological ages, if only temporarily, is among the ways aging clocks are helping us understand that biological age is plastic to some degree. White and his colleagues have also found, for instance, that stress seems to increase biological age, but that the effect can be reversed once the stress stops. Both pregnancy and covid-19 infections have a similar reversible effect.



Poganik wonders if this finding might have applications for human organ transplants. Perhaps thereâ€™s a way to measure the biological age of an organ before it is transplanted and somehow rejuvenate organs before surgery.&nbsp;



But new data from aging clocks suggests that this might be more complicated than it sounds. Poganik and his colleagues have been using methylation clocks to measure the biological age of samples taken from recently transplanted hearts in living people.&nbsp;




If being old is simply a case of losing our youthfulness, then that might give us a clue to how we can somehow regain it.




Young hearts do well in older bodies, but the biological age of these organs eventually creeps up to match that of their recipient. The same is true for older hearts in younger bodies, says Poganik, who has not yet published his findings. â€œAfter a few months, the tissue may assimilate the biological age of the organism,â€ he says.&nbsp;



If thatâ€™s the case, the benefits of young organs might be short-lived. It also suggests that scientists working on ways to rejuvenate individual organs may need to focus their anti-aging efforts on more systemic means of rejuvenationâ€”for example, stem cells that repopulate the blood. Reprogramming these cells to a youthful state, perhaps one a little closer to â€œground zero,â€ might be the way to go.



Whole-body rejuvenation might be some way off, but scientists are still hopeful that aging clocks might help them find a way to reverse aging in people.



â€œWe have the machinery to reset our epigenetic clock to a more youthful state,â€ says White. â€œThat means we have the ability to turn the clock backwards.â€
â€¢ Can we repair the internet?
  From addictive algorithms to exploitative apps, data mining to misinformation, the internet today can be a hazardous place. Books by three influential figuresâ€”the intellect behind â€œnet neutrality,â€ a former Meta executive, and the webâ€™s own inventorâ€”propose radical approaches to fixing it. But are these luminaries the right people for the job? Though each shows conviction, and even sometimes inventiveness, the solutions they present reveal blind spots.



The Age of Extraction: How Tech Platforms Conquered the Economy and Threaten Our Future ProsperityTim WuKNOPF, 2025




In The Age of Extraction: How Tech Platforms Conquered the Economy and Threaten Our Future Prosperity, Tim Wu argues that a few platform companies have too much concentrated power and must be dismantled. Wu, a prominent Columbia professor who popularized the principle that a free internet requires all online traffic to be treated equally, believes that existing legal mechanisms, especially anti-monopoly laws, offer the best way to achieve this goal.



Pairing economic theory with recent digital history, Wu shows how platforms have shifted from giving to users to extracting from them. He argues that our failure to understand their power has only encouraged them to grow, displacing competitors along the way. And he contends that convenience is what platforms most often exploit to keep users entrapped. â€œThe human desire to avoid unnecessary pain and inconvenience,â€ he writes, may be â€œthe strongest force out there.â€



He cites Googleâ€™s and Appleâ€™s â€œecosystemsâ€ as examples, showing how users can become dependent on such services as a result of their all-Â­encompassing seamlessness. To Wu, this isnâ€™t a bad thing in itself. The ease of using Amazon to stream entertainment, make online purchases, or help organize day-to-day life delivers obvious gains. But when powerhouse companies like Amazon, Apple, and Alphabet win the battle of convenience with so many usersâ€”and never let competitors get a footholdâ€”the result is â€œindustry dominanceâ€ that must now be reexamined.



The measures Wu advocatesâ€”and that appear the most practical, as they draw on existing legal frameworks and economic policiesâ€”are federal anti-monopoly laws, utility caps that limit how much companies can charge consumers for service, and â€œline of businessâ€ restrictions that prohibit companies from operating in certain industries.




Columbia Universityâ€™s Tim Wu shows how platforms have shifted from giving to users to extracting from them. He argues that our failure to understand their power has only encouraged them to grow.




Anti-monopoly provisions and antitrust laws are effective weapons in our armory, Wu contends, pointing out that they have been successfully used against technology companies in the past. He cites two well-known cases. The first is the 1960s antitrust case brought by the US government against IBM, which helped create competition in the computer software market that enabled companies like Apple and Microsoft to emerge. The 1982 AT&amp;T case that broke the telephone conglomerate up into several smaller companies is another instance. In each, the public benefited from the decoupling of hardware, software, and other services, leading to more competition and choice in a technology market.





But will past performance predict future results? Itâ€™s not yet clear whether these laws can be successful in the platform age. The 2025 antitrust case against Googleâ€”in which a judge ruled that the company did not have to divest itself of its Chrome browser as the US Justice Department had proposedâ€”reveals the limits of pursuing tech breakups through the law. The 2001 antitrust case brought against Microsoft likewise failed to separate the company from its web browser and mostly kept the conglomerate intact. Wu noticeably doesnâ€™t discuss the Microsoft case when arguing for antitrust action today.



Nick Clegg, until recently Metaâ€™s president of global affairs and a former deputy prime minister of the UK, takes a position very different from Wuâ€™s: that trying to break up the biggest tech companies is misguided and would degrade the experience of internet users. In How to Save the Internet: The Threat to Global Connection in the Age of AI and Political Conflict, Clegg acknowledges Big Techâ€™s monopoly over the web. But he believes punitive legal measures like antitrust laws are unproductive and can be avoided by means of regulation, such as rules for what content social media can and canâ€™t publish. (Itâ€™s worth noting that Meta is facing its own antitrust case, involving whether it should have been allowed to acquire Instagram and WhatsApp.)



How to Save the Internet: The Threat to Global Connection in the Age of AI and Political ConflictNick CleggBODLEY HEAD, 2025




Clegg also believes Silicon Valley should take the initiative to reform itself. He argues that encouraging social media networks to â€œopen up the booksâ€ and share their decision-making power with users is more likely to restore some equilibrium than contemplating legal action as a first resort.



But some may be skeptical of a former Meta exec and politician who worked closely with Mark Zuckerberg and still wasnâ€™t able to usher in such changes to social media sites while working for one. What will only compound this skepticism is the selective history found in Cleggâ€™s book, which briefly acknowledges some scandals (like the one surrounding Cambridge Analyticaâ€™s data harvesting from Facebook users in 2016) but refuses to discuss other pertinent ones. For example, Clegg laments the â€œfracturedâ€ nature of the global internet today but fails to acknowledge Facebookâ€™s own role in this splintering.



Breaking up Big Tech through antitrust laws would hinder innovation, says Clegg, arguing that the idea â€œcompletely ignores the benefits users gain from large network effects.â€ Users stick with these outsize channels because they can find â€œmost of what theyâ€™re looking for,â€ he writes, like friends and content on social media and cheap consumer goods on Amazon and eBay.



Wu might concede this point, but he would disagree with Cleggâ€™s claims that maintaining the status quo is beneficial to users. â€œThe traditional logic of antitrust law doesnâ€™t work,â€ Clegg insists. Instead, he believes less sweeping regulation can help make Big Tech less dangerous while ensuring a better user experience.



Clegg has seen both sides of the regulatory coin: He worked in David Cameronâ€™s government passing national laws for technology companies to follow and then moved to Meta to help the company navigate those types of nation-specific obligations. He bemoans the hassle and complexity Silicon Valley faces in trying to comply with differing rules across the globe, some set by â€œAmerican federal agenciesâ€ and others by â€œIndian nationalists.â€



But with the resources such companies command, surely they are more than equipped to cope? Given that Meta itself has previously meddled in access to the internet (such as in India, whose telecommunications regulator ultimately blocked its Free Basics internet service for violating net neutrality rules), this complaint seems suspect coming from Clegg. What should be the real priority, he argues, is not any new nation-specific laws but a global â€œtreaty that protects the free flow of data between signatory countries.â€




What the former Meta executive Nick Clegg advocatesâ€”unsurprisinglyâ€”is not a breakup of Big Tech but a push for it to become â€œradically transparent.â€




Clegg believes that these nation-specific technology obligationsâ€”a recent one is Australiaâ€™s ban on social media for people under 16â€”usually reflect fallacies about the technologyâ€™s human impact, a subject that can be fraught with anxiety. Such laws have proved ineffective and tend to taint the publicâ€™s understanding of social networks, he says. There is some truth to his argument here, but reading a book in which a former Facebook executive dismisses techno-determinismâ€”that is, the argument that technology makes people do or think certain thingsâ€”may be cold comfort to those who have seen the harm technology can do.



In any case, Cleggâ€™s defensiveness about social networks may not gain much favor from users themselves. He stresses the need for more personal responsibility, arguing that Meta doesnâ€™t ever intend for users to stay on Facebook or Instagram endlessly: â€œHow long you spend on the app in a single session is not nearly as important as getting you to come back over and over again.â€ Social media companies want to serve you content that is â€œmeaningful to you,â€ he claims, not â€œsimply to give you a momentary dopamine spike.â€ All this feels disingenuous at best.



What Clegg advocatesâ€”unsurprisinglyâ€”is not a breakup of Big Tech but a push for it to become â€œradically transparent,â€ whether on its own or, if necessary, with the help of federal legislators. He also wants platforms to bring users more into their governance processes (by using Facebookâ€™s model of community forums to help improve their apps and products, for example). Finally, Clegg also wants Big Tech to give users more meaningful control of their data and how companies such as Meta can use it.



Here Clegg shares common ground with the inventor of the web, Tim Berners-Lee, whose own proposal for reform advances a technically specific vision for doing just that. In his memoir/manifesto This Is for Everyone: The Unfinished Story of the World Wide Web, Berners-Lee acknowledges that his initial visionâ€”of a technology he hoped would remain open-source, collaborative, and completely decentralizedâ€”is a far cry from the web that we know today.



This Is for Everyone: The Unfinished Story of the World Wide WebTim Berners-LeeFARRAR, STRAUS &#038; GIROUX, 2025




If thereâ€™s any surviving manifestation of his original project, he says, itâ€™s Wikipedia, which remains â€œprobably the best single example of what I wanted the web to be.â€ His best idea for moving power from Silicon Valley platforms into the hands of users is to give them more data control. He pushes for a universal data â€œpodâ€ he helped develop, known as â€œSolidâ€ (an abbreviation of â€œsocial linked dataâ€).



The systemâ€”which was originally developed at MITâ€”would offer a central site where people could manage data ranging from credit card information to health records to social media comment history. â€œRather than have all this stuff siloed off with different providers across the web, youâ€™d be able to store your entire digital information trail in a single private repository,â€ Berners-Lee writes.



The Solid product may look like a kind of silver bullet in an age when data harvesting is familiar and data breaches are rampant. Placing greater control with users and enabling them to see â€œwhat data [i]s being generated about themâ€ does sound like a tantalizing prospect.



But some people may have concerns about, for example, merging their confidential health records with data from personal devices (like heart rate info from a smart watch). No matter how much user control and decentralization Berners-Lee may promise, recent data scandals (such as cases in which period-tracking apps misused clientsâ€™ data) may be on peopleâ€™s minds.





Berners-Lee believes that centralizing user data in a product like Solid could save people time and improve daily life on the internet. â€œAn alien coming to Earth would think it was very strange that I had to tell my phone the same things again and again,â€ he complains about the experience of using different airline apps today.



With Solid, everything from vaccination records to credit card transactions could be kept within the digital vault and plugged into different apps. Berners-Lee believes that AI could also help people make more use of this dataâ€”for example, by linking meal plans to grocery bills. Still, if heâ€™s optimistic on how AI and Solid could coordinate to improve usersâ€™ lives, he is vague on how to make sure that chatbots manage such personal data sensitively and safely.



Berners-Lee generally opposes regulation of the web (except in the case of teenagers and social media algorithms, where he sees a genuine need). He believes in internet usersâ€™ individual right to control their own data; he is confident that a product like Solid could â€œcourse-correctâ€ the web from its current â€œexploitativeâ€ and extractive direction.



Of the three writersâ€™ approaches to reform, it is Wuâ€™s that has shown some effectiveness of late. Companies like Google have been forced to give competitors some advantage through data sharing, and they have now seen limits on how their systems can be used in new products and technologies. But in the current US political climate, will antitrust laws continue to be enforced against Big Tech?



Clegg may get his way on one issue: limiting new nation-specific laws. President Donald Trump has confirmed that he will use tariffs to penalize countries that ratify their own national laws targeting US tech companies. And given the posture of the Trump administration, it doesnâ€™t seem likely that Big Tech will see more regulation in the US. Indeed, social networks have seemed emboldened (Meta, for example, removed fact-checkers and relaxed content moderation rules after Trumpâ€™s election win). In any case, the US hasnâ€™t passed a major piece of federal internet legislation since 1996.



If using anti-monopoly laws through the courts isnâ€™t possible, Cleggâ€™s push for a US-led omnibus dealâ€”setting consensual rules for data and acceptable standards of human rightsâ€”may be the only way to make some more immediate improvements.



In the end, there is not likely to be any single fix for what ails the internet today. But the ideas the three writers agree onâ€”greater user control, more data privacy, and increased accountability from Silicon Valleyâ€”are surely the outcomes we should all fight for.



Nathan Smith is a writer whose work has appeared in the Washington Post, the Economist, and the Los Angeles Times.
â€¢ Transforming commercial pharma with agentic AI
  Rising cost of raw materials and supply chain disruptions are squeezing pharma companies . A wave of expiring patents threatens around $300 billion in potential lost sales by 2030 . McKinsey estimates cost per launch is growing 8% each year, reaching $4 billion in 2022 . Patients and health-care providers are seeking more personalized services, leading to greater demand for precision drugs . Personalization, real-time communication channels offer a way of building trust and reaching HCPs in an increasingly competitive market .
â€¢ The Download: planet hunting, and Indiaâ€™s e-scooters
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



An Earthlingâ€™s guide to planet hunting



The pendant on Rebecca Jensen-Clemâ€™s necklace is composed of 36 silver hexagons entwined in a honeycomb mosaic. At the Keck Observatory, in Hawaii, just as many segments make up a mirror that spans 33 feet, reflecting images of uncharted worlds for her to study.



Jensen-Clem, an astronomer at the University of California, Santa Cruz, works with the Keck Observatory to try to detect new planets without leaving our own. Itâ€™s a pursuit that faces a vast array of obstacles, for example wind, and fluctuations in atmospheric density and temperature.&nbsp;



At her lab among the redwoods, Jensen-Clem and her students experiment with new technologies and software to help overcome the challenges, and see into space more clearly. Read more about her and her work.&nbsp;



â€”Jenna Ahart



This story is from the forthcoming print issue of MIT Technology Review, which is all about the body. If youâ€™re not already a subscriber, sign up now to receive issues as soon as they land.







2025 climate tech companies to watch: Ather Energy and its premium e-scooters



While sales of Tesla or BYD cars drove electric vehicle adoption elsewhere in the world, two-wheelers have led the green energy transition in India.As one of the earliest â€œpure playâ€ e-scooter makers, Ather Energy has helped drive micromobility EV penetration throughout India and boosted the shift away from carbon-emitting vehicles. Read the full story.



â€”Nilesh Christopher



Ather Energy is one of our 10 climate tech companies to watchâ€”our annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 The Trump administration has laid off thousands of federal health workersItâ€™s blaming the government shutdownâ€”but labor unions are suing. (Reuters&nbsp;$)+&nbsp;The firings will decimate parts of the CDC that work on disease surveillance.&nbsp;(STAT)2 AI videos of dead celebrities are upsetting their familiesOpenAI is coming under mounting pressure to restrict what its video generator Sora 2 can create. (WP&nbsp;$)+&nbsp;Sora was downloaded over a million times in less than five daysâ€”a rate even faster than ChatGPT.&nbsp;(BBC)3 The Dutch government has taken control of a Chinese-owned chipmakerThe move comes after Beijing tightened restrictions on the export of rare earth elements, which could hurt Europeâ€™s car industry. (CNBC)4 Why some developers reject AI coding toolsEven at their best, they introduce bugs into the code base that can be tricky to spot. (The Information&nbsp;$)+&nbsp;The second wave of AI coding is here.&nbsp;(MIT Technology Review)



5 Police are begging teens to stop pulling the AI homeless man prankKids are using AI to create images of a disheveled person in their home, then sending that to their parents. (The Verge)



6 How Elon Muskâ€™s embrace of Trump continues to hurt TeslaThe result is that his cars are now more expensive and less desirable. (The Atlantic&nbsp;$)+&nbsp;China might force Tesla to redesign its door handles.&nbsp;(Wired&nbsp;$)+&nbsp;How did China come to dominate the world of electric cars?&nbsp;(MIT Technology Review)&nbsp;7 What happened after schools in Australia banned phones?Both students and staff say the impact has been overwhelmingly positive. (The Guardian)8 AI is fantastic at detecting small earthquakes&nbsp;But the really big prize is seeing if it can help with predicting them, too. (Ars Technica)+&nbsp;What we can learn from Japanâ€™s â€œmegaquakeâ€ preparations.&nbsp;(MIT Technology Review)9 Climate change is creating new hybrid speciesThe â€œgrue jayâ€ is half-blue jay, half-green jay. Itâ€™s also a sign of the times we live in. (Nautilus)+&nbsp;How a breakthrough gene-editing tool will help the world cope with climate change. (MIT Technology Review)10 How people gamify Hinge to get the dates they want&nbsp;It puts peopleâ€™s most promising matches behind a paywallâ€”but theyâ€™re finding workarounds. (The Cut&nbsp;$)+&nbsp;Thereâ€™s now a term for daters who use AI to boost their appeal : chatfishers.&nbsp;(The Guardian)







Quote of the day



â€œItâ€™s as emotionally devastating as it is dangerous to the American public.â€



â€”An employee at the Centers for Disease Control and Prevention tells STAT about the impact of the Trump administrationâ€™s decision to carry out mass layoffs at the agency.Â 







One more thing



GETTY IMAGES




How the federal government is tracking changes in the supply of street drugsIn 2021, the Maryland Department of Health and the state police were confronting a crisis: Fatal drug overdoses in the state were at an all-time high, and authorities didnâ€™t know why.Seeking answers, Maryland officials turned to scientists at the National Institute of Standards and Technology, the national metrology institute for the United States, which defines and maintains standards of measurement essential to a wide range of industrial sectors and health and security applications.There, a research chemist named Ed Sisco and his team had developed methods for detecting trace amounts of drugs, explosives, and other dangerous materialsâ€”techniques that could protect law enforcement officials and others who had to collect these samples. And a pilot uncovered new, critical information almost immediately. Read the full story.â€”Adam Bluestein







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ ThisÂ spicy teaÂ is exactly what you need when youâ€™re under the weather.Â + Just a man,Â jammingÂ with his cat.Â + Empathy is not a fixed traitâ€”you can grow it.Â Hereâ€™s how.+ Thereâ€™s something very soothing about JoÃ£o Bernardinoâ€™sÂ photosÂ of Portugal.
â€¢ An Earthlingâ€™s guide to planet hunting
  The pendant on Rebecca Jensen-Clemâ€™s necklace is only about an inch wide, composed of 36 silver hexagons entwined in a honeycomb mosaic. At the Keck Observatory, in Hawaii, just as many segments make up a mirror that spans 33 feet, reflecting images of uncharted worlds for her to study.&nbsp;



Jensen-Clem, an astronomer at the University of California, Santa Cruz, works with the Keck Observatory to figure out how to detect new planets without leaving our own. Typically, this pursuit faces an array of obstacles: Wind, fluctuations in atmospheric density and temperature, or even a misaligned telescope mirror can create a glare from a starâ€™s light that obscures the view of whatâ€™s around it, rendering any planets orbiting the star effectively invisible. And what light Earthâ€™s atmosphere doesnâ€™t obscure, it absorbs. Thatâ€™s why researchers who study these distant worlds often work with space telescopes that circumvent Earthâ€™s pesky atmosphere entirely, such as the $10 billion James Webb Space Telescope.&nbsp;





But thereâ€™s another way over these hurdles. At her lab among the redwoods, Jensen-Clem and her students experiment with new technologies and software to help Keckâ€™s primary honeycomb mirror and its smaller, â€œdeformableâ€ mirror see more clearly. Using measurements from atmospheric sensors, deformable mirrors are designed to adjust shape rapidly, so they can correct for distortions caused by Earthâ€™s atmosphere on the fly.&nbsp;



This general imaging technique, called adaptive optics, has been common practice since the 1990s. But Jensen-Clem is looking to level up the game with extreme adaptive optics technologies, which are aimed to create the highest image quality over a small field of view. Her group, in particular, does so by tackling issues involving wind or the primary mirror itself. The goal is to focus starlight so precisely that a planet can be visible even if its host star is a million to a billion times brighter.



In April, she and her former collaborator Maaike van Kooten were named co-recipients of the Breakthrough Prize Foundationâ€™s New Horizons in Physics Prize. The prize announcement says they earned this early-career research award for their potential â€œto enable the direct detection of the smallest exoÂ­planetsâ€ through a repertoire of methods the two women have spent their careers developing.&nbsp;



In July, Jensen-Clem was also announced as a member of a new committee for the Habitable Worlds Observatory, a concept for a NASA space telescope that would spend its career on the prowl for signs of life in the universe. Sheâ€™s tasked with defining the missionâ€™s scientific goals by the end of the decade.



The Keck Observatoryâ€™s 10-meter primary mirror features a honeycomb structure with 36 individual mirror segments.ETHAN TWEEDIE




â€œIn adaptive optics, we spend a lot of time on simulations, or in the lab,â€ Jensen-Clem says. â€œItâ€™s been a long road to see that Iâ€™ve actually made things better at the observatory in the past few years.â€



Jensen-Clem has long appreciated astronomy for its more mind-bending qualities. In seventh grade, she became fascinated by how time slows down near a black hole when her dad, an aerospace engineer, explained that concept to her. After starting her bachelorâ€™s degree at MIT in 2008, she became taken with how a distant star can seem to disappearâ€”either suddenly winking out or gently fading away, depending on the kind of object that passes in front of it. â€œIt wasnâ€™t quite exoplanet science, but there was a lot of overlap,â€ she says.




â€œIf you just look up at the night sky and see stars twinkling, itâ€™s happening fast. So we have to go fast too.â€




During this time, Jensen-Clem began sowing the seeds for one of her prize-winning methods after her teaching assistant recommended that she apply for an internship at NASAâ€™s Jet Propulsion Laboratory. There, she worked on a setup that could perfect the orientation of a large mirror. Such mirrors are more difficult to realign than the smaller, deformable ones, whose shape-changing segments cater to Earthâ€™s fluctuating atmosphere.



â€œAt the time, we were saying, â€˜Oh, wouldnâ€™t it be really cool to install one of these at Keck Observatory?â€™â€ Jensen-Clem says. The idea stuck around. She even wrote about it in a fellowship application when she was gearing up to start her graduate work at Caltech. And after years of touch-and-go development, Jensen-Clem succeeded in installing the systemâ€”which uses a technology called a Zernike wavefront sensorâ€”on Keckâ€™s primary mirror about a year ago. â€œMy work as a college intern is finally done,â€ she says.&nbsp;





The system, which is currently used for occasional recalibrations rather than continuous adjustments, includes a special kind of glass plate that bends the light rays from the mirror to reveal a specific pattern. The detector can pick up a hairbreadth misalignment in that picture: If one hexagon is pushed too far back or forward, its brightness changes. Even the tiniest misalignment is important to correct, because â€œwhen youâ€™re studying a faint object, suddenly youâ€™re much more susceptible to little mistakes,â€ Jensen-Clem says.



She has also been working to perfect the craft of molding Keckâ€™s deformable mirror. This instrument, which reflects light thatâ€™s been rerouted from the primary mirror, is much smallerâ€”only six inches wideâ€”and is designed to reposition as often as 2,000 times a second to combat atmospheric turbulence and create the clearest picture possible. â€œIf you just look up at the night sky and see stars twinkling, itâ€™s happening fast. So we have to go fast too,â€ Jensen-Clem says.&nbsp;



Even at this rapid rate of readjustment, thereâ€™s still a lag. The deformable mirror is usually about one millisecond behind the actual outdoor conditions at any given time. â€œWhen the [adaptive optics] system canâ€™t keep up,&nbsp;then you arenâ€™t going to get the best resolution,â€ says van Kooten, Jensen-Clemâ€™s former collaborator, who is now at the National Research Council Canada. This lag has proved especially troublesome on windy nights.&nbsp;



Jensen-Clem thought it was an unsolvable problem. â€œThe reason we have that delay is because we need to run computations and then move the deformable mirror,â€ she says. â€œYouâ€™re never going to do those things instantaneously.â€





But while she was still a postdoc at UC Berkeley, she came across a paper that posited a solution. Its authors proposed that using previous measurements and simple algebra to predict how the atmosphere will change, rather than trying to keep up with it in real time, would yield better results. She wasnâ€™t able to test the idea at the time, but coming to UCSC and working with Keck presented the perfect opportunity.&nbsp;



Around this time, Jensen-Clem invited van Kooten to join her team at UCSC as a postdoc because of their shared interest in the predictive software. â€œI didnâ€™t have a place to live at first, so she put me up in her guest room,â€ van Kooten says. â€œSheâ€™s just so supportive at every level.â€



After creating experimental software to try out at Keck, the team compared the predictive version with the more standard adaptive optics, examining how well each imaged an exoplanet without its drowning in starlight. They found that the predictive software could image even faint exoplanets two to three times more clearly. The results, which Jensen-Clem published in 2022, were part of what earned her the New Horizons in Physics Prize.&nbsp;



Thayne Currie, an astronomer at the University of Texas, San Antonio, says that these new techniques will become especially vital as researchers build bigger and bigger ground-based facilities to capture images of exoplanetsâ€”including upcoming projects such as the Extremely Large Telescope at the European Southern Observatory and the Giant Magellan Telescope in Chile. â€œThereâ€™s an incredible amount that weâ€™re learning&nbsp;about the universe, and it is really driven by technology advances that are very, very&nbsp;new,â€ Currie says. â€œDr. Jensen-Clemâ€™s work is an example of that kind of innovation.â€



In May, one of Jensen-Clemâ€™s graduate students went back to Hawaii to reinstall the predictive software at Keck. This time, the program isnâ€™t just a trial run; itâ€™s there to stay. The new software has shown it can refocus artificial starlight. Next, it will have to prove it can handle the real thing.&nbsp;





And in about a year, Jensen-Clem and her students and colleagues will brace themselves for a flood of observations from the European Space Agencyâ€™s Gaia mission, which recently finished measuring the motion, temperature, and composition of billions of stars over more than a decade.&nbsp;



When the project releases its next set of dataâ€”slated for December 2026â€”Jensen-Clemâ€™s team aims to hunt for new exoplanetary systems using clues like the wobbles in a starâ€™s motion caused by the gravitational tugs of planets orbiting around it. Once a system has been identified, exoplanet photographers will then be able to shoot the hidden planets using a new instrument at Keck that can reveal more about their atmospheres and temperatures.&nbsp;



There will be a mountain of data to sort through, and an even steeper supply of starlight to refocus. Thankfully, Jensen-Clem has spent more than a decade refining just the techniques sheâ€™ll need: â€œThis time next year,â€ she says, â€œweâ€™ll be racing to throw all our adaptive optics tricks at these systems and detect as many of these objects as possible.â€



Jenna Ahart is a science journalist specializing in the physical sciences.&nbsp;

ğŸ”’ Cybersecurity & Privacy
â€¢ DDoS Botnet Aisuru Blankets US ISPs in Record DDoS
  The world&#8217;s largest and most disruptive botnet is now drawing a majority of its firepower from compromised Internet-of-Things (IoT) devices hosted on U.S. Internet providers like AT&amp;T, Comcast and Verizon, new evidence suggests. Experts say the heavy concentration of infected devices at U.S. providers is complicating efforts to limit collateral damage from the botnet&#8217;s attacks, which shattered previous records this week with a brief traffic flood that clocked in at nearly 30 trillion bits of data per second.
Since its debut more than a year ago, the Aisuru botnet has steadily outcompeted virtually all other IoT-based botnets in the wild, with recent attacks siphoning Internet bandwidth from an estimated 300,000 compromised hosts worldwide.
The hacked systems that get subsumed into the botnet are mostly consumer-grade routers, security cameras, digital video recorders and other devices operating with insecure and outdated firmware, and/or factory-default settings. Aisuru&#8217;s owners are continuously scanning the Internet for these vulnerable devices and enslaving them for use in distributed denial-of-service (DDoS) attacks that can overwhelm targeted servers with crippling amounts of junk traffic.
As Aisuru&#8217;s size has mushroomed, so has its punch. In May 2025, KrebsOnSecurity was hit with a near-record 6.35 terabits per second (Tbps) attack from Aisuru, which was then the largest assault that Google&#8217;s DDoS protection service Project Shield had ever mitigated. Days later, Aisuru shattered that record with a data blast in excess of 11 Tbps.
By late September, Aisuru was publicly flexing DDoS capabilities topping 22 Tbps. Then on October 6, its operators heaved a whopping 29.6 terabits of junk data packets each second at a targeted host. Hardly anyone noticed because it appears to have been a brief test or demonstration of Aisuru&#8217;s capabilities: The traffic flood lasted less only a few seconds and was pointed at an Internet server that was specifically designed to measure large-scale DDoS attacks.
A measurement of an Oct. 6 DDoS believed to have been launched through multiple botnets operated by the owners of the Aisuru botnet. Image: DDoS Analyzer Community on Telegram.
Aisuru&#8217;s overlords aren&#8217;t just showing off. Their botnet is being blamed for a series of increasingly massive and disruptive attacks. Although recent assaults from Aisuru have targeted mostly ISPs that serve online gaming communities like Minecraft, those digital sieges often result in widespread collateral Internet disruption.
For the past several weeks, ISPs hosting some of the Internet&#8217;s top gaming destinations have been hit with a relentless volley of gargantuan attacks that experts say are well beyond the DDoS mitigation capabilities of most organizations connected to the Internet today.
Steven Ferguson is principal security engineer at Global Secure Layer (GSL), an ISP in Brisbane, Australia. GSL hosts TCPShield, which offers free or low-cost DDoS protection to more than 50,000 Minecraft servers worldwide. Ferguson told KrebsOnSecurity that on October 8, TCPShield was walloped with a blitz from Aisuru that flooded its network with more than 15 terabits of junk data per second.
Ferguson said that after the attack subsided, TCPShield was told by its upstream provider OVH that they were no longer welcome as a customer.
&#8220;This was causing serious congestion on their Miami external ports for several weeks, shown publicly via their weather map,&#8221; he said, explaining that TCPShield is now solely protected by GSL.
Traces from the recent spate of crippling Aisuru attacks on gaming servers can be still seen at the website blockgametracker.gg, which indexes the uptime and downtime of the top Minecraft hosts. In the following example from a series of data deluges on the evening of September 28, we can see an Aisuru botnet campaign briefly knocked TCPShield offline.
An Aisuru botnet attack on TCPShield (AS64199) on Sept. 28Â  can be seen in the giant downward spike in the middle of this uptime graphic. Image: grafana.blockgametracker.gg.
Paging through the same uptime graphs for other network operators listed shows almost all of them suffered brief but repeated outages around the same time. Here is the same uptime tracking for Minecraft servers on the network provider Cosmic (AS30456), and it shows multiple large dips that correspond to game server outages caused by Aisuru.
Multiple DDoS attacks from Aisuru can be seen against the Minecraft host Cosmic on Sept. 28. The sharp downward spikes correspond to brief but enormous attacks from Aisuru. Image: grafana.blockgametracker.gg.
BOTNETS R US
Ferguson said he&#8217;s been tracking Aisuru for about three months, and recently he noticed the botnet&#8217;s composition shifted heavily toward infected systems at ISPs in the United States. Ferguson shared logs from an attack on October 8 that indexed traffic by the total volume sent through each network provider, and the logs showed that 11 of the top 20 traffic sources were U.S. based ISPs.
AT&amp;T customers were by far the biggest U.S. contributors to that attack, followed by botted systems on Charter Communications, Comcast, T-Mobile and Verizon, Ferguson found. He said the volume of data packets per second coming from infected IoT hosts on these ISPs is often so high that it has started to affect the quality of service that ISPs are able to provide to adjacent (non-botted) customers.
&#8220;The impact extends beyond victim networks,&#8221; Ferguson said. &#8220;For instance we have seen 500 gigabits of traffic via Comcast&#8217;s network alone. This amount of egress leaving their network, especially being so US-East concentrated, will result in congestion towards other services or content trying to be reached while an attack is ongoing.&#8221;
Roland Dobbins is principal engineer at Netscout. Dobbins said Ferguson is spot on, noting that while most ISPs have effective mitigations in place to handle large incoming DDoS attacks, many are far less prepared to manage the inevitable service degradation caused by large numbers of their customers suddenly using some or all available bandwidth to attack others.
&#8220;The outbound and cross-bound DDoS attacks can be just as disruptive as the inbound stuff,&#8221; Dobbin said. &#8220;We&#8217;re now in a situation where ISPs are routinely seeing terabit-per-second plus outbound attacks from their networks that can cause operational problems.&#8221;
&#8220;The crying need for effective and universal outbound DDoS attack suppression is something that is really being highlighted by these recent attacks,&#8221; Dobbins continued. &#8220;A lot of network operators are learning that lesson now, and there&#8217;s going to be a period ahead where there&#8217;s some scrambling and potential disruption going on.&#8221;
KrebsOnSecurity sought comment from the ISPs named in Ferguson&#8217;s report. Charter Communications pointed to a recent blog post on protecting its network, stating that Charter actively monitors for both inbound and outbound attacks, and that it takes proactive action wherever possible.
&#8220;In addition to our own extensive network security, we also aim to reduce the risk of customer connected devices contributing to attacks through our Advanced WiFi solution that includes Security Shield, and we make Security Suite available to our Internet customers,&#8221; Charter wrote in an emailed response to questions. &#8220;With the ever-growing number of devices connecting to networks, we encourage customers to purchase trusted devices with secure development and manufacturing practices, use anti-virus and security tools on their connected devices, and regularly download security patches.&#8221;
A spokesperson for Comcast responded, &#8220;Currently our network is not experiencing impacts and we are able to handle the traffic.&#8221;
9 YEARS OF MIRAI
Aisuru is built on the bones of malicious code that was leaked in 2016Â by the original creators of the Mirai IoT botnet. Like Aisuru, Mirai quickly outcompeted all other DDoS botnets in its heyday, and obliterated previous DDoS attack records with a 620 gigabit-per-second siege that sidelined this website for nearly four days in 2016.
The Mirai botmasters likewise used their crime machine to attack mostly Minecraft servers, but with the goal of forcing Minecraft server owners to purchase a DDoS protection service that they controlled. In addition, they rented out slices of the Mirai botnet to paying customers, some of whom used it to mask the sources of other types of cybercrime, such as click fraud.
A depiction of the outages caused by the Mirai botnet attacks against the internet infrastructure firm Dyn on October 21, 2016. Source: Downdetector.com.
Dobbins said Aisuru&#8217;s owners also appear to be renting out their botnet as a distributed proxy network that cybercriminal customers anywhere in the world can use to anonymize their malicious traffic and make it appear to be coming from regular residential users in the U.S.
&#8220;The people who operate this botnet are also selling (it as) residential proxies,&#8221; he said. &#8220;And that&#8217;s being used to reflect application layer attacks through the proxies on the bots as well.&#8221;
The Aisuru botnet harkens back to its predecessor Mirai in another intriguing way. One of its owners is using the Telegram handle &#8220;9gigsofram,&#8221; which corresponds to the nickname used by the co-owner of a Minecraft server protection service called Proxypipe that was heavily targeted in 2016 by the original Mirai botmasters.
Robert Coelho co-ran Proxypipe back then along with his business partner Erik &#8220;9gigsofram&#8221; Buckingham, and has spent the past nine years fine-tuning various DDoS mitigation companies that cater to Minecraft server operators and other gaming enthusiasts. Coelho said he has no idea why one of Aisuru&#8217;s botmasters chose Buckingham&#8217;s nickname, but added that it might say something about how long this person has been involved in the DDoS-for-hire industry.
&#8220;The Aisuru attacks on the gaming networks these past seven day have been absolutely huge, and you can see tons of providers going down multiple times a day,&#8221; Coelho said.
Coelho said the 15 Tbps attack this week against TCPShield was likely only a portion of the total attack volume hurled by Aisuru at the time, because much of it would have been shoved through networks that simply couldn&#8217;t process that volume of traffic all at once. Such outsized attacks, he said, are becoming increasingly difficult and expensive to mitigate.
&#8220;It&#8217;s definitely at the point now where you need to be spending at least a million dollars a month just to have the network capacity to be able to deal with these attacks,&#8221; he said.
RAPID SPREAD
Aisuru has long been rumored to use multiple zero-day vulnerabilities in IoT devices to aid its rapid growth over the past year. XLab, the Chinese security company that was the first to profile Aisuru&#8217;s rise in 2024, warned last month that one of the Aisuru botmasters had compromised the firmware distribution website for Totolink, a maker of low-cost routers and other networking gear.
&#8220;Multiple sources indicate the group allegedly compromised a router firmware update server in April and distributed malicious scripts to expand the botnet,&#8221; XLab wrote on September 15. &#8220;The node count is currently reported to be around 300,000.&#8221;
A malicious script implanted into a Totolink update server in April 2025. Image: XLab.
Aisuru&#8217;s operators received an unexpected boost to their crime machine in August when the U.S. Department JusticeÂ charged the alleged proprietor of Rapper Bot, a DDoS-for-hire botnet that competed directly with Aisuru for control over the global pool of vulnerable IoT systems.
Once Rapper Bot was dismantled, Aisuru&#8217;s curators moved quickly to commandeer vulnerable IoT devices that were suddenly set adrift by the government&#8217;s takedown, Dobbins said.
&#8220;Folks were arrested and Rapper Bot control servers were seized and that&#8217;s great, but unfortunately the botnet&#8217;s attack assets were then pieced out by the remaining botnets,&#8221; he said. &#8220;The problem is, even if those infected IoT devices are rebooted and cleaned up, they will still get re-compromised by something else generally within minutes of being plugged back in.&#8221;
A screenshot shared by XLabs showing the Aisuru botmasters recently celebrating a record-breaking 7.7 Tbps DDoS. The user at the top has adopted the name &#8220;Ethan J. Foltz&#8221; in a mocking tribute to the alleged Rapper Bot operator who was arrested and charged in August 2025.
BOTMASTERS AT LARGE
XLab&#8217;s September blog post cited multiple unnamed sources saying Aisuru is operated by three cybercriminals: &#8220;Snow,&#8221; who&#8217;s responsible for botnet development; &#8220;Tom,&#8221; tasked with finding new vulnerabilities; and &#8220;Forky,&#8221; responsible for botnet sales.
KrebsOnSecurity interviewed Forky in our May 2025 story about the record 6.3 Tbps attack from Aisuru. That story identified Forky as a 21-year-old man from Sao Paulo, Brazil who has been extremely active in the DDoS-for-hire scene since at least 2022. The FBI has seized Forky&#8217;s DDoS-for-hire domains several times over the years.

Like the original Mirai botmasters, Forky also operates a DDoS mitigation service called Botshield. Forky declined to discuss the makeup of his ISPâ€™s clientele, or to clarify whether Botshield was more of a hosting provider or a DDoS mitigation firm. However, Forky has posted on Telegram about Botshield successfully mitigating large DDoS attacks launched against other DDoS-for-hire services.
In our previous interview, Forky acknowledged being involved in the development and marketing of Aisuru, but denied participating in attacks launched by the botnet.
Reached for comment earlier this month, Forky continued to maintain his innocence, claiming that he also is still trying to figure out who the current Aisuru botnet operators are in real life (Forky said the same thing in our May interview).
But after a week of promising juicy details, Forky came up empty-handed once again. Suspecting that Forky was merely being coy, I asked him how someone so connected to the DDoS-for-hire world could still be mystified on this point, and suggested that his inability or unwillingness to blame anyone else for Aisuru would not exactly help his case.
At this, Forky verbally bristled at being pressed for more details, and abruptly terminated our interview.
&#8220;I&#8217;m not here to be threatened with ignorance because you are stressed,&#8221; Forky replied. &#8220;They&#8217;re blaming me for those new attacks. Pretty much the whole world (is) due to your blog.&#8221;

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Transforming the physical world with AI: the next frontier in intelligent automation
  The convergence of artificial intelligence with physical systems marks a pivotal moment in technological evolution. Physical AI, where algorithms transcend digital boundaries to perceive, understand, and manipulate the tangible world, will fundamentally transform how enterprises operate across industries. These intelligent systems bridge the gap between digital intelligence and physical reality, unlocking unprecedented opportunities for efficiency and innovation. For many organizations, this opens the door to entirely new ways to delight their customers and, in turn, transform entire industries. 
To accelerate this transformation, the AWS Generative AI Innovation Center, MassRobotics, and NVIDIA launched the Physical AI Fellowship, providing crucial support to startups developing next-generation robotics and automation solutions. We are pleased to be working with our first cohort fellows: 
 
 Bedrock Robotics â€“ provides same-day hardware and software installation to provide autonomy to existing construction equipment fleets 
 Blue Water Autonomy â€“ integrating hardware, software, and AI to enable uncrewed ships to operate on the open ocean for months at a time 
 Diligent Robotics â€“ develop foundation models for autonomous humanoid robots in dynamic, human-facing environments 
 Generalist AI â€“ developing end-to-end AI foundation models toward general-purpose robots, starting with a focus on dexterity 
 RobCo â€“ offering modular hardware and a no-code system to automate tasks such as machine tending, palletizing, dispensing, or welding without upfront investment or specialist expertise 
 Tutor Intelligence â€“ building AI-powered robots to help manufacturers and warehouses obtain immediate returns on investment 
 Wandercraft â€“ developing exoskeletons to help with rehabilitation and restoring walking ability at home and in outpatient centers 
 Zordi â€“ combining AI, robotics, and machine learning to innovate greenhouse agriculture 
 
For businesses and public sector organizations, this convergence of AI and physical systems goes beyond incremental improvements, fundamentally rethinking whatâ€™s possible in their operations and customer experiences. 
The Physical AI spectrum: from automation to true intelligence 
 
As organizations evaluate their Physical AI initiatives, understanding where different solutions fall on the capability spectrum is crucial for strategic planning. Each level represents a distinct leap in autonomy and sophistication: 
 
 Level 1: Basic Physical Automation:&nbsp;This foundational stage involves systems that perform predefined tasks in tightly controlled environments. Think of industrial robots on assembly linesâ€”highly efficient, but rigid and entirely dependent on human programming and oversight. 
 Level 2: Adaptive Physical Automation:&nbsp;At this stage, systems gain flexibility in task sequencing. While individual actions are still preprogrammed, they can adjust their order based on real-time environmental cues. Collaborative robots that change behavior when humans are nearby is a prime example. 
 Level 3: Partially Autonomous Physical AI:&nbsp;Here, systems demonstrate intelligent behavior, including planning, executing, and adapting tasks with limited human input. Robots that learn new processes through demonstration highlight this emerging autonomy. 
 Level 4: Fully Autonomous Physical AI:&nbsp;The most advanced level features systems capable of operating across varied domains with minimal supervision. These systems adapt fluidly to new scenarios and environmental changes. Although most commercial solutions remain at Levels 1 or 2, momentum toward full autonomy is accelerating. 
 
Enabling technologies: the building blocks of Physical AI 
The progression from basic automation to full autonomy requires sophisticated technological foundations. Several key innovations are driving this evolution: 
 
 Advanced control theory facilitates precise and reliable actuation. 
 High-fidelity perception models, powered by multimodal sensors, enable machines to interpret complex environments. 
 Edge AI accelerators support real-time inference at the point of action, crucial for latency-sensitive tasks. 
 Foundation models, trained on multimodal datasets, help provide generalizable intelligence across domains. 
 Digital twin systems play a pivotal role in enabling simulation, validation, and optimization of physical systems before real-world deployment, significantly accelerating development cycles. 
 
Industry forces and investment momentum 
Physical AI sits at the intersection of multiple high-growth industries, with the AI Robots sector alone projected to reach a staggering $124.26 billion by 2034. Alongside this, the closely related Digital Twin Technology industry is set to hit an even more impressive $379 billion in the same timeframe. These projections signal a fundamental shift in how enterprises approach automation, efficiency, and digital transformation. 
Investors are keenly aware of this potential, focusing their attention on several key themes within the Physical AI space. Humanoid robotics has emerged as a particularly exciting frontier, with startups securing substantial funding rounds to develop general-purpose robotic workers capable of seamlessly operating in environments designed for humans. Simultaneously, thereâ€™s growing interest in foundation models for robotics â€“ the development of sophisticated â€œrobot brainsâ€ that can adapt to various tasks and control diverse robotic systems. This push towards more flexible, intelligent systems is complemented by continued investment in vertical-specific applications, where companies are leveraging Physical AI to address acute industry challenges, from streamlining warehouse logistics to revolutionizing agricultural practices. The breadth of Physical AIâ€™s potential is further demonstrated by emerging applications in fields as diverse as surgical robotics, autonomous delivery systems, and advanced defense technologies. This expansion into new domains underscores the versatility and transformative power of Physical AI across sectors. 
Real-world impact: quantifying the Physical AI transformation 
While investment trends signal strong future potential, Physical AI is already delivering concrete results across industries. For example, Amazonâ€™s&nbsp;supply chain has boosted efficiency by 25% through intelligent automation, while Foxconn cut manufacturing deployment times by 40%. In healthcare, AI-assisted procedures have led to 30% fewer complications and 25% shorter surgery durations, showcasing transformative outcomes. 
According to a 2024 AI in manufacturing &amp; energy report, 64% of manufacturers using AI in production already report positive ROI, with nearly one-third expecting returns of $2 to $5 for every dollar invested. These gains translate into efficiency improvements between 20-40%, cost savings of 15-30%, and the rise of innovative business models like Robot-as-a-Service. 
In retail, digital twins are being used to explore the impact of different store layouts on shopper behavior and to test the integration of Physical AI with autonomous inventory management systems, helping retailers optimize their physical spaces and operations. Meanwhile, agriculture benefits from advancements in precision farming, crop monitoring, and automated harvestingâ€”further highlighting Physical AIâ€™s broad and growing impact. 
The next frontier 
The impact of Physical AI is already evident across industries, with organizations moving well beyond proofs-of-concept to delivering measurable business value. For participating cohorts, the Physical AI Fellowship will play a key role in helping innovative startups accelerate the path from research to commercial applications of this emerging technology. For enterprises of different sizes and sectors, successful integration of AI with physical systems will define industry leaders in the decade to come. 
Learn more:&nbsp; 
Contact us to learn more about evaluating if your organization is set up to work as teammates, or if youâ€™d like to dive deeper into skill development and risk posture for your physical AI plans. 
Learn more about the Generative AI Innovation Center and how we provide expert tailored support from experimentation to production. 
 
About the authors 
Sri Elaprolu is a technology leader with over 25 years of experience spanning artificial intelligence, machine learning, and software engineering. As Director of the AWS Generative AI Innovation Center, Sri leads a global team of ML scientists and engineers applying the latest advances in generative AI to solve complex challenges for enterprises and the public sector. 
Alla Simoneau is a technology and commercial leader with over 15 years of experience, currently serving as the Emerging Technology Physical AI Lead at Amazon Web Services (AWS), where she drives global innovation at the intersection of AI and real-world applications. With over a decade at Amazon, Alla is a recognized leader in strategy, team building, and operational excellence, specializing in turning cutting-edge technologies into real-world transformations for startups and enterprise customers. 
Paul Amadeo is a seasoned technology leader with over 30 years of experience spanning artificial intelligence, machine learning, IoT systems, RF design, optics, semiconductor physics, and advanced engineering. As Technical Lead for Physical AI in the AWS Generative AI Innovation Center, Paul specializes in translating AI capabilities into tangible physical systems, guiding enterprise customers through complex implementations from concept to production. His diverse background includes architecting computer vision systems for edge environments, designing robotic smart card manufacturing technologies that have produced billions of devices globally, and leading cross-functional teams in both commercial and defense sectors. Paul holds an MS in Applied Physics from the University of California, San Diego, a BS in Applied Physics from Caltech, and holds six patents spanning optical systems, communication devices, and manufacturing technologies. 
Randi Larson bridges the gap between AI innovation and executive strategy at the AWS Generative AI Innovation Center, shaping how organizations understand and translate technical breakthroughs into business value. She combines strategic storytelling with data-driven insight through global keynotes, Amazonâ€™s first tech-for-good podcast, and conversations with industry and Amazon leaders on AI transformation. Before Amazon, Randi refined her analytical precision as a Bloomberg journalist and advisor to economic institutions, think tanks, and family offices on technology initiatives. Randi holds an MBA from Duke Universityâ€™s Fuqua School of Business and a B.S. in Journalism and Spanish from Boston University.
â€¢ Medical reports analysis dashboard using Amazon Bedrock, LangChain, and Streamlit
  In healthcare, the ability to quickly analyze and interpret medical reports is crucial for both healthcare providers and patients. While medical reports contain valuable information, they often remain underutilized due to their complex nature and the time-intensive process of analysis. This complexity manifests in several ways: the interpretation of multiple parameters and their relationships (such as various blood cell counts), the comparison of test results against standard reference ranges, and the need to analyze trends in health parameters over time. To address this challenge, weâ€™ve conceptualized a medical reports analysis dashboard that illustrates how healthcare providers could enhance their interaction with medical data through a sample implementation 
In this post, the created dashboard represents a convergent solution that brings together the power of Amazon Bedrock advanced AI capabilities, LangChainâ€˜s document processing, and Streamlitâ€˜s intuitive user interface. By using these technologies, weâ€™ve created a system that not only stores and displays medical reports, but actively helps interpret them through natural language interactions and dynamic visualizations. 
Solution overview 
At the solutionâ€™s foundation are various large language models available through Amazon Bedrock, including Anthropicâ€™s Claude series and Amazon Nova Foundation Models. You can select from options such as Claude Opus 4.1, Claude 3.7 Sonnet, Amazon Nova Pro, and others, each optimized for different performance and capability requirements. The chosen model processes natural language queries with medical context awareness, enabling detailed interpretation of healthcare data. With this flexibility, you can balance factors like accuracy, speed, and cost based on your specific needs. This is enhanced by LangChainâ€™s document processing capabilities, which manage the retrieval system and maintain conversation context, facilitating accurate and relevant responses. 
The solutionâ€™s data flow begins with medical reports securely stored in Amazon Simple Storage Service (Amazon S3), which are then processed through LangChainâ€™s document handling system. When you interact with the Streamlit frontend, your queries are analyzed by Amazon Bedrock, while LangChain maintains the conversation context and manages document retrieval. The system processes this information and presents results through an intuitive interface featuring interactive visualizations. 
These visualizations, powered by Plotly, include range comparison charts that clearly display normal versus actual values, bar charts for parameter comparisons, and trend lines for tracking changes over time. The Streamlit interface ties everything together, providing real-time interaction with the AI system while managing user session state and conversation history. This comprehensive approach helps ensure that medical professionals can quickly access, analyze, and interpret their medical reports through natural language queries while viewing supporting visual data. 
The following is the architecture diagram of the solution that has four layers: 
 
 User Interface Layer: Streamlit Web App, Chat interface, Plotly data visualizations 
 Processing Layer: LangChain document processing, Conversation retrieval chain, Data parsing 
 AI/ML Layer: Amazon Bedrock, Amazon Bedrock embeddings, In-memory vector store 
 Storage Layer: Amazon S3 for medical reports, Conversation buffer memory 
 
 
Prerequisites 
Before deploying the Medical Reports Analysis Dashboard, you need: 
 
 An AWS account with Amazon Bedrock access enabled 
 AWS Identity and Access Management (IAM) permission for Amazon Bedrock and Amazon S3 
 AWS Command Line Interface (AWS CLI) installed and configured 
 An Amazon S3 bucket for storing medical reports in csv format 
   
   Follow Creating a general purpose bucket to create a bucket. 
   Sample reports provided are in the following repository. The command needed to upload reports is in the deployment section. 
    
 Python 3.9 or later with pip 
 Access to Amazon Bedrock Models. The solution supports multiple models including: 
   
   Anthropicâ€™s Claude series (Opus 4.1, 3.7 Sonnet, Sonnet 4, and so on.) 
   Amazon Nova foundation model series (Nova Pro and Nova Lite) 
    
 
Weâ€™ll be using a Python virtual environment (venv) for this project to provide a clean, isolated environment. Virtual environments help avoid package conflicts between projects and make dependency management more straightforward. While weâ€™re using Pythonâ€™s built-in venv, you could alternatively use miniconda or other environment managers. 
Deployment 
To get started with deployment, install the necessary packages on a local machine. 
 
 Clone the repository: 
 
 
 git clone https://github.com/aws-samples/sample-medical-analysis-dashboard.git 
 
 
 Navigate to the project directory. 
 Create and activate a virtual environment (recommended): 
 
For Mac/Linux: 
 
 python3 -m venv venv
source venv/bin/activate 
 
For Windows: 
 
 python3 -m venv venv
venv\Scripts\activate 
 
 
 Update pip to the latest version: 
 
 
 python3 -m pip install --upgrade pip 
 
 
 Install required packages: 
 
 
 pip install -r requirements.txt 
 
Projectâ€™s dependencies are listed in requirements.txt: 
 
 boto3 
 streamlit 
 unstructured 
 langchain-aws 
 langchain-community 
 pandas 
 plotly 
 numpy 
 docarray 
 
These packages will handle AWS integration, web interface, data processing, and visualizations. Theyâ€™ll be installed in our virtual environment during the deployment process. This setup helps ensure that the components are properly installed and isolated in a virtual environment for optimal performance. 
 
 Follow Configuring environment variables for the AWS CLI to configure AWS credentials. 
 
 
 export AWS_ACCESS_KEY_ID='your-access-key'
export AWS_SECRET_ACCESS_KEY='your-secret-key' 
 
 
 Upload sample CSV files to the S3 bucket created in prerequisites section: 
 
Our repository contains two sample files: 
 
 basic_test.csv: Complete blood work with 15 parameters 
 blood_test.csv with basic parameters 
 
The following is the content of basic_test.csv: 
 
 Parameter,Value,Reference_Range,Unit
Hemoglobin,13.8,13.5-17.5,g/dL
RBC,4.8,4.5-5.9,million/ÂµL
WBC,8500,4000-11000,cells/ÂµL
Glucose,92,70-100,mg/dL
Creatinine,1.0,0.7-1.3,mg/dL 
 
Run the following commands to upload sample files to the S3 bucket: 
 
 aws s3 cp basic_test.csv s3://BUCKET_NAME/

aws s3 cp blood_test.csv s3://BUCKET_NAME/ 
 
Go to app.py line 68 and update the S3 bucket name in app.py to match your actual S3 bucket name. 
 
 BUCKET_NAME = "your-bucket-name" 
 
 
 Run the application: 
 
 
 streamlit run app.py 
 
The dashboard will be available at http://localhost:8501. You can now interact with your medical reports through the web interface. 
Using the dashboard 
This section walks through the key features and demonstrates how to effectively use the dashboard for medical data analysis. 
Dashboard interface overview 
The following figures show the complete dashboard where the selected medical report is blood_test.csv from the repo showing the navigation pane and main content. The first figure also shows the first two graphs. 
 
The following figure shows the second graph of the three that are included in this dashboard. 
 
The dashboard interface is organized into three main sections for medical report analysis: 
 
 Document selection and model choice (navigation pane) 
   
   Selection of Amazon Bedrock model (for example: Claude Opus 4.1, Claude 3.7 Sonnet, or Amazon Nova Pro) 
   List of available medical reports in a dropdown menu 
   Currently analyzing blood_test.csv 
   Token usage display (input, output, and total tokens) 
    
 Chat analysis section 
   
   Clean chat interface for natural language queries 
   History of conversation maintained 
   Clear response formatting 
    
 Visualization area 
   
   Range comparison chart showing normal compared to actual values 
   Bar chart displaying the parameters 
   Trend lines for multiple parameters 
    
 
Context-aware query system 
The dashboardâ€™s AI-powered query system demonstrates sophisticated understanding of medical reports through natural conversations. Hereâ€™s a sequence of interactions showing the systemâ€™s capabilities. 
Question 1: Initial query about hemoglobin: 
 
 What is the hemoglobin level in report? 
 
 
Question 2: Follow-up question demonstrating context awareness: 
 
 How does this compare to other parameters in the report? Are there any that stand out? 
 
 
Question 3: Complex analysis request: 
 
 Can you analyze the distribution patterns of percentage-based measurements versus absolute values in this report, and identify any notable patterns in their reference ranges? 
 
 
The system maintains conversation context while providing detailed insights from the medical reports, supporting responses with relevant data visualizations. 
The solution can be further enhanced by fine-tuning the foundational model on organization-specific medical data, clinical questions, and domain expertise. This specialized training helps the model better understand medical terminology, standard protocols, and institution-specific practices. Additionally, organizations can use pre-trained medical LLMs available in AWS Marketplace, which are specifically optimized for healthcare use cases. When combined with the systemâ€™s existing capabilities, these specialized models can provide contextually relevant responses to medical queries while maintaining compliance with healthcare data governance requirements. 
Amazon Bedrock guardrails should be configured to restrict the model from providing medical advice, prescriptions, or diagnoses, making sure responses are limited to data analysis and interpretation only. 
Security considerations 
While our current deployment uses dummy medical data for demonstration purposes, itâ€™s crucial to consider security and compliance measures for real-world healthcare applications. Here are recommendations for enhancing security in a production environment: 
Data privacy: 
 
 HIPAA compliance: Implement HIPAA-compliant practices, including access controls and audit trails. 
 Encryption: Use Amazon S3 server-side encryption (SSE-S3) for data at rest and TLS for data in transit. 
 Personally identifiable information (PII) protection: 
   
   Apply data masking for PII fields. 
   Control data access through role-based permissions. 
   Monitor model invocation using CloudWatch Logs and Amazon S3. 
   Configure Amazon Bedrock Guardrails. You can use guardrails to also restrict the model from providing medical advice, prescriptions, or diagnoses, limiting responses to data analysis and interpretation only. 
    
 Amazon S3 Configuration: Secure your medical data storage with the following S3 bucket settings 
   
   Enable versioning to maintain a complete audit trail and protect against accidental deletions or modifications 
   Block public access at both bucket and account levels 
   Implement strict bucket policies that limit access to specific IAM roles and enforce encryption in transit 
   Configure encryption (AES-256 or KMS) for all objects uploaded to the bucket 
    
 
Recommended AWS security implementation: 
 
 IAM roles: Create specific IAM roles following the principle of least for each service 
 S3 bucket encryption: Enable default AES-256 encryption for all objects 
 Amazon Bedrock API access: Secure access using IAM roles and proper API key management 
 Audit logging: Activate AWS CloudTrail for comprehensive API call logging. 
   
   Log data access events on S3 buckets, Amazon Bedrock API calls, and IAM user and role activities 
   Monitor and record management events for S3 bucket configuration changes and policy updates 
    
 
These are general recommendations. For a production healthcare application, consult with security experts and conduct a risk assessment to make sure all relevant compliance standards are met. 
Clean up 
To avoid ongoing AWS charges, follow these steps to clean up the resources created: 
 
 Delete the created Amazon S3 bucket 
 Delete the created local resources: 
 
 
 # Deactivate virtual environment
deactivate
# Remove project directory and virtual environment
rm -rf medical-analysis-dashboard/ 
 
Conclusion 
In this post, we demonstrated the development of a conceptual Medical Reports Analysis Dashboard that combines Amazon Bedrock AI capabilities, LangChainâ€™s document processing, and Streamlitâ€™s interactive visualization features. The solution transforms complex medical data into accessible insights through a context-aware chat system powered by large language models available through Amazon Bedrock and dynamic visualizations of health parameters. 
This project showcases how cloud and AI technologies can be applied to healthcare analytics, making medical report interpretation more intuitive and efficient. While our implementation uses dummy data for demonstration purposes, the architecture provides a foundation for building secure, compliance-aligned healthcare applications that can be enhanced to meet healthcare organizational requirements and security protocols. 
 
About the authors 
Aditya Ranjan is a Delivery Consultant with AWS, specializing in distributed systems architecture and cloud-native solutions. He collaborates with customers to design and implement well-architected technical solutions using AWSâ€™s latest technologies, including generative AI services, enabling them to achieve their business goals and objectives. 
Shubham Tiwari is a Solutions Architect at AWS specializing in Modernisation, containers and Security. He has been helping customers in deploying highly scalable, resilient and cost optimised architecture on AWS.
â€¢ Kitsa transforms clinical trial site selection with Amazon Quick Automate
  This post was written with Ajay Nyamati from Kitsa. 
The clinical trial industry conducts medical research studies to evaluate the safety, efficacy, and effectiveness of new drugs, treatments, or medical devices before they reach the market. The industry is a cornerstone of medical innovation, yet it continues to face a fundamental bottleneck: selection of the right trial sites based on the requirement by clinical trial sponsors and contract research organizations (CROs). 
Although there are tens of thousands of potential research sites worldwide, the decision-making process is still heavily influenced by personal networks, limited visibility, and incomplete data. The result is delayed trial launches, underutilized site capacity, and missed opportunities for both sponsors and research centers. 
Key challenges in site selection include: 
 
 Data fragmentation: Site performance and operational data are scattered across siloed systems, inconsistent formats, and unstructured online sources. 
 Manual effort and low coverage: Sponsors and CROs often review only a fraction of the available sites due to the time and cost of manual analysis. 
 Over-reliance of Key Opinion Leaders (KOLs): Personal preference and relationships often outweigh objective performance metrics. 
 Missed opportunities for capable sites: Many high-quality sites are overlooked because they lack a centralized platform to showcase their capabilities. 
 Knowledge hoarding: Organizations with large datasets often keep them proprietary, limiting industry-wide progress. 
 
In this post, weâ€™ll show how Kitsa used Amazon Quick Automate to transform their clinical trial site selection solution. Amazon Quick Automate, a capability of Amazon Quick Suite, enables enterprises to build, deploy and maintain resilient workflow automations at scale. Amazon Quick Suite helps business users make better decisions faster and act on them by unifying AI agents for research, business insights, and automation into a single experience. 
Kitsa, a health-tech company specializing in AI-driven clinical trial recruitment and site selection, is tackling the challenge in site selection. By combining demographic data, disease prevalence insights, historical trial performance, and operational site metrics, Kitsa has developed an agentic analytics engine that matches sponsors with the most suitable sites for their studies. This approach requires consolidating and analyzing data from hundreds of fragmented sources, including websites of clinical trial sites, clinical trial registries, investigator resumes, regulatory filings, publications, and conference abstracts. Traditionally, this has been a slow, manual process that pushed trial start dates by months. 
To address this, Kitsa turned to Amazon Web Services (AWS) to build a scalable, secure, and compliant automation pipeline that unifies this data into a single decision-making engine. Using Quick Automate, a generative AIâ€“powered workflow automation capability of Amazon Quick Suite, Kitsa can rapidly extract, normalize, and analyze site data at scale. With an advanced multi-agent automation architecture engineered for enterprise-scale deployment, Quick Automate combines UI automation, API integrations, and workflow orchestration in a single, fully managed solution. 
Quick Automate uses generative AI to analyze inputs from the user and suggests a workflow that can be modified and extended to take action across business systems and UIs, engaging a human when needed. Through specialized AI agents, Quick Automate helps organizations to automate complex processes across applications and departments. It also reduces operational costs through usage-based pricing. 
By using AWS services, Kitsa is transforming site selection from a slow, relationship-driven process into a fast, data-driven, and globally scalable system. 
Solution overview and details 
Kitsa required a process automation solution capable of navigating websites, extracting over 50 distinct data points, and compiling the results in a structured format. The solution needed to be highly reliable, scalable to hundreds of thousands of websites, and accurate. Given that Kitsa operates in the life sciences and healthcare sector, which is heavily regulated, they also needed a secure, compliant solution that meets the industryâ€™s strict standards. 
The automation was built using Quick Automate, designed for enterprise-scale workflow automation. A key component of the solution is a state-of-the-art UI Agent, configured to autonomously perform website navigation and data extraction. The UI Agent is part of Quick Automate, enabling complex browser-based workflows. 
The UI Agent takes natural language input and produces structured outputsâ€”essential for reliably capturing more than 50 data points from each website. It was configured to extract information efficiently and consistently, maintaining both accuracy and compliance. The AWS team collaborated closely with the Kitsa team to design and refine specialized prompts, helping the automation perform optimally for the customerâ€™s needs. The following architecture diagram illustrates the workflow. 
 
Workflow architecture and implementation 
The automation workflow uses the following: 
Case initialization and parallel processing 
The automation begins by fetching cases, where each case contains the URL that needs information extraction. The case management functionality enables parallelization of website processing and evaluation, reducing processing time through concurrent execution of multiple cases. 
Intelligent data extraction 
For each case, the UI Agent navigates to the specified URL and extracts required information while applying AI reasoning concerning the content. The information extraction process utilizes natural language instructions provided to the UI Agent task. It then delivers results in a structured output format, so downstream workflow steps can consume them without extra parsing. 
Human-in-the-loop integration 
When website information extraction shows lower confidence, the system can automatically route cases to human reviewers for manual assessment. This human-in-the-loop (HILO) approach maintains quality control while allowing automated processing. 
Data persistence and storage 
Processed cases are systematically saved and written to an Excel spreadsheet within the workflow. The completed files are then uploaded to an Amazon Simple Storage Service (Amazon S3) bucket through integrated S3 connectors, providing secure and accessible data storage. 
Robust exception handling 
The workflow incorporates exception handling mechanisms to gracefully manage scenarios where websites are not found, under construction, or otherwise inaccessible. The workflow returns accurate error messages and continues processing subsequent websites without interrupting the overall workflow execution, resulting in operational continuity and reliability. 
Results 
With Quick Automate powering the Kitsa large-scale data extraction and integration workflow solution, the impact was immediate and measurable: 
 
 91% cost savings: Compared to the legacy manual process it lowered operational expenses while dramatically expanding the number of sites analyzed. 
 96% faster data acquisition: Kitsa is able to process in days what previously took months, accelerating the entire site feasibility process. 
 96% coverage in data extraction: Surpasses manual review while maintaining consistency across hundreds of thousands of processed websites. 
 Full regulatory compliance: Meets all data security, privacy, and auditability standards required in life sciences and healthcare. 
 
The solution now directly powers the Kitsa Site Finder Agent, which evaluates hundreds of site-specific parameters (from past recruitment speed to infrastructure readiness), and ranks them with a trial-specific algorithm. Sponsors can now compare sites on hard evidence rather than subjective impressions, and eligible sites can showcase their capabilities to pharma companies for the first time in a structured, data-rich format. 
As Rohit Banga, Co-Founder &amp; CTO of Kitsa, explains: 

 â€œWith Amazon Quick Automate, we were able to break through one of the biggest bottlenecks in site selection â€” collecting and unifying high-quality data at scale. This allowed our Site Finder Agent to evaluate more sites, more fairly, and with more precision than ever before. Our results show 96% coverage in data extraction, 91% cost savings compared to legacy manual processes, and 96% faster data acquisition â€“ processing in days what previously took months.â€
 
Conclusion 
Clinical trial site selection has long been a critical bottleneck in medical research, with fragmented data and manual processes causing significant delays and missed opportunities. Kitsa addressed this challenge by using the Automate capability of Amazon Quick Suite in their automated site selection solution. 
With the solution Kitsa can automatically extract and analyze over 50 distinct data points from hundreds of thousands of websites. They are achieving remarkable results with 96% coverage in data extraction and 91% cost savings compared to manual processes. Kitsa also reduced their data acquisition time by 96% while maintaining full regulatory compliance in the heavily regulated healthcare sector. 
Their Site Finder Agent now evaluates hundreds of site-specific parameters objectively, helping pharmaceutical companies to make evidence-based decisions and allowing trial sites to showcase their capabilities in a structured format. This transformation demonstrates how Quick Automate can solve complex industry challenges while significantly improving efficiency, accuracy, and fairness in clinical trial site selection. 
Contact an AWS Representative to know how we can help accelerate your business. 
 
About the authors 
Chethan Shriyan is a Principal Product Manager â€“ Technical at AWS. He has 12+ years of experience in product and business management. Chethan is passionate about building and delivering technology products that create meaningful impact in customersâ€™ lives. 
Ajay Nyamati is the co-founder and CEO of Kitsa â€“ a healthtech company using AI and data automation to transform clinical trials. With 20+ years of Sales &amp; Strategy in global companies, Ajay has spent 10+ years in the Digital Health space across payors, providers and pharma. Before co-founding Kitsa, he was the business leader for clinical trials solutions in Amazon Web Services. 
Reagan Rosario brings over a decade of technical expertise to his role as a Sr. Specialist Solutions Architect in Generative AI at AWS. Reagan transforms enterprise systems through strategic implementation of AI-powered cloud solutions, automated workflows, and innovative architecture design. His specialty lies in guiding organizations through digital evolutionâ€”preserving core business value while implementing cutting-edge generative AI capabilities that dramatically enhance operations and create new possibilities.
â€¢ Connect Amazon Quick Suite to enterprise apps and agents with MCP
  Organizations need solutions for people and AI agents to securely collaborate through a single interface to the organizationâ€™s data and take actions across enterprise applications to improve productivity. The ability of an AI agent to securely and seamlessly connect with organizational knowledge bases, enterprise applications, and other AI agents is foundational to drive adoption and use of AI solutions.&nbsp;Model Context Protocol (MCP)&nbsp;provides a more secure, standardized and simple mechanism for such connectivity, alleviating the need for complex custom integrations. 
In this blog post, you will learn how to use Amazon Quick Suite MCP Actions integrations to connect with hosted MCP servers for enterprise applications such as Asana, Atlassianâ€™s Jira and Confluence, Box, Canva, HubSpot,&nbsp;HuggingFace, Intercom, Linear, Monday, Notion, PagerDuty, Workato and Zapier as well as your existing enterprise solutions and AI agents using&nbsp;Amazon Bedrock AgentCore Gateway. We will cover specific Actions setup examples for Atlassian Jira and Confluence,&nbsp;AWS Knowledge MCP Server, and Amazon Bedrock AgentCore Gateway. 
Solution overview 
With the Amazon Quick Suite MCP client, you can connect to remote hosted MCP servers or your own hosted MCP servers. The Amazon Quick Suite service includes an MCP client that can be used to securely connect Amazon Quick Suite with AI agents and applications through MCP servers. The Amazon Quick Suite MCP client supports server-sent events (SSE) and streamable HTTP for transport, and several authentication mechanisms including three-legged OAuth (3LO), two-legged OAuth (2LO), and No Auth. Amazon Quick Suite supports OAuth 2.0 Dynamic Client Registration protocol to obtain OAuth client IDs without user interaction. 
You can connect Amazon Quick Suite using the MCP client to create integrations with AWS Knowledge MCP Server and the Atlassian MCP server, as shown in the following diagram. 
 
Prerequisites 
Verify that you meet the following prerequisites to deploy the solution in your own AWS account using the step-by-step instructions in this blog post. Before you begin, make sure that you have the following: 
 
 An AWS account 
 Amazon Quick Suite set up with Author Pro subscription 
 An Atlassian account with Jira and Confluence access 
 Permission to create AWS resources and&nbsp;AWS Identity and Access Management (IAM) roles and policies 
 Basic knowledge of AWS&nbsp;services 
 For the Amazon Bedrock AgentCore setup: 
   
   Access to a command line environment with AWS SDK and Python installed 
   Knowledge of AWS CLI and Python 
   Amazon Bedrock with access enabled for Anthropic models 
    
 
Amazon Quick Suite Actions integration with Atlassian MCP server 
In this section, you will create an Amazon Quick Suite Actions integration with the Atlassian MCP server to connect with your Atlassian cloud instance, and then you will use your Amazon Quick Suite Chat Agent to invoke actions defined by this integration. 
 
 Sign on to Amazon Quick Suite using credentials with an Author Pro role using your web browser. 
 From the Amazon Quick Suite Home screen,&nbsp;select Integrations from the Connections section on the left navigation panel and then select the Actions tab in the main panel. Select the plus (+) sign in the Model Context Protocol tile in the Set up a new integration section, as shown in the following screenshot. 
 
 
 
 On the Create integration screen, enter the Name and Description for your Atlassian integration. For MCP server endpoint, enter the URL of Atlassian hosted MCP server as https://mcp.atlassian.com/v1/sse,&nbsp;as shown in the following screenshot, then choose Next. 
 
 
 
 The Atlassian MCP server supports dynamic client registration and three-legged OAuth (3LO). The Amazon Quick Suite MCP client gets further information required for the connection from the Atlassian MCP server. On the next screen and choose&nbsp;Create and continue. 
 
 
 
 In the pop-up window from Atlassian MCP server with a request to approve authorization for the Amazon Quick Suite MCP client. Choose Approve. 
 
 
 
 Enter your Atlassian credentials. You will get a request from the Atlassian MCP to access to your Atlassian account. 
 Select a specific instance, and then choose&nbsp;Accept. Note: The administrator of the Atlassian instance might need to allowlist the dynamic OAuth client. 
 
 
Amazon Quick Suite MCP client works to establish a connection with Atlassian MCP server and retrieving the actions or tools supported by it. It can take a couple of minutes. 
 
 When the actions are retrieved and display on your screen, choose Next. 
 
 
 
 Optionally, you can share the integration with other users and groups. Because Atlassian MCP server supports 3LO, the users with whom you share the connection will only be able to invoke the actions, retrieve the content, and take the actions that they are authorized to access on Atlassian Confluence and Jira. Select&nbsp;Done. 
 
 
Your Actions integration for Atlassian MCP server is now available for use. 
 
 
  
   
   Invoke the atlassian-mcp Actions integration from an Amazon Quick Suite Chat Agent, such as My Assistant, using a simple prompt such as List the Confluence spaces and Jira projects I am authorized to access. This will request a sign-in for you to authorize atlassian-mcp Actions integration to connect with Atlassian MCP server on your behalf, a part of the 3LO authentication. 
    
 
 
 
 The response to your prompt will be similar to the response in the following screenshot, and it will include the details of the Jira projects and Confluence pages you are authorized to access. 
 
 
Using Atlassian MCP connection 
To illustrate the Atlassian MCP server Actions integration, letâ€™s take the use case of a team manager at a fictitious company, AnyOrgApp Corp, who is preparing to welcome a new employee to the team with help from the Amazon Quick Suite default Chat Agent, My Assistant. The Atlassian instance used for this illustration has a Confluence space with the content about AnyOrgApp Corp and a Jira project for this team. Based on the contents of your Confluence spaces and Jira projects, you can also experiment with similar use cases. 
 
 The manager finds the new team member onboarding checklist for the AnyOrgApp project using the following prompt. The response to this prompt includes the checklist of onboarding activities for a new team member. 
 
 
 Find new team member checklist for the AnyOrgApp project from Confluence 
 
 
 
 The manager converts the checklist to tasks for the new team member, creates these tasks in a Jira project and assigns those to the new team member using this prompt: 
 
 
 Make a list of tasks with start date and due date for a new team member Mateo Jackson 
joining the team on Monday October 6th, 2025. Create these tasks in Jira project 
"&lt;replace-with-jira-project-name&gt;" with start date and due date and assign those to 
&lt;replace-with-email-address-for-mateo&gt; 
 
 
 When the atlassian-mcp Actions integration wants to invoke mutable actions such as creating a Jira issue, it requests an action review. The Chat Agent prompts for action review for each Jira issue being created. 
 
 
 
 After creating the tasks, a response with the newly created tasks assigned to the new team member will display. 
 
 
 
 Review the Jira project to verify that the tasks were created. 
 
 
 
 The manager requests a detailed email welcoming the new team member along with the tasks assigned using the context in the conversation. The response to this prompt is a detailed email welcoming the new team member along with a prioritized list of onboarding tasks and information about how to get help from fellow team members. 
 
 
 Write a detailed email welcoming Mateo Jackson to the team based on this thread. 
Use warm and helpful tone. 
 
 
Connecting to the AWS Knowledge MCP Server 
AWS Knowledge MCP Server is a fully managed remote MCP server that provides up-to-date documentation, code samples, and other official AWS content. 
 
 To configure the AWS Knowledge Server as an MCP Actions integration, choose Integrations from the Home screen, and the Actions. In Create new integration, select Model Context Protocol as we did for configuring Atlassian MCP server in the previous section. 
 On the Create integration screen, use the AWS Knowledge Server hosted MCP server URL,&nbsp;https://knowledge-mcp.global.api.aws, then choose Next. 
 
 
 
 This MCP server does not require authentication, so the MCP Client has the information needed to create the integration. Choose Create and continue. 
 
 
 
 When the actions made available to you by AWS Knowledge Server appear, choose Next, to complete the Actions integration creation. 
 
 
 
 After the aws-knowledge-server-mcp Actions integration status is available, use the default Chat Agent, My Assistant, to start asking information about AWS. You can use prompts such as this one:. 
 
 
 Get details of the AWS Well-Architected framework from AWS Knowledge MCP Server and 
summarize for a CXO presentation in the context of the new SaaS application your 
company is building on AWS. Use 200 words or fewer. 
 
 
 
 You can also invoke multiple Actions integrations in one prompt such as this one, where the Actions integrations with the AWS Knowledge MCP Server and the Atlassian MCP server are used: 
 
 
 Get details of Well-Architected Framework from AWS Knowledge MCP Server and the details 
of the AnyOrgApp project from Confluence. Prepare well-architected tenets for the 
AnyOrgApp project team. Be crisp. 
 
 
Connect with solutions&nbsp;and agents using Amazon Bedrock AgentCore Gateway 
Amazon Bedrock AgentCore Gateway is a centralized tools server with a unified interface where agents can discover, access, and invoke tools with native support for MCP. You can connect your enterprise solutions and agents as targets behind an Amazon Bedrock AgentCore Gateway endpoint. The Amazon Quick Suite MCP client can connect to Amazon Bedrock AgentCore Gateway through Actions Integrations, making the tools available to the chat agents and automation workflows configured on Amazon Quick Suite. 
Letâ€™s take an example (shown in the following diagram) where we need to connect to an IT agent implemented using Amazon Bedrock Agent with an Amazon Kendra index that has IT help content, and an HR agent setup on OpenAI needs to be made available to Amazon Quick Suite users. 
 
The architecture includes an Amazon Bedrock AgentCore Gateway configured with an AWS Lambda function as a target along with an MCP schema that specifies the tools it implements. The AWS Lambda function implements the InvokeAgent API to invoke the Bedrock Agent to perform an Amazon Kendra index search, which then triggers a Lambda function to call the Amazon Kendra Retrieve API to search and retrieve content from the enterprise Amazon Kendra index with IT help content, and the OpenAI Responses API to invoke the HR agent based on OpenAI. The AgentCore Gateway uses Amazon Cognito as its inbound identity provider. The Amazon Quick Suite MCP client is configured using a 2LO authentication, allowing inbound connections to the AgentCore Gateway endpoint. 
Create an Amazon Bedrock AgentCore Gateway instance and connect it with solutions 
 
 Create an Amazon Bedrock AgentCore Gateway instance with an Amazon Cognito authorizer and a default AWS Lambda target using&nbsp;Quick Start with creating and using a gateway. 
 Open the Amazon Cognito user pool in a tab of the AWS Management Console and select&nbsp;App clients in the left navigation panel. 
 
 
 
 Create a new App client for use with the Amazon Quick Suite MCP client. Choose Create app client, then select Machine-to-machine application, and then choose Create app client. 
 
 
 
 For the new App client you just created, note the Client ID, the Client secret, and the token URL. You will need these parameters in subsequent steps. 
 
 
 
 Open Amazon Bedrock AgentCore in the AWS Management Console on a new tab in your browser. Select Gateways in the left navigation panel, and then select the gateway you just created. Note the Gateway resource URL. 
 
 
 
 On the Edit Gateway screen, select Use existing identity provider configurations, choose Add client, and then add the Client ID of the Cognito App client you created in a previous step to the list of allowed clients. 
 
 
 
 Note the&nbsp;Lambda ARN and review the inline schema in the&nbsp;Inline schema editor window. Choose&nbsp;Save. 
 
 
 
 You can modify the schema and the lambda functions to add your solutions and agents to the AgentCore gateway. The schema used for the example in this blog is in the following code snippet. For the example used in this blog, the Lambda function is also updated with the InvokeAgent API to invoke the Bedrock Agent that implements the IT agent, and the OpenAI Responses API to invoke the HR agent powered by OpenAI. If you are just experimenting, you can continue with the sample tools, get_weather and get_time, created as part of Quick Start with creating and using a Gateway. 
 
 
 [
  {
    "description": "Get weather for a location",
    "inputSchema": {
      "properties": {
        "location": {
          "type": "string"
        }
      },
      "required": [
        "location"
      ],
      "type": "object"
    },
    "name": "get_weather"
  },
  {
    "description": "Get time for a timezone",
    "inputSchema": {
      "properties": {
        "timezone": {
          "type": "string"
        }
      },
      "required": [
        "timezone"
      ],
      "type": "object"
    },
    "name": "get_time"
  },
  {
    "description": "Get help from IT Agent for questions related to email, networking, computer hardware and password trouble",
    "inputSchema": {
      "properties": {
        "query": {
          "description": "question related to email, networking, computer hardware and password troubleshooting",
          "type": "string"
        }
      },
      "required": [
        "query"
      ],
      "type": "object"
    },
    "name": "get_help_from_it_agent"
  },
  {
    "description": "Get help from HR Agent for questions related to employee benefits and leave policies",
    "inputSchema": {
      "properties": {
        "request": {
          "description": "help query related to employee benefits and leave policies",
          "type": "string"
        }
      },
      "required": [
        "request"
      ],
      "type": "object"
    },
    "name": "get_help_from_hr_agent"
  }
] 
 
Amazon Bedrock AgentCore Gateway is now ready for integration with the Amazon Quick Suite MCP client or other MCP clients. 
Create an Amazon Quick Suite MCP Actions integration with Amazon Bedrock AgentCore Gateway 
 
 As with earlier steps, set up a new MCP integration by entering a a Name, Description, and the Gateway resource URL of your Amazon AgentCore gateway (noted earlier as the MCP server endpoint). Choose Next. The Description includes details of the features the Actions integration provides. Amazon Quick Suite uses this description to decide the Actions to be invoked for a user prompt. 
 
 
 
 Select Service authentication in Authentication settings&nbsp;or&nbsp;2LO authentication. The users with whom this Actions integration is shared will use these credentials as the inbound credentials for Amazon Bedrock AgentCore Gateway. You can create 3LO authentication using identity providers such as Okta or Entra ID. Enter the&nbsp;Client ID, Client secret and&nbsp;Token URL of the Amazon Cognito user pool App client you created earlier. Choose Create and continue. 
 
 
 
 Review the Actions made available as MCP tools by the Amazon Bedrock AgentCore Gateway. 
 
 
 
 Optionally, share the integration with other users, then choose Done. 
 
 
Your Actions integration with the Amazon Bedrock AgentCore gateway is now ready for use. 
 
 You can now test the Amazon Quick Suite Actions integration by opening the newly created integration and choosing Test action APIs. 
 
 
 
 From the&nbsp;Action drop-down menu, select the API you want to test, enter the API parameters â€” Location for this exampleâ€“and select Submit. If successful, the API response will display. 
 
 
Explore the Amazon Quick Suite MCP Actions integration with Amazon Bedrock AgentCore Gateway 
You can start using the Actions integration from your Chat Agents. The following examples use My Assistant, the default Chat Agent. 
 
 To start, invoke an OpenAI-powered HR agent using the following prompt. You will get the employee eligibility criteria in response from the HR agent: 
 
 
 What are the eligibility criteria for an employee to receive health benefits? 
 
 
 
 Now invoke the IT agent implemented using an Amazon Bedrock Agent with an Amazon Kendra index-based IT help search by using the following prompt. In response you will get the troubleshooting help from the IT agent: 
 
 
 My computer is not connecting to network. Please help. 
 
 
Conclusion 
In this blog post, you experienced how the Amazon Quick Suite MCP client supports secure, seamless, and wide connectivity with remote hosted MCP servers such as those by Atlassian, Box, and AWS Knowledge MCP Server. You also saw how Amazon Bedrock AgentCore provides a straightforward mechanism to connect with existing solutions and agents, and how the Amazon Quick Suite MCP client connects with Amazon Bedrock AgentCore Gateway for a collaborative environment for users and AI agents on Amazon Quick Suite. 
For more information on Amazon Quick Suite, and how you can get started, please refer to the blog post Announcing Amazon Quick Suite: your agentic teammate for answering questions and taking action. To know more about Amazon Bedrock AgentCore, please refer to the blog post Introducing Amazon Bedrock AgentCore Gateway: Transforming enterprise AI agent tool development. 
 
About the authors 
Abhinav Jawadekar is a Principal Solutions Architect in the Amazon Quick Suite service team at AWS. Abhinav works with AWS customers and partners to help them build agentic AI solutions on AWS. 
Vignesh Subramanian is a Senior Software Development Engineer at AWS, specializing in application security, fraud prevention, agentic AI, and model-tool interaction frameworks. He currently provides technical leadership for the Amazon Quick Suite and has been building at Amazon for over nine years. Outside of work, he enjoys experimenting with Rust, chasing sunsets, and hiking across the Pacific Northwest. You can connect with him on LinkedIn.
â€¢ Make agents a reality with Amazon Bedrock AgentCore: Now generally available
  Get agents out of prototype purgatory and into production with security, scalability, and reliability 
When we launched AWS in 2006, we believed that cloud computing would transform how organizations build and scale technology. Weâ€™re now at a similar inflection point with AI agents. We envision a world where billions of agents work together, transforming everything from daily operations to complex business processes. However, making this a reality requires more than frameworks or low-code builder tools. Agents that companies are willing to bet their business on need an enterprise-grade operational foundationâ€”one that is secure, reliable, scalable, and purpose-built for the non-deterministic nature of agents. Drawing on our experience building mission-critical systems, Amazon Bedrock AgentCore is a comprehensive agentic platform that enables organizations to get to production with confidence. 
AgentCore: Get agents to production fast 
AgentCore, now generally available, makes it possible for every developer to get agents from pilots to full-scale production fast. AgentCore gives you the complete foundation you need to build, deploy, and operate agents. You can easily equip agents with tools, memory, and data to handle complex workflows. You can deploy agents with a few lines of code on one of the most secure and scalable runtimes available today. And you can operate those agents with the controls and access management required for enterprise deployments. You can do all of this without any infrastructure management, and itâ€™s easy to get started using any model or agent framework of your choice. 
The AgentCore SDK has been downloaded over a million times by customers of all sizes across multiple industries. Some of the early customers include Clearwater Analytics (CWAN), Cox Automotive, Druva, Ericsson, Experian, Heroku, National Australia Bank, Sony, Thomson Reuters, and many more. Supported by AWS Partners, including Accenture, Cisco, Deloitte, and Salesforce, AgentCore is already delivering transformative results. 
AgentCore: A comprehensive agentic platform 
 
Building agents can be hard â€“ you need to figure out how to integrate with identity providers, how to build memory and observability, and integrate with tools. Our agentic platform offers fully-managed services across the agent development lifecycle from build to deploy to operate. You can mix and match, use any model or framework, offering maximum flexibility with access to enterprise-grade infrastructure and tools. Letâ€™s look at its core capabilities. 
Build the way you want: The agent landscape is evolving rapidly, with new frameworks, models, and protocols emerging almost weekly. You can build the way you want with composable AgentCore services that can be used together or independently. Your organization can choose which AgentCore services the team needs while using their preferred frameworks (including CrewAI, Google ADK, LangGraph, LlamaIndex, OpenAI Agents SDK, and Strands Agents) and models (including those available on Amazon Bedrock or models available outside Bedrock including OpenAI and Gemini), so you stay free to build the way you want. 
Foundational tools for agent success: Agents create value with concrete actions â€“ writing and executing code, connecting to company systems, and navigating the web. AgentCore provides these essential services: AgentCore Code Interpreter enables agents to generate and execute code securely in isolated environments, and AgentCore Browser allows agents to interact with web applications at scale. Meanwhile, AgentCore Gateway transforms your existing APIs and AWS Lambda functions into agent-compatible tools, connects to existing MCP servers, and provides seamless integration with essential third-party business tools and services (such as Jira, Asana, and Zendesk). This unified access point enables secure integration across your enterprise systems. With AgentCore Identity, agents can securely access and operate across these tools with proper authentication and authorization using OAuth standards. 
Context-aware agents with intelligent memory: For agents to be truly effective, they need to maintain context and learn from interactions over time. Consider a sales support agent helping a customer explore enterprise software options â€“ it should remember the customerâ€™s industry, budget constraints, and technical requirements across multiple conversations, eliminating repetitive questions and delivering increasingly personalized recommendations. Similarly, when assisting with complex technical troubleshooting, an agent should recall previous debugging attempts and their outcomes to provide more targeted solutions. AgentCore Memory helps developers create these sophisticated, context-aware experiences without managing complex memory infrastructure, helping agents build and maintain detailed understanding of user preferences, historical interactions, and relevant context that enriches every conversation. 
Comprehensive observability for trustworthy agents: Because agents reason in real-time and non-deterministically perform actions, you need complete visibility into the reasoning and actions of agents. AgentCore Observability provides comprehensive monitoring through real-time dashboards and detailed audit trails. Organizations can track every agent action, debug issues quickly, and continuously optimize performance. Through OpenTelemetry (OTEL) compatibility, AgentCore Observability integrates with existing monitoring tools, such as Dynatrace, Datadog, Arize Phoenix, LangSmith, and Langfuse. 
Industry-leading reliability at any scale: Unlike traditional applications, agent workload durations can be inherently unpredictable. AgentCore Runtime is designed for this variability, automatically scaling from zero to thousands of sessions as needed, and it offers an industry-leading runtime of eight hours for long running tasks. 
Enterprise-grade agent security: Agents need to securely access multiple systems and handle sensitive data while acting on behalf of users, making robust security and regulatory compliance non-negotiable. AgentCore embeds security across every service to help agents operate safely. It supports virtual private cloud (VPC) environments and AWS PrivateLink to keep network traffic private and secure. Most importantly, AgentCore Runtime provides industry-leading security through microVM technology, giving each agent session its own isolated computing environment to prevent data leaks and maintain the integrity of every interaction. 
With AgentCore, speed, scale, and security get along: AgentCore makes it easy to build production-ready agents through its MCP server, which works with integrated development environments (IDEs) like Kiro or Cursor AI. While it takes just minutes to get started, these arenâ€™t simplified tools â€“ theyâ€™re full-featured, production-ready solutions that can immediately scale from zero to thousands of sessions while maintaining robust security. This means your team can move quickly from idea to deployment with confidence, knowing your agents are built on a proven foundation. 
Making the promise of AI agents a reality: Customer stories 
Pioneering organizationsâ€”from Cohere Healthâ€™s regulated healthcare environment to Ericssonâ€™s complex technical systems to Sonyâ€™s global transformationâ€”demonstrate how AgentCore is driving the next wave of AI innovation across industries. The organizations that succeed in the AI era wonâ€™t be those who perfectly predict the future, but those who build on proven foundations while maintaining the flexibility to evolve. When you build on AgentCore, youâ€™re not just getting specialized services to deploy and operate agents, youâ€™re getting a partner with nearly two decades of experience helping companies transform securely at global scale. Here are some stories that demonstrate customer impact: 
 
  
 
Watch how Epsilon, part of the worldâ€™s largest advertising company Publicis Groupe, is using AgentCore to revolutionize campaign personalization for large brands. Their Intelligent Campaign Automation solution enables automated campaign design, audience targeting, and real-time optimization across multiple channels â€“ delivering faster execution times and improved customer targeting precision at scale. 
Transforming manufacturing with intelligent workflow automation 
Amazon Devices Operations &amp; Supply Chain team is using AgentCore to develop an agentic manufacturing approach. As part of this new approach, AI agents work together using product specifications to automate manual processes. One agent reads the product requirements and creates detailed test procedures for quality control, while another agent trains the vision systems that robots need on the manufacturing line. As a result, fine-tuning an object detection model, which used to take days of engineering time, can now be done in under an hour with high precision. This proof-of-concept is just the beginning of their vision for smarter manufacturing, where AI agents will streamline the journey from initial product requirements to final production. 
Accelerating healthcare decisions with agents 
In healthcare, every minute matters. Cohere HealthÂ® is a clinical intelligence company focused on strengthening payer-provider collaboration as well as improving the speed and accuracy of clinical decision-making, both pre- and post-care. The companyâ€™s clinically trained AI helps accelerate access to patient care, improve patient outcomes, reduce administrative burden for providers, and improve healthcare economics across the care continuum. 
Using AgentCore, Cohere Health built Cohere Review Resolve, an AI-powered copilot that optimizes the accuracy and efficiency of health plan medical necessity reviews. Cohere Review Resolve analyzes both structured and unstructured dataâ€“such as clinical records, patient notes, and faxes, quickly identifying and surfacing evidence to validate the medical necessity of a requested treatment. The copilot will provide health plansâ€™ reviewers with complete clinical context for prior authorization requests and respond intelligently to reviewersâ€™ queries. 
Cohere Health chose AgentCore because they needed enterprise-grade infrastructure for their first production deployment of agentic AI in a highly regulated healthcare environment. The comprehensive audit trails, extended session support, and ability to maintain history throughout complex multi-hour workflows available in AgentCore are essential for their healthcare use case. 
Cohere Health expects that Review Resolve will reduce review times by 30-40%, helping them to meet critical, mandated turnaround times. For patients, quicker decision-making will accelerate care access, increase adherence to therapy, improve outcomes and reduce costs. Review Resolve will also help health plans improve the accuracy of clinical determinations by approximately 30%, thereby improving medical expense savings and patient outcomes. 
Agents in telecommunications: simplifying complex systems 
Ericsson, a global leader in telecommunications technology, has used AgentCore to tackle its primary challenge in deploying agents. Dag Lindbo, Head of AI and Emerging Technologies in Business Area Networks at Ericsson says â€œAt Ericsson, our 3G/4G/5G/6G systems span millions of lines of code across thousands of interconnected subsystems, representing decades of engineering innovation at the scale of nation-wide critical infrastructure. AgentCore powers our crucial fusion of data and information to deliver AI agents of unprecedented capability in real-world R&amp;D, scaling to double-digit gains across a workforce in the tens of thousands. AgentCore also lets us use any agent framework, which is critical to help us scale across many teams and use cases.â€ 
Agents in entertainment: achieving security, observability and scalability 
At Sony Group, one of the worldâ€™s leading technology and entertainment companies, AgentCore is already having an impact. â€œAgentic AI, which enables a new level of advanced operational efficiency and sophistication, is an essential technology for our AI transformation journey,â€ notes Masahiro Oba, Senior General Manager of AI Acceleration Division, Digital &amp; Technology Platform for Sony Group Corporation. â€œHowever, it is also true that it presents many technical challenges. By leveraging Amazon Bedrock AgentCore, we built a group-wide Agentic AI Platform, achieving enterprise-level security, observability and scalability, along with seamless cross-platform connectivity to AI resourcesâ€”a critical capability for us. By placing Amazon Bedrock AgentCore at the core of our agentic AI ecosystem, we gain the ability to govern and share vast amounts of AI, enabling us to accelerate AI transformation with confidence and security.â€ 
Building the foundation for the AI agent era at AWS 
AgentCore is now generally available in nine AWS Regions to support global deployment needs, including Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Dublin), Europe (Frankfurt), US East (N. Virginia), US East (Ohio), and US West (Oregon). Organizations can accelerate time-to-value with pre-built agents and tools from AWS Marketplace that are designed to work on AgentCore. 
Get started with AgentCore today â€“ Visit aws.amazon.com/bedrock/agentcore/ to start building your agentic future! 
Learn more: 
 
 Accelerate development with the Amazon Bedrock AgentCore MCP server (by Shreyas Subramanian&nbsp;and&nbsp;Primo Mu, AWS ML Blog, 10/02/2025) 
 Introducing Amazon Bedrock AgentCore: Securely deploy and operate AI agents at any scale (preview) (by Danilo Poccia, AWS News Blog, 7/16/2025) 
 AWS announces new innovations for building AI agents at AWS Summit New York 2025 (Amazon news summary, 7/16/2025) 
 
 
About the author 
Swami Sivasubramanian is Vice President for Agentic AI at Amazon Web Services (AWS). At AWS, Swami has led the development and growth of leading AI services like Amazon DynamoDB, Amazon SageMaker, Amazon Bedrock, and Amazon Q. His teamâ€™s mission is to provide the scale, flexibility, and value that customers and partners require to innovate using agentic AI with confidence and build agents that are not only powerful and efficient, but also trustworthy and responsible. Swami also served from May 2022 through May 2025 as a member of the National Artificial Intelligence Advisory Committee, which was tasked with advising the President of the United States and the National AI Initiative Office on topics related to the National AI Initiative.

â¸»