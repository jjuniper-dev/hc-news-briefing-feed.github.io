‚úÖ Morning News Briefing ‚Äì July 15, 2025 10:57

üìÖ Date: 2025-07-15 10:57
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ HEAT WARNING, Pembroke
  Persons in or near this area should be on the lookout for adverse weather conditions and take necessary safety precautions . People in or around this area are advised to take necessary precautions . Weather conditions are expected to worsen in the summer of 2025 . People should also be aware of the weather conditions in the area and take precautions to avoid travel risks in the event of any weather-related travel risk in the
‚Ä¢ Current Conditions:  15.4¬∞C
  Temperature: 15.4&deg;C Pressure / Tendency: 101.7 kPa rising Humidity: 93 % Dewpoint: 14.4¬∞C Wind: WSW calm km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 15 July 2025 . Weather: 15¬∞C (¬∞C) at Pem
‚Ä¢ Tuesday: A mix of sun and cloud. High 32.
  A mix of sun and cloud is expected to hit 32 degrees in the morning . Hazy weather will be followed by a high of 37 degrees with a very high UV index of 9 or very high . High temperatures will be 32 degrees Celsius in the early hours of Tuesday morning, with a low of 32 degrees Fahrenheit in the mid to mid-90s . A high of 33 degrees Celsius

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Planet Money Summer School tackles political economy
  Planet Money Summer School is tackling the biggest economic player of them all: the government . The government is one of the most powerful players in the economy . Planet Money summer school is tackling government in a new way to learn about the government's role in economic policy . Learn more about government in this season's free economics summer school on Planet Money and Summer School at 8 p.m. ET
‚Ä¢ A million veterans gave DNA for medical research. Now the data is in limbo
  Retired service members donated genetic material to a DNA database to help answer health questions for all Americans . The Trump administration is dragging its heels on agreements to analyze the data . Retired members of the U.S. military have donated their DNA to the DNA database . The database will help answer questions about the health of all Americans, including the elderly, who have donated to the database .
‚Ä¢ 4 astronauts splashdown on SpaceX capsule to end Axiom Space's private Ax-4 mission
  The private crew included Ax-4 mission commander and former NASA astronaut Peggy Whitson . It was her fifth trip to space and extended her record-setting duration to 695 days, the most of any American . The mission was her 5th space mission to space, extending her record setting to 696 days . The crew included a crew of four people, including former NASA astronauts .
‚Ä¢ Heavy rains and flash flooding sweep across Northeast
  Flash flood watches and warnings were in place in parts of New Jersey, New York, Pennsylvania and surrounding areas as downpours moved through the region . Flash flood warnings and watches are in place for parts of the region including New Jersey and New York . Flood warnings are also in effect in New York and New Jersey as the region continues to experience heavy rain and thundery showers in the region
‚Ä¢ Thousands continue search for those missing following deadly floods in central Texas
  Recent storms have slowed recovery efforts in central Texas following the July 4 floods that killed more than 130 people . More than 14,000 volunteers are searching for at least 100 people still missing . At least 100 are still missing, including at least 130 people who were killed in the floods in July 4 flooding . About 14,500 volunteers have been searching for missing people since the floods hit central Texas

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ CIOs pause net-new IT investments as global tariff jitters bite
  Gartner has trimmed its growth forecast for worldwide IT spending in 2025 . Uncertainty to blame as businesses wait to see what US Prez Trump does next World War Fee . US President Donald Trump's trade tariff policy has hit net new spending in part by the unpredctability of US President Trump's tariff policy . Businesses are waiting to see if the US president will take
‚Ä¢ HAMR time: Seagate unleashes 30 TB disks to feed the AI beast
  Seagate has released two 30 TB hard drives based on its HAMR technology . Exos and IronWolf drives show spinning rust isn't going anywhere . Seagate is pitching them as more energy efficient and cheaper options for datacenter operators dealing with AI workloads . The company is pitching the drives as cheaper and energy efficient options for data-center operators with big workloads like AI workload
‚Ä¢ Britain's billion-pound F-35s not quite ready for, well, anything
  F-35 stealth fighter is not meeting its potential in British service, says NAO . Availability issues, shortage of support personnel and delays in integrating key weapons are limiting the aircraft's effectiveness . Stealth jets can't fight, can't fly much, and can't shoot UK missiles, the NAO says . NAO: Stealth jets are not meeting potential in UK service because of availability issues
‚Ä¢ Meta reveals plan for several multi-gigawatt datacenter clusters
  Mark Zuckerberg has revealed he plans to build several multi-gigawatt datacenter clusters, with the first to come online in 2026 . The first cluster is expected to be built in New York City, New York, in the fall of 2026, according to Zuckerberg . Zuckerberg plans to use the first cluster in Manhattan to build a super-high-performance super-data
‚Ä¢ Scientists spot massive black hole collision that defies current theories
  Researchers have observed the largest ever collision between two massive black holes witnessed by humans . The finding has sent astrophysicists back to their calculators to re-think models . The discovery is the latest in a long line of gravitational waves to be observed in the sky . It's the first time scientists have observed such a collision in the history of the universe's largest black holes colliding

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Conducting the National Health and Morbidity Survey 2023 in Malaysia with focus on methodology and main findings on non-communicable diseases
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Why do ageing rates vary by country? Massive study says politics play a part
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The Helicobacter pylori AI-clinician harnesses artificial intelligence to personalise H. pylori treatment recommendations
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Trends in stroke incidence and mortality in China, Japan, and South Korea (1990‚Äì2021) with projections to 2035
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Understanding who volunteers globally through an examination of demographic variation in volunteering across 22 countries
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ AI text-to-speech programs could ‚Äúunlearn‚Äù how to imitate certain people
  A technique known as ‚Äúmachine unlearning‚Äù could teach AI models to forget specific voices‚Äîan important step in stopping the rise of audio deepfakes, where someone‚Äôs voice is copied to carry out fraud or scams.



Recent advances in artificial intelligence have revolutionized the quality of text-to-speech technology so that people can convincingly re-create a piece of text in any voice, complete with natural speaking patterns and intonations, instead of having to settle for a robotic voice reading it out word by word. ‚ÄúAnyone‚Äôs voice can be reproduced or copied with just a few seconds of their voice,‚Äù says Jong Hwan Ko, a professor at Sungkyunkwan University in Korea and the coauthor of a new paper that demonstrates one of the first applications of machine unlearning to speech generation.




Copied voices have been used in scams, disinformation, and harassment. Ko, who researches audio processing, and his collaborators wanted to prevent this kind of identity fraud. ‚ÄúPeople are starting to demand ways to opt out of the unknown generation of their voices without consent,‚Äù he says.&nbsp;







AI companies generally keep a tight grip on their models to discourage misuse. For example, if you ask ChatGPT to give you someone‚Äôs phone number or instructions for doing something illegal, it will likely just tell you it cannot help. However, as many examples over time have shown, clever prompt engineering or model fine-tuning can sometimes get these models to say things they otherwise wouldn‚Äôt. The unwanted information may still be hiding somewhere inside the model so that it can be accessed with the right techniques.&nbsp;




At present, companies tend to deal with this issue by applying guardrails; the idea is to check whether the prompts or the AI‚Äôs responses contain disallowed material. Machine unlearning instead asks whether an AI can be made to forget a piece of information that the company doesn‚Äôt want it to know. The technique takes a leaky model and the specific training data to be redacted and uses them to create a new model‚Äîessentially, a version of the original that never learned that piece of data. While machine unlearning has ties to older techniques in AI research, it‚Äôs only in the past couple of years that it‚Äôs been applied to large language models.



Jinju Kim, a master‚Äôs student at Sungkyunkwan University who worked on the paper with Ko and others, sees guardrails as fences around the bad data put in place to keep people away from it. ‚ÄúYou can‚Äôt get through the fence, but some people will still try to go under the fence or over the fence,‚Äù says Kim. But unlearning, she says, attempts to remove the bad data altogether, so there is nothing behind the fence at all.&nbsp;



The way current text-to-speech systems are designed complicates this a little more, though. These so-called ‚Äúzero-shot‚Äù models use examples of people‚Äôs speech to learn to re-create any voice, including those not in the training set‚Äîwith enough data, it can be a good mimic when supplied with even a small sample of someone‚Äôs voice. So ‚Äúunlearning‚Äù means a model not only needs to ‚Äúforget‚Äù voices it was trained on but also has to learn not to mimic specific voices it wasn‚Äôt trained on. All the while, it still needs to perform well for other voices.&nbsp;



To demonstrate how to get those results, Kim taught a recreation of VoiceBox, a speech generation model from Meta, that when it was prompted to produce a text sample in one of the voices to be redacted, it should instead respond with a random voice. To make these voices realistic, the model ‚Äúteaches‚Äù itself using random voices of its own creation.&nbsp;



According to the team‚Äôs results, which are to be presented this week at the International Conference on Machine Learning, prompting the model to imitate a voice it has ‚Äúunlearned‚Äù gives back a result that‚Äîaccording to state-of-the-art tools that measure voice similarity‚Äîmimics the forgotten voice more than 75% less effectively than the model did before. In practice, this makes the new voice unmistakably different. But the forgetfulness comes at a cost: The model is about 2.8% worse at mimicking permitted voices. While these percentages are a bit hard to interpret, the demo the researchers released online offers very convincing results, both for how well redacted speakers are forgotten and how well the rest are remembered. A sample from the demo is given below.&nbsp;



A voice sample of a speaker to be forgotten by the model.



The generated text-to-speech audio from the original model using the above as a prompt.



The generated text-to-speech audio using the same prompt, but now from the model where the speaker was forgotten.



Ko says the unlearning process can take ‚Äúseveral days,‚Äù depending on how many speakers the researchers want the model to forget. Their method also requires an audio clip about five minutes long for each speaker whose voice is to be forgotten.



In machine unlearning, pieces of data are often replaced with randomness so that they can‚Äôt be reverse-engineered back to the original. In this paper, the randomness for the forgotten speakers is very high‚Äîa sign, the authors claim, that they are truly forgotten by the model.&nbsp;



&nbsp;‚ÄúI have seen people optimizing for randomness in other contexts,‚Äù says Vaidehi Patil, a PhD student at the University of North Carolina at Chapel Hill who researches machine unlearning. ‚ÄúThis is one of the first works I‚Äôve seen for speech.‚Äù Patil is organizing a machine unlearning workshop affiliated with the conference, and the voice unlearning research will also be presented there.&nbsp;



She points out that unlearning itself involves inherent trade-offs between efficiency and forgetfulness because the process can take time, and can degrade the usability of the final model. ‚ÄúThere‚Äôs no free lunch. You have to compromise something,‚Äù she says.



Machine unlearning may still be at too early a stage for, say, Meta to introduce Ko and Kim‚Äôs methods into VoiceBox. But there is likely to be industry interest. Patil is researching unlearning for Google DeepMind this summer, and while Meta did not respond with a comment, it has hesitated for a long time to release VoiceBox to the wider public because it is so vulnerable to misuse.&nbsp;



The voice unlearning team seems optimistic that its work could someday get good enough for real-life deployment. ‚ÄúIn real applications, we would need faster and more scalable solutions,‚Äù says Ko. ‚ÄúWe are trying to find those.‚Äù
‚Ä¢ AI‚Äôs giants want to take over the classroom
  School‚Äôs out and it‚Äôs high summer, but a bunch of teachers are plotting how they‚Äôre going to use AI this upcoming school year. God help them.&nbsp;



On July 8, OpenAI, Microsoft, and Anthropic announced a $23 million partnership with one of the largest teachers‚Äô unions in the United States to bring more AI into K‚Äì12 classrooms. Called the National Academy for AI Instruction, the initiative will train teachers at a New York City headquarters on how to use AI both for teaching and for tasks like planning lessons and writing reports, starting this fall



The companies could face an uphill battle. Right now, most of the public perceives AI‚Äôs use in the classroom as nothing short of ruinous‚Äîa surefire way to dampen critical thinking and hasten the decline of our collective attention span (a viral story from New York magazine, for example, described how easy it now is to coast through college thanks to constant access to ChatGPT).&nbsp;



Amid that onslaught, AI companies insist that AI promises more individualized learning, faster and more creative lesson planning, and quicker grading. The companies sponsoring this initiative are, of course, not doing it out of the goodness of their hearts.



No‚Äîas they hunt for profits, their goal is to make users out of teachers and students. Anthropic is pitching its AI models to universities, and OpenAI offers free courses for teachers. In an initial training session for teachers by the new National Academy for AI Instruction, representatives from Microsoft showed teachers how to use the company‚Äôs AI tools for lesson planning and emails, according to the New York Times.¬†



It&#8217;s early days, but what does the evidence actually say about whether AI is helping or hurting students? There‚Äôs at least some data to support the case made by tech companies: A recent survey of 1,500 teens conducted by Harvard‚Äôs Graduate School of Education showed that kids are using AI to brainstorm and answer questions they&#8217;re afraid to ask in the classroom. Studies examining settings ranging from math classes in Nigeria to colleges physics courses at Harvard have suggested that AI tutors can lead students to become more engaged.&nbsp;



And yet there‚Äôs more to the story. The same Harvard survey revealed that kids are also frequently using AI for cheating and shortcuts. And an oft-cited paper from Microsoft found that relying on AI can reduce critical thinking. Not to mention the fact that ‚Äúhallucinations‚Äù of incorrect information are an inevitable part of how large language models work.



There&#8217;s a lack of clear evidence that AI can be a net benefit for students, and it&#8217;s hard to trust that the AI companies funding this initiative will give honest advice on when not to use AI in the classroom.



Despite the fanfare around the academy&#8217;s launch, and the fact the first teacher training is scheduled to take place in just a few months, OpenAI and Anthropic told me they couldn&#8217;t share any specifics.&nbsp;



It&#8217;s not as if teachers themselves aren&#8217;t already grappling with how to approach AI. One such teacher, Christopher Harris, who leads a library system covering 22 rural school districts in New York, has created a curriculum aimed at AI literacy. Topics range from privacy when using smart speakers (a lesson for second graders) to misinformation and deepfakes (instruction for high schoolers). I asked him what he‚Äôd like to see in the curriculum used by the new National Academy for AI Instruction.





‚ÄúThe real outcome should be teachers that are confident enough in their understanding of how AI works and how it can be used as a tool that they can teach students about the technology as well,‚Äù he says. The thing to avoid would be overfocusing on tools and pre-built prompts that teachers are instructed to use without knowing how they work.&nbsp;



But all this will be for naught without an adjustment to how schools evaluate students in the age of AI, Harris says: ‚ÄúThe bigger issue will be shifting the fundamental approaches to how we assign and assess student work in the face of AI cheating.‚Äù



The new initiative is led by the American Federation of Teachers, which represents 1.8 million members, as well as the United Federation of Teachers, which represents 200,000 members in New York. If they win over these groups, the tech companies will have significant influence over how millions of teachers learn about AI. But some educators are resisting the use of AI entirely, including several hundred who signed an open letter last week.



Helen Choi is one of them. ‚ÄúI think it is incumbent upon educators to scrutinize the tools that they use in the classroom to look past hype,‚Äù says Choi, an associate professor at the University of Southern California, where she teaches writing. ‚ÄúUntil we know that something is useful, safe, and ethical, we have a duty to resist mass adoption of tools like large language models that are not designed by educators with education in mind.‚Äù



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.
‚Ä¢ The Download: California‚Äôs AI power plans, and and why it‚Äôs so hard to make welfare AI fair
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



California is set to become the first US state to manage power outages with AI



California&#8217;s statewide power grid operator is poised to become the first in North America to deploy artificial intelligence to manage outages, MIT Technology Review has learned.&nbsp;



At an industry summit in Minneapolis tomorrow, the California Independent System Operator is set to announce a deal to run a pilot program using new AI software called Genie, from the energy-services giant OATI.&nbsp;



The software uses generative AI to analyze and carry out real-time analyses for grid operators and comes with the potential to autonomously make decisions about key functions on the grid, a switch that might resemble going from uniformed traffic officers to sensor-equipped stoplights. Read the full story.



‚ÄîAlexander C. Kaufman







Why it‚Äôs so hard to make welfare AI fair



There are plenty of stories about AI that‚Äôs caused harm when deployed in sensitive situations, and in many of those cases, the systems were developed without much concern to what it meant to be fair or how to implement fairness.But the city of Amsterdam did spend a lot of time and money to try to create ethical AI‚Äîin fact, it followed every recommendation in the responsible AI playbook. But when it deployed it in the real world, it still couldn‚Äôt remove biases. So why did Amsterdam fail? And more importantly: Can this ever be done right?Join our editor Amanda Silverman, investigative reporter Eileen Guo and Gabriel Geiger, an investigative reporter from Lighthouse Reports, for a subscriber-only Roundtables conversation at 1pm ET on Wednesday July 30 to explore if algorithms can ever be fair. Register here!







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Trump‚Äôs ‚Äòbig, beautiful bill‚Äô is already hurting sick childrenAnd hundreds of hospitals are likely to close, too. (New Yorker $)+ His administration is going after easy targets, which includes sick children. (Salon $)



2 The US overseas worker purge is hitting Amazon hardIts warehouse employees are losing their right to work in the US. (NYT $)+ The US State Department has fired more than 1,350 workers so far. (Reuters)



3 Nvidia‚Äôs CEO claims China‚Äôs military probably won‚Äôt use its AI chipsBut then he would say that, wouldn‚Äôt he. (Bloomberg $)+ Even after the Trump administration has eased chip software tool export restrictions. (FT $)+ Rival Huawei is planning a major AI chip overhaul. (The Information $)



4 Scientists are reportedly hiding LLM instructions in their papers¬†Instructing models to give their work positive peer reviews. (The Guardian)



5 Amazon is dragging its heels launching its web version of AlexaIt appears the company underestimated just how much work they had to do. (WP $)



6 SpaceX‚Äôs revenue is on the up¬†As Tesla continues to struggle. (WSJ $)+ Musk is not in favor of merging Tesla with xAI. (Reuters)+ Trump is still planning to slash NASA‚Äôs budget. (The Atlantic $)+ Rivals are rising to challenge the dominance of SpaceX. (MIT Technology Review)



7 The Air India crash was caused by a cut in the plane‚Äôs fuel supplyCockpit voice recordings reveal that one pilot asked another why he‚Äôd cut off the supply. (CNN)



8 The UK‚Äôs attempt to ape DOGE isn‚Äôt going wellCouncils are already blocking Reform UK‚Äôs attempts to access sensitive data. (FT $)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



9 Even crypto executives can fall for crypto scamsJust ask the top brass from MoonPay, which lost $250,000 worth of Ethereum. (The Verge)+ The people using humour to troll their spam texts. (MIT Technology Review)



10 Why landline phones refuse to die The business world still loves them. (WSJ $)







Quote of the day



&#8220;We don&#8217;t like to work like that. I&#8217;m a Buddhist, so I believe in karma. I don&#8217;t want to steal anyone&#8217;s money.&#8221;



‚ÄîA man forced to work in an online scam center in Myanmar recounts his experience to Nikkei.







One more thing







China wants to restore the sea with high-tech marine ranchesA short ferry ride from the port city of Yantai, on the northeast coast of China, sits Genghai No. 1, a 12,000-metric-ton ring of oil-rig-style steel platforms, advertised as a hotel and entertainment complex.Genghai is in fact an unusual tourist destination, one that breeds 200,000 ‚Äúhigh-quality marine fish‚Äù each year. The vast majority are released into the ocean as part of a process known as marine ranching.The Chinese government sees this work as an urgent and necessary response to the bleak reality that fisheries are collapsing both in China and worldwide. But just how much of a difference can it make? Read the full story.



‚ÄîMatthew Ponsford







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ You can easily make ice cream at home with just two ingredients.¬†+ Pink Floyd fans, this lecture is for you.¬†+ Lose yourself for a few minutes in the story behind an ancient Indian painting. (NYT $)+ Remember the days of idly surfing the web? Here‚Äôs how you can still recreate them.
‚Ä¢ California is set to become the first US state to manage power outages with AI
  California&#8217;s statewide power grid operator is poised to become the first in North America to deploy artificial intelligence to manage outages, MIT Technology Review has learned.&nbsp;



‚ÄúWe wanted to modernize our grid operations. This fits in perfectly with that,‚Äù says Gopakumar Gopinathan, a senior advisor on power system technologies at the California Independent System Operator‚Äîknown as the CAISO and pronounced KAI-so. ‚ÄúAI is already transforming different industries. But we haven‚Äôt seen many examples of it being used in our industry.‚Äù&nbsp;



At the DTECH Midwest utility industry summit in Minneapolis on July 15, CAISO is set to announce a deal to run a pilot program using new AI software called Genie, from the energy-services giant OATI. The software uses generative AI to analyze and carry out real-time analyses for grid operators and comes with the potential to autonomously make decisions about key functions on the grid, a switch that might resemble going from uniformed traffic officers to sensor-equipped stoplights.&nbsp;



But while CAISO may deliver electrons to cutting-edge Silicon Valley companies and laboratories, the actual task of managing the state‚Äôs electrical system is surprisingly analog.&nbsp;



Today, CAISO engineers scan outage reports for keywords about maintenance that‚Äôs planned or in the works, read through the notes, and then load each item into the grid software system to run calculations on how a downed line or transformer might affect power supply.



‚ÄúEven if it takes you less than a minute to scan one on average, when you amplify that over 200 or 300 outages, it adds up,‚Äù says Abhimanyu Thakur, OATI‚Äôs vice president of platforms, visualization, and analytics. ‚ÄúThen different departments are doing it for their own respective keywords. Now we consolidate all of that into a single dictionary of keywords and AI can do this scan and generate a report proactively.‚Äù&nbsp;



If CAISO finds that Genie produces reliable, more efficient data analyses for managing outages, Gopinathan says, the operator may consider automating more functions on the grid. ‚ÄúAfter a few rounds of testing, I think we‚Äôll have an idea about what is the right time to call it successful or not,‚Äù he says.&nbsp;





Regardless of the outcome, the experiment marks a significant shift. Most grid operators are using the same systems that utilities have used ‚Äúfor decades,‚Äù says Richard Doying, who spent more than 20 years as a top executive at the Midcontinent Independent System Operator, the grid operator for an area encompassing 15 states from the upper Midwest down to Louisiana.&nbsp;



‚ÄúThese organizations are carved up for people working on very specific, specialized tasks and using their own proprietary tools that they‚Äôve developed over time,‚Äù says Doying, now a vice president at the consultancy Grid Strategies. ‚ÄúTo the extent that some of these new AI tools are able to draw from data across different areas of an organization and conduct more sophisticated analysis, that‚Äôs only helpful for grid operators.‚Äù



Last year, a Department of Energy report found that AI had potential to speed up studies on grid capacity and transmission, improve weather forecasting to help predict how much energy wind and solar plants would produce at a given time, and optimize planning for electric-vehicle charging networks. Another report by the energy department‚Äôs Loan Programs Office concluded that adding more ‚Äúadvanced‚Äù technology such as sensors to various pieces of equipment will generate data that can enable AI to do much more over time.&nbsp;



In April, the PJM Interconnection‚Äîthe nation‚Äôs largest grid system, spanning 13 states along the densely populated mid-Atlantic and Eastern Seaboard‚Äîtook a big step toward embracing AI by inking a deal with Google to use its Tapestry software to improve regional planning and speed up grid connections for new power generators.&nbsp;



ERCOT, the Texas grid system, is considering adopting technology similar to what CAISO is now set to use, according to a source with knowledge of the plans who requested anonymity because they were not authorized to speak publicly. ERCOT did not respond to a request for comment.&nbsp;



Australia offers an example of what the future may look like. In New South Wales, where grid sensors and smart technology are more widely deployed, AI software rolled out in February is now predicting the production and flow of electricity from rooftop solar units across the state and automatically adjusting how much power from those panels can enter the grid.&nbsp;



Until now, much of the discussion around AI and energy has focused on the electricity demands of AI data centers (check out MIT Technology Review‚Äôs Power Hungry series for more on this).



‚ÄúWe‚Äôve been talking a lot about what the grid can do for AI and not nearly as much about what AI can do for the grid,‚Äù says Charles Hua, a coauthor of one of last year‚Äôs Energy Department reports who now serves executive director of PowerLines, a nonprofit that advocates for improving the affordability and reliability of US grids. ‚ÄúIn general, there‚Äôs a huge opportunity for grid operators, regulators, and other stakeholders in the utility regulatory system to use AI effectively and harness it for a more resilient, modernized, and strengthened grid.‚Äù&nbsp;



For now, Gopinathan says, he‚Äôs remaining cautiously optimistic.&nbsp;



‚ÄúI don‚Äôt want to overhype it,‚Äù he says.&nbsp;



Still, he adds, ‚Äúit‚Äôs a first step for bigger automation.‚Äù



‚ÄúRight now, this is more limited to our outage management system. Genie isn‚Äôt talking to our other parts yet,‚Äù he says. ‚ÄúBut I see a world where AI agents are able to do a lot more.‚Äù
‚Ä¢ The Download: cybersecurity‚Äôs shaky alert system, and mobile IVF
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Cybersecurity‚Äôs global alarm system is breaking down



Every day, billions of people trust digital systems to run everything from communication to commerce to critical infrastructure. But the global early warning system that alerts security teams to dangerous software flaws is showing critical gaps in coverage‚Äîand most users have no idea their digital lives are likely becoming more vulnerable.



Over the past eighteen months, two pillars of global cybersecurity have been shaken by funding issues: the US-backed National Vulnerability Database (NVD)‚Äîrelied on globally for its free analysis of security threats‚Äîand the Common Vulnerabilities and Exposures (CVE) program, the numbering system for tracking software flaws.&nbsp;



Although the situation for both has stabilized, organizations and governments are confronting a critical weakness in our digital infrastructure: Essential global cybersecurity services depend on a complex web of US agency interests and government funding that can be cut or redirected at any time. Read the full story.&nbsp;



‚ÄîMatthew King







The first babies have been born following ‚Äúsimplified‚Äù IVF in a mobile lab



This week I‚Äôm sending congratulations to two sets of new parents in South Africa. Babies Milayah and Rossouw arrived a few weeks ago. All babies are special, but these two set a new precedent. They‚Äôre the first to be born following &#8220;simplified&#8221; IVF performed in a mobile lab.



This new mobile lab is essentially a trailer crammed with everything an embryologist needs to perform IVF on a shoestring. It was designed to deliver reproductive treatments to people who live in rural parts of low-income countries, where IVF can be prohibitively expensive or even nonexistent. And best of all: it seems to work! Read our story about why it‚Äôs such an exciting development.&nbsp;



‚ÄîJessica Hamzelou¬†



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Trump is seeking huge cuts to basic scientific researchIf he gets his way, federal science funding will be slashed by a third for the next fiscal year. (NYT&nbsp;$)+&nbsp;The foundations of America‚Äôs prosperity are being dismantled.&nbsp;(MIT Technology Review)+&nbsp;Senators are getting ready to push back against proposed NASA cuts. (Bloomberg&nbsp;$)



2 Conspiracy theorists are starting to turn on TrumpHe whipped them all up over the supposed existence of Epstein‚Äôs client list, and now they‚Äôre mad nothing‚Äôs being released. (The Atlantic&nbsp;$)3 AI actually slows experienced software developers downThey end up wasting lots of time checking and correcting AI models‚Äô output. (Reuters&nbsp;$)4 The Pentagon is becoming the largest shareholder in a rare earth minerals companyIt shows just how much competition is hotting up to secure a steady supply of these materials. (Quartz&nbsp;$)+&nbsp;The race to produce rare earth elements. (MIT Technology Review)&nbsp;5 Solar power is starting to truly transform the world‚Äôs energy system&nbsp;Globally, roughly a third more power was generated from the sun this spring than last. (New Yorker&nbsp;$)6 Cops‚Äô favorite AI tool auto-deletes evidence of AI being used&nbsp;A pretty breathtaking attempt to avoid any sort of audit, transparency or accountability. (Ars Technica)+&nbsp;How a new type of AI is helping police skirt facial recognition bans.&nbsp;(MIT Technology Review)7 Why Chinese EV brands are being forced to go globalCompetition at home is becoming so intense that many have no choice but to seek profits elsewhere. (Rest of World)+&nbsp;China‚Äôs EV giants are betting big on humanoid robots.&nbsp;(MIT Technology Review)8 Which Big Tech execs are closest to the White House?&nbsp;Check out this scorecard showing how they‚Äôre all doing trying to stay in Trump‚Äôs good graces. (WSJ&nbsp;$)9 Elon Musk says Grok is coming to Tesla vehiclesYes, that‚Äôs the same Grok that keeps being racist. Shareholders must be delighted. (Insider&nbsp;$)+&nbsp;X is basically becoming a strip mine for AI training data.&nbsp;(Axios)10 Trump Mobile is charging people‚Äôs credit cards without explanationBut I‚Äôm sure it‚Äôs all perfectly explicable and above board, right? Right?! (404 Media)











Quote of the day



‚ÄúIt has been nonstop pandemonium.‚Äù



‚ÄîAugustus Doricko, who founded a cloud seeding startup two years ago, tells the¬†Washington Post¬†he‚Äôs received a deluge of fury online from conspiracy theorists who blame him for the catastrophic Texas floods.







One more thing



STEPHANIE ARNETT/MIT TECHNOLOGY REVIEW | LUMMI




What‚Äôs next for AI in 2025



For the last couple of years we‚Äôve had a go at predicting what‚Äôs coming next in AI. A fool‚Äôs game given how fast this industry moves. But we gave it a go anyway back in January. As we sail pass this year‚Äôs halfway mark, it‚Äôs a good time to ask: how well did we do?&nbsp;Check out our predictions, and see for yourself!



‚ÄîJames O&#8217;Donnell, Will Douglas Heaven &amp; Melissa Heikkil√§



This piece is part of MIT Technology Review‚Äôs What‚Äôs Next series, looking across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&nbsp;here.







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Let‚Äôs have more pop culture references¬†in journal article titles, please.+ Here‚Äôs some¬†inspiration¬†for things to cook this month (or, if it‚Äôs hot, just assemble).+ There‚Äôs something so relaxing about gazing at¬†these (award-winning!) landscape photos.¬†+ If you like birds, you‚Äôll enjoy¬†this artist‚Äôs work

üîí Cybersecurity & Privacy
‚Ä¢ DOGE Denizen Marko Elez Leaked API Key for xAI
  Marko Elez, a 25-year-old employee at Elon Musk&#8217;s Department of Government Efficiency (DOGE), has been granted access to sensitive databases at the U.S. Social Security Administration, the Treasury and Justice departments, and the Department of Homeland Security. So it should fill all Americans with a deep sense of confidence to learn that Mr. Elez over the weekend inadvertently published a private key that allowed anyone to interact directly with more than four dozen large language models (LLMs) developed by Musk&#8217;s artificial intelligence company xAI.
Image: Shutterstock, @sdx15.
On July 13, Mr. Elez committed a code script to GitHub called &#8220;agent.py&#8221; that included a private application programming interface (API) key for xAI. The inclusion of the private key was first flagged by GitGuardian, a company that specializes in detecting and remediating exposed secrets in public and proprietary environments. GitGuardian‚Äôs systems constantly scan GitHub and other code repositories for exposed API keys, and fire off automated alerts to affected users.
Philippe Caturegli, ‚Äúchief hacking officer‚Äù at the security consultancy Seralys,¬†said the exposed API key allowed access to at least 52 different LLMs used by xAI. The most recent LLM in the list was called &#8220;grok-4-0709&#8221; and was created on July 9, 2025.
Grok, the generative AI chatbot developed by xAI and integrated into Twitter/X, relies on these and other LLMs (a query to Grok before publication shows Grok currently uses Grok-3, which was launched in Feburary 2025). Earlier today, xAI announced that the Department of Defense will begin using Grok as part of a contract worth up to $200 million. The contract award came less than a week after Grok began spewing antisemitic rants and invoking Adolf Hitler.
Mr. Elez did not respond to a request for comment. The code repository containing the private xAI key was removed shortly after Caturegli notified Elez via email. However, Caturegli said the exposed API key still works and has not yet been revoked.
&#8220;If a developer can&#8217;t keep an API key private, it raises questions about how they&#8217;re handling far more sensitive government information behind closed doors,&#8221; Caturegli told KrebsOnSecurity.
Prior to joining DOGE, Marko Elez worked for a number of Musk&#8217;s companies. His DOGE career began at the Department of the Treasury, and a legal battle over DOGE&#8217;s access to Treasury databases showed Elez was sending unencrypted personal information in violation of the agency&#8217;s policies.
While still at Treasury, Elez resigned after The Wall Street Journal linked him to social media posts that advocated racism and eugenics. When Vice President J.D. Vance lobbied for Elez to be rehired, President Trump agreed and Musk reinstated him.
Since his re-hiring as a DOGE employee, Elez has been granted access to databases at one federal agency after another. TechCrunch reported in February 2025 that he was working at the Social Security Administration. In March, Business Insider found Elez was part of a DOGE detachment assigned to the Department of Labor.
Marko Elez, in a photo from a social media profile.
In April, The New York Times reported that Elez held positions at the U.S. Customs and Border Protection and the Immigration and Customs Enforcement (ICE) bureaus, as well as the Department of Homeland Security. The Washington Post later reported that Elez, while serving as a DOGE advisor at the Department of Justice, had gained access to the Executive Office for Immigration Review&#8217;s Courts and Appeals System (EACS).
Elez is not the first DOGE worker to publish internal API keys for xAI: In May, KrebsOnSecurity detailed how another DOGE employee leaked a private xAI key on GitHub for two months, exposing LLMs that were custom made for working with internal data from Musk&#8217;s companies, including SpaceX, Tesla and Twitter/X.
Caturegli said it&#8217;s difficult to trust someone with access to confidential government systems when they can&#8217;t even manage the basics of operational security.
&#8220;One leak is a mistake,&#8221; he said. &#8220;But when the same type of sensitive key gets exposed again and again, it‚Äôs not just bad luck, it‚Äôs a sign of deeper negligence and a broken security culture.&#8221;
‚Ä¢ UK Arrests Four in ‚ÄòScattered Spider‚Äô Ransom Group
  Authorities in the United Kingdom this week arrested four people aged 17 to 20 in connection with recent data theft and extortion attacks against the retailers Marks &amp; Spencer and Harrods, and the British food retailer Co-op Group. The breaches have been linked to a prolific but loosely-affiliated cybercrime group dubbed &#8220;Scattered Spider,&#8221; whose other recent victims include multiple airlines.
The U.K.&#8217;s National Crime Agency (NCA) declined verify the names of those arrested, saying only that they included two males aged 19, another aged 17, and 20-year-old female.
Scattered Spider is the name given to an English-speaking cybercrime group known for using social engineering tactics to break into companies and steal data for ransom, often impersonating employees or contractors to deceive IT help desks into granting access. The FBI warned last month that Scattered Spider had recently shifted to targeting companies in the retail and airline sectors.
KrebsOnSecurity has learned the identities of two of the suspects. Multiple sources close to the investigation said those arrested include Owen David Flowers, a U.K. man alleged to have been involved in the cyber intrusion and ransomware attack that shut down several MGM Casino properties in September 2023. Those same sources said the woman arrested is or recently was in a relationship with Flowers.
Sources told KrebsOnSecurity that Flowers, who allegedly went by the hacker handles &#8220;bo764,&#8221; &#8220;Holy,&#8221; and &#8220;Nazi,&#8221; was the group member who anonymously gave interviews to the media in the days after the MGM hack. His real name was omitted from a September 2024 story about the group because he was not yet charged in that incident.
The bigger fish arrested this week is 19-year-old Thalha Jubair,¬†a U.K. man whose alleged exploits under various monikers have been well-documented in stories on this site. Jubair is believed to have used the nickname &#8220;Earth2Star,&#8221; which corresponds to a founding member of the cybercrime-focused Telegram channel &#8220;Star Fraud Chat.&#8221;
In 2023, KrebsOnSecurity published an investigation into the work of three different SIM-swapping groups that phished credentials from T-Mobile employees and used that access to offer a service whereby any T-Mobile phone number could be swapped to a new device. Star Chat was by far the most active and consequential of the three SIM-swapping groups, who collectively broke into T-Mobile&#8217;s network more than 100 times in the second half of 2022.
Jubair allegedly used the handles &#8220;Earth2Star&#8221; and &#8220;Star Ace,&#8221; and was a core member of a prolific SIM-swapping group operating in 2022. Star Ace posted this image to the Star Fraud chat channel on Telegram, and it lists various prices for SIM-swaps.
Sources tell KrebsOnSecurity that Jubair also was a core member of the LAPSUS$ cybercrime group that broke into dozens of technology companies in 2022, stealing source code and other internal data from tech giants including Microsoft, Nvidia, Okta, Rockstar Games, Samsung, T-Mobile, and Uber.
In April 2022, KrebsOnSecurity published internal chat records from LAPSUS$, and those chats indicated Jubair was using the nicknames Amtrak and Asyntax. At one point in the chats, Amtrak told the LAPSUS$ group leader not to share T-Mobile&#8217;s logo in images sent to the group because he&#8217;d been previously busted for SIM-swapping and his parents would suspect he was back at it again.
As shown in those chats, the leader of LAPSUS$ eventually decided to betray Amtrak by posting his real name, phone number, and other hacker handles into a public chat room on Telegram.
In March 2022, the leader of the LAPSUS$ data extortion group exposed Thalha Jubair&#8217;s name and hacker handles in a public chat room on Telegram.
That story about the leaked LAPSUS$ chats connected Amtrak/Asyntax/Jubair to the identity &#8220;Everlynn,&#8221; the founder of a cybercriminal service that sold fraudulent &#8220;emergency data requests&#8221; targeting the major social media and email providers. In such schemes, the hackers compromise email accounts tied to police departments and government agencies, and then send unauthorized demands for subscriber data while claiming the information being requested can‚Äôt wait for a court order because it relates to an urgent matter of life and death.
The roster of the now-defunct &#8220;Infinity Recursion&#8221; hacking team, from which some member of LAPSUS$ hail.
Sources say Jubair also used the nickname &#8220;Operator,&#8221; and that until recently he was the administrator of the Doxbin, a long-running and highly toxic online community that is used to ‚Äúdox‚Äù or post deeply personal information on people. In May 2024, several popular cybercrime channels on Telegram ridiculed Operator after it was revealed that he&#8217;d staged his own kidnapping in a botched plan to throw off law enforcement investigators.
In November 2024, U.S. authorities charged five men aged 20 to 25 in connection with the Scattered Spider group, which has long relied on recruiting minors to carry out its most risky activities. Indeed, many of the group&#8217;s core members were recruited from online gaming platforms like Roblox and Minecraft in their early teens, and have been perfecting their social engineering tactics for years.
&#8220;There is a clear pattern that some of the most depraved threat actors first joined cybercrime gangs at an exceptionally young age,&#8221; said Allison Nixon, chief research officer at the New York based security firm Unit 221B. &#8220;Cybercriminals arrested at 15 or younger need serious intervention and monitoring to prevent a years long massive escalation.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from cybersecurity
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool.&nbsp;AI Testing and Evaluation: Learnings from Science and Industry,&nbsp;hosted by Microsoft Research‚Äôs&nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Sullivan speaks with Professor Ciaran Martin (opens in new tab) of the University of Oxford about risk assessment and testing in the field of cybersecurity. They explore the importance of differentiated standards for organizations of varying sizes, the role of public-private partnerships, and the opportunity to embed security into emerging technologies from the outset. Later, Tori Westerhoff (opens in new tab), a principal director on the Microsoft AI Red Team, joins Sullivan to talk about identifying vulnerabilities in AI products and services. Westerhoff describes AI security in terms she‚Äôs heard cybersecurity professionals use for their work‚Äîa team sport‚Äîand points to cybersecurity‚Äôs establishment of a shared language and understanding of risk as a model for AI security.








Learn more:




Introducing AI Red Teaming Agent: Accelerate your AI safety and security journey with Azure AI Foundry (opens in new tab)Azure AI Foundry Blog | April 2025



Lessons From Red Teaming 100 Generative AI ProductsPublication | January 2025



Learning from other domains to advance AI evaluation and testingMicrosoft Research Blog | June 2025



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Research










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today, I&#8217;m excited to welcome Ciaran Martin to the podcast to explore testing and risk assessment in cybersecurity. Ciaran is a professor of practice in the management of public organizations at the University of Oxford. He had previously founded and served as chief executive of the National Cyber Security Centre within the UK&#8217;s intelligence, security, and cyber agency.



And after our conversation, we&#8217;ll talk to Microsoft&#8217;s Tori Westerhoff, a principal director on Microsoft‚Äôs AI Red Team, about how we should think about these insights in the context of AI.



Hi, Ciaran. Thank you so much for being here today.



				
				
					



CIARAN MARTIN: Well, thanks so much for inviting me. It‚Äôs great to be here.



SULLIVAN: Ciaran, before we get into some regulatory specifics, it&#8217;d be great to hear a little bit more about your origin story, and just take us to that day‚Äîwho tapped you on the shoulder and said, ‚ÄúCiaran, we need you to run a national cyber center! Do you fancy building one?‚Äù



MARTIN: You could argue that I owe my job to Edward Snowden. Not an obvious thing to say. So the National Cyber Security Centre, which didn&#8217;t exist at the time‚ÄîI was invited to join the British government&#8217;s cybersecurity effort in a leadership role‚Äîis now a subset of GCHQ. That&#8217;s the digital intelligence agency. The equivalent in the US obviously is the NSA [National Security Agency]. It had been convulsed by the Snowden disclosures. It was an unprecedented challenge.



I was a 17-year career government fixer with some national security experience. So I was asked to go out and help with the policy response, the media response, the legal response. But I said, look, any crisis, even one as big as this, is over one way or the other in six months. What should I do long term? And they said, well, we were thinking of asking you to try to help transform our cybersecurity mission. So the National Cyber Security Centre was born, and I was very proud to lead it, and all in all, I did it for seven years from startup to handing it on to somebody else.



SULLIVAN: I mean, it&#8217;s incredible. And just building on that, people spend a significant portion of their lives online now with a variety of devices, and maybe for listeners who are newer to cybersecurity, could you give us the 90-second lightning talk? Kind of, what does risk assessment and testing look like in this space?



MARTIN: Well, risk assessment and testing, I think, are two different things. You can&#8217;t defend everything. If you defend everything, you&#8217;re defending nothing. So broadly speaking, organizations face three threats. One is complete disruption of their systems. So just imagine not being able to access your system. The second is data protection, and that could be sensitive customer information. It could be intellectual property. And the third is, of course, you could be at risk of just straightforward being stolen from. I mean, you don&#8217;t want any of them to happen, but you have to have a hierarchy of harm.



SULLIVAN: Yes.



MARTIN: So that&#8217;s your risk assessment.



The testing side, I think, is slightly different. One of the paradoxes, I think, of cybersecurity is for such a scientific, data-rich subject, the sort of metrics about what works are very, very hard to come by. So you&#8217;ve got boards and corporate leadership and senior governmental structures, and they say, ‚ÄúLook, how do I run this organization safely and securely?‚Äù And a cybersecurity chief within the organization will say, ‚ÄúWell, we could get this capability in.‚Äù Well, the classic question for a leadership team to ask is, well, what risk and harm will this reduce, by how much, and what&#8217;s the cost-benefit analysis? And we find that really hard.



So that&#8217;s really where testing and assurance comes in. And also as technology changes so fast, we have to figure out, well, if we&#8217;re worried about post-quantum cryptography, for example, what standards does it have to meet? How do you assess whether it&#8217;s meeting those standards? So it&#8217;s a huge issue in cybersecurity and one that we&#8217;re always very conscious of. It‚Äôs really hard.



SULLIVAN: Given the scope of cybersecurity, are there any differences in testing, let&#8217;s say, for maybe a small business versus a critical infrastructure operator? Are there any, sort of, metrics we can look at in terms of distinguishing risk or assessment?



MARTIN: There have to be. One of the reasons I think why we have to be is that no small business can be expected to take on a hostile nation-state that&#8217;s well equipped. You have to be realistic.



If you look at government guidance, certainly in the UK 15 years ago on cybersecurity, you were telling small businesses that are living hand to mouth, week by week, trying to make payments at the end of each month, we were telling them they needed sort of nation-state-level cyber defenses. That was never going to happen, even if they could afford it, which they couldn&#8217;t. So you have to have some differentiation. So again, you&#8217;ve got assessment frameworks and so forth where you have to meet higher standards. So there absolutely has to be that distinction. Otherwise, you end up in a crazy world of crippling small businesses with just unmanageable requirements which they&#8217;re never going to meet.



SULLIVAN: It&#8217;s such a great point. You touched on this a little bit earlier, as well, but just cybersecurity governance operates in a fast-moving technology and threat environment. How have testing standards evolved, and where do new technical standards usually originate?



MARTIN: I keep saying this is very difficult, and it is. [LAUGHTER] So I think there are two challenges. One is actually about the balance, and this applies to the technology of today as well as the technology of tomorrow. This is about, how do you make sure things are good enough without crowding out new entrants? You want people to be innovative and dynamic. You want disruptors in this business.



But if you say to them, ‚ÄúLook, well, you have to meet these 14 impossibly high technical standards before you can even sell to anybody or sell to the government,‚Äù whatever, then you&#8217;ve got a problem. And I think we&#8217;ve wrestled with that, and there&#8217;s no perfect answer. You just have to try and go to ‚Ä¶ find the sweet spot between two ends of a spectrum. And that&#8217;s going to evolve.



The second point, which in some respects if you&#8217;ve got the right capabilities is slightly easier but still a big call, is around, you know, those newer and evolving technologies. And here, having, you know, been a bit sort of gloomy and pessimistic, here I think is actually an opportunity. So one of the things we always say in cybersecurity is that the internet was built and developed without security in mind. And that was kind of true in the ‚Äô90s and the noughties, as we call them over here.



But I think as you move into things like post-quantum computing, applied use of AI, and so on, you can actually set the standards at the beginning. And that&#8217;s really good because it&#8217;s saying to people that these are the things that are going to matter in the post-quantum age. Here&#8217;s the outline of the standards you&#8217;re going to have to meet; start looking at them. So there&#8217;s an opportunity actually to make technology safer by design, by getting ahead of it. And I think that&#8217;s the era we&#8217;re in now.



SULLIVAN: That makes a lot of sense. Just building on that, do businesses and the public trust these standards? And I guess, which standard do you wish the world would just adopt already, and what&#8217;s the real reason they haven&#8217;t?



MARTIN: Well, again, where do you start? I mean, most members of the public quite rightly haven&#8217;t heard of any of these standards. I think public trust and public capital in any society matters. But I think it is important that these things are credible.



And there&#8217;s quite a lot of convergence between, you know, the top-level frameworks. And obviously in the US, you know, the NIST [National Institute of Standards and Technology] framework is the one that&#8217;s most popular for cybersecurity, but it bears quite a strong resemblance to the international one, ISO[/IEC] 27001, and there are others, as well. But fundamentally, they boil down to kind of five things. Do a risk assessment; work out what your crown jewels are. Protect your perimeter as best you can. Those are the first two.



The third one then is when your perimeter&#8217;s breached, be able to detect it more times than not. And when you can&#8217;t do that, you go to the fourth one, which is, can you mitigate it? And when all else fails, how quickly can you recover and manage it? I mean, all the standards are expressed in way more technical language than that, but fundamentally, if everybody adopted those five things and operated them in a simple way, you wouldn&#8217;t eliminate the harm, but you would reduce it quite substantially.



SULLIVAN: Which policy initiatives are most promising for incentivizing companies to undertake, you know, these cybersecurity testing parameters that you‚Äôve just outlined? Governments, including the UK, have used carrots and sticks, but what do you think will actually move the needle?



MARTIN: I think there are two answers to that, and it comes back to your split between smaller businesses and critically important businesses. In the critically important services, I think it&#8217;s easier because most industries are looking for a level playing field. In other words, they realize there have to be rules and they want to apply them to everyone.



We had a fascinating experience when I was in government back in around 2018 where the telecom sector, they came to us and they said, we&#8217;ve got a very good cooperative relationship with the British government, but it needs to be put on a proper legal footing because you&#8217;re just asking us nicely to do expensive things. And in a regulated sector, if you actually put in some rules‚Äîand please develop them jointly with us; that&#8217;s the crucial part‚Äîthen that will help because it means that we&#8217;re not going to our boards and saying, or our shareholders, and saying that we should do this, and they&#8217;re saying, ‚ÄúWell, do you have to do it? Are our competitors doing it?‚Äù And if the answer to that is, yes, we have to, and, yes, our competitors are doing it, then it tends to be OK.



The harder nut to crack is the smaller business. And I think there&#8217;s a real mystery here: why has nobody cracked a really good and easy solution for small business? We need to be careful about this because, you know, you can&#8217;t throttle small businesses with onerous regulation. At the same time, we&#8217;re not brilliant, I think, in any part of the world at using the normal corporate governance rules to try and get people to figure out how to do cybersecurity.



There are initiatives there that are not the sort of pretty heavy stick that you might have to take to a critical function, but they could help. But that is a hard nut to crack. And I look around the world, and, you know, I think if this was easy, somebody would have figured it out by now. I think most of the developed economies around the world really struggle with cybersecurity for smaller businesses.



SULLIVAN: Yeah, it&#8217;s a great point. Actually building on one of the comments you made on the role of, kind of, government, how do you see the role of private-public partnerships scaling and strengthening, you know, robust cybersecurity testing?



MARTIN: I think they&#8217;re crucial, but they have to be practical. I&#8217;ve got a slight, sort of, high horse on this, if you don&#8217;t mind, Kathleen. It&#8217;s sort of ‚Ä¶ [LAUGHS]



SULLIVAN: Of course.



MARTIN: I think that there are two types of public-private partnership. One involves committees saying that we should strengthen partnerships and we should all work together and collaborate and share stuff. And we tried that for a very long time, and it didn&#8217;t get us very far. There are other types.



We had some at the National Cyber Security Centre where we paid companies to do spectacularly good technical work that the market wouldn&#8217;t provide. So I think it&#8217;s sort of partnership with a purpose. I think sometimes, and I understand the human instinct to do this, particularly in governments and big business, they think you need to get around a table and work out some grand strategy to fix everything, and the scale of the ‚Ä¶ not just the problem but the scale of the whole technology is just too big to do that.



So pick a bit of the problem. Find some ways of doing it. Don&#8217;t over-lawyer it. [LAUGHTER] I think sometimes people get very nervous. Oh, well, is this our role? You know, should we be doing this, that, and the other? Well, you know, sometimes certainly in this country, you think, well, who&#8217;s actually going to sue you over this, you know? So I wouldn&#8217;t over-programmatize it. Just get stuck practically into solving some problems.



SULLIVAN: I love that. Actually, [it] made me think, are there any surprising allies that you&#8217;ve gained‚Äîyou know, maybe someone who you never expected to be a cybersecurity champion‚Äîthrough your work?



MARTIN: Ooh! That&#8217;s a ‚Ä¶ that&#8217;s a‚Ä¶ what a question! To give you a slightly disappointing answer, but it relates to your previous question. In the early part of my career, I was working in institutions like the UK Treasury long before I was in cybersecurity, and the treasury and the British civil service in general, but the treasury in particular sort of trained you to believe that the private sector was amoral, not immoral, amoral. It just didn&#8217;t have values. It just had bottom line, and, you know, its job essentially was to provide employment and revenue then for the government to spend on good things that people cared about. And when I got into cybersecurity and people said, look, you need to develop relations with this cybersecurity company, often in the US, actually. I thought, well, what&#8217;s in it for them?



And, sure, sometimes you were paying them for specific services, but other times, there was a real public spiritedness about this. There was a realization that if you tried to delineate public-private boundaries, that it wouldn&#8217;t really work. It was a shared risk. And you could analyze where the boundaries fell or you could actually go on and do something about it together. So I was genuinely surprised at the allyship from the cybersecurity sector. Absolutely, I really, really was. And I think it&#8217;s a really positive part of certainly the UK cybersecurity ecosystem.



SULLIVAN: Wonderful. Well, we&#8217;re coming to the end of our time here, but is there any maybe last thoughts or perhaps requests you have for our listeners today?



MARTIN: I think that standards, assurance, and testing really matter, but it&#8217;s a bit like the discussion we&#8217;re having over AI. Get all these things to take you 80, 90% of the way and then really apply your judgment. There&#8217;s been some bad regulation under the auspices of standards and assurance. First of all, it‚Äôs, have you done this assessment? Have you done that? Have you looked at this? Well, fine. And you can tick that box, but what does it actually mean when you do it? What bits that you know in your heart of hearts are really important to the defense of your organization that may not be covered by this and just go and do those anyway. Because sure it helps, but it&#8217;s not everything.



SULLIVAN: No. Great, great closing sentiment. Well, Ciaran, thank you for joining us today. This has been just a super fun conversation and really insightful. Just really enjoyed the conversation. Thank you.



MARTIN: My pleasure, Kathleen, thank you.



[TRANSITION MUSIC]



SULLIVAN: Now, I&#8217;m happy to introduce Tori Westerhoff. As a principal director on the Microsoft AI Red Team, Tori leads all AI security and safety red team operations, as well as dangerous capability testing, to directly inform C-suite decision-makers.



So, Tori, welcome!



TORI WESTERHOFF: Thanks. I am so excited to be here.



SULLIVAN: I&#8217;d love to just start a little bit more learning about your background. You&#8217;ve worn some very intriguing hats. I mean, cognitive neuroscience grad from Yale, national security consultant, strategist in augmented and virtual reality ‚Ä¶ how do those experiences help shape the way you lead the Microsoft AI Red Team?



WESTERHOFF: I always joke this is the only role I think will always combine the entire patchwork LinkedIn r√©sum√©. [LAUGHS]



I think I use those experiences to help me understand the really broad approach that AI Red Team‚Äîartist also known as AIRT; I&#8217;m sure I&#8217;ll slip into our acronym‚Äîhow we frame up the broad security implications of AI. So I think the cognitive neuroscience element really helped me initially approach AI hacking, right. There&#8217;s a lot of social engineering and manipulation within chat interfaces that are enabled by AI. And also, kind of, this, like, metaphor for understanding how to find soft spots in the way that you see human heuristics show up, too. And so I think that was actually my personal ‚Äúin‚Äù to getting hooked into AI red teaming generally.



But my experience in national security and I&#8217;d also say working through the AR/VR/metaverse space at the time where I was in it helped me balance both how our impact is framed, how we&#8217;re thinking about critical industries, how we&#8217;re really trying to push our understanding of where security of AI can help people the most. And also do it in a really breakneck speed in an industry that&#8217;s evolving all of the time, that&#8217;s really pushing you to always be at the bleeding edge of your understanding. So I draw a lot of the energy and the mission criticality and the speed from those experiences as we&#8217;re shaping up how we approach it.



SULLIVAN: Can you just give us a quick rundown? What does the Red Team do? What actually, kind of, is involved on a day-to-day basis? And then as we think about, you know, our engagements with large enterprises and companies, how do we work alongside some of those companies in terms of testing?



WESTERHOFF: The way I see our team is almost like an indicator light that works really part and parcel with product development. So the way we&#8217;ve organized our expert red teaming efforts is that we work with product development before anything ships out to anyone who can use it. And our job is to act as expert AI manipulators, AI hackers. And we are supposed to take the theories and methods and new research and harness it to find examples of vulnerabilities or soft spots in products to enable product teams to harden those soft spots before anything actually reaches someone who wants to use it.



So if we&#8217;re the indicator light, we are also not the full workup, right. I see that as measurement and evals. And we also are not the mechanic, which is that product development team that&#8217;s creating mitigations. It&#8217;s platform-security folks who are creating mitigations at scale. And there&#8217;s a really great throughput of insights from those groups back into our area where we love to inform about them, but we also love to add on to, how do we break the next thing, right? So it&#8217;s a continuous cycle.



And part of that is just being really creative and thinking outside of a traditional cybersecurity box. And part of that is also really thinking about how we pull in research‚Äîwe have a research function within our AI Red Team‚Äîand how we automate and scale. This year, we&#8217;ve pulled a lot of those assets and insights into the Azure [AI] Foundry AI Red Teaming Agent (opens in new tab). And so folks can now access a lot of our mechanisms through that. So you can get a little taste of what we do day to day in the AI Red Teaming Agent.



SULLIVAN: You recently‚Äîactually, with your team‚Äîpublished a report that outlined lessons from testing over a hundred generative AI products. But could you share a bit about what you learned? What were some of the important lessons? Where do you see opportunities to improve the state of red teaming as a method for probing AI safety?



WESTERHOFF: I think the most important takeaway from those lessons is that AI security is truly a team sport. You&#8217;ll hear cybersecurity folks say that a lot. And part of the rationale there is that the defense in depth and integrating and a view towards AI security through the entire development of AI systems is really the way that we&#8217;re going to approach this with intentionality and responsibility.



So in our space, we really focus on novel harm categories. We are pushing bleeding edge, and we also are pushing iterative and, like, contextually based red teaming in product dev. So outside of those hundred that we&#8217;ve done, there&#8217;s a community [LAUGHS] through the entire, again, multistage life cycle of a product that is really trying to push the cost of attacking those AI systems higher and higher with all of the expertise they bring. So we may be, like, the experts in AI hacking in that line, but there are also so many partners in the Microsoft ecosystem who are thinking about their market context or they really, really know the people who love their products. How are they using it?



And then when you bubble out, you also have industry and government who are working together to push towards the most secure AI implementation for people, right? And I think our team in particular, we feel really grateful to be part of the big AI safety and security ecosystem at Microsoft and also to be able to contribute to the industry writ large. 



SULLIVAN: As you know, we had a chance to speak with Professor Ciaran Martin from the University of Oxford about the cybersecurity industry and governance there. What are some of the ideas and tools from that space that are surfacing in how we think about approaching red teaming and AI governance broadly?



WESTERHOFF: Yeah, I think it&#8217;s such a broad set of perspectives to bring in, in the AI instance. Something that I&#8217;ve noticed interjecting into security at the AI junction, right, is that cybersecurity has so many decades of experience of working through how to build trustworthy computing, for example, or bring an entire industry to bear in that way. And I think that AI security and safety can learn a lot of lessons of how to bring clarity and transparency across the industry to push universal understanding of where the threats really are.



So frameworks coming out of NIST, coming out of MITRE that help us have a universal language that inform governance, I think, are really important because it brings clarity irrespective of where you are looking into AI security, irrespective of your company size, what you&#8217;re working on. It means you all understand, ‚ÄúHey, we are really worried about this fundamental impact.‚Äù And I think cybersecurity has done a really good job of driving towards impact as their organizational vector. And I am starting to see that in the AI space, too, where we&#8217;re trying to really clarify terms and threats.&nbsp;And you see it in updates of those frameworks, as well, that I really love.



So I think that the innovation is in transparency to folks who are really innovating and doing the work so we all have a shared language, and from that, it really creates communal goals across security instead of a lot of people being worried about the same thing and talking about it in a different way.



SULLIVAN: Mm-hmm. In the cybersecurity context, Ciaran really stressed matching risk frameworks to an organization&#8217;s role and scale. Microsoft plays many roles, including building models and shipping applications. How does your red teaming approach shift across those layers?&nbsp;



WESTERHOFF: I love this question also because I love it as part of our work. So one of the most fascinating things about working on this team has been the diversity of the technology that we end up red teaming and testing. And it feels like we&#8217;re in the crucible in that way. Because we see AI applied to so many different architectures, tech stacks, individual features, models, you name it.



Part of my answer is that we still care about the highest-impact things. And so irrespective of the iteration, which is really fascinating and I love, I still think that our team drives to say, ‚ÄúOK, what is that critical vulnerability that is going to affect people in the largest ways, and can we battle test to see if that can occur?‚Äù



So in some ways, the task is always the same. I think in the ways that we change our testing, we customize a lot to the access to systems and data and also people&#8217;s trust almost as different variables that could affect the impact, right.



So a good example is if we&#8217;re thinking through agentic frameworks that have access to functions and tools and preferential ability to act on data, it&#8217;s really different to spaces where that action may not be feasible, right. And so I think the tailoring of the way to get to that impact is hyper-custom every time we start an engagement. And part of it is very thesis driven and almost mechanizing empathy.



You almost need to really focus on how people could use, or misuse, in such a way that you can emulate it before to a really great signal to product development, to say this is truly what people could do and we want to deliver the highest-impact scenarios so you can solve for those and also solve the underlying patterns, actually, that could contribute to maybe that one piece of evidence but also all the related pieces of evidence. So singular drive but like hyper-, hyper-customization to what that piece of tech could do and has access to.



SULLIVAN: What are some of the unexplored testing approaches or considerations from cybersecurity that you think we should encourage AI technologists, policymakers, and other stakeholders to focus on? 



WESTERHOFF: I do love that AI humbles us each and every day with new capabilities and the potential for new capabilities. It&#8217;s not just saying, ‚ÄúHey, there&#8217;s one test that we want to try,‚Äù but more, ‚ÄúHey, can we create a methodology that we feel really, really solid about so that when we are asked a question we haven&#8217;t even thought of, we feel confident that we have the resources and the system?‚Äù



So part of me is really intrigued by the process that we&#8217;re asked to make without knowing what those capabilities are really going to bring. And then I think tactically, AIRT is really pushing on how we create new research methodologies. How are we investing in, kind of, these longer-term iterations of red teaming? So we&#8217;re really excited about pushing out those insights in an experimental and longer-term way.



I think another element is a little bit of that evolution of how industry standards and frameworks are updating to the AI moment and really articulating where AI is either furthering adversarial ability to create those harms or threats or identifying where AI has a net new harm. And I think that demystifies a little bit about what we talked about in terms of the lessons learned, that fundamentally, a lot of the things that we talk about are traditional security vulnerabilities, and we are standing on kind of that cybersecurity shoulder. And I&#8217;m starting to see those updates translate in spaces that are already considered trustworthy and kind of the basis on which not only cybersecurity folks build their work but also business decision-makers make decisions on those frameworks.



So to me, integration of AI into those frameworks by those same standards means that we&#8217;re evolving security to include AI. We aren&#8217;t creating an entirely new industry of AI security and that, I think, really helps anchor people in the really solid foundation that we have in cybersecurity anyways.



I think there&#8217;s also some work around how the cyber, like, defenses will actually benefit from AI. So we think a lot about threats because that&#8217;s our job. But the other side of cybersecurity is offense. And I&#8217;m seeing a ton of people come out with frameworks and methodologies, especially in the research space, on how defensive networks are going to be benefited from things like agentic systems.



Generally speaking, I think the best practice is to realize that we&#8217;re fundamentally still talking about the same impacts, and we can use the same avenues, conversations, and frameworks. We just really want them to be crisply updated with that understanding of AI applications.



SULLIVAN: How do you think about bringing others into the fold there? I think those standards and frameworks are often informed by technologists. But I&#8217;d love for you to expand [that to] policymakers or other kind of stakeholders in our ecosystem, even, you know, end consumers of these products. Like, how do we communicate some of this to them in a way that resonates and it has an impactful meaning?



WESTERHOFF: I&#8217;ve found the AI security-safety space to be one of the more collaborative. I actually think the fact that I&#8217;m talking to you today is probably evidence that a ton of people are bringing in perspectives that don&#8217;t only come from a long-term cybersecurity view. And I see that as a trend in how AI is being approached opposed to how those areas were moving earlier. So I think that speed and the idea of conversations and not always having the perfect answer but really trying to be transparent with what everyone does know is kind of a communal energy in the communities, at least, where we&#8217;re playing. [LAUGHS] So I am pretty biased but at least the spaces where we are.



SULLIVAN: No, I think we&#8217;re seeing that across the board. I mean, I&#8217;d echo [that] sitting in research, as well, like, that ability to have impact now and at speed to getting the amazing technology and models that we&#8217;re creating into the hands of our customers and partners and ecosystem is just underscored.



So on the note of speed, let&#8217;s shift gears a little bit to just a quick lightning round. I&#8217;d love to get maybe some quick thoughts from you, just 30-second answers here. I&#8217;ll start with one.



Which headline-grabbing AI threat do you think is mostly hot air?



WESTERHOFF: I think we should pay attention to it all. I&#8217;m a red team lead. I love a good question to see if we can find an answer in real life. So no hot air, just questions.



SULLIVAN: Is there some sort of maybe new tool that you can&#8217;t wait to sneak into the red team arsenal?



WESTERHOFF: I think there are really interesting methodologies that break our understanding of cybersecurity by looking at the intersection between different layers of AI and how you can manipulate AI-to-AI interaction, especially now when we&#8217;re looking at agentic systems. So I would say a method, not a tool.



SULLIVAN: So maybe ending on a little bit of a lighter note, do you have a go-to snack during an all-night red teaming session?



WESTERHOFF: Always coffee. I would love it to be a protein smoothie, but honestly, it is probably Trader Joe&#8217;s elote chips. Like the whole bag. [LAUGHTER] It‚Äôs going to get me through. I&#8217;m going to not love that I did it.



[MUSIC]



SULLIVAN: Amazing. Well, Tori, thanks so much for joining us today, and just a huge thanks also to Ciaran for his insights, as well.



WESTERHOFF: Thank you so much for having me. This was a joy.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!‚ÄØ



[MUSIC FADES]

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series

Opens in a new tabThe post AI Testing and Evaluation: Learnings from cybersecurity appeared first on Microsoft Research.
‚Ä¢ How AI will accelerate biomedical research and discovery
  In November 2022, OpenAI‚Äôs ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4&#8217;s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right‚Äîand what they didn‚Äôt foresee.



In this episode, Daphne Koller (opens in new tab), Noubar Afeyan (opens in new tab), and Dr. Eric Topol (opens in new tab), leaders in AI-driven medicine, join Lee to explore the rapidly evolving role of AI across the biomedical and healthcare landscape. Koller, founder and CEO of Insitro, shares how machine learning is transforming drug discovery, especially target identification for complex diseases like ALS, by uncovering biological patterns across massive datasets. Afeyan, founder and CEO of Flagship Pioneering and co-founder and chairman of Moderna, discusses how AI is being applied across biotech research and development, from protein design to autonomous science platforms. Topol, executive vice president of Scripps Research and founder and director of the Scripps Research Translational Institute, highlights how AI can today help mitigate and prevent the core diseases that erode our health and the possibility of realizing a virtual cell. Through his conversations with the three, Lee investigates how AI is reshaping the discovery, deployment, and delivery of medicine.¬†








Learn more:




How Machine Learning Is Revolutionising Drug Discovery (opens in new tab) (Koller)&nbsp;WIRED Health talk | March 2025



Insitro and Lilly Enter Strategic Agreements to Advance Novel Treatments for Metabolic Diseases (opens in new tab)&nbsp;(Koller)&nbsp;Insitro release | October 2024&nbsp;



2025 Annual Letter: Polyintelligence (opens in new tab) (Afeyan)&nbsp;Flagship Pioneering | 2025



The ultimate in mind extenders (opens in new tab) (Afeyan)&nbsp;McGill Daily article (college newspaper) | October 1982&nbsp;



Super Agers: An Evidence-Based Approach to Longevity (opens in new tab) (Topol)&nbsp;Book | May 2025&nbsp;



Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again (opens in new tab) (Topol)&nbsp;Book | March 2019&nbsp;



The AI Revolution in Medicine: GPT-4 and BeyondBook | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript&nbsp;



[MUSIC]



[BOOK PASSAGE]&nbsp;



PETER LEE: ‚ÄúCan GPT-4 indeed accelerate the progression of medicine ‚Ä¶ ? It seems like a tall order, but if I had been told six months ago that it could rapidly summarize any published paper, that alone would have satisfied me as a strong contribution to research productivity. ‚Ä¶ But now that I&#8217;ve seen what GPT-4 can do with the healthcare process, I expect a lot more in the realm of research.‚Äù&nbsp;



[END OF BOOK PASSAGE]



[THEME MUSIC]



This is The AI Revolution in Medicine, Revisited. I‚Äôm your host, Peter Lee.



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?



In this series, we‚Äôll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



[THEME MUSIC FADES]



The book passage I read at the top was from ‚ÄúChapter 8: Smarter Science,‚Äù which was written by Zak.



In writing the book, we were optimistic about AI‚Äôs potential to accelerate biomedical research and help get new and much-needed treatments and drugs to patients sooner. One area we explored was generative AI as a designer of clinical trials. We looked at generative AI‚Äôs adeptness at summarizing helping speed up pre-trial triage and research. We even went so far as to predict the arrival of a large language model that can serve as a central intellectual tool.&nbsp;



For a look at how AI is impacting biomedical research today, I‚Äôm excited to welcome Daphne Koller, Noubar Afeyan, and Eric Topol.&nbsp;



				
				
					



Daphne Koller is the CEO and founder of Insitro, a machine learning-driven drug discovery and development company that recently made news for its identification of a novel drug target for ALS and its collaboration with Eli Lilly to license Lilly&#8217;s biochemical delivery systems. Prior to founding Insitro, Daphne was the co-founder, co-CEO, and president of the online education platform Coursera.



Noubar Afeyan is the founder and CEO of Flagship Pioneering, which creates biotechnology companies focused on transforming human health and environmental sustainability. He is also co-founder and chairman of the messenger RNA company Moderna. An entrepreneur and biochemical engineer, Noubar has numerous patents to his name and has co-founded many startups in science and technology.



Dr. Eric Topol is the executive vice president of the biomedical research non-profit Scripps Research, where he founded and now directs the Scripps Research Translational Institute. One of the most cited researchers in medicine, Eric has focused on promoting human health and individualized medicine through the use of genomic and digital data and AI.&nbsp;



These three are likely to have an outsized influence on how drugs and new medical technologies soon will be developed.



[TRANSITION MUSIC]&nbsp;



Here‚Äôs my interview with Daphne Koller:



LEE: Daphne, I&#8217;m just thrilled to have you join us.&nbsp;



DAPHNE KOLLER: Thank you for having me, Peter. It&#8217;s a pleasure to be here.&nbsp;



LEE: Well, you know, you&#8217;re quite well-known across several fields. But maybe for some audience members of this podcast, they might not have encountered you before. So where I&#8217;d like to start is a question I&#8217;ve been asking all of our guests.



How would you describe what you do? And the way I kind of put it is, you know, how do you explain to someone like your parents what you do for a living?&nbsp;



KOLLER: So that answer obviously has shifted over the years.



What I would say now is that we are working to leverage the incredible convergence of very powerful technologies, of which AI is one but not the only one, to change the way in which we discover and develop new treatments for diseases for which patients are currently suffering and even dying.&nbsp;



LEE: You know, I think I&#8217;ve known you for a long time.&nbsp;



KOLLER: Longer than I think either of us care to admit.&nbsp;



LEE: [LAUGHS] In fact, I think I remember you even when you were still a graduate student. But of course, I knew you best when you took up your professorship at Stanford. And I always, in my mind, think of you as a computer scientist and a machine learning person. And in fact, you really made a big name for yourself in computer science research in machine learning.



But now you&#8217;re, you know, leading one of the most important biotech companies on the planet. How did that happen?



KOLLER: So people often think that this is a recent transition. That is, after I left Coursera, I looked around and said, ‚ÄúHmm. What should I do next? Oh, biotech seems like a good thing,‚Äù but that&#8217;s actually not the way it transpired.



This goes all the way back to my early days at Stanford, where, in fact, I was, you know, as a young faculty member in machine learning, because I was the first machine learning hire into Stanford&#8217;s computer science department, I was looking for really exciting places in which this technology could be deployed, and applications back then, because of scarcity of data, were just not that inspiring.



And so I looked around, and this was around the late ‚Äô90s, and realized that there was interesting data emerging in biology and medicine. My first application actually was in, interestingly, in epidemiology‚Äîpatient tracking and tuberculosis. You know, you can think of it as a tiny microcosm of the very sophisticated models that COVID then enabled in a much later stage.



LEE: Right.&nbsp;



KOLLER: And so initially, this was based almost entirely on just technical interest. It&#8217;s kind of like, oh, this is more interesting as a question to tackle than spam filtering. But then I became interested in biology in its own right, biology and medicine, and ended up having a bifurcated existence as a Stanford professor where half my lab continued to do core computer science research published in, you know, NeurIPS and ICML. And the other half actually did biomedical research that was published in, you know, Nature Cell [and] Science. So that was back in, you know, the early, early 2000s, and for most of my Stanford career, I continued to have both interests.



And then the Coursera experience kind of took me out of Stanford and put me in an industry setting for the first time in my life actually. But then when my time at Coursera came to an end, you know, I&#8217;d been there for five years. And if you look at the timeline, I left Stanford in early 2012, right as the machine learning revolution was starting. So I missed the beginning.



And it was only in like 2016 or so that, as I picked my head up over the trenches, like, ‚ÄúOh my goodness, this technology is going to change the world.‚Äù And I wanted to deploy that big thing towards places where it would have beneficial impact on the world, like to make the world a better place.



LEE:‚ÄØYeah.‚ÄØ



KOLLER: And so I decided that one of the areas where I could make a unique, differentiated impact was in really bringing AI and machine learning to the life sciences, having spent, you know, the majority of my career at the boundary of those two disciplines. And notice I say ‚Äúboundary‚Äù with deliberation because there wasn&#8217;t very much of an intersection.



LEE: Right.&nbsp;



KOLLER: I felt like I could do something that was unique.&nbsp;



LEE: So just to stick on you for a little bit longer, you know, we have been sort of getting into your origin story about what we call AI today‚Äîbut machine learning, so deep learning.&nbsp;



And, you know, there has always been a kind of an emotional response for people like you and me and now the general public about their first encounters with what we now call generative AI. I‚Äôd love to hear what your first encounter was with generative AI and how you reacted to this.&nbsp;



KOLLER: I think my first encounter was actually an indirect one. Because, you know, the earlier generations of generative AI didn‚Äôt directly touch our work at Insitro (opens in new tab).&nbsp;



And yet at the same time, I had always had an interest in computer vision. That was a large part of my non-bio work when I was at Stanford.&nbsp;



And so some of my earlier even presentations, when I was trying to convey to people back in 2016 how this technology was going to transform the world, I was talking about the incredible progress in image recognition that had happened up until that point.&nbsp;



So my first interaction was actually in the generative AI for images, where you are able to go the other way ‚Ä¶&nbsp;



LEE: Yes.&nbsp;



KOLLER: ‚Ä¶ where you can take a verbal description of an image and create‚Äîand this was back in the days when the images weren&#8217;t particularly photorealistic, but still a natural language description to an image was magic given that only two or three years before that, we were barely able to look at an image and write a short phrase saying, ‚ÄúThis is a dog on the beach.‚Äù And so that arc, that hockey curve, was just mind blowing to me.&nbsp;



LEE: Did you have moments of skepticism?&nbsp;



KOLLER: Yeah, I mean the early, you know, early versions of ChatGPT, where it was more like parlor tricks and poking it a little bit revealed all of the easy ways that one could break it and make it do really stupid things. I was like, yeah, OK, this is kind of cute, but is it going to actually make a difference? Is it going to solve a problem that matters?&nbsp;



And I mean, obviously, I think now everyone agrees that the answer is yes, although there are still people who are like, yeah, but maybe it&#8217;s around the edges. I&#8217;m not among them, by the way, but &#8230; yeah, so initially there were like, ‚ÄúYeah, this is cute and very impressive, but is it going to make a difference to a problem that matters?‚Äù&nbsp;



LEE: Yeah. So now, maybe this is a good time to get into what you&#8217;ve been doing with ALS [amyotrophic lateral sclerosis]. You know, there&#8217;s a knee-jerk reaction from the technology side to focus on designing small molecules, on predicting, you know, their properties, you know, maybe binding affinity or aspects of ADME [absorption, distribution, metabolism, and excretion], you know, like absorption or dispersion or whatever.&nbsp;



And all of that is very useful, but if I understand the work on ALS, you went to a much harder place, which is to actually identify and select targets.&nbsp;



KOLLER: That‚Äôs right.&nbsp;



LEE: So first off, just for the benefit of the standard listeners of this podcast, explain what that problem is in general.&nbsp;



KOLLER: No, for sure. And I think maybe I&#8217;ll start by just very quickly talking about the drug discovery and development arc, &#8230;



LEE: Yeah.



KOLLER: &#8230; which, by and large, consists of three main phases. That&#8217;s the standard taxonomy.&nbsp;The first is what&#8217;s called sometimes target discovery or identifying a therapeutic hypothesis, which looks like: if I modulate this target in this disease, something beneficial will happen.&nbsp;



Then, you have to take that target and turn it into a molecule that you can actually put into a person. It could be a small molecule. It could be a large molecule like an antibody, whatever. And then you have that construct, that molecule. And the last piece is you put it into a person in the context of a clinical trial, and you measure what has happened. And there&#8217;s been AI deployed towards each of those three stages in different ways.&nbsp;



The last one is mostly like an efficiency gain. You know, the trial is kind of already defined, and you want to deploy technology to make it more efficient and effective, which is great because those are expensive operations.&nbsp;



LEE: Yep.&nbsp;



KOLLER: The middle one is where I would say the vast majority of efforts so far has been deployed in AI because it is a nice, well-defined problem. It doesn&#8217;t mean it&#8217;s easy, but it&#8217;s one where you can define the problem. It is, I need to inhibit this protein by this amount, and the molecule needs to be soluble and whatever and go past the blood-brain barrier. And you know probably within a year and a half or so, or two, if you succeeded or not.&nbsp;



The first stage is the one where I would say the least amount of energy has gone because when you&#8217;re uncovering a novel target in the context of an indication, you don&#8217;t know that you&#8217;ve been successful until you go all the way to the end, which is the clinical trial, which is what makes this a long and risky journey. And not a lot of people have the appetite or the capital to actually do that.&nbsp;



However, in my opinion, and that of, I think, quite a number of others, it is where the biggest impact can be made. And the reason is that while pharma has its deficiencies, making good molecules is actually something they&#8217;re pretty good at.&nbsp;



It might take them longer than it should, maybe it&#8217;s not as efficient as it could be, but at the end of the day, if you tell them to drug A target, pharma is actually pretty good at generating those molecules. However, when you put those molecules into the clinic, 90% of them fail. And the reason they fail is not by and large because the molecule wasn&#8217;t good. In the majority of cases, it&#8217;s because the target you went after didn&#8217;t do anything useful in the context of the patient population in which you put it.&nbsp;



And so in order to fix the inefficiency of this industry, which is incredible inefficiency, you need to address the problem at the root, and the root is picking the right targets to go after. And so that is what we elected to do.&nbsp;



It doesn&#8217;t mean we don&#8217;t make molecules. I mean, of course, you can&#8217;t just end up with a target because a target is not actionable. You need to turn it into a molecule. And we absolutely do that. And by the way, the partnership with Lilly (opens in new tab) is actually one where they help us make a molecule.&nbsp;



LEE: Yes.&nbsp;



KOLLER: I mean, it&#8217;s our target. It&#8217;s our program. But Lilly is deploying its very state-of-the-art molecule-making capabilities to help us turn that target into a drug.&nbsp;



LEE: So let&#8217;s get now into the machine learning of this. Again, this just strikes me as such a difficult problem to solve.&nbsp;



KOLLER: Yeah.&nbsp;



LEE: So how does machine learning &#8230; how does AI help you?&nbsp;



KOLLER: So I think when you look at how people currently select targets, it&#8217;s a combination of oftentimes at this point, with an increasing respect for the power of human genetics, some search for a genetic association, oftentimes with a human-defined, highly subjective, highly noisy clinical outcome, like some ICD [International Classification of Diseases] code.&nbsp;



And those are often underpowered and very difficult to deconvolute the underlying biology. You combine that with some mechanistic interrogation in a highly reductionist model system looking at a small number of readouts, biochemical readouts, that a biologist thinks are relevant to the disease. Like does this make this, whatever, cholesterol go up or amyloid beta go down? Or whatever. And then you take that as the second stage, and you pick, based on typically human intuition about, Oh, this one looks good to me, and then you take that forward.&nbsp;



What we&#8217;re doing is an attempt to be as unbiased and holistic as possible. So, first of all, rather than rely on human-defined clinical endpoints, like this person has been diagnosed with diabetes or fatty liver, we try and measure as much as we can a holistic physiological state and then use machine learning to find structure, patterns in that human physiological readouts, imaging readouts, and omics readouts from blood, from tissue, different kinds of imaging, and say, these are different vectors that this disease takes, this group of individuals, and here&#8217;s a different group of individuals that maybe from a diagnostical perspective are all called the same thing, but they are actually exhibiting a very different biology underlying it.&nbsp;



And so that is something that doesn&#8217;t emerge when a human being takes a reductionist view to looking at this high-content data, and oftentimes, they don&#8217;t even look at it and produce an ICD code.&nbsp;



LEE: Right. Yep.&nbsp;



KOLLER: The same approach, actually even the same code base, is taken in the cellular data. So we don&#8217;t just say, ‚ÄúWell, the thing that matters is, you know, the total amount of lipid in the cell or whatever.‚Äù Rather, we say, ‚ÄúLet&#8217;s look at multiple readouts, multiple ways of looking at the cells, combine them using the power of machine learning.‚Äù And again, looking at imaging readouts where a human&#8217;s eyes just glaze over looking at even a few dozen cells, far less a few hundreds of millions of cells, and understand what are the different biological processes that are going on. What are the vectors that the disease might take you in this direction, in this group of cells, or in that direction?&nbsp;



And then importantly, we take all of that information from the human side, from the cellular side, across these different readouts, and we combine them using an integrative approach that looks at the combined weight of evidence and says, these are the targets that I have the greatest amount of conviction about by looking across all of that information. Whereas we know, and we know this, I&#8217;m sure you&#8217;ve seen this analysis done for clinicians, a human being typically is able to keep three or four things in their head at the same time.&nbsp;



LEE: Right.&nbsp;



KOLLER: A really good human being who&#8217;s really expert at what they do can maybe get to six to eight.&nbsp;



LEE: Yeah.&nbsp;



KOLLER: The machine learning has no problem doing a few hundred.&nbsp;



LEE: Right.&nbsp;



KOLLER: And so you put that together, and that allows you, to your earlier question, really select the targets around which you have the highest conviction. And then those are the ones that we then prioritize for interrogation in more expensive systems like mice and monkeys and then at the end of the day pick the small handful that one can afford to actually take into clinical trials.&nbsp;



LEE: So now, Insitro recently received $25 million in milestone payments from Bristol Myers Squibb (opens in new tab) after discovering and selecting a novel drug target for ALS. Can you tell us a little bit more about that?‚ÄØ



KOLLER: We are incredibly excited about the first novel target, and there is a couple of others just behind it in line that seem, you know, quite efficacious, as well, that truly seem to reverse, albeit in a cellular system, what we now understand to be ALS pathology across multiple different dimensions. There&#8217;s been obviously many attempts made to try and address ALS, which by the way, horrible, horrible disease, worse than most cancers. It kills you almost inevitably in three to five years in a particularly horrific way.&nbsp;



And what we have in our hands is a target that seems to revert a lot of the pathologies that are associated with the disease, which we now understand has to do with the mis-splicing of multiple proteins within the cell and creating defective versions of those proteins that are just not operational. And we are seeing reversion of many of those.&nbsp;



So can I tell you for sure it&#8217;ll work in a human? No, there&#8217;s many steps between now and then. But we couldn&#8217;t be more excited about the opportunity to provide what we hope will be a disease-modifying intervention for these patients who really desperately need something.&nbsp;



LEE: Well, it&#8217;s certainly been making waves in the biotech and biomedical world.&nbsp;



KOLLER: Thank you.&nbsp;



LEE: So we&#8217;ll be really watching very closely.&nbsp;



So, you know, I think just reflecting on, you know, what we missed and what we got right in our book, I think in our book, we did have the insight that there would be an ability to connect, say, genotypic and phenotypic data and, you know, just broadly the kinds of clinical measurements that get made on real patients and that these things could be brought together. And I think the work that you&#8217;re doing really illustrates that in a very, very sophisticated, very ambitious way.&nbsp;



But the fact that this could be connected all the way down to the biology, to the biochemistry, I think we didn&#8217;t have any clue what would happen, at least not this quickly.&nbsp;



KOLLER: Well, I think the &#8230;&nbsp;



LEE: And I realize, you&#8217;ve been at this for quite a few years, but still, it&#8217;s quite amazing.&nbsp;



KOLLER: The thread that connects them is human genetics. And I think that has, to us, been, sort of, the, kind of, the connective tissue that allows you to translate across different systems and say, ‚ÄúWhat does this gene do? What does this gene do in this organ and in that organ? What does it do in this type of cell and in that type of cell?‚Äù&nbsp;



And then use that as sort of the thread, if you will, that follows the impact of modulating this gene all the way from the simple systems where you can do the experiment to the complex systems where you can&#8217;t do the experiment until the very end, but you have the human genetics as a way of looking at the statistics and understanding what the impact might be.&nbsp;



LEE: So I&#8217;d like to now switch gears and take ‚Ä¶ I want to take two steps in the remainder of this conversation towards the future. So one step into that future, of course, we&#8217;re living through now, which is just all of the crazy pace of work and advancement in generative AI generally, you know, just the scale of transformers, of post-training, and now inference scale and reasoning models and so on. And where do you see all of that going with respect to the goals that you have and that Insitro has?&nbsp;



KOLLER: So I think first and foremost is the parallel, if you will, to the predictions that you focused on in your book, which is this will transform a lot of the core data processing tasks, the information tasks. And sure, the doctors and nurses is one thing. But if you just think of clinical trial operations or the submission of regulatory documents, these are all kind of simple data ‚Ä¶ they&#8217;re not simple, obviously, but they&#8217;re data processing tasks. They involve natural language. That&#8217;s not going to be our focus, but I hope that others will use that to make clinical trials faster, more efficient, less expensive.&nbsp;



There&#8217;s already a lot of progress that&#8217;s happening on the molecular design side of things and taking hypotheses and turning them quickly and effectively into molecules. As I said, this is part of our work that we absolutely do and we don&#8217;t talk about it very much, simply because it&#8217;s a very crowded landscape and a lot of companies are engaged on that. But I think it&#8217;s really important to be able to take biological insights and turn them into new molecules.&nbsp;



And then, of course, the transformer models and their likes play a very significant role in that sort of turning insights into molecules because you can have foundation models for proteins. There are increasing efforts to create foundation models for other categories of molecules. And so that will undoubtedly accelerate the process by which you can quickly generate different molecular hypotheses and test them and learn from what you did so that you can do fewer iterations ‚Ä¶&nbsp;



LEE: Right.&nbsp;



KOLLER: ‚Ä¶ before you converge on a successful molecule.&nbsp;



I do think that arguably the biggest impact as yet to be had is in that understanding of core human biology and what are the right ways to intervene in it. And that plays a role in a couple different ways. First of all, it certainly plays a role in which ‚Ä¶ if we are able to understand the human physiological state and, you know, the state of different systems all the way down to the cell level, that will inform our ability to pick hypotheses that are more likely to actually impact the right biologies underneath.&nbsp;



LEE: Yep. Yeah.&nbsp;



KOLLER: And the more data we&#8217;re able to collect about humans and about cells, the more successful our models will be at representing that human physiological state or the cell biological state and making predictions reliably on the impact of these interventions.&nbsp;



The other side of it, though, and this comes back, I think, to themes that were very much in your book, is this will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient.&nbsp;



LEE: Right.&nbsp;



KOLLER: And I think there&#8217;s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.&nbsp;



But I think the opportunity is still there. We just haven&#8217;t been able to bring it to life because of the lack of the right kind of data. And I think with the increasing amount of human, kind of, foundational data that we&#8217;re able to acquire, things that are not sort of distilled through the eye of a clinician, for example, ‚Ä¶&nbsp;



LEE: Yes.&nbsp;



KOLLER: ‚Ä¶ but really measurements of human pathology, we can start to get to some of that precision, carving out of the human population and then get to a world where we can prescribe the right medicine to the right patient and not only in cancer but also in other diseases that are also not a single disease.&nbsp;



LEE: All right, so now to wrap up this time together, I always try to ask one more provocative last question. One of the dreams that comes naturally to someone like me or any of my colleagues, probably even to you, is this idea of, you know, wouldn&#8217;t it be possible someday to have a foundation model for biology or for human biology or foundation model for the human cell or something along these lines?&nbsp;



And in fact, there are, of course, you and I are both aware of people who are taking that idea seriously and chasing after it. I have people in our labs that think hard about this kind of thing. Is it a reasonable thought at all?&nbsp;



KOLLER: I have learned over the years to avoid saying the word never because technology proceeds in ways that you often don&#8217;t expect. And so will we at some point be able to measure the cell in enough different ways across enough different channels at the same time that you can piece together what a cell does? I think that is eminently feasible, not today, but over time.&nbsp;



I don&#8217;t think it&#8217;s feasible using today&#8217;s technology, although the efforts to get there may expose where the biggest opportunities lie to, you know, build that next layer. So I think it&#8217;s good that people are working on really hard problems. I would also point out that even if one were to solve that really challenging problem of creating a model of a cell, there is thousands of different types of cells within the human body.&nbsp;



They&#8217;re very different. They also talk to each other ‚Ä¶&nbsp;



LEE: Yep.&nbsp;



KOLLER: ‚Ä¶ both within the cell type and across different cell types. So the combinatorial complexity of that system is, I think, unfathomable to many people. I mean, I would say to all of us.&nbsp;



LEE: Yeah.&nbsp;



KOLLER: And so even from that very lofty goal, there is multiple big steps that would need to be taken to a mechanistic model of the full organism. So will we ever get there? Again, you know, I don&#8217;t see a reason why this is impossible to do. So I think over time, technology will get better and will allow us to build more and more elaborate models of more and more complex systems.&nbsp;



Patients can&#8217;t wait &#8230;



LEE: Right. Yeah.&nbsp;



KOLLER: ‚Ä¶ for that to happen in order for us to get them better medicines. So I think there is a great basic science initiative on that side of things. And, in parallel, we need to make do with the data that we have or can collect or can print. We print a lot of data in our internal wet labs and get to drugs that are effective even though they don&#8217;t benefit from having a full-blown mechanistic model.&nbsp;



LEE: Last question: where do you think we&#8217;ll be in five years?&nbsp;



KOLLER: Phew. If I had answered that question five years ago, I would have been very badly embarrassed at the inaccuracy of my answer. [LAUGHTER] So I will not answer it today either.&nbsp;



I will say that the thing about exponential curves is that they are very, very tricky, and they move in unexpected ways. I would hope that in five years, we will have made a sufficient investment in the generation of scientific data that we will be able to move beyond data that was generated entirely by humans and therefore insights that are derivative of what people already know to things that are truly novel discoveries.&nbsp;



And I think in order to do that in, you know, math, maybe because math is entirely conceptual, maybe you can do that today. Math is effectively a construct of the human mind. I don&#8217;t think biology is a construct of the human mind, and therefore one needs to collect enough data to really build those models that will give rise to those novel insights.&nbsp;



And that&#8217;s where I hope we will have made considerable progress in five years.&nbsp;



LEE: Well, I&#8217;m with you. I hope so, too. Well, you know, thank you, Daphne, so much for this conversation. I learn a lot talking to you, and it was great to, you know, connect again on this. And congratulations on all of this success. It&#8217;s really groundbreaking.&nbsp;



KOLLER: Thank you very much, Peter. It was a pleasure chatting with you, as well.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: I still think of Daphne first and foremost as an AI researcher. And for sure, her research work in machine learning continues to be incredibly influential to this day. But it&#8217;s her work on AI-enhanced drug development that now is on the verge of making a really big difference on some of the most difficult diseases afflicting people today.&nbsp;



In our book, Carey, Zak, and I predicted that AI might be a meaningful accelerant in biomedical research, but I don&#8217;t know that we foresaw the incredible potential specifically in drug development.&nbsp;



Today, we&#8217;re seeing a flurry of activity at companies, universities, and startups on generative AI systems that aid and maybe even completely automate the design of new molecules as drug candidates. But now, in our conversation with Daphne, seeing AI go even further than that to do what one might reasonably have assumed to be impossible, to identify and select novel drug targets, especially for a neurodegenerative disease like ALS, it&#8217;s just, well, mind blowing.‚ÄØ



Let&#8217;s continue our deep dive on AI and biomedical research with this conversation with Noubar Afeyan:&nbsp;



LEE: Noubar, thanks so much for joining. I&#8217;m really looking forward to this conversation.&nbsp;



NOUBAR AFEYAN: Peter, thanks. Thrilled to be here.&nbsp;



LEE: While I think most of the listeners to this podcast have heard of Flagship Pioneering (opens in new tab), it&#8217;s still worth hearing from you, you know, what is Flagship? And maybe a little bit about your background. And finally, you found a way to balance science and business creation. And so, you know, your approach and philosophy to all of that.&nbsp;



AFEYAN: Well, great. So maybe I&#8217;ll just start out by way of quick background. You know, my &#8230; and since we&#8217;re going talk about AI, I&#8217;ll also highlight my first contact with the topic of AI. So as an undergraduate in 1980 up at McGill University, I was an engineering student, but I was really captivated by, at that time, the talk on the campus around the expert system, heuristic-based, rule-based kind of programs.&nbsp;



LEE: Right.&nbsp;



AFEYAN: And so actually I had the dubious distinction of writing my one and only college newspaper article. [LAUGHTER] That was a short career. And it was all about how artificial intelligence would be impacting medicine, would be impacting, you know, speech capture, translation, and some of the ideas that were there that it&#8217;s interesting to see now 45 years later re-emerge with some of the new learning-based models.&nbsp;



My journey after college ended up taking me into biotechnology. In the early ‚Äô80s, I came to MIT to do a PhD. At the time, the field was brand new. I ended up being the first PhD graduate from MIT in this combination biology and engineering degree. And since then, I&#8217;ve basically been‚Äîso since 1987‚Äîa founder, a technologist in the space of biotechnology for human health and as well for planetary health.&nbsp;



And then in 1999/2000 formed what is now Flagship Pioneering, which essentially was an attempt to bring together the three elements of what we know are important in startups. That is scientific capital, human capital, and financial capital. Right now, startups get that from different places. The science in our fields mostly come from academia, research hospitals. The human capital comes from other startups ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: ‚Ä¶ or large companies or some academics leave. And then the financial capital is usually venture capital, but there&#8217;s also now more and more other deeper pockets of money.&nbsp;



What we thought was, what if all that existed in one entity and instead of having to convince each other how much they should believe the other if we just said, ‚ÄúLet&#8217;s use that power to go work on much further out things‚Äù? But in a way where nobody would believe it in the beginning, but we could give ourselves a little bit of time to do impactful big things.&nbsp;



Twenty-five years later, that&#8217;s the road we&#8217;ve stayed on.&nbsp;



LEE: OK. So let&#8217;s get into AI. Now, you know, what I&#8217;ve been asking guests is kind of an origin story. And there&#8217;s the origin story of contact with AI, you know, before the emergence of generative AI and afterwards. I don&#8217;t think there&#8217;s much of a point to asking you the pre-ChatGPT. But ‚Ä¶ so let&#8217;s focus on your first encounter with ChatGPT or generative AI. When did that happen, and what went through your head?&nbsp;



AFEYAN: Yeah. So, if you permit me, Peter, just for very briefly, let me actually say I had the interesting opportunity over the last 25 years to actually stay pretty close to the machine learning world ‚Ä¶&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: ‚Ä¶ because one, as you well know, among the most prolific users of machine learning has been the bioinformatics computational biology world because it&#8217;s been so data rich that anything that can be done, people have thrown at these problems because unlike most other things, we&#8217;re not working on man-made data. We&#8217;re looking at data that comes from nature, the complexity of which far exceeds our ability to comprehend.&nbsp;



So you could imagine that any approach to statistically reduce complexity, get signal out of scant data‚Äîthat&#8217;s a problem that&#8217;s been around.&nbsp;



The other place where I&#8217;ve been exposed to this, which I&#8217;m going to come back to because that&#8217;s where it first felt totally different to me, is that some 25 years ago, actually the very first company we started was a company that attempted to use evolutionary algorithms to essentially iteratively evolve consumer-packaged goods online. Literally, we tried to, you know, consider features of products as genes and create little genomes of them. And by recombination and mutation, we could create variety. And then we could get people through panels online‚Äîthis was 2002/2003 timeframe‚Äîwe could essentially get people through iterative cycles of voting to create a survival of the fittest. And that&#8217;s a company that was called Affinnova.&nbsp;



The reason I say that is that I knew that there‚Äôs a much better way to do this if only: one, you can generate variety ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: ‚Ä¶ without having to prespecify genes. We couldn‚Äôt do that before. And, two, which we‚Äôve come back to nowadays, you can actually mimic how humans think about voting on things and just get rid of that element of it.&nbsp;



So then to your question of when does this kind of begin to feel different? So you could imagine that in biotechnology, you know, as an engineer by background, I always wanted to do CAD, and I picked the one field in which CAD doesn&#8217;t exist, which is biology. Computer-aided design is kind of a notional thing in that space. But boy, have we tried. For a long time, &#8230;



LEE: Yep.&nbsp;



AFEYAN: &#8230; people would try to do, you know, hidden Markov models of genomes to try to figure out what should be the next, you know, base that you may want to or where genes might be, etc. But the notion of generating in biology has been something we&#8217;ve tried for a while. And in the late teens, so kind of 2018, ‚Äô17, ‚Äô18, because we saw deep learning come along, and you could basically generate novelty with some of the deep learning models ‚Ä¶ and so we started asking, ‚ÄúCould you generate a protein basically by training a correspondence table, if you will, between protein structures and their underlying DNA sequence?‚Äù Not their protein sequence, but their DNA sequence.&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: So that&#8217;s a big leap. So ‚Äô17/‚Äô18, we started this thing. It was called 56. It was FL56, Flagship Labs 56, our 56th project.&nbsp;



By the way, we started this parallel one called ‚Äú57‚Äù that did it in a very different way. So one of them did pure black box model-building. The other one said, you know what, we don&#8217;t want to do the kind of &#8230; at that time, AlphaFold was in its very early embodiments. And we said, ‚ÄúIs there a way we could actually take little, you know, multi amino acid kind of almost grammars, if you will, a little piece, and then see if we could compose a protein that way?‚Äù So we were experimenting.&nbsp;



And what we found was that actually, if you show enough instances and you could train a transformer model‚Äîback in the day, that&#8217;s what we were using‚Äîyou could actually, say, predict another sequence that should have the same activity as the first one.&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: So we trained on green fluorescent proteins. Now, we&#8217;re talking about seven years ago. We trained on enzymes, and then we got to antibodies.&nbsp;



With antibodies, we started seeing that, boy, this could be a pretty big deal because it has big market impact. And we started bringing in some of the diffusion models that were beginning to come along at that time. And so we started getting much more excited. This was all done in a company that subsequently got renamed from FL56 to Generate:Biomedicines (opens in new tab), ‚Ä¶&nbsp;



LEE: Yep, yep.&nbsp;



AFEYAN: ‚Ä¶ which is one of the leaders in protein design using the generative techniques. It was interesting because Generate:Biomedicines is a company that was called that before generative AI was a thing, [LAUGHTER] which was kind of very ironic.&nbsp;



And, of course, that team, which operates today very, very kind of at the cutting edge, has published their models. They came up with this first Chroma (opens in new tab) model, which is a diffusion-based model, and then started incorporating a lot of the LLM capabilities and fusing them.&nbsp;



Now we&#8217;re doing atomistic models and many other things. The point being, that gave us a glimpse of how quickly the capability was gaining, ‚Ä¶&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: ‚Ä¶ just like evolution shows you. Sometimes evolution is super silent, and then all of a sudden, all hell breaks loose. And that&#8217;s what we saw.&nbsp;



LEE: Right. One of the things that I reflect on just in my own journey through this is there are other emotions that come up. One that was prominent for me early on was skepticism. Were there points when even in your own work, transformer-based work on this early on, that you had doubts or skepticism that these transformer architectures would be or diffusion-based approaches would be worth anything?&nbsp;



AFEYAN: You know, it&#8217;s interesting, I think that, I&#8217;m going to say this to you in a kind of a friendly way, but you&#8217;ll understand what I mean. In the world I live in, it&#8217;s kind of like the slums of innovation, [LAUGHTER] kind of like just doing things that are not supposed to work. The notion of skepticism is a luxury, right. I assume everything we do won&#8217;t work. And then once in a while I&#8217;m wrong.&nbsp;



And so I don&#8217;t actually try to evaluate whether before I bring something in, like just think about it. We, some hundred or so times a year, ask ‚Äúwhat if‚Äù questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that&#8217;s testable. Then we go into a lab, and we test it.&nbsp;



So in that world, right, sitting there going, like, ‚ÄúHow do I know this transformer is going to work?‚Äù The answer is, ‚ÄúFor what?‚Äù Like, it&#8217;s going to work. To make something up &#8230; well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.&nbsp;



So it&#8217;s just such a different use that, of course, I have trained scientific skepticism, but it&#8217;s a little bit like looking at a competitive situation in an ecology and saying, ‚ÄúI bet that thing&#8217;s going to die.‚Äù Well, you&#8217;d be right‚Äîmost of the time, you&#8217;d be right. [LAUGHTER]&nbsp;



So I just don&#8217;t ‚Ä¶ like, it ‚Ä¶ and that&#8217;s why‚ÄîI guess, call me an early adopter‚Äîfor us, things that could move the needle even a little, but then upon repetition a lot, let alone this, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: ‚Ä¶ you have to embrace. You can&#8217;t wait there and say, I&#8217;ll embrace it once it&#8217;s ready. And so that&#8217;s what we did.&nbsp;



LEE: Hmm. All right. So let&#8217;s get into some specifics and what you are seeing either in your portfolio companies or in the research projects or out in the industry. What is going on today with respect to AI really being used for something meaningful in the design and development of drugs?&nbsp;



AFEYAN: In companies that are doing as diverse things as‚Äîlet me give you a few examples‚Äîa project that&#8217;s now become a named company called ProFound Therapeutics (opens in new tab) that literally discovered three, four years ago, and would not have been able to without some of the big data-model-building capabilities, that our cells make literally thousands, if not tens of thousands, of more proteins than we were aware of, full stop.&nbsp;



We had done the human genome sequence, there was 20,000 genes, we thought that there was ‚Ä¶&nbsp;



LEE: Wow.&nbsp;



AFEYAN: ‚Ä¶ maybe 70-80,000, 100,000 proteins, and that&#8217;s that. And it turns out that our cells have a penchant to express themselves in the form of proteins, and they have many other ways than we knew to do that.&nbsp;



Now, so what does that mean? That means that we have generated a massive amount of data, the interpretation of which, the use of which to guide what you do and what these things might be involved with is purely being done using the most cutting-edge data-trained models that allow you to navigate such complexity.&nbsp;



LEE: Wow. Hmm.&nbsp;



AFEYAN: That&#8217;s just one example. Another example: a company called Quotient Therapeutics (opens in new tab), again three, four years old. I can talk about the ones that are three, four years old because we&#8217;ve kind of gotten to a place where we&#8217;ve decided that it&#8217;s not going to fail yet, [LAUGHTER] so we can talk about it.&nbsp;



You know, we discovered‚Äîour team discovered‚Äîthat in our cells, right, so we know that when we get cancer, our cells have genetic mutations in them or DNA mutations that are correlated and often causal to the hyperproliferative stages of cancer. But what we assume is that all the other cells in our body, pretty much, have one copy of their genes from our mom, one copy from our dad, and that&#8217;s that.&nbsp;



And when very precise deep sequencing came along, we always asked the question, ‚ÄúHow much variation is there cell to cell?‚Äù&nbsp;



LEE: Right.&nbsp;



AFEYAN: And the answer was it&#8217;s kind of noise, random variation. Well, our team said, ‚ÄúWell, what if it&#8217;s not really that random?‚Äù because upon cell division cycles, there&#8217;s selection happening on these cells. And so not just in cancer but in liver cells, in muscle cells, in skin cells ‚Ä¶&nbsp;



LEE: Oh, interesting.&nbsp;



AFEYAN: ‚Ä¶ can you imagine that there&#8217;s an evolutionary experiment that is favoring either compensatory mutations that are helping you avoid disease or disease-caused mutations that are gaining advantage as a way to understand the mechanism? Sure enough‚ÄîI wouldn&#8217;t be telling you otherwise‚Äîwith massive amount of single cell sequencing from individual patient samples, we&#8217;ve now discovered that the human genome is mutated on average in our bodies 10,000 times, like over every base, like, it&#8217;s huge numbers.&nbsp;



And we&#8217;re finding very interesting big signals come out of this massive amount of data. By the way, data of the sort that the human mind, if it tries to assign causal explanations to what&#8217;s happening ‚Ä¶&nbsp;



LEE: Right.&nbsp;



AFEYAN: ‚Ä¶ is completely inadequate.&nbsp;



LEE: When you think about a language model, we&#8217;re learning from human language, and the totality of human language‚Äîat least relative to what we&#8217;re able to compute today in terms of constructing a model‚Äîthe totality of human language is actually pretty limited. And in fact, you know, as is always written about in click-baity titles, you know, the big model builders are actually starting to run short.&nbsp;



AFEYAN: Running out, running out, yes. [LAUGHTER]&nbsp;



LEE: But one of the things that perplexes me and maybe even worries me‚Äîlike these two examples‚Äîare generally in the realm of cellular biology and the complexity. Let&#8217;s just take the example of your company, ProFound. You know, the complexity of what&#8217;s going on and the potential genetic diversity is such that, can we ever have enough data? You know, because there just aren&#8217;t that many human beings. There just aren&#8217;t that many samples.&nbsp;



AFEYAN: Well, it depends on what you want to train, right. So if you want to train a de novo evolutionary model that could take you from bacteria to human mammalian cells and the like, there may not be‚Äîand I&#8217;m not an expert in that‚Äîbut that&#8217;s a question that we often kind of think about.&nbsp;



But if you&#8217;re trying to train a &#8230; like you know what the proteins we know about, how they interact with pathways and disease mechanisms and the like. Now all of a sudden you find out that there&#8217;s a whole continent of them missing in your explanations. But there are things you can reason, in quotations, through analogy, functional analogy, sequence analogy, homology. So there&#8217;s a lot of things that we could do to essentially make use of this, even though you may not have the totality of data needed to, kind of, predict, based on a de novo sequence, exactly what it&#8217;s going to do.&nbsp;



So I agree with the comparison. But &#8230; but you&#8217;re right. The complexity is ‚Ä¶ just keep in mind, on average, a protein may be interacting with 50 to 100 other proteins.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So if you find thousands of proteins, you&#8217;ve found a massive interaction space through which information is being processed in a living cell.&nbsp;



LEE: But do you find in your AI companies that access to data ends up being a key challenge? Or, you know, how central is that?&nbsp;



AFEYAN: Access to data is a key challenge for the companies we have that are trying to build just models. But that&#8217;s the minority of things we do. The majority of things we do is to actually co-develop the data and the models. And as you know well, because you guys, you know, have given us some ideas around this space, that, you know, you could generate data and then think about what you&#8217;re to do with it, which is the way biotech is operated with bioinformatics.&nbsp;



LEE: Right, right.&nbsp;



AFEYAN: Or you could generate bespoke data that is used to train the model that&#8217;s quite separate from what you would have done in the natural course of biology. So we&#8217;re doing much more of the latter of late, and I think that&#8217;ll continue. So, but these things are proliferating.&nbsp;



I mean, it&#8217;s hard to find a place where we&#8217;re not using this. And the ‚Äúthis‚Äù is any and all data-driven model building, generative, LLM-based, but also every other technique to make progress.&nbsp;



LEE: Sure. So now moving away from the straight biochemistry applications, what about AI in the process of building a business, of making investment decisions, of actually running an operation? What are you seeing there?&nbsp;



AFEYAN: So, well, you know, Moderna, which is a company that I&#8217;m quite proud of being a founder and chairman of, has adopted a significant, significant amount of AI embedded into their operations in all aspects: from the manufacturing, quality control, the clinical monitoring, the design‚Äîevery aspect. And in fact, they&#8217;ve had a partnership that they&#8217;ve had for a little while here with OpenAI, and they&#8217;ve tried many different ways to stay at the cutting edge of that.&nbsp;



So we see that play out at some scale. That‚Äôs a 5,000-, 6,000-person organization, and what they&#8217;re doing is a good example of what early adopters would do, at least in our kind of biotechnology company.&nbsp;



But then, you know, in our space, I would say the efficiency impact is kind of no different, than, you know, anywhere else in academia you might adopt it or in other kinds of companies. But where I find it an interesting kind of maybe segue is the degree to which it may fundamentally change the way we think about how to do science, which is a whole other use, right?&nbsp;



LEE: Right.&nbsp;



AFEYAN: So it&#8217;s not an efficiency gain per se, although it&#8217;s maybe an effectiveness gain when it comes to science, but can you just fundamentally train models to generate hypotheses?&nbsp;



LEE: Yep.&nbsp;



AFEYAN: And we have done that, and we&#8217;ve been doing this for the last three years. And now it&#8217;s getting better and better, the better these reasoning engines are getting and kind of being able to extrapolate and train for novelty. Can you convert that to the world&#8217;s best experimental protocol to very precisely falsify your hypothesis, on and on?&nbsp;



That closing of that loop, kind of what we call autonomous science, which we&#8217;ve been trying to do for the last two, three years and are making some progress in, that to me is another kind of bespoke use of these things, not to generate molecules in its chemistry, but to change the behavior of how science is done.&nbsp;



LEE: Yeah. So I always end with a couple of provocative questions, but I need‚Äîbefore we do that, while we&#8217;re on this subject‚Äîto get your take on Lila Sciences (opens in new tab).&nbsp;



And there is a vision there that I think is very interesting. It&#8217;d be great to hear it described by you.&nbsp;



AFEYAN: Sure. So Lila, after operating for two to three years in kind of a preparatory kind of stealth mode, we&#8217;ve now had a little bit more visibility around, and essentially what we&#8217;re trying to do there is to create what we call automated science factories, and such a factory would essentially be able to take problems, either computationally specified or human-specified, and essentially do the experimental work in order to either make an optimization happen or enable something that just didn‚Äôt exist. And it‚Äôs really, at this point, we‚Äôve shown proof of concept in narrow areas.&nbsp;



LEE: Yep.&nbsp;



AFEYAN: But it‚Äôs hard to say that if you can do this, you can‚Äôt do some other things, so we‚Äôre just expanding it that way. We don‚Äôt think we need a complete proof or complete demonstration of it for every aspect.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So we&#8217;re just kind of being opportunistic. The idea for Lila is to partner with a number of companies. The good news is, within Flagship, there&#8217;s 48 of them. And so there&#8217;s a whole lot of them they can partner with to get their learning cycles. But eventually they want to be a real alternative to every time somebody has an idea, having to kind of go into a lab and manually do this.&nbsp;



I do want to say one thing we touched on, Peter, though, just on that front, which is &#8230;&nbsp;



LEE: Yep.&nbsp;



AFEYAN: &#8230; if you say, like, ‚ÄúWhat problem is this going to solve?‚Äù It&#8217;s several but an important one is just the flat-out human capacity to reason on this much data and this much complexity that is real. Because nature doesn&#8217;t try to abstract itself in a human understandable form.&nbsp;



LEE: Right. Yeah.&nbsp;



AFEYAN: In biology, since it&#8217;s kind of like progress happens through evolutionary kind of selections, the evidence of which [has] long been lost, and so therefore, you just see what you have, and then it has a behavior. I really do think that there&#8217;s something to be said, and I want to‚Äîjust for your audience‚Äîlay out a provocative, at least, thought on all this, which Lila is a beginning embodiment of, which is that I really think that what&#8217;s going to happen over the next five, 10 years, even while we&#8217;re all fascinated with the impending arrival of AGI [artificial general intelligence] is really what I call poly-intelligence, which is the combination of human intelligence, machine intelligence, AI, and nature&#8217;s intelligence.&nbsp;



We&#8217;re all fascinated at the human-machine interface. We know the human-nature interface, but imagine the machine-nature interface‚Äîthat is, actually letting loose a digital kind of information processing life form through the algorithms that are being developed and the commensurately complex, maybe much more complex. We&#8217;ll see. And so now the question becomes, what does the human do?&nbsp;



And we&#8217;re living in a world which is human dominated, which means the humans say, ‚ÄúIf I don&#8217;t understand it, it&#8217;s not real, basically. And if I don&#8217;t understand it, I can&#8217;t regulate it.‚Äù And we&#8217;re going to have to make peace with the fact that we&#8217;re not going to be able to predictably affect things without necessarily understanding them the way we could if we just forced ourselves to only work on problems we can understand. And that world we&#8217;re not ready for at all.&nbsp;



LEE: Yeah. All right. So this one I predict is going to be a little harder for you because I think while you think about the future, you live very much in the present. But I&#8217;d like you to make some predictions about what the biotech and biopharmaceutical industries are going to be able to do two years from now, five years from now, 10 years from now.&nbsp;



AFEYAN: Yeah, well, it&#8217;s hard for me because you know my nature, which is that I think this is all emergent.&nbsp;



LEE: Right.&nbsp;



AFEYAN: And so I would be the conceit of predicting. So I would say with likelihood positive predictive value of less than 10%, I&#8217;m happy to answer your question. So I&#8217;m not trying to score high [LAUGHTER] because I really think that my job is to envision it, not to predict it. And that&#8217;s a little bit different, right?&nbsp;



LEE: Yeah, I actually was trying to pick what would be the hardest possible question I could ask you, [LAUGHTER] and this is what I came up with.&nbsp;



AFEYAN: Yeah, no, no, I&#8217;m kidding here. So now look, I think that we will cross this threshold of understandability. And of course you&#8217;re seeing that in a lot of LLM things today. And of course, people are trying to train for things that are explainers and all that whole, there&#8217;s a whole world of that. But I think at some point we&#8217;re going to have to kind of let go and get comfortable working on things that, you know ‚Ä¶&nbsp;



I sometimes tell people, you know, and I&#8217;m not the first, but scientists and engineers are different, it&#8217;s said, in that engineers work on things that they don&#8217;t wait until they get a full understanding of before they work with them. Well, now scientists are going to have to get used to that, too, right?&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: Because insisting that it&#8217;s only valid if it&#8217;s understandable. So, I would say, look, I hope that the time ‚Ä¶ for example, I think major improvements will be made in patient selection. If we can test drugs on patients that are more synchronized as to the stage of their disease ‚Ä¶&nbsp;



LEE: Yep.&nbsp;



AFEYAN: &#8230; I think the answer will be much better. We&#8217;re working on that. It&#8217;s a company called Etiome (opens in new tab), very, very early stage. It&#8217;s really beautiful data, very early data that shows that when we talk about MASH [metabolic dysfunction-associated steatohepatitis], liver disease, when we talk about Parkinson&#8217;s, there&#8217;s such a heterogeneity, not only of the subset type of the disease, but the stage of the disease, that this notion that you have stage one cancer, stage two cancer, again, nobody told nature there&#8217;s stages of that kind. It&#8217;s a continuum.&nbsp;



But if you can synchronize based on training, kind of, the ability to detect who are the patients that are in enough of a close proximity that should be treated so that the trial‚Äîmuch smaller a trial size‚Äîcould give you a drug, then afterwards, you can prescribe it using these approaches.&nbsp;



Kind of we&#8217;re going to find that what we thought is one disease is more like 15 diseases. That&#8217;s bad news because we&#8217;re not going to be able to claim that we can treat everything which we can. It&#8217;s good news in that there&#8217;s going to be people who are going to start making much more specific solutions to things.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So I can imagine that. I can imagine a generation of, kind of, students who are going to be able to play in this space without having 25 years of graduate education on the subject. So what is deemed knowledge sufficient to do creative things will change. I can go on and on, but I think all this is very close by and it&#8217;s very exciting.&nbsp;



LEE: Noubar, I just always have so much fun, and I learn really a lot. It&#8217;s high-density learning when I talk to you. And so I hope our listeners feel the same way. It&#8217;s something I really appreciate.&nbsp;



AFEYAN: Well, Peter, thanks for this. And I think your listeners know that if I was asking you questions, you would be answering them with equal if not more fascinating stuff. So, thanks for giving me the chance to do that today.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: I‚Äôm always fascinated by Noubar‚Äôs perspectives on fundamental research and how it connects to human health and the building of successful companies. I see him as a classic ‚Äúsystems thinker,‚Äù and by that, I mean he builds impressive things like Flagship Pioneering itself, which he created as a kind of biomedical innovation system.&nbsp;



In our conversation, I was really struck by the fact that he‚Äôs been thinking about the potential impact of transformers‚Äîtransformers being the fundamental building block of large language models‚Äîas far back as 2017, when the first paper on the attention mechanism in transformers was published by Google.&nbsp;



But, you know, it isn‚Äôt only about using AI to do things like understand and design molecules and antibodies faster. It&#8217;s interesting that he is also pushing really hard towards a future where AI might ‚Äúclose the loop‚Äù from hypothesis generation, to experiment design, to analysis, and so on.&nbsp;



Now, here‚Äôs my conversation with Dr. Eric Topol:&nbsp;



LEE: Eric, it&#8217;s really great to have you here.&nbsp;



ERIC TOPOL: Oh, Peter, I&#8217;m thrilled to be here with you here at Microsoft.&nbsp;



LEE: You&#8217;re a super famous person. Extremely well known to researchers even in computer science, as we have here at Microsoft Research.&nbsp;



But the question I&#8217;d like to ask is, how would you explain to your parents what you do every day?&nbsp;



TOPOL: [LAUGHS] That&#8217;s a good question. If I was just telling them I&#8217;m trying to come up with better ways to keep people healthy, that probably would be the easiest way to do it because if I ever got in deeper, I would lose them real quickly. They&#8217;re not around, but just thinking about what they could understand.&nbsp;



LEE: Right.&nbsp;



TOPOL: I think as long as they knew it was work centered on innovative paths to promoting and preserving human health, that would get to them, I think.&nbsp;



LEE: OK, so now, kind of the second topic, and then we let the conversation flow, is about origin stories with respect to AI. And with most of our guests, you know, I factor that into two pieces: the encounters with AI before ChatGPT and what we call generative AI and then the first contacts after.&nbsp;



And, of course, you have extensive contact with both now. But let&#8217;s start with how you got interested in machine learning and AI prior to ChatGPT. How did that happen?&nbsp;



TOPOL: Yeah, it was out of necessity. So back, you know, when I started at Scripps at the end of ‚Äô06, we started accumulating, you know, massive datasets. First, it was whole genomes. We did one of the early big cohorts of 1,400 people of healthy aging. We called the Wellderly whole genome sequence (opens in new tab).&nbsp;



And then we started big in the sensor world, and then we started saying, what are we going to do with all this data, with electronic health records and all those sensors? And now we got whole genomes.&nbsp;



And basically, what we were doing, we were in hoarding mode. We didn&#8217;t have a way to meaningfully analyze it.&nbsp;



LEE: Right.&nbsp;



TOPOL: You would read about how, you know, data is the new oil and, you know, gold and whatnot. But we just didn&#8217;t have a way to extract the juice. And even when we wanted to analyze genomes, it was incredibly laborious.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And we weren&#8217;t extracting a lot of the important information. So that&#8217;s why &#8230; not having any training in computer science, when I was doing the &#8230; about three years of work to do the book Deep Medicine, I started really, first auto-didactic about, you know, machine learning. And then I started contacting a lot of the real top people in the field and hanging out with them, and learning from them, getting their views as to, you know, where we are today, what models are coming in the future.&nbsp;



And then I said, ‚ÄúYou know what? We are going to be able to fix this mess.‚Äù [LAUGHS] We&#8217;re going to get out of the hoarding phase, and we&#8217;re going to get into, you know, really making a difference.&nbsp;



So that&#8217;s when I embraced the future of AI. And I knew, you know, back‚Äîthat was six years ago when it was published and probably eight or nine years ago when I was doing the research, and I knew that we weren&#8217;t there yet.&nbsp;



You know, at the time, we were seeing the image interpretation. That was kind of the early promise. But really, the models that were transformative, the transformer models, they were incubating back in 2017. So people knew something was brewing.&nbsp;



LEE: Right. Yes.&nbsp;



TOPOL: And everyone said we&#8217;re going to get there.&nbsp;



LEE: So then, ChatGPT comes out November of 2022; there‚Äôs GPT-4 in 2023, and now a lot has happened. Do you remember what your first encounter with that technology was?&nbsp;



TOPOL: Oh, sure. First, ChatGPT. You know, in the last days of November ‚Äô22, I was just blown away. I mean, I&#8217;m having a conversation. I&#8217;m having fun. And this is humanoid responding to me. I said, ‚ÄúWhat?‚Äù You know? So that was to me, a moment I&#8217;ll never forget. And so I knew that the world was, you know, at a very kind of momentous changing point.&nbsp;



Of course, knowing, too, that this is going to be built on, and built on quickly. Of course, I didn&#8217;t know how soon GPT-4 and all the others were going to come forward, but that was a wake-up call that the capabilities of AI had just made a humongous jump, which seemingly was all of a sudden, although I did know this had been percolating ‚Ä¶&nbsp;



LEE: Right.&nbsp;



TOPOL: ‚Ä¶ you know, for what, at least five years, that, you know, it really was getting into its position to do this.&nbsp;



LEE: I know one of the things that was challenging psychologically and emotionally for me is, it made me rethink a lot of things that were going on in Microsoft Research in areas like causal reasoning, natural language processing, speech processing, and so on.&nbsp;



I&#8217;m imagining you must have had some emotional struggles too because you have this amazing book, Deep Medicine. Did you have to ‚Ä¶ did it go through your mind to rethink what you wrote in Deep Medicine in light of this or, or, you know, how did that feel?&nbsp;



TOPOL: It&#8217;s funny you ask that because in this one chapter I have on the virtual health coach, I wrote a whole bunch of scenarios &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: ‚Ä¶ that were very kind of futuristic. You know, about how the AI interacts with the person&#8217;s health and schedules their appointment for this and their scan and tells them what lab tests they should tell their doctor to have, and, you know, all these things. And I sent a whole bunch of these, thinking that they were a little too far-fetched.&nbsp;



LEE: Yes.&nbsp;



TOPOL: And I sent them to my editor when I wrote the book, and he says, ‚ÄúOh, these are great. You should put them all in.‚Äù [LAUGHTER] What I didn&#8217;t realize is they weren&#8217;t that, you know, they were all going to happen.&nbsp;



&nbsp;LEE: Yeah. They weren&#8217;t that far-fetched at all.&nbsp;



TOPOL: Not at all. If there&#8217;s one thing I&#8217;ve learned from all this, is our imagination isn&#8217;t big enough.&nbsp;



&nbsp;LEE: Yeah.&nbsp;



TOPOL: We think too small.&nbsp;



LEE: Now in our book that Carey, Zak, and I wrote, you know, we made, you know, we sort of guessed that GPT-4 might help biomedical researchers, but I don&#8217;t think that any of us had the thought in mind that the architecture around generative AI would be so directly applicable to, you know, say, protein structures or, you know, to clinical health records and so on.&nbsp;



And so a lot of that seems much more obvious today. But two years ago, it wasn&#8217;t. But we did guess that biomedical researchers would find this interesting and be helped along.&nbsp;



So as you reflect over the past two years, you know, do you have things that you think are very important, kind of, meaningful applications of generative AI in the kinds of research that Scripps does?&nbsp;



TOPOL: Yeah. I mean, I think for one, you pointed out how the term generative AI is a misnomer.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And so it really was prescient about how, you know, it had a pluripotent capability in every respect, you know, of editing and creating. So that was something that I think was telling us, an indicator that this is, you know, a lot bigger than how it&#8217;s being labeled. And our expectations can actually be more than what we had seen previously with the earlier version.&nbsp;



So I think what&#8217;s happened is that now, we keep jumping. It&#8217;s so quick that we can&#8217;t ‚Ä¶ you know, first we think, oh, well, we‚Äôve gone into the agentic era, and then we could pass that with reasoning. [LAUGHTER] And, you know, we just can&#8217;t ‚Ä¶&nbsp;



LEE: Right.&nbsp;



TOPOL: It&#8217;s just wild.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So I think so many of us now will put in prompts that will necessitate or ideally result in a not-immediate gratification, but rather one that requires, you know, quite a bit of combing through the corpus of knowledge &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: ‚Ä¶ and getting, with all the citations, a report or a response. And I think now this has been a reset because to do that on our own, it takes, you know, many, many hours. And it&#8217;s usually incomplete.&nbsp;



But one of the things that was so different in the beginning was you would get the references from up to a year and a half previously.&nbsp;



LEE: Yep.&nbsp;



TOPOL: And that&#8217;s not good enough. [LAUGHS]&nbsp;



LEE: Right.&nbsp;



TOPOL: And now you get references, like, from the day before.&nbsp;



LEE: Yes. Yeah.&nbsp;



TOPOL: And so, you say, ‚ÄúWhy would you do a regular search for anything when you could do something like this?‚Äù&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And then, you know, the reasoning power. And a lot of people who are not using this enough still are talking about, ‚ÄúWell, there&#8217;s no reasoning.‚Äù&nbsp;



LEE: Yeah.



TOPOL: Which you dealt with really well in the book. But what, of course, you couldn&#8217;t have predicted is the new dimensions.&nbsp;



LEE: Right.&nbsp;



TOPOL: I think you nailed it with GPT-4. But it&#8217;s all these just, kind of, stepwise progressions that have been occurring because of the velocity that&#8217;s unprecedented. I just can&#8217;t believe it.&nbsp;



LEE: We were aware of the idea of multi-modality, but we didn&#8217;t appreciate, you know, what that would mean. Like AlphaFold (opens in new tab) [protein structure database], you know, the ability for AI to understand‚Äîor crystal structures‚Äîto really start understanding something more fundamental about biochemistry or medicinal chemistry.&nbsp;



I have to admit, when we wrote the book, we really had no idea.&nbsp;



TOPOL: Well, I feel the same way. I still today can&#8217;t get over it because the reason AlphaFold and Demis [Hassabis] and John Jumper [AlphaFold‚Äôs co-creators] were so successful is there was this protein databank.&nbsp;



LEE: Yes.&nbsp;



TOPOL: And it had been kept for decades. And so, they had the substrate to work with.&nbsp;



LEE: Right.&nbsp;



TOPOL: So, you say, ‚ÄúOK, we can do proteins.‚Äù But then how do you do everything else?&nbsp;



LEE: Right.&nbsp;



TOPOL: And so this whole, what I call, ‚Äúlarge language of life model‚Äù work, which has gone into high gear like I&#8217;ve never seen.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: You know, now to this holy grail of a virtual cell, and &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: You know, it&#8217;s basically &#8230; it&#8217;s &#8230; it was inspired by proteins. But now it&#8217;s hitting on, you know, ligands and small molecules, cells. I mean, nothing is being held back here.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So how could anybody have predicted that?&nbsp;



LEE: Right.&nbsp;



TOPOL: I sure wouldn&#8217;t have thought it would be possible at this point.&nbsp;



LEE: Yeah. So just to challenge you, where do you think that is going to be two years from now? Five years from now? Ten years from now? Like, so you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?&nbsp;



TOPOL: No, I think within 10 years for sure. You know the group that got assembled that Steve Quake (opens in new tab) pulled together?&nbsp;



LEE: Right.&nbsp;



TOPOL: I think has 42 authors in a paper (opens in new tab) in Cell. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



TOPOL: ‚Ä¶ that not only is this a worthy goal, but it&#8217;s actually going to be realized, that was impressive.&nbsp;



I challenged him about that. How did you get these people all to agree? So many of them were naysayers. And by the time the workshop finished, they were fully convinced. I think that what we&#8217;re seeing is so much progress happening so quickly. And then all the different models, you know, across DNA, RNA, and everything are just zooming forward.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And it&#8217;s just a matter of pulling this together. Now when we have that, and I think it could easily be well before a decade and possibly, you know, between the five- and 10-year mark‚Äîthat&#8217;s just a guess‚Äîbut then we&#8217;re moving into another era of life science because right now, you know, this whole buzz about drug discovery.&nbsp;



LEE: Yep.&nbsp;



TOPOL: It&#8217;s not&#8230; with the ability to do all these perturbations at a cellular level.&nbsp;



LEE: Right.&nbsp;



TOPOL: Or the cell of interest.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Or the cell-to-cell interactions or the intra-cell interaction. So once you nail that, yeah, it takes it to a kind of another predictive level that we haven&#8217;t really fathomed. So, yes, there&#8217;s going to be drug discovery that&#8217;s accelerated. But this would make that and also the underpinnings of diseases.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So the idea that there&#8217;s so many diseases we don&#8217;t understand now. And if you had virtual cell, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



TOPOL: ‚Ä¶ you would probably get to that answer ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



TOPOL: ‚Ä¶ much more quickly. So whether it&#8217;s underpinnings of diseases or what it&#8217;s going to take to really come up with far better treatments‚Äîpreventions‚ÄîI think that&#8217;s where virtual cell will get us.&nbsp;



LEE: There&#8217;s a technical question &#8230; I wonder if you have an opinion. You may or may not. There is sort of what I would refer to as ab initio approaches to this. You know, you start from the fundamental physics and chemistry, and we know the laws, we have the math and, you know, we can try to derive from there ‚Ä¶ in fact, we can even run simulations of that math to generate training data to build generative models and work up to a cell, or forget all of that and just take as many observations and measurements of, say, living cells as possible, and just have faith that hidden amongst all of the observational data, there is structure and language that can be derived.&nbsp;



So that&#8217;s sort of bottom-up versus top-down approaches. Do you have an opinion about which way?&nbsp;



TOPOL: Oh, I think you go after both. And clearly whenever you&#8217;re positing that you&#8217;ve got a virtual cell model that&#8217;s working, you&#8217;ve got to do the traditional methods as well to validate it, and ‚Ä¶ so all that. You know, I think if you&#8217;re going to go out after this seriously, you have to pull out all the stops. Both approaches, I think, are going to be essential.&nbsp;



LEE: You know, if what you&#8217;re saying is true, and it is amazing to hear the confidence, the one thing I tried to explain to someone nontechnical is that for a lot of problems in medicine, we just don&#8217;t have enough data in a really profound way. And the most profound way to say that is, since Adam and Eve, there have only been an estimated 106 billion people who have ever lived.&nbsp;



So even if we had the DNA of every human being, every individual of Homo sapiens, there are certain problems for which we would not have enough data.&nbsp;



TOPOL: Sure.&nbsp;



LEE: And so I think another thing that seems profound to me, if we can actually have a virtual cell, is we can actually make trillions of virtual ‚Ä¶&nbsp;



TOPOL: Yeah&nbsp;



LEE: ‚Ä¶ human beings. The true genetic diversity could be realized for our species.&nbsp;



TOPOL: I think you nailed it. The ability to have that type of data, no less synthetic data, I mean, it‚Äôs just extraordinary.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: We will get there someday. I&#8217;m confident of that. We may be wrong in projections. And I do think [science writer] Philip Ball won&#8217;t be right that it will never happen, though. [LAUGHTER] No, I think that if there&#8217;s a holy grail of biology, this is it.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And I think you&#8217;re absolutely right about where that will get us.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Transcending the beginning of the species.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Of our species.&nbsp;



LEE: Yeah. All right. So now, we&#8217;re starting to run short on time here. And so I wanted to ask you about, I&#8217;m in my 60s, so I actually think about this a lot more. [LAUGHTER] And I know you&#8217;ve been thinking a lot about longevity. And, of course, your new book, Super Agers.&nbsp;



And one of the reasons I&#8217;m so eager to read is it&#8217;s a topic very top of mind for me and actually for a lot of people. Where is this going? Because this is another area where you hear so much hype. At the same time, you see Nobel laureate scientists &#8230;&nbsp;



TOPOL: Yeah.&nbsp;



LEE: &#8230; working on this.&nbsp;



TOPOL: Yeah.&nbsp;



LEE: So, so what&#8217;s, what&#8217;s real there?&nbsp;



TOPOL: Yeah. Well, it&#8217;s really ‚Ä¶ the real deal is the science of aging is zooming forward.&nbsp;



And that&#8217;s exciting. But I see it bifurcating. On the one hand, all these new ideas, strategies to reverse aging are very ambitious. Like cell reprogramming and senolytics and, you know, the rejuvenation of our thymus gland, and it&#8217;s a long list.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And they‚Äôre really cool science, and it used to be the mouse lived longer. Now it&#8217;s the old mouse looks really young.&nbsp;



LEE: Yeah. Yeah.&nbsp;



TOPOL: All the different features. A blind mouse with cataracts is all of a sudden there&#8217;s no cataracts. I mean, so these things are exciting, but none of them are proven in people, and they all have significant risk, no less, you know, the expense that might be attached.&nbsp;



LEE: Right.&nbsp;



TOPOL: And some people are jumping the gun. They&#8217;re taking rapamycin, which can really knock out their immune system. So they all carry a lot of risk. And people are just getting a little carried away. We&#8217;re not there yet.&nbsp;



But the other side, which is what I emphasize in the book, which is exciting, is that we have all these new metrics that came out of the science of aging.&nbsp;



LEE: Yes.&nbsp;



TOPOL: So we have clocks of the body. Our biological clock versus our chronological clock, and we have organ clocks. So I can say, you know, Peter, we&#8217;ve assessed all your organs and your immune system. And guess what? Every one of them is either at or less than your actual age.&nbsp;



LEE: Right.&nbsp;



TOPOL: And that&#8217;s very reassuring. And by the way, your methylation clock is also ‚Ä¶ I don&#8217;t need to worry about you so much. And then I have these other tests that I can do now, like, for example, the brain. We have an amazing protein p-Tau217 that we can say over 20 years in advance of you developing Alzheimer&#8217;s, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



TOPOL: ‚Ä¶ we can look at that, and it&#8217;s modifiable by lifestyle, bringing it down. It should be you can change the natural history. So what we&#8217;ve seen is an explosion of knowledge of metrics, proteins, no less, you know, our understanding at the gene level, the gut microbiome, the immune system. So that&#8217;s what&#8217;s so exciting. How our immune system ages. Immunosenescence. How we have more inflammation‚Äîinflammaging‚Äîwith aging. So basically, we have three diseases that kill us, that take away our health: heart, cancer, and neurodegenerative.&nbsp;



LEE: Yep.&nbsp;



TOPOL: And they all take more than 20 years. They all have a defective immune system inflammation problem, and they&#8217;re all going to be preventable.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: That&#8217;s what&#8217;s so exciting.&nbsp;So we don&#8217;t have to have reverse aging. We can actually work on ‚Ä¶&nbsp;



LEE: Just prevent aging in the first place.&nbsp;



TOPOL: ‚Ä¶ the age-related diseases. So basically, what it means is: I got to find out if you have a risk, if you&#8217;re in this high-risk group for this particular condition, because if you are‚Äîand we have many levels, layers, orthogonal ways to check‚Äîwe don&#8217;t just bank it all on one polygenic test. We&#8217;re going to have several ways, say this is the one we are going &#8230;&nbsp;



And then we go into high surveillance, where, let&#8217;s say if it&#8217;s your brain, we do more p-Tau, if we need to do brain imaging‚Äîwhatever it takes. And also, we do preventive treatments on top of the lifestyle [changes], that one of the problems we have today is a lot of people know generally, what are good lifestyle factors. Although, I go through a lot more than people generally acknowledge.&nbsp;



But they don&#8217;t incorporate them because they don&#8217;t know that they&#8217;re at risk and they could change their &#8230; extend their health span and prevent that disease. So what I at least put out there, a blueprint, is how we can use AI, because it&#8217;s multimodal AI, with all these layers of data, and then temporally, it&#8217;s like today you could say if you have two protein tests, not only are you going to have Alzheimer&#8217;s, but within a two-year time frame when &#8230;&nbsp;



LEE: Yep.&nbsp;



TOPOL: &#8230; and if you don&#8217;t change things, if we don&#8217;t gear up ‚Ä¶ you know, we can &#8230; we can completely prevent this, so ‚Ä¶ or at least defer it for a decade or more. So that&#8217;s why I&#8217;m excited, is that we made these strides in the science of aging. But we haven&#8217;t acknowledged the part that doesn&#8217;t require reversing aging. There&#8217;s this much less flashy, attainable, less risky approach &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: &#8230; than the one that ‚Ä¶ when you reverse aging, you&#8217;re playing with the hallmarks of cancer. They are like, if you look at the hallmarks of cancer ‚Ä¶&nbsp;



LEE: That has been one of the primary challenges.&nbsp;



TOPOL: They&#8217;re lined up.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: They‚Äôre all the same, you know, whether it&#8217;s telomeres, or whether it&#8217;s &#8230; you know &#8230; so this is the problem. I actually say in the book, I do think one of these‚Äîwe have so many shots on goal‚Äîone of these reverse aging things will likely happen someday. But we&#8217;re nowhere close.&nbsp;



On the other hand, let&#8217;s gear up. Let&#8217;s do what we can do. Because we have these new metrics that&#8217;s &#8230; people don&#8217;t ‚Ä¶ like, when I read the organ clock paper (opens in new tab) from Tony Wyss-Coray from Stanford. It was published end of ‚Äô23; it was the cover of Nature. It blew me away.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And I wrote a Substack (opens in new tab) [article] on it. And Tony said, ‚ÄúWell, that&#8217;s so nice of you.‚Äù I said, ‚ÄúSo nice? This is revolutionary, you know.‚Äù [LAUGHTER] So ‚Ä¶&nbsp;



LEE: By the way, what&#8217;s so interesting is, how these things, this kind of understanding and AI, are coming together.



TOPOL: Yes.&nbsp;



LEE: It&#8217;s almost eerie the timing of these things.&nbsp;



TOPOL: Absolutely. Because you couldn&#8217;t take all these layers of data, just like we were talking about data hoarding.



LEE: Yep.



TOPOL: Now we have data hoarding on individual with no way to be able to make these assessments of what level of risk, when, what are we going to do in this individual to prevent that? We can do that now.&nbsp;



We can do it today. And we could keep building on that. So I&#8217;m really excited about it. I think that, you know, when I wrote the last book on deep medicine, it was our overarching goal should be to bring back the patient-doctor relationship. I&#8217;m an old dog, and I know what it used to be when I got out of medical school.&nbsp;



It&#8217;s totally &#8230; you couldn&#8217;t imagine how much erosion from the ‚Äô70s, ‚Äô80s to now. But now I have a new overarching goal. I&#8217;m thinking that that still is really important‚Äîhumanity in medicine‚Äîbut let&#8217;s prevent these three &#8230; big three diseases because it&#8217;s an opportunity that we&#8217;re not ‚Ä¶ you know, in medicine, all my life we&#8217;ve been hearing and talking about we need to prevent diseases.&nbsp;



Curing is much harder than prevention. And the economics. Oh my gosh. But we haven&#8217;t done it.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Now we can do it. Primary prevention. We‚Äôd do really well. Somebody‚Äôs had heart attack.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Oh, we&#8217;re going to get all over it. Why did they have a heart attack in the first place?&nbsp;



LEE: Well, the thing that makes so much sense in what you&#8217;re saying is that we understand we have an understanding both economically and medically that prevention is a good thing. And extending the concept of prevention to these age-related conditions, I think, makes all the sense in the world.&nbsp;



You know, Eric, maybe on that optimistic note, it‚Äôs time to wrap up this conversation. Really appreciate you coming. Let me just brag in closing that I&#8217;m now the proud owner of an autographed copy of your latest book, and, really, thank you for that.&nbsp;



TOPOL: Oh, thank you. I could spend the rest of the day talking to you. I&#8217;ve really enjoyed it. Thanks.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: For me, the biggest takeaway from our conversation was Eric‚Äôs supremely optimistic predictions about what AI will allow us to do in much less than 10 years.&nbsp;



You know, for me personally, I started off several years ago with the typical techie naivete that if we could solve protein folding using machine learning, we would solve human biology. But as I‚Äôve gotten smarter, I‚Äôve realized that things are way, way more complicated than that, and so hearing Eric‚Äôs techno-optimism on this is really both heartening and so interesting.&nbsp;



Another thing that really caught my attention are Eric‚Äôs views on AI in medical diagnosis. That really stood out to me because within our labs here at Microsoft Research, we have been doing a lot of work on this, for example in creating foundation models for whole-slide digital pathology.&nbsp;



The bottom line, though, is that biomedical research and development is really changing and changing quickly. It&#8217;s something that we thought about and wrote briefly about in our book, but just hearing it from these three people gives me reason to believe that this is going to create tremendous benefits in the diagnosis and treatment of disease.&nbsp;



And in fact, I wonder now how regulators, such as the Food and Drug Administration here in the United States, will be able to keep up with what might become a really big increase in the number of animal and human studies that need to be approved. On this point, it&#8217;s clear that the FDA and other regulators will need to use AI to help process the likely rise in the pace of discovery and experimentation. And so stay tuned for more information about that.&nbsp;



[THEME MUSIC]‚ÄØ



I&#8217;d like to thank Daphne, Noubar, and Eric again for their time and insights. And to our listeners, thank you for joining us. There are several episodes left in the series, including discussions on medical students‚Äô experiences with AI and AI‚Äôs influence on the operation of health systems and public health departments. We hope you&#8217;ll continue to tune in.&nbsp;



Until next time.&nbsp;



[MUSIC FADES]‚ÄØ

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series

Opens in a new tabThe post How AI will accelerate biomedical research and discovery appeared first on Microsoft Research.
‚Ä¢ Build AI-driven policy creation for vehicle data collection and automation using Amazon Bedrock
  Vehicle data is critical for original equipment manufacturers (OEMs) to drive continuous product innovation and performance improvements and to support new value-added services. Similarly, the increasing digitalization of vehicle architectures and adoption of software-configurable functions allow OEMs to add new features and capabilities efficiently. Sonatus‚Äôs Collector AI and Automator AI products address these two aspects of the move towards Software-Defined Vehicles (SDVs) in the automotive industry. 
Collector AI lowers the barrier to using data across the entire vehicle lifecycle using data collection policies that can be created without changes to vehicle electronics or requiring modifications to embedded code. However, OEM engineers and other consumers of vehicle data struggle with the thousands of vehicle signals to choose to drive their specific use cases and outcomes. Likewise, Automator AI‚Äôs no-code methodology for automating vehicle functions using intuitive if-then-style scripted workflows can also be challenging, especially for OEM users who aren‚Äôt well-versed in the events and signals available on vehicles to incorporate in a desired automated action. 
To address these challenges, Sonatus partnered with the AWS Generative AI Innovation Center to develop a natural language interface to generate data collection and automation policies using generative AI. This innovation aims to reduce the policy generation process from days to minutes while making it accessible to both engineers and non-experts alike. 
In this post, we explore how we built this system using Sonatus‚Äôs Collector AI and Amazon Bedrock. We discuss the background, challenges, and high-level solution architecture. 
Collector AI and Automator AI 
Sonatus has developed a sophisticated vehicle data collection and automation workflow tool, which comprises two main products: 
 
 Collector AI ‚Äì Gathers and transmits precise vehicle data based on configurable trigger events 
 Automator AI ‚Äì Executes automated actions within the vehicle based on analyzed data and trigger conditions 
 
The current process requires engineers to create data collection or automation policies manually. Depending on the range of an OEM‚Äôs use cases, there could be hundreds of policies for a given vehicle model. Also, identifying the correct data to collect for the given intent required sifting through multiple layers of information and organizational challenges. Our goal was to develop a more intelligent and intuitive way to accomplish the following: 
 
 Generate policies from the user‚Äôs natural language input 
 Significantly reduce policy creation time from days to minutes 
 Provide complete control over the intermediate steps in the generation process 
 Expand policy creation capabilities to non-engineers such as vehicle product owners, product planners, and even procurement 
 Implement a human-in-the-loop review process for both existing and newly created policies 
 
Key challenges 
During implementation, we encountered several challenges: 
 
 Complex event structures ‚Äì Vehicle models and different policy entities use diverse representations and formats, requiring flexible policy generation 
 Labeled data limitations ‚Äì Labeled data mapping natural language inputs to desired policies is limited 
 Format translation ‚Äì The solution must handle different data formats and schemas across customers and vehicle models 
 Quality assurance ‚Äì Generated policies must be accurate and consistent 
 Explainability ‚Äì Clear explanations for how policies are generated can help build trust 
 
Success metrics 
We defined the following key metrics to measure the success of our solution: 
 
 Business metrics: 
   
   Reduced policy generation time 
   Increased number of policies per customer 
   Expanded user base for policy creation 
    
 Technical metrics: 
   
   Accuracy of generated policies 
   Quality of results for modified prompts 
    
 Operational metrics: 
   
   Reduced policy generation effort and turnaround time compared to manual process 
   Successful integration with existing systems 
    
 
Solution overview 
The Sonatus Advanced Technology team and Generative AI Innovation Center team built an automated policy generation system, as shown in the following diagram. 
 
This is a chain of large language models (LLMs) that perform individual tasks, including entity extraction, signal translation, and signal parametrization. 
Entity extraction 
A fully generated vehicle policy consists of multiple parts, which could be captured within one single user statement. These are triggers and target data for collector policies, and triggers, actions, and associated tasks for automator policies. The user‚Äôs statement is first broken down into its entities using the following steps and rules: 
 
 Few-shot examples are provided for each entity 
 Trigger outputs must be self-contained with the appropriate signal value and comparison operator information: 
   
   Query example: ‚ÄúGenerate an automation policy that locks the doors automatically when the car is moving‚Äù 
   Trigger output: &lt;response&gt;vehicle speed above 0, vehicle signal&lt;/response&gt; 
    
 Triggers and actions are secondarily verified using a classification prompt 
 For Automator AI, triggers and actions must be associated with their corresponding tasks 
 The final output of this process is the intermediate structured XML representation of the user query in natural language: 
   
   Query example: ‚ÄúGenerate an automation policy that locks the doors automatically when the car is moving‚Äù 
   Generated XML: 
    
 
 
 &lt;response&gt;
&lt;task&gt; Lock doors when moving &lt;/task&gt;
&lt;triggers&gt; vehicle speed above 0, vehicle signal &lt;/triggers&gt;
&lt;actions&gt; lock doors, vehicle signal &lt;/actions&gt;
&lt;/response&gt;  
 
The following is a diagram of our improved solution, which converts a user query into XML output. 
 
Signal translation and parametrization 
To get to the final JSON policy structure from the intermediate structured XML output, the correct signals must be identified, the signal parameters need to be generated, and this information must be combined to follow the application‚Äôs expected JSON schema. 
The output signal format of choice at this stage is Vehicle Signal Specification (VSS), an industry-standard specification driven by COVESA. VSS is a standard specifying vehicle signal naming conventions and strategies that make vehicle signals descriptive and understandable when compared to their physical Control Area Network (CAN) signal counterparts. This makes it not only suitable but also essential in the generative AI generation process because descriptive signal names and availability of their meanings are necessary. 
The VSS signals, along with their descriptions and other necessary metadata, are embedded into a vector index. For every XML structure requiring a lookup of a vehicle signal, the process of signal translation includes the following steps: 
 
 Available signal data is preprocessed and stored into a vector database. 
 Each XML representation‚Äîtriggers, actions, and data‚Äîis converted into their corresponding embeddings. In some cases, the XML phrases can also be enhanced for better embedding representation. 
 For each of the preceding entities: 
   
   Top-k similar vector embeddings are identified (assume k as 20). 
   Candidate signals are reranked based on name and descriptions. 
   The final signal is selected using a LLM selection prompt. 
    
 In the case of triggers, after the selection of the correct signal, the trigger value and condition comparator operator are also generated using few-shot examples. 
 This retrieved and generated information is combined into a predefined trigger, action, data, and task JSON object structure. 
 Individual JSON objects are assembled to construct the final JSON policy. 
 This is run through a policy schema validator before it is saved. 
 
The following diagram illustrates the step-by-step process of signal translation. To generate the JSON output from the intermediate XML structure, correct signals are identified using vector-based lookups and reranking techniques. 
 
Solution highlights 
In this section, we discuss key components and features of the solution. 
Improvement of task adjacency 
In automator policies, a task is a discrete unit of work within a larger process. It has a specific purpose and performs a defined set of actions‚Äîboth within and outside a vehicle. It also optionally defines a set of trigger conditions that, when evaluated to be true, the defined actions start executing. The larger process‚Äîthe workflow‚Äîdefines a dependency graph of tasks and the order in which they are executed. The workflow follows the following rules: 
 
 Every automator policy starts with exactly one task 
 A task can point to one or more next tasks 
 One task can only initiate one other task 
 Multiple possible next tasks can exist, but only one can be triggered at a time 
 Each policy workflow runs one task at a given time 
 Tasks can be arranged in linear or branching patterns 
 If none of the conditions satisfy, the default is monitoring the trigger conditions for the next available tasks 
 
For example: 
 
 # Linear Task Adjacency
t1 ‚Üí t2 ‚Üí t3 ‚Üí t4 ‚Üí t1*
# Branching Task Adjacency
t1 ‚Üí t2, t3, t4
t3 ‚Üí t5
t5 ‚Üí t4 
 
*Loops back to start. 
In some of the generated outputs, we identified that there can be two adjacent tasks in which one doesn‚Äôt have an action, and another doesn‚Äôt have a trigger. Task merging aims to resolve this issue by merging those into a single task. To address this, we implemented task merging using Anthropic‚Äôs Claude on Amazon Bedrock. Our outcomes were as follows: 
 
 Solve the task merging issue, where multiple tasks with incomplete information are merged into one task 
 Properly generate tasks that point to multiple next tasks 
 Change the prompt style to decision tree-based planning to make it more flexible 
 
Multi-agent approach for parameter generation 
During the signal translation process, an exhaustive list of signals is fed into a vector store, and when corresponding triggers or actions are generated, they are used to search the vector store and select the signal with the highest relevancy. However, this sometimes generates less accurate or ambiguous results. 
For example, the following policy asks to cool down the car: 
Action: &lt;response&gt; cool down the car &lt;/response&gt; 
The corresponding signal should try to cool the car cabin, as shown in the following signal: 
Vehicle.Cabin.HVAC.Station.Row1.Driver.Temperature 
It should not cool the car engine, as shown in the following incorrect signal: 
Vehicle.Powertrain.CombustionEngine.EngineCoolant.Temperature 
We mitigated this issue by introducing a multi-agent approach. Our approach has two agents: 
 
 ReasoningAgent ‚Äì Proposes initial signal names based on the query and knowledge base 
 JudgeAgent ‚Äì Evaluates and refines the proposed signals 
 
The agents interact iteratively up to a set cycle threshold before claiming success for signal identification. 
Reduce redundant LLM calls 
To reduce latency, parts of the pipeline were identified that could be merged into a single LLM call. For example, trigger condition value generation and trigger condition operator generation were individual LLM calls.We addressed this by introducing a faster Anthropic‚Äôs Claude 3 Haiku model and merging prompts where it is possible to do so. The following is an example of a set of prompts before and after merging.The first example is before merging, with the trigger set to when the temperature is above 20 degrees Celsius: 
 
 Operator response: &lt;operator&gt; &gt; &lt;/operator&gt;
Parameter response: &lt;value&gt; 20 &lt;/value&gt; 
 
The following is the combined response for the same trigger: 
 
 &lt;response&gt;
&lt;operator&gt; &gt; &lt;/operator&gt;
&lt;value&gt; 20 &lt;/value&gt;
&lt;/response&gt; 
 
Context-driven policy generation 
The goal here is to disambiguate the signal translation, similar to the multi-agent approach for parameter generation. To make policy generation more context-aware, we proposed a customer intent clarifier that carries out the following tasks: 
 
 Retrieves relevant subsystems using knowledge base lookups 
 Identifies the intended target subsystem 
 Allows user verification and override 
 
This approach works by using external and preprocessed information like available vehicle subsystems, knowledge bases, and signals to guide the signal selection. Users can also clarify or override intent in cases of ambiguity early on to reduce wasted iterations and achieve the desired result more quickly. For example, in the case of the previously stated example on an ambiguous generation of ‚Äúcool the car,‚Äù users are asked to clarify which subsystem they meant‚Äîto choose from ‚ÄúEngine‚Äù or ‚ÄúCabin.‚Äù 
Conclusion 
Combining early feedback loops and a multi-agent approach has transformed Sonatus‚Äôs policy creation system into a more automated and efficient solution. By using Amazon Bedrock, we created a system that not only automates policy creation, reducing time taken by 70%, but also provides accuracy through context-aware generation and validation. So, organizations can achieve similar efficiency gains by implementing this multi-agent approach with Amazon Bedrock for their own complex policy creation workflows. Developers can leverage these techniques to build natural language interfaces that dramatically reduce technical complexity while maintaining precision in business-critical systems. 
 
About the authors 
 Giridhar Akila Dhakshinamoorthy is the Senior Staff Engineer and AI/ML Tech Lead in the CTO Office at Sonatus. 
Tanay Chowdhury is a Data Scientist at Generative AI Innovation Center at Amazon Web Services who helps customers solve their business problems using generative AI and machine learning. He has done MS with Thesis in Machine Learning from University of Illinois and has extensive experience in solving customer problem in the field of data science. 
Parth Patwa is a Data Scientist in the Generative AI Innovation Center at Amazon Web Services. He has co-authored research papers at top AI/ML venues and has 1000+ citations. 
Yingwei Yu is an Applied Science Manager at Generative AI Innovation Center, AWS, where he leverages machine learning and generative AI to drive innovation across industries. With a PhD in Computer Science from Texas A&amp;M University and years of working experience, Yingwei brings extensive expertise in applying cutting-edge technologies to real-world applications. 
Hamed Yazdanpanah was a Data Scientist in the Generative AI Innovation Center at Amazon Web Services. He helps customers solve their business problems using generative AI and machine learning.
‚Ä¢ How Rapid7 automates vulnerability risk scores with ML pipelines using Amazon SageMaker AI
  This post is cowritten with Jimmy Cancilla from Rapid7. 
Organizations are managing increasingly distributed systems, which span on-premises infrastructure, cloud services, and edge devices. As systems become interconnected and exchange data, the potential pathways for exploitation multiply, and vulnerability management becomes critical to managing risk. Vulnerability management (VM) is the process of identifying, classifying, prioritizing, and remediating security weaknesses in software, hardware, virtual machines, Internet of Things (IoT) devices, and similar assets. When new vulnerabilities are discovered, organizations are under pressure to remediate them. Delayed responses can open the door to exploits, data breaches, and reputational harm. For organizations with thousands or millions of software assets, effective triage and prioritization for the remediation of vulnerabilities are critical. 
To support this process, the Common Vulnerability Scoring System (CVSS) has become the industry standard for evaluating the severity of software vulnerabilities. CVSS v3.1, published by the Forum of Incident Response and Security Teams (FIRST), provides a structured and repeatable framework for scoring vulnerabilities across multiple dimensions: exploitability, impact, attack vector, and others. With new threats emerging constantly, security teams need standardized, near real-time data to respond effectively. CVSS v3.1 is used by organizations such as NIST and major software vendors to prioritize remediation efforts, support risk assessments, and comply with standards. 
There is, however, a critical gap that emerges before a vulnerability is formally standardized. When a new vulnerability is disclosed, vendors aren‚Äôt required to include a CVSS score alongside the disclosure. Additionally, third-party organizations such as NIST aren‚Äôt obligated or bound by specific timelines to analyze vulnerabilities and assign CVSS scores. As a result, many vulnerabilities are made public without a corresponding CVSS score. This situation can leave customers uncertain about how to respond: should they patch the newly discovered vulnerability immediately, monitor it for a few days, or deprioritize it? Our goal with machine learning (ML) is to provide Rapid7 customers with a timely answer to this critical question. 
Rapid7 helps organizations protect what matters most so innovation can thrive in an increasingly connected world. Rapid7‚Äôs comprehensive technology, services, and community-focused research remove complexity, reduce vulnerabilities, monitor for malicious behavior, and shut down attacks. In this post, we share how Rapid7 implemented end-to-end automation for the training, validation, and deployment of ML models that predict CVSS vectors. Rapid7 customers have the information they need to accurately understand their risk and prioritize remediation measures. 
Rapid7‚Äôs solution architecture 
Rapid7 built their end-to-end solution using Amazon SageMaker AI, the Amazon Web Services (AWS) fully managed ML service to build, train, and deploy ML models into production environments. SageMaker AI provides powerful compute for ephemeral tasks, orchestration tools for building automated pipelines, a model registry for tracking model artifacts and versions, and scalable deployment to configurable endpoints. 
Rapid7 integrated SageMaker AI with their DevOps tools (GitHub for version control and Jenkins for build automation) to implement continuous integration and continuous deployment (CI/CD) for the ML models used for CVSS scoring. By automating model training and deployment, Rapid7‚Äôs CVSS scoring solutions stay up to date with the latest data without additional operational overhead. 
The following diagram illustrates the solution architecture. 
 
Orchestrating with SageMaker AI Pipelines 
The first step in the journey toward end-to-end automation was removing manual activities previously performed by data scientists. This meant migrating experimental code from Jupyter notebooks to production-ready Python scripts. Rapid7 established a project structure to support both development and production. Each step in the ML pipeline‚Äîdata download, preprocessing, training, evaluation, and deployment‚Äîwas defined as a standalone Python module in a common directory. 
Designing the pipeline 
After refactoring, pipeline steps were moved to SageMaker Training and Processing jobs for remote execution. Steps in the pipeline were defined using Docker images with the required libraries, and orchestrated using SageMaker Pipelines in the SageMaker Python SDK. 
CVSS v3.1 vectors consist of eight independent metrics combined into a single vector. To produce an accurate CVSS vector, eight separate models were trained in parallel. However, the data used to train these models was identical. This meant that the training process could share common download and preprocessing steps, followed by separate training, validation, and deployment steps for each metric. The following diagram illustrates the high-level architecture of the implemented pipeline. 
 
Data loading and preprocessing 
The data used to train the model comprised existing vulnerabilities and their associated CVSS vectors. This data source is updated constantly, which is why Rapid7 decided to download the most recent data available at training time and uploaded it to Amazon Simple Storage Service (Amazon S3) to be used by subsequent steps. After being updated, Rapid7 implemented a preprocessing step to: 
 
 Structure the data to facilitate ingestion and use in training. 
 Split the data into three sets: training, validation, and testing (80%, 10%, and 10%). 
 
The preprocessing step was defined with a dependency on the data download step so that the new dataset was available before a new preprocessing job was started. The outputs of the preprocessing job‚Äîthe resulting training, validation, and test sets‚Äîare also uploaded to Amazon S3 to be consumed by the training steps that follow. 
Model training, evaluation, and deployment 
For the remaining pipeline steps, Rapid7 executed each step eight times‚Äîone time for each metric in the CVSS vector. Rapid7 iterated through each of the eight metrics to define the corresponding training, evaluation, and deployment steps using the SageMaker Pipelines SDK. 
The loop follows a similar pattern for each metric. The process starts with a training job using PyTorch framework images provided by Amazon SageMaker AI. The following is a sample script for defining a training job. 
 
 estimator = PyTorch(
        entry_point="train.py",
        source_dir="src",
        role=role,
        instance_count=1,
        instance_type=TRAINING_INSTANCE_TYPE
        output_path=f"s3://{s3_bucket}/cvss/trained-model",
        framework_version="2.2",
        py_version="py310",
        disable_profiler=True,
        environment={"METRIC": cvss_metric}
        )
step_train = TrainingStep(
    name=f"TrainModel_{cvss_metric}",
    estimator=estimator,
    inputs={
        "train": TrainingInput(
            s3_data=&lt;&lt;INPUT_DATA_S3_URI&gt;&gt;,
            content_type="text/plain"
        ),
        "validation": TrainingInput(
            s3_data=&lt;&lt;VALIDATION_DATA_S3_URI&gt;&gt;,
            content_type="text/plain"
        )
    }
)
training_steps.append(step_train) 
 
The PyTorch Estimator creates model artifacts that are automatically uploaded to the Amazon S3 location defined in the output path parameter. The same script is used for each one of the CVSS v3.1 metrics while focusing on a different metric by passing a different cvss_metric to the training script as an environment variable. 
The SageMaker Pipeline is configured to trigger the execution of a model evaluation step when the model training job for that CVSS v3.1 metric is finished. The model evaluation job takes the newly trained model and test data as inputs, as shown in the following step definition. 
 
 script_eval = Processor(...)
eval_args = script_eval.run(
    inputs=[
        ProcessingInput(
            source=&lt;&lt;MODEL_ARTIFACTS_IN_AMAZON_S3&gt;&gt;,
            destination="/opt/ml/processing/model"
        ),
        ProcessingInput(
            source=&lt;&lt;TEST_DATA_IN_AMAZON_S3&gt;&gt;,
            destination="/opt/ml/processing/test"
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="evaluation",
            source="/opt/ml/processing/evaluation/",
            destination=f"s3://{s3_bucket}/cvss/evaluation/{cvss_metric}/"
        )
    ],
    source_dir="src",
    code="evaluate.py"
)
evaluation_report = PropertyFile(
    name="EvaluationReport",
    output_name="evaluation",
    path="evaluation.json"
)
step_eval = ProcessingStep(
    name=f"Evaluate_{cvss_metric}",
    step_args=eval_args,
    property_files=[evaluation_report],
)
evaluation_steps.append(step_eval) 
 
The processing job is configured to create a PropertyFile object to store the results from the evaluation step. Here is a sample of what might be found in this file: 
 
 {
  "ac": {
    "metrics": {
      "accuracy": 99
    }
  }
} 
 
This information is critical in the last step of the sequence followed for each metric in the CVSS vector. Rapid7 wants to ensure that models deployed in production meet quality standards, and they do that by using a ConditionStep that allows only models whose accuracy is above a critical value to be registered in the SageMaker Model Registry. This process is repeated for all eight models. 
 
 cond_gte = ConditionGreaterThanOrEqualTo(
            left=JsonGet(
                step_name=step_eval.name,
                property_file=evaluation_report,
                json_path=f"{cvss_metric}.metrics.accuracy"
            ),
            right=accuracy_threshold_param
        )
step_cond = ConditionStep(
    name=f"CVSS_{cvss_metric}_Accuracy_Condition",
    conditions=[cond_gte],
    if_steps=[step_model_create],
    else_steps=[]
)
conditional_steps.append(step_cond)  
 
Defining the pipeline 
With all the steps defined, a pipeline object is created with all the steps for all eight models. The graph for the pipeline definition is shown in the following image. 
 
Managing models with SageMaker Model Registry 
SageMaker Model Registry is a repository for storing, versioning, and managing ML models throughout the machine learning operations (MLOps) lifecycle. The model registry enables the Rapid7 team to track model artifacts and their metadata (such as performance metrics), and streamline model version management as their CVSS models evolve. Each time a new model is added, a new version is created under the same model group, which helps track model iterations over time. Because new versions are evaluated for accuracy before registration, they‚Äôre registered with an Approved status. If a model‚Äôs accuracy falls below this threshold, the automated deployment pipeline will detect this and send an alert to notify the team about the failed deployment. This enables Rapid7 to maintain an automated pipeline that serves the most accurate model available to date without requiring manual review of new model artifacts. 
Deploying models with inference components 
When a set of CVSS scoring models has been selected, they can be deployed in a SageMaker AI endpoint for real-time inference, allowing them to be invoked to calculate a CVSS vector as soon as new vulnerability data is available. SageMaker AI endpoints are accessible URLs where applications can send data and receive predictions. Internally, the CVSS v3.1 vector is prepared using predictions from the eight scoring models, followed by postprocessing logic. Because each invocation runs each of the eight CVSS scoring models one time, their deployment can be optimized for efficient use of compute resources. 
When the deployment script runs, it checks the model registry for new versions. If it detects an update, it immediately deploys the new version to a SageMaker endpoint. 
Ensuring Cost Efficiency 
Cost efficiency was a key consideration in designing this workflow. Usage patterns for vulnerability scoring are bursty, with periods of high activity followed by long idle intervals. Maintaining dedicated compute resources for each model would be unnecessarily expensive given these idle times. To address this issue, Rapid7 implemented Inference Components in their SageMaker endpoint. Inference components allow multiple models to share the same underlying compute resources, significantly improving cost efficiency‚Äîparticularly for bursty inference patterns. This approach enabled Rapid7 to deploy all eight models on a single instance. Performance tests showed that inference requests could be processed in parallel across all eight models, consistently achieving sub-second response times (100-200ms). 
Monitoring models in production 
Rapid7 continually monitors the models in production to ensure high availability and efficient use of compute resources. The SageMaker AI endpoint automatically uploads logs and metrics into Amazon CloudWatch, which are then forwarded and visualized in Grafana. As part of regular operations, Rapid7 monitors these dashboards to visualize metrics such as model latency, the number of instances behind the endpoint, and invocations and errors over time. Additionally, alerts are configured on response time metrics to maintain system responsiveness and prevent delays in the enrichment pipeline. For more information on the various metrics and their usage, refer to the AWS blog post, Best practices for load testing Amazon SageMaker real-time inference endpoints. 
Conclusion 
End-to-end automation of vulnerability scoring model development and deployment has given Rapid7 a consistent, fully automated process. The previous manual process for retraining and redeploying these models was fragile, error-prone, and time-intensive. By implementing an automated pipeline with SageMaker, the engineering team now saves at least 2‚Äì3 days of maintenance work each month. By eliminating 20 manual operations, Rapid7 software engineers can focus on delivering higher-impact work for their customers. Furthermore, by using inference components, all models can be consolidated onto a single ml.m5.2xlarge instance, rather than deploying a separate endpoint (and instance) for each model. This approach nearly halves the hourly compute cost, resulting in approximately 50% cloud compute savings for this workload. In building this pipeline, Rapid7 benefited from features that reduced time and cost across multiple steps. For example, using custom containers with the necessary libraries improved startup times, while inference components enabled efficient resource utilization‚Äîboth were instrumental in building an effective solution. 
Most importantly, this automation means that Rapid7 customers always receive the most recently published CVEs with a CVSSv3.1 score assigned. This is especially important for InsightVM because Active Risk Scores, Rapid7‚Äôs latest risk strategy for understanding vulnerability impact, rely on the CVSSv3.1 score as a key component in their calculation. Providing accurate and meaningful risk scores is critical for the success of security teams, empowering them to prioritize and address vulnerabilities more effectively. 
In summary, automating model training and deployment with Amazon SageMaker Pipelines has enabled Rapid7 to deliver scalable, reliable, and efficient ML solutions. By embracing these best practices and lessons learned, teams can streamline their workflows, reduce operational overhead, and remain focused on driving innovation and value for their customers. 
 
About the authors 
Jimmy Cancilla is a Principal Software Engineer at Rapid7, focused on applying machine learning and AI to solve complex cybersecurity challenges. He leads the development of secure, cloud-based solutions that use automation and data-driven insights to improve threat detection and vulnerability management. He is driven by a vision of AI as a tool to augment human work, accelerating innovation, enhancing productivity, and enabling teams to achieve more with greater speed and impact. 
Felipe Lopez&nbsp;is a Senior AI/ML Specialist Solutions Architect at AWS. Prior to joining AWS, Felipe worked with GE Digital and SLB, where he focused on modeling and optimization products for industrial applications. 
Steven Warwick is a Senior Solutions Architect at AWS, where he leads customer engagements to drive successful cloud adoption and specializes in SaaS architectures and Generative AI solutions. He produces educational content including blog posts and sample code to help customers implement best practices, and has led programs on GenAI topics for solution architects. Steven brings decades of technology experience to his role, helping customers with architectural reviews, cost optimization, and proof-of-concept development.
‚Ä¢ Build secure RAG applications with AWS serverless data lakes
  Data is your generative AI differentiator, and successful generative AI implementation depends on a robust data strategy incorporating a comprehensive data governance approach. Traditional data architectures often struggle to meet the unique demands of generative such as applications. An effective generative AI data strategy requires several key components like seamless integration of diverse data sources, real-time processing capabilities, comprehensive data governance frameworks that maintain data quality and compliance, and secure access patterns that respect organizational boundaries. In particular, Retrieval Augmented Generation (RAG) applications have emerged as one of the most promising developments in this space. RAG is the process of optimizing the output of a foundation model (FM), so it references a knowledge base outside of its training data sources before generating a response. Such systems require secure, scalable, and flexible data ingestion and access patterns to enterprise data. 
In this post, we explore how to build a secure RAG application using serverless data lake architecture, an important data strategy to support generative AI development. We use Amazon Web Services (AWS) services including Amazon S3, Amazon DynamoDB, AWS Lambda, and Amazon Bedrock Knowledge Bases to create a comprehensive solution supporting unstructured data assets which can be extended to structured data. The post covers how to implement fine-grained access controls for your enterprise data and design metadata-driven retrieval systems that respect security boundaries. These approaches will help you maximize the value of your organization‚Äôs data while maintaining robust security and compliance. 
Use case overview 
As an example, consider a RAG-based generative AI application. The following diagram shows the typical conversational workflow that is initiated with a user prompt, for example, operation specialists in a retail company querying internal knowledge to get procurement and supplier details. Each user prompt is augmented with relevant contexts from data residing in an enterprise data lake. 
 
In the solution, the user interacts with the Streamlit frontend, which serves as the application interface. Amazon Cognito that enables IdP integration through IAM Identity Center, so that only authorized users can access the application. For production use, we recommend that you use a more robust frontend framework such as AWS Amplify, which provides a comprehensive set of tools and services for building scalable and secure web applications. After the user has successfully signed in, the application retrieves the list of datasets associated with the user‚Äôs ID from the DynamoDB table. The list of datasets is used to filter while querying the knowledge base to get answers from datasets the user is authorized to access. This is made possible because when the datasets are ingested, the knowledge base is prepopulated with metadata files containing user principal-dataset mapping stored in Amazon S3. The knowledge base returns the relevant results, which are then sent back to the application and displayed to the user. 
The datasets reside in a serverless data lake on Amazon S3 and are governed using Amazon S3 Access Grants with IAM Identity Center trusted identity propagation enabling automated data permissions at scale. When an access grant is created or deleted for a user or group, the information is added to the DynamoDB table through event-driven architecture using AWS CloudTrail and Amazon EventBridge. 
The workflow includes the following key data foundation steps: 
 
 Access policies to extract permissions based on relevant data and filter out results based on the prompt user role and permissions. 
 Enforce data privacy policies such as personally identifiable information (PII) redactions. 
 Enforce fine-grained access control. 
 Grant the user role permissions for sensitive information and compliance policies based on dataset classification in the data lake. 
 Extract, transform, and load multimodal data assets into a vector store. 
 
In the following sections, we explain why a modern data strategy is important for generative AI and what challenges it solves. 
Serverless data lakes powering RAG applications 
Organizations implementing RAG applications face several critical challenges that impact both functionality and cost-effectiveness. At the forefront is security and access control. Applications must carefully balance broad data access with strict security boundaries. These systems need to allow access to data sources by only authorized users, apply dynamic filtering based on permissions and classifications, and maintain security context throughout the entire retrieval and generation process. This comprehensive security approach helps prevent unauthorized information exposure while still enabling powerful AI capabilities. 
Data discovery and relevance present another significant hurdle. When dealing with petabytes of enterprise data, organizations must implement sophisticated systems for metadata management and advanced indexing. These systems need to understand query context and intent while efficiently ranking retrieval results to make sure users receive the most relevant information. Without proper attention to these aspects, RAG applications risk returning irrelevant or outdated information that diminishes their utility. 
Performance considerations become increasingly critical as these systems scale. RAG applications must maintain consistent low latency while processing large document collections, handling multiple concurrent users, integrating data from distributed sources and retrieving relevant data. The challenge of balancing real-time and historical data access adds another layer of complexity to maintaining responsive performance at scale.Cost management represents a final key challenge that organizations must address. Without careful architectural planning, RAG implementations can lead to unnecessary expenses through duplicate data storage, excessive vector database operations, and inefficient data transfer patterns. Organizations need to optimize their resource utilization carefully to help prevent these costs from escalating while maintaining system performance and functionality. 
A modern data strategy addresses the complex challenges of RAG applications through comprehensive governance frameworks and robust architectural components. At its core, the strategy implements sophisticated governance mechanisms that go beyond traditional data management approaches. These frameworks enable AI systems to dynamically access enterprise information while maintaining strict control over data lineage, access patterns, and regulatory compliance. By implementing comprehensive provenance tracking, usage auditing, and compliance frameworks, organizations can operate their RAG applications within established ethical and regulatory boundaries. 
Serverless data lakes serve as the foundational component of this strategy, offering an elegant solution to both performance and cost challenges. Their inherent scalability automatically handles varying workloads without requiring complex capacity planning, and pay-per-use pricing models facilitate cost efficiency. The ability to support multiple data formats‚Äîfrom structured to unstructured‚Äîmakes them particularly well-suited for RAG applications that need to process and index diverse document types. 
To address security and access control challenges, the strategy implements enterprise-level data sharing mechanisms. These include sophisticated cross-functional access controls and federated access management systems that enable secure data exchange across organizational boundaries. Fine-grained permissions at the row, column, and object levels enforce security boundaries while maintaining necessary data access for AI systems.Data discoverability challenges are met through centralized cataloging systems that help prevent duplicate efforts and enable efficient resource utilization. This comprehensive approach includes business glossaries, technical catalogs, and data lineage tracking, so that teams can quickly locate and understand available data assets. The catalog system is enriched with quality metrics that help maintain data accuracy and consistency across the organization. 
Finally, the strategy implements a structured data classification framework that addresses security and compliance concerns. By categorizing information into clear sensitivity levels from public to restricted, organizations can create RAG applications that only retrieve and process information appropriate to user access levels. This systematic approach to data classification helps prevent unauthorized information disclosure while maintaining the utility of AI systems across different business contexts.Our solution uses AWS services to create a secure, scalable foundation for enterprise RAG applications. The components are explained in the following sections. 
Data lake structure using Amazon S3 
Our data lake will use Amazon S3 as the primary storage layer, organized with the following structure: 
 
 s3://amzn-s3-demo-enterprise-datalake/
‚îú‚îÄ‚îÄ retail/
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ product-catalog/
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ customer-data/
‚îÇ &nbsp; ‚îî‚îÄ‚îÄ sales-history/
‚îú‚îÄ‚îÄ finance/
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ financial-statements/
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ tax-documents/
‚îÇ &nbsp; ‚îî‚îÄ‚îÄ budget-forecasts/
‚îú‚îÄ‚îÄ supply-chain/
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ inventory-reports/
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ supplier-contracts/
‚îÇ &nbsp; ‚îî‚îÄ‚îÄ logistics-data/
‚îî‚îÄ‚îÄ shared/
&nbsp;&nbsp; &nbsp;‚îú‚îÄ‚îÄ company-policies/
&nbsp;&nbsp; &nbsp;‚îú‚îÄ‚îÄ knowledge-base/
&nbsp;&nbsp; &nbsp;‚îî‚îÄ‚îÄ public-data/ 
 
Each business domain has dedicated folders containing domain-specific data, with common data stored in a shared folder. 
Data sharing options 
There are two options for data sharing. The first option is Amazon S3 Access Points, which provide a dedicated access endpoint policy for different applications or user groups. This approach enables fine-grained control without modifying the base bucket policy. 
The following code is an example access point configuration. This policy grants the RetailAnalyticsRole read-only access (GetObject and ListBucket permissions) to data in both the retail-specific directory and the shared directory, but it restricts access to other business domain directories. The policy is attached to a dedicated S3 access point, allowing users with this role to retrieve only data relevant to retail operations and commonly shared resources: 
 
 {
&nbsp;&nbsp;"Version": "2012-10-17",
&nbsp;&nbsp;"Statement": [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"AWS": "arn:aws:iam::123456789012:role/RetailAnalyticsRole"
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"Action": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3:GetObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3:ListBucket"
&nbsp;&nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:us-east-1:123456789012:accesspoint/retail-access-point/object/retail/*",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:us-east-1:123456789012:accesspoint/retail-access-point/object/shared/*"
&nbsp;&nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;]
} 
 
The second option for data sharing is using bucket policies with path-based access control. Bucket policies can implement path-based restrictions to control which user roles can access specific data directories.The following code is an example bucket policy. This bucket policy implements domain-based access control by granting different permissions based on user roles and data paths. The FinanceUserRole can only access data within the finance and shared directories, and the RetailUserRole can only access data within the retail and shared directories. This pattern enforces data isolation between business domains while facilitating access to common resources. Each role is limited to read-only operations (GetObject and ListBucket) on their authorized directories, which means users can only retrieve data relevant to their business functions. 
 
 {
&nbsp;&nbsp;"Version": "2012-10-17",
&nbsp;&nbsp;"Statement": [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"AWS": "arn:aws:iam::123456789012:role/FinanceUserRole"
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"Action": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3:GetObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3:ListBucket"
&nbsp;&nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::amzn-s3-demo-enterprise-datalake/finance/*",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::amzn-s3-demo-enterprise-datalake/shared/*"
&nbsp;&nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"AWS": "arn:aws:iam::123456789012:role/RetailUserRole"
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"Action": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3:GetObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3:ListBucket"
&nbsp;&nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::amzn-s3-demo-enterprise-datalake/retail/*",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::amzn-s3-demo-enterprise-datalake/shared/*"
&nbsp;&nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;]
} 
 
As your number of datasets and use cases scale, you might require more policy space. Bucket policies work as long as the necessary policies fit within the policy size limits of S3 bucket policies (20 KB), AWS Identity and Access Management (IAM) policies (5 KB), and within the number of IAM principals allowed per account. With an increasing number of datasets, access points offer a better alternative of having a dedicated policy for each access point in such cases. You can define quite granular access control patterns because you can have thousands of access points per AWS Region per account, with a policy up to 20 KB in size for each access point. Although S3 Access Points increases the amount of policy space available, it requires a mechanism for clients to discover the right access point for the right dataset. To manage scale, S3 Access Points provides a simplified model to map identities in directories such as Active Directory or IAM principals to datasets in Amazon S3 by prefix, bucket, or object. With the simplified access scheme in S3 Access Grants, you can grant read-only, write-only, or read-write access according to Amazon S3 prefix to both IAM principals and directly to users or groups from a corporate directory. As a result, you can manage automated data permissions at scale. 
Amazon Comprehend PII redaction job identifies and redacts (or masks) sensitive data in documents residing in Amazon S3. After redaction, documents are verified for redaction effectiveness using Amazon Macie. Documents flagged by Macie are sent to another bucket for manual review, and cleared documents are moved to a redacted bucket ready for ingestion. For more details, refer to Protect sensitive data in RAG applications with Amazon Comprehend. 
User-dataset mapping with DynamoDB 
To dynamically manage access permissions, you can use DynamoDB to store mapping information between users or roles and datasets. You can automate the mapping from AWS Lake Formation to DynamoDB using CloudTrail and event-driven Lambda invocation. The DynamoDB structure consists of a table named UserDatasetAccess. Its primary key structure is: 
 
 Partition key ‚Äì UserIdentifier (string) ‚Äì IAM role Amazon Resource Name (ARN) or user ID 
 Sort key ‚Äì DatasetID (string) ‚Äì Unique identifier for each dataset 
 
Additional attributes consist of: 
 
 DatasetPath (string) ‚Äì S3 path to the dataset 
 AccessLevel (string) ‚Äì READ, WRITE, or ADMIN 
 Classification (string) ‚Äì PUBLIC, INTERNAL, CONFIDENTIAL, RESTRICTED 
 Domain (string) ‚Äì Business domain (such as retail or finance) 
 ExpirationTime (number) ‚Äì Optional Time To Live (TTL) for temporary access 
 
The following DynamoDB item represents an access mapping between a user role (RetailAnalyst) and a specific dataset (retail-products). It defines that this role has READ access to product catalog data in the retail domain with an INTERNAL security classification. When the RAG application processes a query, it references this mapping to determine which datasets the querying user can access, and the application only retrieves and uses data appropriate for the user‚Äôs permissions level. 
 
 {
&nbsp;&nbsp;"UserIdentifier": "arn:aws:iam::123456789012:role/RetailAnalyst",
&nbsp;&nbsp;"DatasetID": "retail-products",
&nbsp;&nbsp;"DatasetPath": "s3://amzn-s3-demo-enterprise-datalake/retail/product-catalog/",
&nbsp;&nbsp;"AccessLevel": "READ",
&nbsp;&nbsp;"Classification": "INTERNAL",
&nbsp;&nbsp;"Domain": "retail"
} 
 
This approach provides a flexible, programmatic way to control which users can access specific datasets, enabling fine-grained permission management for RAG applications. 
Amazon Bedrock Knowledge Bases for unstructured data 
Amazon Bedrock Knowledge Bases provides a managed solution for organizing, indexing, and retrieving unstructured data to support RAG applications. For our solution, we use this service to create domain-specific knowledge bases. With the metadata filtering feature provided by Amazon Bedrock Knowledge Bases, you can retrieve not only semantically relevant chunks but a well-defined subset of those relevant chunks based on applied metadata filters and associated values. In the next sections, we show how you can set this up. 
Configuring knowledge bases with metadata filtering 
We organize our knowledge bases to support filtering based on: 
 
 Business domain (such as finance, retail, or supply-chain) 
 Security classification (such as public, internal, confidential, or restricted) 
 Document type (such as policy, report, or guide) 
 
Each document ingested into our knowledge base includes a standardized metadata structure: 
 
 {

&nbsp;&nbsp;"source_uri": "s3://amzn-s3-demo-enterprise-datalake/retail/product-catalog/shoes-inventory-2023.pdf",
&nbsp;&nbsp;"title": "Shoes Inventory Report 2023",
&nbsp;&nbsp;"language": "en",
&nbsp;&nbsp;"last_updated": "2023-12-15T14:30:00Z",
&nbsp;&nbsp;"author": "Inventory Management Team",
&nbsp;&nbsp;"business_domain": "retail",
&nbsp;&nbsp;"security_classification": "internal",
&nbsp;&nbsp;"document_type": "inventory_report",
&nbsp;&nbsp;"departments": ["retail", "supply-chain"],
&nbsp;&nbsp;"tags": ["footwear", "inventory", "2023"],
&nbsp;&nbsp;"version": "1.2"
} 
 
Code examples shown throughout this post are for reference only and highlight key API calls and logic. Additional implementation code is required for production deployments. 
Amazon Bedrock Knowledge Bases API integration 
To demonstrate how our RAG application will interact with the knowledge base, here‚Äôs a Python sample using the AWS SDK: 
 
 # High-level logic for querying knowledge base with security filters
def query_knowledge_base(query_text, user_role, business_domain=None):
&nbsp;&nbsp; &nbsp;# Get permitted classifications based on user role
&nbsp;&nbsp; &nbsp;permitted_classifications = get_permitted_classifications(user_role)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;# Build security filter expression
&nbsp;&nbsp; &nbsp;filter_expression = build_security_filters(permitted_classifications, business_domain)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;# Key API call for retrieval with security filtering
&nbsp;&nbsp; &nbsp;response = bedrock_agent_runtime.retrieve(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;knowledgeBaseId='your-kb-id',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;retrievalQuery={'text': query_text},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;retrievalConfiguration={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'vectorSearchConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'numberOfResults': 5,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'filter': filter_expression &nbsp;# Apply security filters here
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;return response['retrievalResults'] 
 
Conclusion 
In this post, we‚Äôve explored how to build a secure RAG application using a serverless data lake architecture. The approach we‚Äôve outlined provides several key advantages: 
 
 Security-first design ‚Äì Fine-grained access controls at scale mean that users only access data they‚Äôre authorized for 
 Scalability ‚Äì Serverless components automatically handle varying workloads 
 Cost-efficiency ‚Äì Pay-as-you-go pricing models optimize expenses 
 Flexibility ‚Äì Seamless adaptation to different business domains and use cases 
 
By implementing a modern data strategy with proper governance, security controls, and serverless architecture, organizations can make the most of their data assets for generative AI applications while maintaining security and compliance.The RAG architecture we‚Äôve described enables contextualized, accurate responses that respect security boundaries, providing a powerful foundation for enterprise AI applications across diverse business domains.For next steps, consider implementing monitoring and observability to track performance and usage patterns. 
For performance and usage monitoring: 
 
 Deploy Amazon CloudWatch metrics and dashboards to track key performance indicators such as query latency, throughput, and error rates 
 Set up CloudWatch Logs Insights to analyze usage patterns and identify optimization opportunities 
 Implement AWS X-Ray tracing to visualize request flows across your serverless components 
 
For security monitoring and defense: 
 
 Enable Amazon GuardDuty to detect potential threats targeting your S3 data lake, Lambda functions, and other application resources 
 Implement Amazon Inspector for automated vulnerability assessments of your Lambda functions and container images 
 Configure AWS Security Hub to consolidate security findings and measure cloud security posture across your RAG application resources 
 Use Amazon Macie for continuous monitoring of S3 data lake contents to detect sensitive data exposures 
 
For authentication and activity auditing: 
 
 Analyze AWS CloudTrail logs to audit API calls across your application stack 
 Implement CloudTrail Lake to create SQL-queryable datasets for security investigations 
 Enable Amazon Cognito advanced security features to detect suspicious sign-in activities 
 
For data access controls: 
 
 Set up CloudWatch alarms to send alerts about unusual data access patterns 
 Configure AWS Config rules to monitor for compliance with access control best practices 
 Implement AWS IAM Access Analyzer to identify unintended resource access 
 
Other important considerations include: 
 
 Adding feedback loops to continuously improve retrieval quality 
 Exploring multi-Region deployment for improved resilience 
 Implementing caching layers to optimize frequently accessed content 
 Extending the solution to support structured data assets using AWS Glue and AWS Lake Formation for data transformation and data access 
 
With these foundations in place, your organization will be well-positioned to use generative AI technologies securely and effectively across the enterprise. 
 
About the authors 
Venkata Sistla is a Senior Specialist Solutions Architect in the Worldwide team at Amazon Web Services (AWS), with over 12 years of experience in cloud architecture. He specializes in designing and implementing enterprise-scale AI/ML systems across financial services, healthcare, mining and energy, independent software vendors (ISVs), sports, and retail sectors. His expertise lies in helping organizations transform their data challenges into competitive advantages through innovative cloud solutions while mentoring teams and driving technological excellence. He focuses on architecting highly scalable infrastructures that accelerate machine learning initiatives and deliver measurable business outcomes. 
Aamna Najmi is a Senior GenAI and Data Specialist in the Worldwide team at Amazon Web Services (AWS). She assists customers across industries and Regions in operationalizing and governing their generative AI systems at scale, ensuring they meet the highest standards of performance, safety, and ethical considerations, bringing a unique perspective of modern data strategies to complement the field of AI. In her spare time, she pursues her passion of experimenting with food and discovering new places. 
 &nbsp;

‚∏ª