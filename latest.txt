âœ… Morning News Briefing â€“ July 31, 2025 10:50

ğŸ“… Date: 2025-07-31 10:50
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  11.4Â°C
  Temperature: 11.4&deg;C Pressure / Tendency: 102.3 kPa rising Humidity: 94 % Humidity is 94 % Dewpoint: 10.4Â°C Wind: NW 6 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 31 July 2025 . Weather forecast: 10/11Â°C
â€¢ Thursday: Mainly sunny. High 23.
  Local smoke early this morning . Becoming a mix of sun and cloud near noon . Wind becoming north 30 km/h this afternoon . High 23. UV index 8 or very high for most of the day . Forecast issued 5:00 AM EDT Thursday 31 July 2025 . For the rest of the week, see www.dailymailonline.co.uk for the latest forecast .

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ Not enough money for retirement? NPR wants to hear from you
  Many adults over age 50 have no retirement savings or worry they won't be able to support themselves during their post-working years . If that's you, NPR wants to hear your story . If you have a story to tell, please email us at iReport@dailymailonline.com or write to iReport.com . In the U.S. call the National Suicide Prevention
â€¢ The transformative power of keeping a daily journal
  Suleika Jaouad explains why journaling is a form of alchemy in a new book . She offers tips on how to reignite your writing if you're feeling stuck, bored or uninspired by your own writing . The book is available now on Amazon.com and iReporter.com, go on sale now at $9.99. For more information, visit
â€¢ Christian refugees caught in the crosshairs of U.S. immigration policy
  The Trump administration's overhaul of the U.S. asylum and refugee systems has taken a toll on people fleeing religious persecution, including many Christians . Many Christians are fleeing persecution in the United States, including Christians, fleeing persecution of Christians . The government overhauls the asylum, refugee and asylum system has taken tolls on Christians fleeing persecution, many of whom they say are Christians . In the
â€¢ Canada and Malta to recognize Palestinian state, joining France and possibly Britain
  Both countries said they would make the formal announcement at the UN General Assembly in New York in September . The announcement will be made at the United Nations General Assembly . Both countries say they will make the announcement in September at the U.N. General Assembly, New York, for the first time in a year . The decision is expected to be announced at the end of this month's UN
â€¢ Brown University strikes agreement with White House to restore lost federal funding
  Brown University will pay $50 million to Rhode Island workforce development organizations in a deal with the Trump administration . The deal restores lost federal research funding, officials said Wednesday . Brown University has been criticized by the White House for its lack of funding for research in the past . The university has also been criticized for its decision to withdraw funding for Rhode Island research organizations . The White House announced the deal

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ AWS Lambda loves charging for idle time: Vercel claims it found a way to dodge the bill
  Startupâ€™s workaround reuses stuck compute slots to rein in runaway function costs . Vercel claims it's slashed Lambda costs by up to 95 percent by reusing idle instances that would otherwise rack up charges while waiting on slow external services like LLMs or databases .â€¦â€¦â€¦and using idle instances to reuse idle instances would otherwise charge up to $1,000 per hour
â€¢ Capgemini wins Â£107M HMRC extension â€“ no competition needed
  UK tax collector His Majesty's Revenue and Customs (HMRC) awarded Capgemini a Â£107 million support and services deal, without competition, under a relationship that started more than twenty years ago . Deal for legacy applications support reaches Â£322M as they continue to be decommissioned by HMRC . The deal is without competition under the terms of the deal, under the relationship that
â€¢ Banning VPNs to protect kids? Good luck with that
  UK's Online Safety Act (OSA) kicks off about as well as everyone expected . It was only a matter of time before tech-savvy under-18s figured out how to bypass the rules and regain access to adult content . OSA is now in effect, and it is not the first time the law has been implemented in the U.S. has been in effect
â€¢ Bitter fight over 2020 Microsoft quantum paper both resolved and unresolved
  Science is preparing to remove an editorial expression of concern that cast doubt on a five-year-old Microsoft quantum computing research paper . The journal Science will remove the editorial expression that casts doubt on the paper . Microsoft's quantum computing paper was published five years ago in the journal Science . The paper is set to be removed from Science magazine's online edition of this week's issue of this article
â€¢ Internet exchange points are ignored, vulnerable, and absent from infrastructure protection plans
  Internet Exchange Points are an underappreciated resource that all internet users rely on, but governments have unfortunately ignored them, despite their status as critical infrastructure . Italy's Italian operator calls for lawmakers to wake up to the critical role played by peering Internet Exchange points .â€¦â€¦â€¦ Italy's internet operator says it's time for the government to take responsibility for peering points to the internet

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ How allergens make us cough and weeze â€” by poking holes in airway cells
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Permissive gun laws in the USA linked to thousands of child deaths
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ The relationship between parental psychological well-being and preadolescentsâ€™ social media use
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Access to translated invitations online increases involvement of linguistically diverse households in a population-based study: a cluster randomized controlled study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Evaluating the potential of phenotypic age to enhance cardiovascular risk prediction over chronological age in the UK Biobank
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ The two people shaping the future of OpenAIâ€™s research
  For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firmâ€™s roster. Even his bungled ouster ended with him back on topâ€”and more famous than ever. But look past the charismatic frontman and you get a clearer sense of where this company is going. After all, Altman is not the one building the technology on which its reputation rests.&nbsp;



That responsibility falls to OpenAIâ€™s twin heads of researchâ€”chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.



I sat down with Chen and Pachocki for an exclusive conversation during a recent trip the pair made to London, where OpenAI set up its first international office in 2023. We talked about how they manage the inherent tension between research and product. We also talked about why they think coding and math are the keys to more capable all-purpose models; what they really mean when they talk about AGI; and what happened to OpenAIâ€™s superalignment team, set up by the firmâ€™s cofounder and former chief scientist Ilya Sutskever to prevent a hypothetical superintelligence from going rogue, which disbanded soon after he quit.&nbsp;



In particular, I wanted to get a sense of where their heads are at in the run-up to OpenAIâ€™s biggest product release in months: GPT-5.



Reports are out that the firmâ€™s next-generation model will be launched in August. OpenAIâ€™s official lineâ€”well, Altmanâ€™sâ€”is that it will release GPT-5 â€œsoon.â€ Anticipation is high. The leaps OpenAI made with GPT-3 and then GPT-4 raised the bar of what was thought possible with this technology. And yet delays to the launch of GPT-5 have fueled rumors that OpenAI has struggled to build a model that meets its ownâ€”not to mention everyone elseâ€™sâ€”expectations.



Altman has been uncharacteristically modest: â€œ[GPT-5] is an experimental model that incorporates new research techniques we will use in future models,â€ he posted on Xâ€”which makes it sound more like a work in progress than another horizon-shifting release.Â 



But expectation management is part of the job for a company that for the last several years has set the agenda for the industry. And Chen and Pachocki set the agenda inside OpenAI.



Twin peaks&nbsp;



The firmâ€™s main London office is in St Jamesâ€™s Park, a few hundred meters east of Buckingham Palace. But I met Chen and Pachocki in a conference room in a coworking space near Kingâ€™s Cross, which OpenAI keeps as a kind of pied-Ã -terre in the heart of Londonâ€™s tech neighborhood (Google DeepMind and Meta are just around the corner). OpenAIâ€™s head of research communications, Laurance Fauconnet, sat with an open laptop at the end of the table.&nbsp;



Chen, who was wearing a maroon polo shirt, is clean-cut, almost preppy. Heâ€™s media trained and comfortable talking to a reporter. (Thatâ€™s him flirting with a chatbot in the â€œIntroducing GPT-4oâ€ video.) Pachocki, in a black elephant-logo tee, has more of a TV-movie hacker look. He stares at his hands a lot when he speaks.



But the pair are a tighter double act than they first appear. Pachocki summed up their roles. Chen shapes and manages the research teams, he said. â€œI am responsible for setting the research roadmap and establishing our long-term technical vision.â€





â€œBut thereâ€™s fluidity in the roles,â€ Chen said. â€œWeâ€™re both researchers, we pull on technical threads. Whatever we see that we can pull on and fix, thatâ€™s what we do.â€



Chen joined the company in 2018 after working as a quantitative trader at the Wall Street firm Jane Street Capital, where he developed machine-learning models for futures trading. At OpenAI he spearheaded the creation of DALL-E, the firmâ€™s breakthrough generative image model. He then worked on adding image recognition to GPTâ€‘4 and led the development of Codex, the generative coding model that powers GitHub Copilot.



Pachocki left an academic career in theoretical computer science to join OpenAI in 2017 and replaced Sutskever as chief scientist in 2024. He is the key architect of OpenAIâ€™s so-called reasoning modelsâ€”especially o1 and o3â€”which are designed to tackle complex tasks in science, math, and coding.&nbsp;



When we met they were buzzing, fresh off the high of two new back-to-back wins for their companyâ€™s technology.



On July 16, one of OpenAIâ€™s large language models came in second in the AtCoder World Tour Finals, one of the worldâ€™s most hardcore programming competitions. On July 19, OpenAI announced that one of its models had achieved gold-medal-level results on the 2025 International Math Olympiad, one of the worldâ€™s most prestigious math contests.



The math result made headlines, not only because of OpenAIâ€™s remarkable achievement, but because rival Google DeepMind revealed two days later that one of its models had achieved the same score in the same competition. Google DeepMind had played by the competitionâ€™s rules and waited for its results to be checked by the organizers before making an announcement; OpenAI had in effect marked its own answers.



For Chen and Pachocki, the result speaks for itself. Anyway, itâ€™s the programming win theyâ€™re most excited about. â€œI think thatâ€™s quite underrated,â€ Chen told me. A gold medal result in the International Math Olympiad puts you somewhere in the top 20 to 50 competitors, he said. But in the AtCoder contest OpenAIâ€™s model placed in the top two: â€œTo break into a really different tier of human performanceâ€”thatâ€™s unprecedented.â€



Ship, ship, ship!



People at OpenAI still like to say they work at a research lab. But the company is very different from the one it was before the release of ChatGPT three years ago. The firm is now in a race with the biggest and richest technology companies in the world and valued at $300 billion. Envelope-pushing research and eye-catching demos no longer cut it. It needs to ship products and get them into peopleâ€™s handsâ€”and boy, it does.&nbsp;



OpenAI has kept up a run of new releasesâ€”putting out major updates to its GPT-4 series, launching a string of generative image and video models, and introducing the ability to talk to ChatGPT with your voice. Six months ago it kicked off a new wave of so-called reasoning models with its o1 release, soon followed by o3. And last week it released its browser-using agent Operator to the public. It now claims that more than 400 million people use its products every week and submit 2.5 billion prompts a day.&nbsp;



OpenAIâ€™s incoming CEO of applications, Fidji Simo, plans to keep up the momentum. In a memo to the company, she told employees she is looking forward to â€œhelping get OpenAIâ€™s technologies into the hands of more people around the world,â€ where they will â€œunlock more opportunities for more people than any other technology in history.â€ Expect the products to keep coming.



I asked how OpenAI juggles open-ended research and product development. â€œThis is something we have been thinking about for a very long time, long before ChatGPT,â€ Pachocki said. â€œIf we are actually serious about trying to build artificial general intelligence, clearly there will be so much that you can do with this technology along the way, so many tangents you can go down that will be big products.â€ In other words, keep shaking the tree and harvest what you can.



A talking point that comes up with OpenAI folks is that putting experimental models out into the world was a necessary part of research. The goal was to make people aware of how good this technology had become. â€œWe want to educate people about whatâ€™s coming so that we can participate in what will be a very hard societal conversation,â€ Altman told me back in 2022. The makers of this strange new technology were also curious what it might be for: OpenAI was keen to get it into peopleâ€™s hands to see what they would do with it.



Is that still the case? They answered at the same time. â€œYeah!â€ Chen said. â€œTo some extent,â€ Pachocki said. Chen laughed: â€œNo, go ahead.â€&nbsp;



â€œI wouldnâ€™t say research iterates on product,â€ said Pachocki. â€œBut now that models are at the edge of the capabilities that can be measured by classical benchmarks and a lot of the long-standing challenges that weâ€™ve been thinking about are starting to fall, weâ€™re at the point where it really is about what the models can do in the real world.â€



Like taking on humans in coding competitions. The person who beat OpenAIâ€™s model at this yearâ€™s AtCoder contest, held in Japan, was a programmer named PrzemysÅ‚aw DÄ™biak, also known as Psyho. The contest was a puzzle-solving marathon in which competitors had 10 hours to find the most efficient way to solve a complex coding problem. After his win, Psyho posted on X: â€œIâ€™m completely exhausted &#8230; Iâ€™m barely alive.â€&nbsp;&nbsp;



Chen and Pachocki have strong ties to the world of competitive coding. Both have competed in international coding contests in the past and Chen coaches the USA Computing Olympiad team. I asked whether that personal enthusiasm for competitive coding colors their sense of how big a deal it is for a model to perform well at such a challenge.





They both laughed. â€œDefinitely,â€ said Pachocki. â€œSo: Psyho is kind of a legend. Heâ€™s been the number one competitor for many years. Heâ€™s also actually a friend of mineâ€”we used to compete together in these contests.â€ DÄ™biak also used to work with Pachocki at OpenAI.



When Pachocki competed in coding contests he favored those that focused on shorter problems with concrete solutions. But DÄ™biak liked longer, open-ended problems without an obvious correct answer.



â€œHe used to poke fun at me, saying that the kind of contest I was into will be automated long before the ones he liked,â€ Pachocki recalled. â€œSo I was seriously invested in the performance of this model in this latest competition.â€



Pachocki told me he was glued to the late-night livestream from Tokyo, watching his model come in second: â€œPsyho resists for now.â€&nbsp;



â€œWeâ€™ve tracked the performance of LLMs on coding contests for a while,â€ said Chen. â€œWeâ€™ve watched them become better than me, better than Jakub. It feels something like Lee Sedol playing Go.â€



Lee is the master Go player who lost a series of matches to DeepMindâ€™s game-playing model AlphaGo in 2016. The results stunned the international Go community and led Lee to give up professional play. Last year he told the New York Times: â€œLosing to AI, in a sense, meant my entire world was collapsing &#8230; I could no longer enjoy the game.â€ And yet, unlike Lee, Chen and Pachocki are thrilled to be surpassed.&nbsp;&nbsp;&nbsp;



But why should the rest of us care about these niche wins? Itâ€™s clear that this technologyâ€”designed to mimic and, ultimately, stand in for human intelligenceâ€”is being built by people whose idea of peak intelligence is acing a math contest or holding your own against a legendary coder. Is it a problem that this view of intelligence is skewed toward the mathematical, analytical end of the scale?



â€œI mean, I think you are right thatâ€”you know, selfishly, we do want to create models which accelerate ourselves,â€ Chen told me. â€œWe see that as a very fast factor to progress.â€&nbsp;&nbsp;



The argument researchers like Chen and Pachocki make is that math and coding are the bedrock for a far more general form of intelligence, one that can solve a wide range of problems in ways we might not have thought of ourselves. â€œWeâ€™re talking about programming and math here,â€ said Pachocki. â€œBut itâ€™s really about creativity, coming up with novel ideas, connecting ideas from different places.â€



Look at the two recent competitions: â€œIn both cases, there were problems which required very hard, out-of-the-box thinking. Psyho spent half the programming competition thinking and then came up with a solution that was really novel and quite different from anything that our model looked at.â€



â€œThis is really what weâ€™re after,â€ Pachocki continued. â€œHow do we get models to discover this sort of novel insight? To actually advance our knowledge? I think they are already capable of that in some limited ways. But I think this technology has the potential to really accelerate scientific progress.â€&nbsp;



I returned to the question about whether the focus on math and programming was a problem, conceding that maybe itâ€™s fine if what weâ€™re building are tools to help us do science. We don&#8217;t necessarily want large language models to replace politicians and have people skills, I suggested.



Chen pulled a face and looked up at the ceiling: â€œWhy not?â€



Whatâ€™s missing



OpenAI was founded with a level of hubris that stood out even by Silicon Valley standards, boasting about its goal of building AGI back when talk of AGI still sounded kooky. OpenAI remains as gung-ho about AGI as ever, and it has done more than most to make AGI a mainstream multibillion-dollar concern. Itâ€™s not there yet, though. I asked Chen and Pachocki what they think is missing.



â€œI think the way to envision the future is to really, deeply study the technology that we see today,â€ Pachocki said. â€œFrom the beginning, OpenAI has looked at deep learning as this very mysterious and clearly very powerful technology with a lot of potential. Weâ€™ve been trying to understand its bottlenecks. What can it do? What can it not do?â€&nbsp;&nbsp;



At the current cutting edge, Chen said, are reasoning models, which break down problems into smaller, more manageable steps, but even they have limits: â€œYou know, you have these models which know a lot of things but canâ€™t chain that knowledge together. Why is that? Why canâ€™t it do that in a way that humans can?â€



OpenAI is throwing everything at answering that question.



â€œWe are probably still, like, at the very beginning of this reasoning paradigm,â€ Pachocki told me. â€œReally, we are thinking about how to get these models to learn and explore over the long term and actually deliver very new ideas.â€



Chen pushed the point home: â€œI really donâ€™t consider reasoning done. Weâ€™ve definitely not solved it. You have to read so much text to get a kind of approximation of what humans know.â€



OpenAI wonâ€™t say what data it uses to train its models or give details about their size and shapeâ€”only that it is working hard to make all stages of the development process more efficient.



Those efforts make them confident that so-called scaling lawsâ€”which suggest that models will continue to get better the more compute you throw at themâ€”show no sign of breaking down.



â€œI donâ€™t think thereâ€™s evidence that scaling laws are dead in any sense,â€ Chen insisted. â€œThere have always been bottlenecks, right? Sometimes theyâ€™re to do with the way models are built. Sometimes theyâ€™re to do with data. But fundamentally itâ€™s just about finding the research that breaks you through the current bottleneck.â€&nbsp;



The faith in progress is unshakeable. I brought up something Pachocki had said about AGI in an interview with Nature in May: â€œWhen I joined OpenAI in 2017, I was still among the biggest skeptics at the company.â€ He looked doubtful.&nbsp;



â€œIâ€™m not sure I was skeptical about the concept,â€ he said. â€œBut I think I wasâ€”â€ He paused, looking at his hands on the table in front of him. â€œWhen I joined OpenAI, I expected the timelines to be longer to get to the point that we are now.â€





â€œThereâ€™s a lot of consequences of AI,â€ he said. â€œBut the one I think the most about is automated research. When we look at human history, a lot of it is about technological progress, about humans building new technologies. The point when computers can develop new technologies themselves seems like a very important, um, inflection point.



â€œWe already see these models assist scientists. But when they are able to work on longer horizonsâ€”when theyâ€™re able to establish research programs for themselvesâ€”the world will feel meaningfully different.â€



For Chen, that ability for models to work by themselves for longer is key. â€œI mean, I do think everyone has their own definitions of AGI,â€ he said. â€œBut this concept of autonomous timeâ€”just the amount of time that the model can spend making productive progress on a difficult problem without hitting a dead endâ€”thatâ€™s one of the big things that weâ€™re after.â€



Itâ€™s a bold visionâ€”and far beyond the capabilities of todayâ€™s models. But I was nevertheless struck by how Chen and Pachocki made AGI sound almost mundane. Compare this with how Sutskever responded when I spoke to him 18 months ago. â€œItâ€™s going to be monumental, earth-shattering,â€ he told me. â€œThere will be a before and an after.â€ Faced with the immensity of what he was building, Sutskever switched the focus of his career from designing better and better models to figuring out how to control a technology that he believed would soon be smarter than himself.



Two years ago Sutskever set up what he called a superalignment team that he would co-lead with another OpenAI safety researcher, Jan Leike. The claim was that this team would funnel a full fifth of OpenAIâ€™s resources into figuring out how to control a hypothetical superintelligence. Today, most of the people on the superalignment team, including Sutskever and Leike, have left the company and the team no longer exists.&nbsp;&nbsp;&nbsp;



When Leike quit, he said it was because the team had not been given the support he felt it deserved. He posted this on X: â€œBuilding smarter-than-human machines is an inherently dangerous endeavor. OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.â€ Other departing researchers shared similar statements.



I asked Chen and Pachocki what they make of such concerns. â€œA lot of these things are highly personal decisions,â€ Chen said. â€œYou know, a researcher can kind of, you knowâ€”â€



He started again. â€œThey might have a belief that the field is going to evolve in a certain way and that their research is going to pan out and is going to bear fruit. And, you know, maybe the company doesnâ€™t reshape in the way that you want it to. Itâ€™s a very dynamic field.â€



â€œA lot of these things are personal decisions,â€ he repeated. â€œSometimes the field is just evolving in a way that is less consistent with the way that youâ€™re doing research.â€



But alignment, both of them insist, is now part of the core business rather than the concern of one specific team. According to Pachocki, these models donâ€™t work at all unless they work as you expect them to. Thereâ€™s also little desire to focus on aligning a hypothetical superintelligence with your objectives when doing so with existing models is already enough of a challenge.



â€œTwo years ago the risks that we were imagining were mostly theoretical risks,â€ Pachocki said. â€œThe world today looks very different, and I think a lot of alignment problems are now very practically motivated.â€



Still, experimental technology is being spun into mass-market products faster than ever before. Does that really never lead to disagreements between the two of them?



â€œI am often afforded the luxury of really kind of thinking about the long term, where the technology is headed,â€ Pachocki said. â€œContending with the reality of the processâ€”both in terms of people and also, like, the broader company needsâ€”falls on Mark. Itâ€™s not really a disagreement, but there is a natural tension between these different objectives and the different challenges that the company is facing that materializes between us.â€



Chen jumped in: â€œI think itâ€™s just a very delicate balance.â€&nbsp;&nbsp;
â€¢ The AI Hype Index: The White Houseâ€™s war on â€œwoke AIâ€
  The Trump administration recently declared war on so-called â€œwoke AI,â€ issuing an executive order aimed at preventing companies whose models exhibit a liberal bias from landing federal contracts . The Pentagon inked a deal with Elon Muskâ€™s xAI just days after its chatbot, Grok, spouted harmful antisemitic stereotypes . The White House has partnered with an anti-DEI nonprofit to create AI slop videos .
â€¢ An EPA rule change threatens to gut US climate regulations
  This story is part of MIT Technology Reviewâ€™s â€œAmerica Undoneâ€ series, examining how the foundations of US success in science and innovation are currently under threat.Â You can read the rest here.



The mechanism that allows the US federal government to regulate climate change is on the chopping block.



On Tuesday, US Environmental Protection Agency administrator Lee Zeldin announced that the agency is taking aim at the endangerment finding, a 2009 rule thatâ€™s essentially the tentpole supporting federal greenhouse-gas regulations.



This might sound like an obscure legal situation, but itâ€™s a really big deal for climate policy in the US. So buckle up, and letâ€™s look at what this rule says now, what the proposed change looks like, and what it all means.



To set the stage, we have to go back to the Clean Air Act of 1970, the law that essentially gave the EPA the power to regulate air pollution. (Stick with meâ€”I promise Iâ€™ll keep this short and not get too into the legal weeds.)



There were some pollutants explicitly called out in this law and its amendments, including lead and sulfur dioxide. But it also required the EPA to regulate new pollutants that were found to be harmful. In the late 1990s and early 2000s, environmental groups and states started asking for the agency to include greenhouse-gas pollution.



In 2007, the Supreme Court ruled that greenhouse gases qualify as air pollutants under the Clean Air Act, and that the EPA should study whether theyâ€™re a danger to public health. In 2009, the incoming Obama administration looked at the science and ruled that greenhouse gases pose a threat to public health because they cause climate change. Thatâ€™s the endangerment finding, and itâ€™s what allows the agency to pass rules to regulate greenhouse gases.Â Â 





The original case and argument were specifically about vehicles and the emissions from tailpipes, but this finding was eventually used to allow the agency to set rules around power plants and factories, too. It essentially underpins climate regulations in the US.



Fast-forward to today, and the Trump administration wants to reverse the endangerment finding. In a proposed rule released on Tuesday, the EPA argues that the Clean Air Act does not, in fact, authorize the agency to set emissions standards to address global climate change. Zeldin, in an appearance on the conservative politics and humor podcast Ruthless that preceded the official announcement, called the proposal the â€œlargest deregulatory action in the history of America.â€



The administration was already moving to undermine the climate regulations that rely on this rule. But this move directly targets a â€œfundamental building block of EPAâ€™s climate policy,â€ says Deborah Sivas, an environmental-law professor at Stanford University.



The proposed rule will go up for public comment, and the agency will then take that feedback and come up with a final version. Itâ€™ll almost certainly get hit with legal challenges and will likely wind up in front of the Supreme Court.



One note here is that the EPA makes a mostly legal argument in the proposed rule reversal rather than focusing on going after the science of climate change, says Madison Condon, an associate law professor at Boston University. That could make it easier for the Supreme Court to eventually uphold it, she says, though this whole process is going to take a while.Â 



If the endangerment finding goes down, it would have wide-reaching ripple effects. â€œWe could find ourselves in a couple years with no legal tools to try and address climate change,â€ Sivas says.



To take a step back for a moment, itâ€™s wild that weâ€™ve ended up in this place where a single rule is so central to regulating emissions. US climate policy is held up by duct tape and a dream. Congress could have, at some point, passed a law that more directly allows the EPA to regulate greenhouse-gas emissions (the last time we got close was a 2009 bill that passed the House but never made it to the Senate). But here we are.



This move isnâ€™t a surprise, exactly. The Trump administration has made it very clear that it is going after climate policy in every way that it can. But whatâ€™s most striking to me is that weâ€™re not operating in a shared reality anymore when it comes to this subject.Â 



While top officials tend to acknowledge that climate change is real, thereâ€™s often a â€œbutâ€ followed by talking points from climate denialâ€™s list of greatest hits. (One of the more ridiculous examples isÂ the statement that carbon dioxide is good, actually, because it helps plants.)Â 



Climate change is real, and itâ€™s a threat. And the US has emitted more greenhouse gases into the atmosphere than any other country in the world. It shouldnâ€™t be controversial to expect the government to be doing something about it.&nbsp;



This article is from The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
â€¢ Roundtables: Why Itâ€™s So Hard to Make Welfare AI Fair
  Amsterdam tried using algorithms to fairly assess welfare applicants, but bias still crept in . Hear from MIT Technology Review editor Amanda Silverman, investigative reporter Eileen Guo, and Lighthouse Reports investigative reporter Gabriel Geiger as they explore if algorithms can ever be fair . The true dangers of AI are closer than we think, and the true dangers are closer to us, they say . The coming war on the hidden algorithms that trap people in poverty is on the rise .
â€¢ The Download: a 30-year old baby, and OpenAIâ€™s push into colleges
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Exclusive: A record-breaking baby has been born from an embryo thatâ€™s over 30 years old



A baby boy has just won the new record for the â€œoldest baby.â€ Thaddeus Daniel Pierce, who arrived on July 26, developed from an embryo that had been in storage for 30 and a half years.Lindsey and her husband, Tim Pierce, who live in London, Ohio, â€œadoptedâ€ the embryo from Linda Archerd, who had it created in 1994. The couple, aged 35 and 34, respectively, had been trying for a baby for seven years. Read more about their remarkable story.



â€”Jessica Hamzelou







OpenAI is launching a version of ChatGPT for college students



OpenAI is launching Study Mode, a version of ChatGPT for college students that it promises will act less like a lookup tool and more like a friendly, always-available tutor.&nbsp;



The chatbot begins by asking what the student wants to know and then attempts to build an exchange, where the pair work methodically toward the answer together. OpenAI says the tool was built after consulting with pedagogy experts from over 40 institutions.



But thereâ€™s an ambitious vision behind Study Mode: Itâ€™s part of a wider push by OpenAI to get AI more deeply embedded into classrooms when the new academic year starts in September. Read the full story.â€”James O&#8217;Donnell







MIT Technology Review Narrated: Are we ready to hand AI agents the keys?



In recent months, a new class of agents has arrived on the scene: ones built using large language models. Any action that can be captured by textâ€”from playing a video game using written commands to running a social media accountâ€”is potentially within the purview of this type of system.



LLM agents donâ€™t have much of a track record yet, but to hear CEOs tell it, they will transform the economyâ€”and soon. Despite that, like chatbot LLMs, agents can be chaotic and unpredictable.This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we publish each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as itâ€™s released.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 The first tsunami waves have reached the US West CoastBut early damage from the powerful Russian earthquake has been thankfully limited. (WP $)+ Itâ€™ll take some time before we can be confident thereâ€™s no danger, though. (WSJ $)+ These underwater cables can improve tsunami detection. (MIT Technology Review)



2 Google has signed the EU code of practiceDespite criticisms from the US that it stands to stifle growth. (FT $)+ Europe and America are taking very different paths. (The Register)



3 NASA is launching a new Earth-observing satellite todayItâ€™ll keep a watch over precursors to earthquakes, landslides and volcanoes. (BBC)+ Its data will be turned into maps to help scientists better respond. (NYT $)



4 US antibiotics research is likely to suffer without federal fundingIt plays a critical role in antibiotic discovery. (Undark)+ How bacteria-fighting viruses could go mainstream. (MIT Technology Review)



5 Russia is building its own new webAnd at its heart is VK Co, a social network controlled by its government. (Bloomberg $)+ How Russia killed its tech industry. (MIT Technology Review)



6 How Anthropic became so good at codingEveryone else in Silicon Valley is dying to know. (Insider $)+ The second wave of AI coding is here. (MIT Technology Review)



7 Demand for Vietnamâ€™s chips is boomingItâ€™s reaping the benefits of the world looking for alternatives to Chinaâ€™s products. (Rest of World)+ Things arenâ€™t looking great for AI chipmaker Groq. (The Information $)



8 Yelp has started making its own AI restaurant videosAnd users canâ€™t opt out of having their photos used in them. (The Verge)



9 Are memes the new comics?If comics didnâ€™t have a plot, that is. (Ars Technica)+ Generative AI is reshaping South Koreaâ€™s webcomics industry. (MIT Technology Review)



10 Starbucks is abandoning launching stores that only accept mobile orders The vibes are off, apparently. (WSJ $)







Quote of the day



â€œAny lawyer unaware that using generative AI platforms to do legal research is playing with fire is living in a cloud.â€



â€”Judge Michael Slade criticizes a lawyer who used AI-generated citations in a legal case, PC Gamer reports.







One more thing







The return of pneumatic tubesPneumatic tubes were once touted as something that would revolutionize the world. In science fiction, they were envisioned as a fundamental part of the futureâ€”even in dystopias like George Orwellâ€™s 1984, where they help to deliver orders for the main character, Winston Smith, in his job rewriting history to fit the ruling partyâ€™s changing narrative.In real life, the tubes were expected to transform several industries in the late 19th century through the mid-20th. For a while, the United States took up the systems with gusto.But by the mid to late 20th century, use of the technology had largely fallen by the wayside, and pneumatic tube technology became virtually obsolete. Except in hospitals. Read the full story.



â€”Vanessa Armstrong







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This sweet baby pudu fawn is just too cute for words.+ Thereâ€™s some great picks in this list of the 100 best podcasts (and some shocking omissions).+ The infamous gigantic Home Depot skeleton is getting a voice!+ If youâ€™re never not thinking about the Roman empire, hereâ€™s what happened after it all came crashing down.

ğŸ”’ Cybersecurity & Privacy
â€¢ Scammers Unleash Flood of Slick Online Gaming Sites
  Fraudsters are flooding Discord and other social media platforms with ads for hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. Here&#8217;s a closer look at the social engineering tactics and remarkable traits of this sprawling network of more than 1,200 scam sites.
The scam begins with deceptive ads posted on social media that claim the wagering sites are working in partnership with popular social media personalities, such as Mr. Beast, who recently launched a gaming business called Beast Games. The ads invariably state that by using a supplied &#8220;promo code,&#8221; interested players can claim a $2,500 credit on the advertised gaming website.
An ad posted to a Discord channel for a scam gamblingÂ website that the proprietors falsely claim was operating in collaboration with the Internet personality Mr. Beast. Image: Reddit.com.
The gaming sites all require users to create a free account to claim their $2,500 credit, which they can use to play any number of extremely polished video games that ask users to bet on each action. At the scam website gamblerbeast[.]com, for example, visitors can pick from dozens of games like B-Ball Blitz, in which you play a basketball pro who is taking shots from the free throw line against a single opponent, and you bet on your ability to sink each shot.
The financial part of this scam begins when users try to cash out any &#8220;winnings.&#8221; At that point, the gaming site will reject the request and prompt the user to make a &#8220;verification deposit&#8221; of cryptocurrency &#8212; typically around $100 &#8212; before any money can be distributed. Those who deposit cryptocurrency funds are soon asked for additional payments.

However, any &#8220;winnings&#8221; displayed by these gaming sites are a complete fantasy, and players who deposit cryptocurrency funds will never see that money again. Compounding the problem, victims likely will soon be peppered with come-ons from &#8220;recovery experts&#8221; who peddle dubious claims on social media networks about being able to retrieve funds lost to such scams.
KrebsOnSecurity first learned about this network of phony betting sites from a Discord user who asked to be identified only by their screen name: &#8220;Thereallo&#8221; is a 17-year-old developer who operates multiple Discord servers and said they began digging deeper after users started complaining of being inundated with misleading spam messages promoting the sites.
&#8220;We were being spammed relentlessly by these scam posts from compromised or purchased [Discord] accounts,&#8221; Thereallo said. &#8220;I got frustrated with just banning and deleting, so I started to investigate the infrastructure behind the scam messages. This is not a one-off site, it&#8217;s a scalable criminal enterprise with a clear playbook, technical fingerprints, and financial infrastructure.&#8221;
After comparing the code on the gaming sites promoted via spam messages, Thereallo found they all invoked the same API key for an online chatbot that appears to be in limited use or else is custom-made. Indeed, a scan for that API key at the threat hunting platform Silent Push reveals at least 1,270 recently-registered and active domains whose names all invoke some type of gaming or wagering theme.
The &#8220;verification deposit&#8221; stage of the scam requires the user to deposit cryptocurrency in order to withdraw their &#8220;winnings.&#8221;
Thereallo said the operators of this scam empire appear to generate a unique Bitcoin wallet for each gaming domain they deploy.
&#8220;This is a decoy wallet,&#8221; Thereallo explained. &#8220;Once the victim deposits funds, they are never able to withdraw any money. Any attempts to contact the &#8216;Live Support&#8217; are handled by a combination of AI and human operators who eventually block the user. The chat system is self-hosted, making it difficult to report to third-party service providers.&#8221;
Thereallo discovered another feature common to all of these scam gambling sites [hereafter referred to simply as &#8220;scambling&#8221; sites]: If you register at one of them and then very quickly try to register at a sister property of theirs from the same Internet address and device, the registration request is denied at the second site.
&#8220;I registered on one site, then hopped to another to register again,&#8221; Thereallo said. Instead, the second site returned an error stating that a new account couldn&#8217;t be created for another 10 minutes.
The scam gaming site spinora dot cc shares the same chatbot API as more than 1,200 similar fake gaming sites.
&#8220;They&#8217;re tracking my VPN IP across their entire network,&#8221; Thereallo explained. &#8220;My password manager also proved it. It tried to use my dummy email on a site I had never visited, and the site told me the account already existed. So it&#8217;s definitely one entity running a single platform with 1,200+ different domain names as front-ends. This explains how their support works, a central pool of agents handling all the sites. It also explains why they&#8217;re so strict about not giving out wallet addresses; it&#8217;s a network-wide policy.&#8221;
In many ways, these scambling sites borrow from the playbook of &#8220;pig butchering&#8221; schemes, a rampant and far more elaborate crime in which people are gradually lured by flirtatious strangers online into investing in fraudulent cryptocurrency trading platforms.
Pig butchering scams are typically powered by people in Asia who have been kidnapped and threatened with physical harm or worse unless they sit in a cubicle and scam Westerners on the Internet all day. In contrast, these scambling sites tend to steal far less money from individual victims, but their cookie-cutter nature and automated support components may enable their operators to extract payments from a large number of people in far less time, and with considerably less risk and up-front investment.
Silent Push&#8217;s Zach Edwards said the proprietors of this scambling empire are spending big money to make the sites look and feel like some fancy new type of casino.
&#8220;That&#8217;s a very odd type of pig butchering network and not like what we typically see, with much lower investments in the sites and lures,&#8221; Edwards said.
Here is a list of all domains that Silent Push found were using the scambling network&#8217;s chat API.

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Automate the creation of handout notes using Amazon Bedrock Data Automation
  Organizations across various sectors face significant challenges when converting meeting recordings or recorded presentations into structured documentation. The process of creating handouts from presentations requires lots of manual effort, such as reviewing recordings to identify slide transitions, transcribing spoken content, capturing and organizing screenshots, synchronizing visual elements with speaker notes, and formatting content. These challenges impact productivity and scalability, especially when dealing with multiple presentation recordings, conference sessions, training materials, and educational content. 
In this post, we show how you can build an automated, serverless solution to transform webinar recordings into comprehensive handouts using Amazon Bedrock Data Automation for video analysis. We walk you through the implementation of Amazon Bedrock Data Automation to transcribe and detect slide changes, as well as the use of Amazon Bedrock foundation models (FMs) for transcription refinement, combined with custom AWS Lambda functions orchestrated by AWS Step Functions. Through detailed implementation details, architectural patterns, and code, you will learn how to build a workflow that automates the handout creation process. 
Amazon Bedrock Data Automation 
Amazon Bedrock Data Automation uses generative AI to automate the transformation of multimodal data (such as images, videos and more) into a customizable structured format. Examples of structured formats include summaries of scenes in a video, unsafe or explicit content in text and images, or organized content based on advertisements or brands. The solution presented in this post uses Amazon Bedrock Data Automation to extract audio segments and different shots in videos. 
Solution overview 
Our solution uses a serverless architecture orchestrated by Step Functions to process presentation recordings into comprehensive handouts. The workflow consists of the following steps: 
 
 The workflow begins when a video is uploaded to Amazon Simple Storage Service (Amazon S3), which triggers an event notification through Amazon EventBridge rules that initiates our video processing workflow in Step Functions. 
 After the workflow is triggered, Amazon Bedrock Data Automation initiates a video transformation job to identify different shots in the video. In our case, this is represented by a change of slides. The workflow moves into a waiting state, and checks for the transformation job progress. If the job is in progress, the workflow returns to the waiting state. When the job is complete, the workflow continues, and we now have extracted both visual shots and spoken content. 
 These visual shots and spoken content feed into a synchronization step. In this Lambda function, we use the output of the Amazon Bedrock Data Automation job to match the spoken content to the correlating shots based on the matching of timestamps. 
 After function has matched the spoken content to the visual shots, the workflow moves into a parallel state. One of the steps of this state is the generation of screenshots. We use a FFmpeg-enabled Lambda function to create images for each identified video shot. 
 The other step of the parallel state is the refinement of our transformations. Amazon Bedrock processes and improves each raw transcription section through a Map state. This helps us remove speech disfluencies and improve the sentence structure. 
 Lastly, after the screenshots and refined transcript are created, the workflow uses a Lambda function to create handouts. We use the Python-PPTX library, which generates the final&nbsp;presentation with synchronized content. These final handouts are stored in Amazon S3 for distribution. 
 
The following diagram illustrates this workflow. 
 
If you want to try out this solution, we have created an AWS Cloud Development Kit (AWS CDK) stack available in the accompanying GitHub repo that you can deploy in your account. It deploys the Step Functions state machine to orchestrate the creation of handout notes from the presentation video recording. It also provides you with a sample video to test out the results. 
To deploy and test the solution in your own account, follow the instructions in the GitHub repositoryâ€™s README file. The following sections describe in more detail the technical implementation details of this solution. 
Video upload and initial processing 
The workflow begins with Amazon S3, which serves as the entry point for our video processing pipeline. When a video is uploaded to a dedicated S3 bucket, it triggers an event notification that, through EventBridge rules, initiates our Step Functions workflow. 
Shot detection and transcription using Amazon Bedrock Data Automation 
This step uses Amazon Bedrock Data Automation to detect slide transitions and create video transcriptions. To integrate this as part of the workflow, you must create an Amazon Bedrock Data Automation project. A project is a grouping of output configurations. Each project can contain standard output configurations as well as custom output blueprints for documents, images, video, and audio. The project has already been created as part of the AWS CDK stack. After you set up your project, you can process content using the InvokeDataAutomationAsync API. In our solution, we use the Step Functions service integration to execute this API call and start the asynchronous processing job. A job ID is returned for tracking the process. 
The workflow must now check the status of the processing job before continuing with the handout creation process. This is done by polling Amazon Bedrock Data Automation for the job status using the GetDataAutomationStatus API on a regular basis. Using a combination of the Step Functions Wait and Choice states, we can ask the workflow to poll the API on a fixed interval. This not only gives you the ability to customize the interval depending on your needs, but it also helps you control the workflow costs, because every state transition is billed in Standard workflows, which this solution uses. 
When the GetDataAutomationStatus API output shows as SUCCESS, the loop exits and the workflow continues to the next step, which will match transcripts to the visual shots. 
Matching audio segments with corresponding shots 
To create comprehensive handouts, you must establish a mapping between the visual shots and their corresponding audio segments. This mapping is crucial to make sure the final handouts accurately represent both the visual content and the spoken narrative of the presentation. 
A shot represents a series of interrelated consecutive frames captured during the presentation, typically indicating a distinct visual state. In our presentation context, a shot corresponds to either a new slide or a significant slide animation that adds or modifies content. 
An audio segment is a specific portion of an audio recording that contains uninterrupted spoken language, with minimal pauses or breaks. This segment captures a natural flow of speech. The Amazon Bedrock Data Automation output provides an audio_segments array, with each segment containing precise timing information such as the start and end time of each segment. This allows for accurate synchronization with the visual shots. 
The synchronization between shots and audio segments is critical for creating accurate handouts that preserve the presentationâ€™s narrative flow. To achieve this, we implement a Lambda function that manages the matching process in three steps: 
 
 The function retrieves the processing results from Amazon S3, which contains both the visual shots and audio segments. 
 It creates structured JSON arrays from these components, preparing them for the matching algorithm. 
 It executes a matching algorithm that analyzes the different timestamps of the audio segments and the shots, and matches them based on these timestamps. This algorithm also considers timestamp overlaps between shots and audio segments. 
 
For each shot, the function examines audio segments and identifies those whose timestamps overlap with the shotâ€™s duration, making sure the relevant spoken content is associated with its corresponding slide in the final handouts. The function returns the matched results directly to the Step Functions workflow, where it will serve as input for the next step, where Amazon Bedrock will refine the transcribed content and where we will create screenshots in parallel. 
Screenshot generation 
After you get the timestamps of each shot and associated audio segment, you can capture the slides of the presentation to create comprehensive handouts. Each detected shot from Amazon Bedrock Data Automation represents a distinct visual state in the presentationâ€”typically a new slide or significant content change. By generating screenshots at these precise moments, we make sure our handouts accurately represent the visual flow of the original presentation. 
This is done with a Lambda function using the ffmpeg-python library. This library acts as a Python binding for the FFmpeg media framework, so you can run FFmpeg terminal commands using Python methods. In our case, we can extract frames from the video at specific timestamps identified by Amazon Bedrock Data Automation. The screenshots are stored in an S3 bucket to be used in creating the handouts, as described in the following code. To use ffmpeg-python in Lambda, we created a Lambda ZIP deployment containing the required dependencies to run the code. Instructions on how to create the ZIP file can be found in our GitHub repository. 
The following code shows how a screenshot is taken using ffmpeg-python. You can view the full Lambda code on GitHub. 
 
 ## Taking a screenshot at a specific timestamp 
ffmpeg.input(video_path, ss=timestamp).output(screenshot_path, vframes=1).run() 
 
Transcript refinement with Amazon Bedrock 
In parallel with the screenshot generation, we refine the transcript using a large language model (LLM). We do this to improve the quality of the transcript and filter out errors and speech disfluencies. This process uses an Amazon Bedrock model to enhance the quality of the matched transcription segments while maintaining content accuracy. We use a Lambda function that integrates with Amazon Bedrock through the Python Boto3 client, using a prompt to guide the modelâ€™s refinement process. The function can then process each transcript segment, instructing the model to do the following: 
 
 Fix typos and grammatical errors 
 Remove speech disfluencies (such as â€œuhâ€ and â€œumâ€) 
 Maintain the original meaning and technical accuracy 
 Preserve the context of the presentation 
 
In our solution, we used the following prompt with three example inputs and outputs: 
 
 prompt = '''This is the result of a transcription. 
I want you to look at this audio segment and fix the typos and mistakes present. 
Feel free to use the context of the rest of the transcript to refine (but don't leave out any info). 
Leave out parts where the speaker misspoke. 
Make sure to also remove works like "uh" or "um". 
Only make change to the info or sentence structure when there are mistakes. 
Only give back the refined transcript as output, don't add anything else or any context or title. 
If there are no typos or mistakes, return the original object input. 
Do not explain why you have or have not made any changes; I just want the JSON object. 

These are examples: 
Input: &lt;an example-input&gt; 
Output: &lt;an example-output&gt;

Input: &lt;an example-input&gt; 
Output: &lt;an example-output&gt;

Input: &lt;an example-input&gt; 
Output: &lt;an example-output&gt;

Here is the object: ''' + text 
 
The following is an example input and output: 
 
 Input: Yeah. Um, so let's talk a little bit about recovering from a ransomware attack, right?

Output: Yes, let's talk a little bit about recovering from a ransomware attack. 
 
To optimize processing speed while adhering to the maximum token limits of the Amazon Bedrock InvokeModel API, we use the Step Functions Map state. This enables parallel processing of multiple transcriptions, each corresponding to a separate video segment. Because these transcriptions must be handled individually, the Map state efficiently distributes the workload. Additionally, it reduces operational overhead by managing integrationâ€”taking an array as input, passing each element to the Lambda function, and automatically reconstructing the array upon completion.The Map state returns the refined transcript directly to the Step Functions workflow, maintaining the structure of the matched segments while providing cleaner, more professional text content for the final handout generation. 
Handout generation 
The final step in our workflow involves creating the handouts using the python-pptx library. This step combines the refined transcripts with the generated screenshots to create a comprehensive presentation document. 
The Lambda function processes the matched segments sequentially, creating a new slide for each screenshot while adding the corresponding refined transcript as speaker notes. The implementation uses a custom Lambda layer containing the python-pptx package. To enable this functionality in Lambda, we created a custom layer using Docker. By using Docker to create our layer, we make sure the dependencies are compiled in an environment that matches the Lambda runtime. You can find the instructions to create this layer and the layer itself in our GitHub repository. 
The Lambda function implementation uses python-pptx to create structured presentations: 
 
 import boto3
from pptx import Presentation
from pptx.util import Inches
import os
import json

def lambda_handler(event, context):
    # Create new presentation with specific dimensions
    prs = Presentation()
    prs.slide_width = int(12192000)  # Standard presentation width
    prs.slide_height = int(6858000)  # Standard presentation height
    
    # Process each segment
    for i in range(num_images):
        # Add new slide
        slide = prs.slides.add_slide(prs.slide_layouts[5])
        
        # Add screenshot as full-slide image
        slide.shapes.add_picture(image_path, 0, 0, width=slide_width)
        
        # Add transcript as speaker notes
        notes_slide = slide.notes_slide
        transcription_text = transcription_segments[i].get('transcript', '')
        notes_slide.notes_text_frame.text = transcription_text
    
    # Save presentation
    pptx_path = os.path.join(tmp_dir, "lecture_notes.pptx")
    prs.save(pptx_path)
 
 The function processes segments sequentially, creating a presentation that combines visual shots with their corresponding audio segments, resulting in handouts ready for distribution. 
 The following screenshot shows an example of a generated slide with notes. The full deck has been added as a file in the GitHub repository. 
  
 Conclusion 
 In this post, we demonstrated how to build a serverless solution that automates the creation of handout notes from recorded slide presentations. By combining Amazon Bedrock Data Automation with custom Lambda functions, weâ€™ve created a scalable pipeline that significantly reduces the manual effort required in creating handout materials. Our solution addresses several key challenges in content creation: 
  
  Automated detection of slide transitions, content changes, and accurate transcription of spoken content using the video modality capabilities of Amazon Bedrock Data Automation 
  Intelligent refinement of transcribed text using Amazon Bedrock 
  Synchronized visual and textual content with a custom matching algorithm 
  Handout generation using the ffmpeg-python and python-pptx libraries in Lambda 
  
 The serverless architecture, orchestrated by Step Functions, provides reliable execution while maintaining cost-efficiency. By using Python packages for FFmpeg and a Lambda layer for python-pptx, weâ€™ve overcome technical limitations and created a robust solution that can handle various presentation formats and lengths. This solution can be extended and customized for different use cases, from educational institutions to corporate training programs. Certain steps such as the transcript refinement can also be improved, for instance by adding translation capabilities to account for diverse audiences. 
 To learn more about Amazon Bedrock Data Automation, refer to the following resources: 
  
  Transform unstructured data into meaningful insights using Amazon Bedrock Data Automation 
  New Amazon Bedrock capabilities enhance data processing and retrieval 
  Simplify multimodal generative AI with Amazon Bedrock Data Automation 
  Guidance for Multimodal Data Processing Using Amazon Bedrock Data Automation 
  
  
  
 About the authors 
 Laura Verghote is the GenAI Lead for PSI Europe at Amazon Web Services (AWS), driving Generative AI adoption across public sector organizations. She partners with customers throughout Europe to accelerate their GenAI initiatives through technical expertise and strategic planning, bridging complex requirements with innovative AI solutions. 
 Elie Elmalem is a solutions architect at Amazon Web Services (AWS) and supports Education customers across the UK and EMEA. He works with customers to effectively use AWS services, providing architectural best practices, advice, and guidance. Outside of work, he enjoys spending time with family and friends and loves watching his favorite football team play.
â€¢ Streamline GitHub workflows with generative AI using Amazon Bedrock and MCP
  Customers are increasingly looking to use the power of large language models (LLMs) to solve real-world problems. However, bridging the gap between these LLMs and practical applications has been a challenge. AI agents have appeared as an innovative technology that bridges this gap. 
The foundation models (FMs) available through Amazon Bedrock serve as the cognitive engine for AI agents, providing the reasoning and natural language understanding capabilities essential for interpreting user requests and generating appropriate responses. You can integrate these models with various agent frameworks and orchestration layers to create AI applications that can understand context, make decisions, and take actions. You can build with Amazon Bedrock Agents or other frameworks like LangGraph and the recently launched Strands Agent SDK. 
This blog post explores how to create powerful agentic applications using the Amazon Bedrock FMs, LangGraph, and the Model Context Protocol (MCP), with a practical scenario of handling a GitHub workflow of issue analysis, code fixes, and pull request generation. 
For teams seeking a managed solution to streamline GitHub workflows, Amazon Q Developer in GitHub offers native integration with GitHub repositories. It provides built-in capabilities for code generation, review, and code transformation without requiring custom agent development. While Amazon Q Developer provides out-of-the-box functionality for common development workflows, organizations with specific requirements or unique use cases may benefit from building custom solutions using Amazon Bedrock and agent frameworks. This flexibility allows teams to choose between a ready-to-use solution with Amazon Q Developer or a customized approach using Amazon Bedrock, depending on their specific needs, technical requirements, and desired level of control over the implementation. 
Challenges with the current state of AI agents 
Despite the remarkable advancements in AI agent technology, the current state of agent development and deployment faces significant challenges that limit their effectiveness, reliability, and broader adoption. These challenges span technical, operational, and conceptual domains, creating barriers that developers and organizations must navigate when implementing agentic solutions. 
One of the significant challenges is tool integration. Although frameworks like Amazon Bedrock Agents, LangGraph, and the Strands Agent SDK provide mechanisms for agents to interact with external tools and services, the current approaches often lack standardization and flexibility. Developers must create custom integrations for each tool, define precise schemas, and handle a multitude of edge cases in tool invocation and response processing. Furthermore, the rigid nature of many tool integration frameworks means that agents struggle to adapt to changes in tool interfaces or to discover and use new capabilities dynamically. 
How MCP helps in creating agents 
Appearing as a response to the limitations and challenges of current agent architectures, MCP provides a standardized framework that fundamentally redefines the relationship between FMs, context management, and tool integration. This protocol addresses many of the core challenges that have hindered the broader adoption and effectiveness of AI agents, particularly in enterprise environments and complex use cases. 
The following diagram illustrates an example architecture. 
 
Tool integration is dramatically simplified through MCPâ€™s Tool Registry and standardized invocation patterns. Developers can register tools with the registry using a consistent format, and the protocol manages the complexities of tool selection, parameter preparation, and response processing. This not only reduces the development effort required to integrate new tools but also enables more sophisticated tool usage patterns, such as tool chaining and parallel tool invocation, that are challenging to implement in current frameworks. 
This combination takes advantage of the strengths of each technologyâ€”high-quality FMs in Amazon Bedrock, MCPâ€™s context management capabilities, and LangGraphâ€™s orchestration frameworkâ€”to create agents that can tackle increasingly complex tasks with greater reliability and effectiveness. 
Imagine your development team wakes up to find yesterdayâ€™s GitHub issues already analyzed, fixed, and waiting as pull requests â€” all handled autonomously overnight. 
Recent advances in AI, particularly LLMs with code generation capabilities, have resulted in an impactful approach to development workflows. By using agents, development teams can automate simple changesâ€”such as dependency updates or straightforward bug fixes. 
Solution Overview 
Amazon Bedrock is a fully managed service that makes high-performing FMs from leading AI companies and Amazon available through a unified API. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. 
LangGraph orchestrates agentic workflows through a graph-based architecture that handles complex processes and maintains context across agent interactions. It uses supervisory control patterns and memory systems for coordination. For more details, refer to Build multi-agent systems with LangGraph and Amazon Bedrock. 
The Model Context Protocol (MCP) is an open standard that empowers developers to build secure, two-way connections between their data sources and AI-powered tools. The GitHub MCP Server is an MCP server that provides seamless integration with GitHub APIs. It offers a standard way for AI tools to work with GitHubâ€™s repositories. Developers can use it to automate tasks, analyze code, and improve workflows without handling complex API calls. 
This post uses these three technologies in a complementary fashion. Amazon Bedrock offers the AI capabilities for understanding issues and generating code fixes. LangGraph orchestrates the end-to-end workflow, managing the state and decision-making throughout the process. The GitHub MCP Server interfaces with GitHub repositories, providing context to the FM and implementing the generated changes. Together, these technologies enable an automation system that can understand and analyze GitHub issues, extract relevant code context, generate code fixes, create well-documented pull requests, and integrate seamlessly with existing GitHub workflows. 
The figure below shows high-level view of how LangGraph integrates with GitHub through MCP while leveraging LLMs from Amazon Bedrock. 
 
In the following sections, we explore the technical approach for building an AI-powered automation system, using Amazon Bedrock, LangGraph, and the GitHub MCP Server. We discuss the core concepts of building the solution; we donâ€™t focus on deploying the agent or running the MCP server in the AWS environment. For a detailed explanation, refer to the GitHub repository. 
Prerequisites 
You must have the following prerequisites before you can deploy this solution. For this post, we use the us-west-2 AWS Region. For details on available Regions, see Amazon Bedrock endpoints and quotas. 
 
 A valid AWS account. 
 An AWS Identity and Access Management (IAM) role in the account that has sufficient permissions to invoke Amazon Bedrock models. If youâ€™re planning to run your code on a Amazon SageMaker Jupyter notebook instance (rather than locally), you will also need permissions to set up and manage SageMaker resources. If you have administrator access, no action is needed for this step. 
 Access to Anthropicâ€™s Claude 3.5 Haiku on Amazon Bedrock. For instructions, see Access Amazon Bedrock foundation models. 
 Docker or Finch to run GitHub MCP server as a container. 
 A fine-grained personal access token. The GitHub MCP server can use supported GitHub APIs, so enable the least permission needed for this post. Assign repository permissions for contents, issues, and pull requests. 
 
Environment configuration and setup 
The MCP server acts as a bridge between our LangGraph agent and GitHubâ€™s API. Instead of directly calling GitHub APIs, we use the containerized the GitHub MCP Server, which provides standardized tool interfaces. 
You need to define the MCP configuration using the personal access token that you defined in the prerequisites. This configuration will start the GitHub MCP Server using Docker or Finch. 
 
  
   
   mcp_config = {
    "mcp": {
        "inputs": [
            {
                "type": "promptString",
                "id": "github_token",
                "description": "GitHub Personal Access Token",
                "password": "true",
            }
        ],
        "servers": {
            "github": {
                "command": "/usr/local/bin/docker",
                "args": [
                    "run",
                    "-i",
                    "--rm",
                    "-e",
                    "GITHUB_PERSONAL_ACCESS_TOKEN",
                    "ghcr.io/github/github-mcp-server",
                ],
                "env": {
                    "GITHUB_PERSONAL_ACCESS_TOKEN": os.environ.get("GITHUB_TOKEN")
                },
            }
        },
    }
} 
   
  
 
Agent state 
LangGraph needs a shared state object that flows between the nodes in the workflow. This state acts as memory, allowing each step to access data from earlier steps and pass results to later ones. 
 
 class AgentState(TypedDict):
    issues: List[Dict[str, Any]] 
    current_issue_index: int 
    analysis_result: Optional[Dict[str, Any]] 
    action_required: Optional[str] 
 
Structured output 
Instead of parsing free-form LLM responses, we use Pydantic models to enforce consistent, machine-readable outputs. This reduces parsing errors and make sure downstream nodes receive data in expected formats. The Field descriptions guide the LLM to provide exactly what we need. 
 
  
  class IssueAnalysis(BaseModel):
    """Analysis of the GitHub issue."""
    analysis: str = Field(
        description="Brief summary of the issue's core problem or request."
    )
    action_required: str = Field(
        description="Decision on next step. Must be one of: 'code_change_required', 'no_change_needed', 'needs_clarification'."
    ) 
  
 
MCP tools integration 
The load_mcp_tools function from the LangChainâ€™s MCP adapter automatically converts the MCP server capabilities into LangChain-compatible tools. This abstraction makes it possible to use the GitHub operations (list issues, create branches, update files) as if they were built-in LangChain tools. 
 
 async def get_mcp_tools(session: ClientSession) -&gt; List[Any]:
     """Loads tools from the connected MCP session."""
     tools = await load_mcp_tools(session)
     return tools  
 
Workflow structure 
Each node is stateless â€” it takes the current state, performs one specific task, and returns state updates. This makes the workflow predictable, testable, and straightforward to debug. These nodes are connected using edges or conditional edges. Not every GitHub issue requires code changes. Some might be documentation requests, duplicates, or need clarification. The routing functions use the structured LLM output to dynamically decide the next step, making the workflow adaptive rather than rigid. 
Finally, we start the agent by invoking the compiled graph with an initial state. The agent then follows the steps and decisions defined in the graph. The following diagram illustrates the workflow. 
 
Agent Execution and Result 
We can invoke the compiled graph with the initial_state and recursion_limit. It will fetch open issues from the given GitHub repository, analyze them one at a time, make the code changes if needed and then create the pull request in GitHub. 
Considerations 
To enable automated workflows, Amazon EventBridge offers an integration with GitHub through its SaaS partner event sources. After itâ€™s configured, EventBridge receives these GitHub events in near real-time. You can create rules that match specific issue patterns and route them to various AWS services like AWS Lambda functions, AWS Step Functions state machines, or Amazon Simple Notification Service (Amazon SNS) topics for further processing. This integration enables automated workflows that can trigger your analysis pipelines or code generation processes when relevant GitHub issue activities occur. 
When deploying the system, consider a phased rollout strategy. Start with a pilot phase in two or three non-critical repositories to confirm effectiveness and find issues. During this pilot phase, itâ€™s crucial to thoroughly evaluate the solution across a diverse set of code files. This test should cover different programming languages, frameworks, formats (such as â€“ Jupyter notebook), and varying levels of complexity in number and size of code files. Gradually expand to more repositories, prioritizing those with high maintenance burdens or standardized code patterns. 
Infrastructure best practices include containerization, designing for scalability, providing high availability, and implementing comprehensive monitoring for application, system, and business metrics. Security considerations are paramount, including operating with least privilege access, proper secrets management, input validation, and vulnerability management through regular updates and security scanning. 
It is crucial to align with your companyâ€™s generative AI operations and governance frameworks. Prior to deployment, verify alignment with your organizationâ€™s AI safety protocols, data handling policies, and model deployment guidelines. Although this architectural pattern offers significant benefits, you should adapt implementation to fit within your organizationâ€™s specific AI governance structure and risk management frameworks. 
Clean up 
Clean up your environment by completing the following steps: 
 
 Delete IAM roles and policies created specifically for this post. 
 Delete the local copy of this postâ€™s code. 
 If you no longer need access to an Amazon Bedrock FM, you can remove access to it. For instructions, see Add or remove access to Amazon Bedrock foundation models. 
 Delete the personal access token. For instructions, see Deleting a personal access token. 
 
Conclusion 
The integration of Amazon Bedrock FMs with the MCP and LangGraph is a significant advancement in the field of AI agents. By addressing the fundamental challenges of context management and tool integration, this combination enables the development of more sophisticated, reliable, and powerful agentic applications. 
The GitHub issues workflow scenario demonstrates benefits that include productivity enhancement, consistency improvement, faster response times, scalable maintenance, and knowledge amplification. Important insights include the role of FMs as development partners, the necessity of workflow orchestration, the importance of repository context, the need for confidence assessment, and the value of feedback loops for continuous improvement. 
The future of AI-powered development automation will see trends like multi-agent collaboration systems, proactive code maintenance, context-aware code generation, enhanced developer collaboration, and ethical AI development. Challenges include skill evolution, governance complexity, quality assurance, and integration complexity, whereas opportunities include developer experience transformation, accelerated innovation, knowledge democratization, and accessibility improvements. Organizations can prepare by starting small, investing in knowledge capture, building feedback loops, developing AI literacy, and experimenting with new capabilities. The goal is to enhance developer capabilities, not replace them, fostering a collaborative future where AI and human developers work together to build better software. 
For the example code and demonstration discussed in this post, refer to the accompanying GitHub repository. 
Refer to the following resources for additional guidance to get started: 
 
 Model Context Protocol documentation 
 Amazon Bedrock Documentation 
 
 
About the authors 
Jagdeep Singh Soni is a Senior Partner Solutions Architect at AWS based in the Netherlands. He uses his passion for generative AI to help customers and partners build generative AI applications using AWS services. Jagdeep has 15 years of experience in innovation, experience engineering, digital transformation, cloud architecture, and ML applications. 
Ajeet Tewari is a Senior Solutions Architect for Amazon Web Services. He works with enterprise customers to help them navigate their journey to AWS. His specialties include architecting and implementing scalable OLTP systems and leading strategic AWS initiatives. 
Mani Khanuja is a Tech Lead â€“ Generative AI Specialists, author of the book Applied Machine Learning and High-Performance Computing on AWS, and a member of the Board of Directors for Women in Manufacturing Education Foundation Board. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach.
â€¢ Mistral-Small-3.2-24B-Instruct-2506 is now available on Amazon Bedrock Marketplace and Amazon SageMaker JumpStart
  Today, weâ€™re excited to announce that Mistral-Small-3.2-24B-Instruct-2506â€”a 24-billion-parameter large language model (LLM) from Mistral AI thatâ€™s optimized for enhanced instruction following and reduced repetition errorsâ€”is available for customers through Amazon SageMaker JumpStart and Amazon Bedrock Marketplace. Amazon Bedrock Marketplace is a capability in Amazon Bedrock that developers can use to discover, test, and use over 100 popular, emerging, and specialized foundation models (FMs) alongside the current selection of industry-leading models in Amazon Bedrock. 
In this post, we walk through how to discover, deploy, and use Mistral-Small-3.2-24B-Instruct-2506 through Amazon Bedrock Marketplace and with SageMaker JumpStart. 
Overview of Mistral Small 3.2 (2506) 
Mistral Small 3.2 (2506) is an update of Mistral-Small-3.1-24B-Instruct-2503, maintaining the same 24-billion-parameter architecture while delivering improvements in key areas. Released under Apache 2.0 license, this model maintains a balance between performance and computational efficiency. Mistral offers both the pretrained (Mistral-Small-3.1-24B-Base-2503) and instruction-tuned (Mistral-Small-3.2-24B-Instruct-2506) checkpoints of the model under Apache 2.0. 
Key improvements in Mistral Small 3.2 (2506) include: 
 
 Improves in following precise instructions with 84.78% accuracy compared to 82.75% in version 3.1 from Mistralâ€™s benchmarks 
 Produces twice as fewer infinite generations or repetitive answers, reducing from 2.11% to 1.29% according to Mistral 
 Offers a more robust and reliable function calling template for structured API interactions 
 Now includes image-text-to-text capabilities, allowing the model to process and reason over both textual and visual inputs. This makes it ideal for tasks such as document understanding, visual Q&amp;A, and image-grounded content generation. 
 
These improvements make the model particularly well-suited for enterprise applications on AWS where reliability and precision are critical. With a 128,000-token context window, the model can process extensive documents and maintain context throughout longer conversation. 
SageMaker JumpStart overview 
SageMaker JumpStart is a fully managed service that offers state-of-the-art FMs for various use cases such as content writing, code generation, question answering, copywriting, summarization, classification, and information retrieval. It provides a collection of pre-trained models that you can deploy quickly, accelerating the development and deployment of machine learning (ML) applications. One of the key components of SageMaker JumpStart is model hubs, which offer a vast catalog of pre-trained models, such as Mistral, for a variety of tasks. 
You can now discover and deploy Mistral models in Amazon SageMaker Studio or programmatically through the Amazon SageMaker Python SDK, deriving model performance and MLOps controls with SageMaker features such as Amazon SageMaker Pipelines, Amazon SageMaker Debugger, or container logs. The model is deployed in a secure AWS environment and under your virtual private cloud (VPC) controls, helping to support data security for enterprise security needs. 
Prerequisites 
To deploy Mistral-Small-3.2-24B-Instruct-2506, you must have the following prerequisites: 
 
 An AWS account that will contain all your AWS resources. 
 An AWS Identity and Access Management (IAM) role to access SageMaker. To learn more about how IAM works with SageMaker, see Identity and Access Management for Amazon SageMaker. 
 Access to SageMaker Studio, a SageMaker notebook instance, or an interactive development environment (IDE) such as PyCharm or Visual Studio Code. We recommend using SageMaker Studio for straightforward deployment and inference. 
 Access to accelerated instances (GPUs) for hosting the model. 
 
If needed, request a quota increase and contact your AWS account team for support. This model requires a GPU-based instance type (approximately 55 GB of GPU RAM in bf16 or fp16) such as ml.g6.12xlarge. 
Deploy Mistral-Small-3.2-24B-Instruct-2506 in Amazon Bedrock Marketplace 
To access Mistral-Small-3.2-24B-Instruct-2506 in Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, in the navigation pane under Discover, choose Model catalog. 
 Filter for Mistral as a provider and choose the Mistral-Small-3.2-24B-Instruct-2506 model. 
 
 
The model detail page provides essential information about the modelâ€™s capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration.The page also includes deployment options and licensing information to help you get started with Mistral-Small-3.2-24B-Instruct-2506 in your applications. 
 
 To begin using Mistral-Small-3.2-24B-Instruct-2506, choose Deploy. 
 You will be prompted to configure the deployment details for Mistral-Small-3.2-24B-Instruct-2506. The model ID will be pre-populated. 
   
   For Endpoint name, enter an endpoint name (up to 50 alphanumeric characters). 
   For Number of instances, enter a number between 1â€“100. 
   For Instance type, choose your instance type. For optimal performance with Mistral-Small-3.2-24B-Instruct-2506, a GPU-based instance type such as ml.g6.12xlarge is recommended. 
   Optionally, configure advanced security and infrastructure settings, including VPC networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, review these settings to align with your organizationâ€™s security and compliance requirements. 
    
 Choose Deploy to begin using the model. 
 
 
When the deployment is complete, you can test Mistral-Small-3.2-24B-Instruct-2506 capabilities directly in the Amazon Bedrock playground, a tool on the Amazon Bedrock console to provide a visual interface to experiment with running different models. 
 
 Choose Open in playground to access an interactive interface where you can experiment with different prompts and adjust model parameters such as temperature and maximum length. 
 
 
The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results. 
To invoke the deployed model programmatically with Amazon Bedrock APIs, you need to get the endpoint Amazon Resource Name (ARN). You can use the Converse API for multimodal use cases. For tool use and function calling, use the Invoke Model API. 
Reasoning of complex figures 
VLMs excel at interpreting and reasoning about complex figures, charts, and diagrams. In this particular use case, we use Mistral-Small-3.2-24B-Instruct-2506 to analyze an intricate image containing GDP data. Its advanced capabilities in document understanding and complex figure analysis make it well-suited for extracting insights from visual representations of economic data. By processing both the visual elements and accompanying text, Mistral Small 2506 can provide detailed interpretations and reasoned analysis of the GDP figures presented in the image. 
We use the following input image. 
 
We have defined helper functions to invoke the model using the Amazon Bedrock Converse API: 
 
 def get_image_format(image_path):
    with Image.open(image_path) as img:
        # Normalize the format to a known valid one
        fmt = img.format.lower() if img.format else 'jpeg'
        # Convert 'jpg' to 'jpeg'
        if fmt == 'jpg':
            fmt = 'jpeg'
    return fmt

def call_bedrock_model(model_id=None, prompt="", image_paths=None, system_prompt="", temperature=0.6, top_p=0.9, max_tokens=3000):
    
    if isinstance(image_paths, str):
        image_paths = [image_paths]
    if image_paths is None:
        image_paths = []
    
    # Start building the content array for the user message
    content_blocks = []

    # Include a text block if prompt is provided
    if prompt.strip():
        content_blocks.append({"text": prompt})

    # Add images as raw bytes
    for img_path in image_paths:
        fmt = get_image_format(img_path)
        # Read the raw bytes of the image (no base64 encoding!)
        with open(img_path, 'rb') as f:
            image_raw_bytes = f.read()

        content_blocks.append({
            "image": {
                "format": fmt,
                "source": {
                    "bytes": image_raw_bytes
                }
            }
        })

    # Construct the messages structure
    messages = [
        {
            "role": "user",
            "content": content_blocks
        }
    ]

    # Prepare additional kwargs if system prompts are provided
    kwargs = {}
    
    kwargs["system"] = [{"text": system_prompt}]

    # Build the arguments for the `converse` call
    converse_kwargs = {
        "messages": messages,
        "inferenceConfig": {
            "maxTokens": 4000,
            "temperature": temperature,
            "topP": top_p
        },
        **kwargs
    }

    
    converse_kwargs["modelId"] = model_id

    # Call the converse API
    try:
        response = client.converse(**converse_kwargs)
    
        # Parse the assistant response
        assistant_message = response.get('output', {}).get('message', {})
        assistant_content = assistant_message.get('content', [])
        result_text = "".join(block.get('text', '') for block in assistant_content)
    except Exception as e:
        result_text = f"Error message: {e}"
    return result_text 
 
Our prompt and input payload are as follows: 
 
 import boto3
import base64
import json
from PIL import Image
from botocore.exceptions import ClientError

# Create a Bedrock Runtime client in the AWS Region you want to use.
client = boto3.client("bedrock-runtime", region_name="us-west-2")

system_prompt='You are a Global Economist.'
task = 'List the top 5 countries in Europe with the highest GDP'
image_path = './image_data/gdp.png'

print('Input Image:\n\n')
Image.open(image_path).show()

response = call_bedrock_model(model_id=endpoint_arn, 
                   prompt=task, 
                   system_prompt=system_prompt,
                   image_paths = image_path)

print(f'\nResponse from the model:\n\n{response}') 
 
The following is a response using the Converse API: 
 
 Based on the image provided, the top 5 countries in Europe with the highest GDP are:

1. **Germany**: $3.99T (4.65%)
2. **United Kingdom**: $2.82T (3.29%)
3. **France**: $2.78T (3.24%)
4. **Italy**: $2.07T (2.42%)
5. **Spain**: $1.43T (1.66%)

These countries are highlighted in green, indicating their location in the Europe region. 
 
Deploy Mistral-Small-3.2-24B-Instruct-2506 in SageMaker JumpStart 
You can access Mistral-Small-3.2-24B-Instruct-2506 through SageMaker JumpStart in the SageMaker JumpStart UI and the SageMaker Python SDK. SageMaker JumpStart is an ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. With SageMaker JumpStart, you can customize pre-trained models to your use case, with your data, and deploy them into production using either the UI or SDK. 
Deploy Mistral-Small-3.2-24B-Instruct-2506 through the SageMaker JumpStart UI 
Complete the following steps to deploy the model using the SageMaker JumpStart UI: 
 
 On the SageMaker console, choose Studio in the navigation pane. 
 First-time users will be prompted to create a domain. If not, choose Open Studio. 
 On the SageMaker Studio console, access SageMaker JumpStart by choosing JumpStart in the navigation pane. 
 
 
 
 Search for and choose Mistral-Small-3.2-24B-Instruct-2506 to view the model card. 
 
 
 
 Click the model card to view the model details page. Before you deploy the model, review the configuration and model details from this model card. The model details page includes the following information: 
 
 
 The model name and provider information. 
 A Deploy button to deploy the model. 
 About and Notebooks tabs with detailed information. 
 The Bedrock Ready badge (if applicable) indicates that this model can be registered with Amazon Bedrock, so you can use Amazon Bedrock APIs to invoke the model. 
 
 
 
 Choose Deploy to proceed with deployment. 
   
   For Endpoint name, enter an endpoint name (up to 50 alphanumeric characters). 
   For Number of instances, enter a number between 1â€“100 (default: 1). 
   For Instance type, choose your instance type. For optimal performance with Mistral-Small-3.2-24B-Instruct-2506, a GPU-based instance type such as ml.g6.12xlarge is recommended. 
    
 
 
 
 Choose Deploy to deploy the model and create an endpoint. 
 
When deployment is complete, your endpoint status will change to InService. At this point, the model is ready to accept inference requests through the endpoint. You can invoke the model using a SageMaker runtime client and integrate it with your applications. 
Deploy Mistral-Small-3.2-24B-Instruct-2506 with the SageMaker Python SDK 
Deployment starts when you choose Deploy. After deployment finishes, you will see that an endpoint is created. Test the endpoint by passing a sample inference request payload or by selecting the testing option using the SDK. When you select the option to use the SDK, you will see example code that you can use in the notebook editor of your choice in SageMaker Studio. 
To deploy using the SDK, start by selecting the Mistral-Small-3.2-24B-Instruct-2506 model, specified by the model_id with the value mistral-small-3.2-24B-instruct-2506. You can deploy your choice of the selected models on SageMaker using the following code. Similarly, you can deploy Mistral-Small-3.2-24B-Instruct-2506 using its model ID. 
 
 from sagemaker.jumpstart.model import JumpStartModel 
accept_eula = True 
model = JumpStartModel(model_id="huggingface-vlm-mistral-small-3-2-24b-instruct-2506") 
predictor = model.deploy(accept_eula=accept_eula)
This deploys the model on SageMaker with default configurations, including the default instance type and default VPC configurations. You can change these configurations by specifying non-default values in JumpStartModel. The EULA value must be explicitly defined as True to accept the end-user license agreement (EULA). 
 
After the model is deployed, you can run inference against the deployed endpoint through the SageMaker predictor: 
 
 prompt = "Hello!"
payload = {
    "messages": [
        {
            "role": "user",
            "content": prompt
        }
    ],
    "max_tokens": 4000,
    "temperature": 0.15,
    "top_p": 0.9,
}
    
response = predictor.predict(payload)
print(response['choices'][0]['message']['content'])
We get following response:

Hello! ğŸ˜Š How can I assist you today? 
 
Vision reasoning example 
Using the multimodal capabilities of Mistral-Small-3.2-24B-Instruct-2506, you can process both text and images for comprehensive analysis. The following example highlights how the model can simultaneously analyze a tuition ROI chart to extract visual patterns and data points. The following image is the input chart.png. 
 
Our prompt and input payload are as follows: 
 
 # Read and encode the image
image_path = "chart.png"
with open(image_path, "rb") as image_file:
 base64_image = base64.b64encode(image_file.read()).decode('utf-8')


# Create a prompt focused on visual analysis of the box plot chart
visual_prompt = """Please analyze this box plot chart showing the relationship between Annual Tuition (x-axis) and&nbsp;
40-Year Net Present Value (y-axis) in US$.&nbsp;
Describe the key trend between tuition and net present value shown in this chart. What's one notable insight?"""

# Create payload with image input
payload = {
"messages": [
{
"role": "user",
"content": [
{"type": "text", "text": visual_prompt},
{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
]
}
],
"max_tokens": 800,
"temperature": 0.15
}

# Make a prediction
response = predictor.predict(payload)

# Display the visual analysis
message = response['choices'][0]['message']
if message.get('content'):
print("Vision Analysis:")
print(message['content']) 
 
We get following response: 
 
 Vision Analysis:
This box plot chart illustrates the relationship between annual tuition costs (x-axis) and the 40-year net present value (NPV) in US dollars (y-axis). Each box plot represents a range of annual tuition costs, showing the distribution of NPV values within that range.

### Key Trend:
1. **General Distribution**: Across all tuition ranges, the median 40-year NPV (indicated by the line inside each box) appears to be relatively consistent, hovering around the $1,000,000 mark.
2. **Variability**: The spread of NPV values (indicated by the height of the boxes and whiskers) is wider for higher tuition ranges, suggesting greater variability in outcomes for more expensive schools.
3. **Outliers**: There are several outliers, particularly in the higher tuition ranges (e.g., 35-40k, 40-45k, and &gt;50k), indicating that some individuals experience significantly higher or lower NPVs.

### Notable Insight:
One notable insight from this chart is that higher tuition costs do not necessarily translate into a higher 40-year net present value. For example, the median NPV for the highest tuition range (&gt;50k) is not significantly higher than that for the lowest tuition range (&lt;5k). This suggests that the return on investment for higher tuition costs may not be proportionally greater, and other factors beyond tuition cost may play a significant role in determining long-term financial outcomes.

This insight highlights the importance of considering factors beyond just tuition costs when evaluating the potential return on investment of higher education. 
 
Function calling example 
This following example shows Mistral Small 3.2â€™s function calling by demonstrating how the model identifies when a user question needs external data and calls the correct function with proper parameters.Our prompt and input payload are as follows: 
 
 # Define a simple weather function
weather_function = {
"type": "function",
"function": {
"name": "get_weather",
"description": "Get weather for a location",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City name"
}
},
"required": ["location"]
}
}
}

# User question
user_question = "What's the weather like in Seattle?"

# Create payload
payload = {
"messages": [{"role": "user", "content": user_question}],
"tools": [weather_function],
"tool_choice": "auto",
"max_tokens": 200,
"temperature": 0.15
}

# Make prediction
response = predictor.predict(payload)

# Display raw response to see exactly what we get
print(json.dumps(response['choices'][0]['message'], indent=2))

# Extract function call information from the response content
message = response['choices'][0]['message']
content = message.get('content', '')

if '[TOOL_CALLS]' in content:
print("Function call details:", content.replace('[TOOL_CALLS]', '')) 
 
We get following response: 
 
 {
"role": "assistant",
"reasoning_content": null,
"content": "[TOOL_CALLS]get_weather{\"location\": \"Seattle\"}",
"tool_calls": []
}
Function call details: get_weather{"location": "Seattle"} 
 
Clean up 
To avoid unwanted charges, complete the following steps in this section to clean up your resources. 
Delete the Amazon Bedrock Marketplace deployment 
If you deployed the model using Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, under Tune in the navigation pane, select Marketplace model deployment. 
 In the Managed deployments section, locate the endpoint you want to delete. 
 Select the endpoint, and on the Actions menu, choose Delete. 
 Verify the endpoint details to make sure youâ€™re deleting the correct deployment: 
   
   Endpoint name 
   Model name 
   Endpoint status 
    
 Choose Delete to delete the endpoint. 
 In the deletion confirmation dialog, review the warning message, enter confirm, and choose Delete to permanently remove the endpoint. 
 
Delete the SageMaker JumpStart predictor 
After youâ€™re done running the notebook, make sure to delete the resources that you created in the process to avoid additional billing. For more details, see Delete Endpoints and Resources. You can use the following code: 
 
 predictor.delete_model()
predictor.delete_endpoint() 
 
Conclusion 
In this post, we showed you how to get started with Mistral-Small-3.2-24B-Instruct-2506 and deploy the model using Amazon Bedrock Marketplace and SageMaker JumpStart for inference. This latest version of the model brings improvements in instruction following, reduced repetition errors, and enhanced function calling capabilities while maintaining performance across text and vision tasks. The modelâ€™s multimodal capabilities, combined with its improved reliability and precision, support enterprise applications requiring robust language understanding and generation. 
Visit SageMaker JumpStart in Amazon SageMaker Studio or Amazon Bedrock Marketplace now to get started with Mistral-Small-3.2-24B-Instruct-2506. 
For more Mistral resources on AWS, check out the Mistral-on-AWS GitHub repo. 
 
About the authors 
Niithiyn Vijeaswaran is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is AWS AI accelerators (AWS Neuron). He holds a Bachelorâ€™s degree in Computer Science and Bioinformatics. 
Breanne Warner is an Enterprise Solutions Architect at Amazon Web Services supporting healthcare and life science (HCLS) customers. She is passionate about supporting customers to use generative AI on AWS and evangelizing model adoption for first- and third-party models. Breanne is also Vice President of the Women at Amazon board with the goal of fostering inclusive and diverse culture at Amazon. Breanne holds a Bachelorâ€™s of Science in Computer Engineering from the University of Illinois Urbana-Champaign. 
Koushik Mani is an Associate Solutions Architect at AWS. He previously worked as a Software Engineer for 2 years focusing on machine learning and cloud computing use cases at Telstra. He completed his Masterâ€™s in Computer Science from the University of Southern California. He is passionate about machine learning and generative AI use cases and building solutions.
â€¢ Generate suspicious transaction report drafts for financial compliance using generative AI
  Financial regulations and compliance are constantly changing, and automation of compliance reporting has emerged as a game changer in the financial industry. Amazon Web Services (AWS) generative AI solutions offer a seamless and efficient approach to automate this reporting process. The integration of AWS generative AI into the compliance framework not only enhances efficiency but also instills a greater sense of confidence and trust in the financial sector by promoting precision and timely delivery of compliance reports. These solutions help financial institutions avoid the costly and reputational consequences of noncompliance. This, in turn, contributes to the overall stability and integrity of the financial ecosystem, benefiting both the industry and the consumers it serves. 
Amazon Bedrock is a managed generative AI service that provides access to a wide array of advanced foundation models (FMs). It includes features that facilitate the efficient creation of generative AI applications with a strong focus on privacy and security. Getting a good response from an FM relies heavily on using efficient techniques for providing prompts to the FM. Retrieval Augmented Generation (RAG) is a pivotal approach to augmenting FM prompts with contextually relevant information from external sources. It uses vector databases such as Amazon OpenSearch Service to enable semantic searching of the contextual information. 
Amazon Bedrock Knowledge Bases, powered by vector databases such as Amazon OpenSearch Serverless, helps in implementing RAG to supplement model inputs with relevant information from factual resources, thereby reducing potential hallucinations and increasing response accuracy. 
Amazon Bedrock Agents enables generative AI applications to execute multistep tasks using action groups and enable interaction with APIs, knowledge bases, and FMs. Using agents, you can design intuitive and adaptable generative AI applications capable of understanding natural language queries and creating engaging dialogues to gather details required for using the FMs effectively. 
A suspicious transaction report (STR) or suspicious activity report (SAR) is a type of report that a financial organization must submit to a financial regulator if they have reasonable grounds to suspect any financial transaction that has occurred or was attempted during their activities. There are stipulated timelines for filing these reports and it typically takes several hours of manual effort to create one report for one customer account. 
In this post, we explore a solution that uses FMs available in Amazon Bedrock to create a draft STR. We cover how generative AI can be used to automate the manual process of draft generation using account information, transaction details, and correspondence summaries as well as creating a knowledge base of information about fraudulent entities involved in such transactions. 
Solution overview 
The solution uses Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, AWS Lambda, Amazon Simple Storage Service (Amazon S3), and OpenSearch Service. The workflow is as follows: 
 
 The user requests for creation of a draft STR report through the business application. 
 The application calls Amazon Bedrock Agents, which has been preconfigured with detailed instructions to engage in a conversational flow with the user. The agent follows these instructions to gather the required information from the user, completes the missing information by using actions groups to invoke the Lambda function, and generates the report in the specified format. 
 Following its instructions, the agent invokes Amazon Bedrock Knowledge Bases to find details about fraudulent entities involved in the suspicious transactions. 
 Amazon Bedrock Knowledge Bases queries OpenSearch Service to perform semantic search for the entities required for the report. If the information about fraudulent entities is available in Amazon Bedrock Knowledge Bases, the agent follows its instructions to generate a report for the user. 
 If the information isnâ€™t found in the knowledge base, the agent uses the chat interface to prompt the user to provide the website URL that contains the relevant information. Alternatively, the user can provide a description about the fraudulent entity in the chat interface. 
 If the user provides a URL for a publicly accessible website, the agent follows its instructions to call the action group to invoke a Lambda function to crawl the website URL. The Lambda function scrapes the information from the website and returns it to the agent for use in the report. 
 The Lambda function also stores the scraped content in an S3 bucket for future use by the search index. 
 Amazon Bedrock Knowledge Bases can be programmed to periodically scan the S3 bucket to index the new content in OpenSearch Service. 
 
The following diagram illustrates the solution architecture and workflow. 
 
You can use the full code available in GitHub to deploy the solution using the AWS Cloud Development Kit (AWS CDK). Alternatively, you can follow a step-by-step process for manual deployment. We walk through both approaches in this post. 
Prerequisites 
To implement the solution provided in this post, you must enable model access in Amazon Bedrock for Amazon Titan Text Embeddings V2 and Anthropic Claude 3.5 Haiku. 
Deploy the solution with the AWS CDK 
To set up the solution using the AWS CDK, follow these steps: 
 
 Verify that the AWS CDK has been installed in your environment. For installation instructions, refer to the AWS CDK Immersion Day Workshop. 
 Update the AWS CDK to version 36.0.0 or higher: 
 
 
 npm install -g aws-cdk 
 
 
 Initialize the AWS CDK environment in the AWS account: 
 
 
 cdk bootstrap 
 
 
 Clone the GitHub repository containing the solution files: 
 
 
 git clone https://github.com/aws-samples/suspicious-financial-transactions-reporting 
 
 
 Navigate to the solution directory: 
 
 
 cd financial-transaction-report-drafting-for-compliance 
 
 
 Create and activate the virtual environment: 
 
 
 python3 -m venv .venv
source .venv/bin/activate 
 
Activating the virtual environment differs based on the operating system. Refer to the AWS CDK workshop for information about activating in other environments. 
 
 After the virtual environment is activated, install the required dependencies: 
 
 
 pip install -r requirements.txt 
 
 
 Deploy the backend and frontend stacks: 
 
 
 cdk deploy -a ./app.py --all 
 
 
 When the deployment is complete, check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following two screenshots. 
 
 
 
Manual deployment 
To implement the solution without using the AWS CDK, complete the following steps: 
 
 Set up an S3 bucket. 
 Create a Lambda function. 
 Set up Amazon Bedrock Knowledge Bases. 
 Set up Amazon Bedrock Agents. 
 
Visual layouts in some screenshots in this post might look different than those on your AWS Management Console. 
Set up an S3 bucket 
Create an S3 bucket with a unique bucket name for the document repository, as shown in the following screenshot. This will be a data source for Amazon Bedrock Knowledge Bases. 
 
Create the website scraper Lambda function 
Create a new Lambda function called Url-Scraper using the Python 3.13 runtime to crawl and scrape the website URL provided by Amazon Bedrock Agents. The function will scrape the content, send the information to the agent, and store the contents in the S3 bucket for future references. 
 
Error handling has been skipped in this code snippet for brevity. The full code is available in GitHub. 
Create a new file called search_suspicious_party.py with the following code snippet: 
 
 import boto3
from bs4 import BeautifulSoup
import os
import re
import urllib.request
BUCKET_NAME = os.getenv('S3_BUCKET')
s3 = boto3.client('s3')
def get_receiving_entity_from_url(start_url):
    response = urllib.request.urlopen(
        urllib.request.Request(url=start_url, method='GET'),
        timeout=5)
    soup = BeautifulSoup(response.read(), 'html.parser')
    # Extract page title
    title = soup.title.string if soup.title else 'Untitled'
    # Extract page content for specific HTML elements
    content = ' '.join(p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']))
    content = re.sub(r'\s+', ' ', content).strip()
    s3.put_object(Body=content, Bucket=BUCKET_NAME, Key=f"docs/{title}.txt")
return content 
 
Replace the default generated code in lambda_function.py with the following code: 
 
 import json
from search-suspicious-party import *
def lambda_handler(event, context):
    # apiPath should match the path specified in action group schema
    if event['apiPath'] == '/get-receiving-entity-details':
        # Extract the property from request data
        start_url = get_named_property(event, 'start_url')
        scraped_text = get_receiving_entity_from_url(start_url)
        action_response = {
            'actionGroup': event['actionGroup'],
            'apiPath': event['apiPath'],
            'httpMethod': event['httpMethod'],
            'httpStatusCode': 200,
            'responseBody': {
                'application/json': {
                    'body': json.dumps({'scraped_text': scraped_text})
                }
            }
        }
        return {'response': action_response}
    # Return an error if apiPath is not recognized
    return {
        'statusCode': 400,
        'body': json.dumps({'error': 'Invalid API path'})
    }
def get_named_property(event, name):
    return next(
        item for item in
        event['requestBody']['content']['application/json']['properties']
        if item['name'] == name
    )['value'] 
 
Configure the Lambda function 
Set up a Lambda environment variable S3_BUCKET, as shown in the following screenshot. For Value, use the S3 bucket you created previously. 
 
Increase the timeout duration for Lambda function to 30 seconds. You can adjust this value based on the time it takes for the crawler to complete its work. 
 
Set up Amazon Bedrock Knowledge Bases 
Complete the following steps to create a new knowledge base in Amazon Bedrock. This knowledge base will use OpenSearch Serverless to index the fraudulent entity data stored in Amazon S3. For more information, refer to Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases. 
 
 On the Amazon Bedrock console, choose Knowledge bases in the navigation pane and choose Create knowledge base. 
 For Knowledge base name, enter a name (for example, str-knowledge-base). 
 For Service role name, keep the default system generated value. 
 
 
 
 Select Amazon S3 as the data source. 
 
 
 
 Configure the Amazon S3 data source: 
   
   For Data source name, enter a name (for example, knowledge-base-data-source-s3). 
   For S3 URI, choose Browse S3 and choose the bucket where information scraped by web crawler about fraudulent entities is available for the knowledge base to use. 
   Keep all other default values. 
    
 
 
 
 For Embeddings model, choose Titan Text Embeddings V2. 
 
 
 
 For Vector database, select Quick create a new vector store to create a default vector store with OpenSearch Serverless. 
 
 
 
 Review the configurations and choose Create knowledge base. 
 
After the knowledge base is successfully created, you can see the knowledge base ID, which you will need when creating the agent in Amazon Bedrock. 
 
 Select knowledge-base-data-source-s3 from the list of data sources and choose Sync to index the documents. 
 
 
Set up Amazon Bedrock Agents 
To create a new agent in Amazon Bedrock, complete the following steps. For more information, refer to Create and configure agent manually. 
 
 On the Amazon Bedrock console, choose Agents in the navigation pane and choose Create Agent. 
 For Name, enter a name (for example, agent-str). 
 Choose Create. 
 
 
 
 For Agent resource role, keep the default setting (Create and use a new service role). 
 For Select model, choose a model provider and model name (for example, Anthropicâ€™s Claude 3.5 Haiku) 
 For Instructions for the Agent, provide the instructions that allow the agent to invoke the large language model (LLM). 
 
You can download the instructions from the agent-instructions.txt file in the GitHub repo. Refer to next section in this post to understand how to write the instructions. 
 
 Keep all other default values. 
 Choose Save. 
 
 
 
 Under Action groups, choose Add to create a new action group. 
 
 
An action is a task the agent can perform by making API calls. A set of actions comprises an action group. 
 
 Provide an API schema that defines all the APIs in the action group. 
 For Action group details, enter an action group name (for example, agent-group-str-url-scraper). 
 For Action group type, select Define with API schemas. 
 For Action group invocation, select Select an existing Lambda function, which is the Lambda function that you created previously. 
 
 
 
 For Action group schema, choose Define via in-line schema editor. 
 Replace the default sample code with the following example to define the schema to specify the input parameters with default and mandatory values: 
 
 
 openapi: 3.0.0
info:
  title: Gather suspicious receiving entity details from website
  version: 1.0.0
paths:
  /:
    post:
      description: Get details about suspicious receiving entity from the URL
      operationId: getReceivingEntityDetails
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ScrapeRequest"
      responses:
        "200":
          description: Receiving entity details gathered successfully
components:
  schemas:
    ScrapeRequest:
      type: object
      properties:
        :
          type: string
          description: The URL to start scraping from
      required:
        - start_url 
 
 
 Choose Create. 
 Under Knowledge bases, choose Add. 
 
 
 
 For Select knowledge base, choose knowledge-base-str, which you created previously, and add the following instructions: 
 
Use the information in the knowledge-base-str knowledge base to select transaction reports. 
 
 
 Choose Save to save all changes. 
 Finally, choose Prepare to prepare this agent to get it ready for testing. 
 
You can also create a Streamlit application to create a UI for this application. The source code is available in GitHub. 
Agent instructions 
Agent instructions for Amazon Bedrock Agents provide the mechanism for a multistep user interaction to gather the inputs an agent needs to invoke the LLM with a rich prompt to generate the response in the required format. Provide logical instructions in plain English. There are no predefined formats for these instructions. 
 
 Provide an overview of the task including the role: 
 
 
 You are a financial user creating Suspicious Transaction Report (STR) draft for a financial compliance use case. 
 
 
 Provide the message that the agent can use for initiating the user interaction: 
 
 
 Greet the user with the message â€œHi &lt;name&gt;. Welcome to STR report drafting. How can I help?â€
Ask the user to provide the transactions details. From the transaction details, capture the response in the &lt;answer&gt; tag and include the &lt;thinking&gt; tag to understand the rationale behind the response. 
 
 
 Specify the processing that needs to be done on the output received from the LLM: 
 
 
 For the transaction input provided by user, create a narrative description for financial risk reporting of the provided bank account and transaction details.
1. Add a summary of correspondence logs that includes title, summary, correspondence history, and analysis in the narrative description.
2. Add the details about the receiving entity in the narrative description. You can get details about receiving entities from the agent action group. 
 
 
 Provide the optional messages that the agent can use for a multistep interaction to gather the missing inputs if required: 
 
 
 If you don't have knowledge about Receiving entity, you should ask the Human for more details about it with a message â€œUnfortunately I do not have enough context or details about the receiving entity &lt;entity name&gt; to provide an accurate risk assessment or summary. Can you please provide some additional background information about &lt;entity name&gt;? What is the URL of the &lt;entity name&gt; or the description?â€ 
 
 
 Specify the actions that the agent can take to process the user input using action groups: 
 
 
 If user provides the URL of &lt;entity name&gt;, call the action group &lt;add action group name&gt; to get the details. If user provides the description of &lt;entity name&gt;, then summarize and add it to the narrative description as a receiving entity. 
 
 
 Specify how the agent should provide the response, including the format details: 
 
 
 Once you have all the necessary input (financial transaction details and receiving entity details), create a detailed well-formatted draft report for financial risk reporting of the provided bank account and transaction details containing the following sections:
1. Title
2. Summary of transactions
3. Correspondence History &amp; Analysis
4. Receiving entity summary 
 
Test the solution 
To test the solution, follow these steps: 
 
 Choose Test to start testing the agent. 
 Initiate the chat and observe how the agent uses the instructions you provided in the configuration step to ask for required details for generating the report. 
 Try different prompts, such as â€œGenerate an STR for an account.â€ 
 
The following screenshot shows an example chat. 
 
The following screenshot shows an example chat with the prompt, â€œGenerate an STR for account number 49179-180-2092803.â€ 
 
Another option is to provide all the details at the same time, for example, â€œGenerate an STR for account number 12345-999-7654321 with the following transactions.â€ 
 
 Copy and paste the sample transactions from the sample-transactions.txt file in GitHub. 
 
The agent keeps asking for missing information, such as account number, transaction details, and correspondence history. After it has all the details, it will generate a draft STR document. 
The code in GitHub also contains a sample StreamLit application that you can use to test the application. 
Clean up 
To avoid incurring unnecessary future charges, clean up the resources you created as part of this solution. If you created the solution using the GitHub code sample and the AWS CDK, empty the S3 bucket and delete the CloudFormation stack. If you created the solution manually, complete the following steps: 
 
 Delete the Amazon Bedrock agent. 
 Delete the Amazon Bedrock knowledge base. 
 Empty and delete the S3 bucket if you created one specifically for this solution. 
 Delete the Lambda function. 
 
Conclusion 
In this post, we showed how Amazon Bedrock offers a robust environment for building generative AI applications, featuring a range of advanced FMs. This fully managed service prioritizes privacy and security while helping developers create AI-driven applications efficiently. A standout feature, RAG, uses external knowledge bases to enrich AI-generated content with relevant information, backed by OpenSearch Service as its vector database. Additionally, you can include metadata fields in the knowledge base and agent session context with Amazon Verified Permissions to pass fine-grained access context for authorization. 
With careful prompt engineering, Amazon Bedrock minimizes inaccuracies and makes sure that AI responses are grounded in factual documentation. This combination of advanced technology and data integrity makes Amazon Bedrock an ideal choice for anyone looking to develop reliable generative AI solutions. You can now explore extending this sample code to use Amazon Bedrock and RAG for reliably generating draft documents for compliance reporting. 
 
About the Authors 
Divyajeet (DJ) Singh is a Senior Solutions Architect at AWS Canada. He loves working with customers to help them solve their unique business challenges using the cloud. Outside of work, he enjoys spending time with family and friends and exploring new places. 
Parag Srivastava is a Senior Solutions Architect at AWS, where he has been helping customers successfully apply generative AI to real-life business scenarios. During his professional career, he has been extensively involved in complex digital transformation projects. He is also passionate about building innovative solutions around geospatial aspects of addresses. 
Sangeetha Kamatkar is a Senior Solutions Architect at AWS who helps customers with successful cloud adoption and migration. She works with customers to craft highly scalable, flexible, and resilient cloud architectures that address customer business problems. In her spare time, she listens to music, watches movies, and enjoys gardening during summertime. 
Vineet Kachhawaha is a Senior Solutions Architect at AWS focusing on AI/ML and generative AI. He co-leads the AWS for Legal Tech team within AWS. He is passionate about working with enterprise customers and partners to design, deploy, and scale AI/ML applications to derive business value.
â€¢ Fine-tune and deploy Meta Llama 3.2 Vision for generative AI-powered web automation using AWS DLCs, Amazon EKS, and Amazon Bedrock
  Fine-tuning of large language models (LLMs) has emerged as a crucial technique for organizations seeking to adapt powerful foundation models (FMs) to their specific needs. Rather than training models from scratchâ€”a process that can cost millions of dollars and require extensive computational resourcesâ€”companies can customize existing models with domain-specific data at a fraction of the cost. This approach has become particularly valuable as organizations across healthcare, finance, and technology sectors look to use AI for specialized tasks while maintaining cost-efficiency. However, implementing a production-grade fine-tuning solution presents several significant challenges. Organizations must navigate complex infrastructure setup requirements, enforce robust security measures, optimize performance, and establish reliable model hosting solutions. 
In this post, we present a complete solution for fine-tuning and deploying the Llama-3.2-11B-Vision-Instruct model for web automation tasks. We demonstrate how to build a secure, scalable, and efficient infrastructure using AWS Deep Learning Containers (DLCs) on Amazon Elastic Kubernetes Service (Amazon EKS). By using AWS DLCs, you can gain access to well-tested environments that come with enhanced security features and pre-installed software packages, significantly simplifying the optimization of your fine-tuning process. This approach not only accelerates development, but also provides robust security and performance in production environments. 
Solution overview 
In this section, we explore the key components of our architecture for fine-tuning a Meta Llama model and using it for web task automation. We explore the benefits of different components and how they interact with each other, and how we can use them to build a production-grade fine-tuning pipeline. 
AWS DLCs for training and hosting AI/ML workloads 
At the core of our solution are AWS DLCs, which provide optimized environments for machine learning (ML) workloads. These containers come preconfigured with essential dependencies, including NVIDIA drivers, CUDA toolkit, and Elastic Fabric Adapter (EFA) support, along with preinstalled frameworks like PyTorch for model training and hosting. AWS DLCs tackle the complex challenge of packaging various software components to work harmoniously with training scripts, so you can use optimized hardware capabilities out of the box. Additionally, AWS DLCs implement unique patching algorithms and processes that continuously monitor, identify, and address security vulnerabilities, making sure the containers remain secure and up-to-date. Their pre-validated configurations significantly reduce setup time and reduce compatibility issues that often occur in ML infrastructure setup. 
AWS DLCs, Amazon EKS, and Amazon EC2 for seamless infrastructure management 
We deploy these DLCs on Amazon EKS, creating a robust and scalable infrastructure for model fine-tuning. Organizations can use this combination to build and manage their training infrastructure with unprecedented flexibility. Amazon EKS handles the complex container orchestration, so you can launch training jobs that run within DLCs on your desired Amazon Elastic Compute Cloud (Amazon EC2) instance, producing a production-grade environment that can scale based on training demands while maintaining consistent performance. 
AWS DLCs and EFA support for high-performance networking 
AWS DLCs come with pre-configured support for EFA, enabling high-throughput, low-latency communication between EC2 nodes. An EFA is a network device that you can attach to your EC2 instance to accelerate AI, ML, and high performance computing applications. DLCs are pre-installed with EFA software that is tested and compatible with the underlying EC2 instances, so you donâ€™t have to go through the hassle of setting up the underlying components yourself. For this post, we use setup scripts to create EKS clusters and EC2 instances that will support EFA out of the box. 
AWS DLCs with FSDP for enhanced memory efficiency 
Our solution uses PyTorchâ€™s built-in support for Fully Sharded Data Parallel (FSDP) training, a cutting-edge technique that dramatically reduces memory requirements during training. Unlike traditional distributed training approaches where each GPU must hold a complete model copy, FSDP shards model parameters, optimizer states, and gradients across workers. The optimized implementation of FSDP within AWS DLCs makes it possible to train larger models with limited GPU resources while maintaining training efficiency. 
For more information, see Scale LLMs with PyTorch 2.0 FSDP on Amazon EKS â€“ Part 2. 
Model deployment on Amazon Bedrock 
For model deployment, we use Amazon Bedrock, a fully managed service for FMs. Although we can use AWS DLCs for model hosting, we use Amazon Bedrock for this post to demonstrate diversity in service utilization. 
Web automation integration 
Finally, we implement the SeeAct agent, a sophisticated web automation tool, and demonstrate its integration with our hosted model on Amazon Bedrock. This combination creates a powerful system capable of understanding visual inputs and executing complex web tasks autonomously, showcasing the practical applications of our fine-tuned model.In the following sections, we demonstrate how to: 
 
 Set up an EKS cluster for AI workloads. 
 Use AWS DLCs to fine-tune Meta Llama 3.2 Vision using PyTorch FSDP. 
 Deploy the fine-tuned model on Amazon Bedrock. 
 Use the model with SeeAct for web task automation. 
 
Prerequisites 
You must have the following prerequisites: 
 
 An AWS account. 
 An AWS Identity and Access Management (IAM) role with appropriate policies. Because this post deals with creating clusters, nodes, and infrastructure, administrator-level permissions would work well. However, if you must have restricted permissions, you should at least have the following permissions: AmazonEC2FullAccess, AmazonSageMakerFullAccess, AmazonBedrockFullAccess, AmazonS3FullAccess, AWSCloudFormationFullAccess, AmazonEC2ContainerRegistryFullAccess. For more information about other IAM policies needed, see Minimum IAM policies. 
 The necessary dependencies installed for Amazon EKS. For instructions, see Set up to use Amazon EKS. 
 For this post, we use P5 instances. To request a quota increase, see Requesting a quota increase. 
 An EC2 key pair. For instructions, see Create a key pair for your Amazon EC2 instance. 
 
Run export AWS_REGION=&lt;region_name&gt; in your bash script from where you are running the commands. 
Set up the EKS cluster 
In this section, we walk through the steps to create your EKS cluster and install the necessary plugins, operators, and other dependencies. 
Create an EKS cluster 
The simplest way to create an EKS cluster is to use the cluster configuration YAML file. You can use the following sample configuration file as a base and customize it as needed. Provide the EC2 key pair created as a prerequisite. For more configuration options, see Using Config Files. 
 
 ---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
&nbsp;&nbsp;name: MyCluster
&nbsp;&nbsp;region: us-west-2

managedNodeGroups: 
&nbsp;&nbsp;- name: p5
&nbsp;&nbsp; &nbsp;instanceType: p5.48xlarge
&nbsp;&nbsp; &nbsp;minSize: 0
&nbsp;&nbsp; &nbsp;maxSize: 2
&nbsp;&nbsp; &nbsp;desiredCapacity: 2
&nbsp;&nbsp; &nbsp;availabilityZones: ["us-west-2a"]
&nbsp;&nbsp; &nbsp;volumeSize: 1024
&nbsp;&nbsp; &nbsp;ssh:
&nbsp;&nbsp; &nbsp; &nbsp;publicKeyName: &lt;your-ec2-key-pair&gt;
&nbsp;&nbsp; &nbsp;efaEnabled: true
&nbsp;&nbsp; &nbsp;privateNetworking: true
&nbsp; &nbsp; ## In case you have an On Demand Capacity Reservation (ODCR) and want to&nbsp;use it, uncomment the lines below.
&nbsp; &nbsp; # capacityReservation:
&nbsp; &nbsp; #&nbsp; &nbsp;capacityReservationTarget:
&nbsp; &nbsp; #&nbsp; &nbsp; &nbsp;capacityReservationResourceGroupARN: arn:aws:resource-groups:us-west-2:897880167187:group/eks_blog_post_capacity_reservation_resource_group_p5 
 
Run the following command to create the EKS cluster: 
eksctl create cluster --config-file cluster.yamlThe following is an example output: 
 
 YYYY-MM-DD HH:mm:SS [â„¹] eksctl version x.yyy.z
YYYY-MM-DD HH:mm:SS [â„¹] using region &lt;region_name&gt;
...
YYYY-MM-DD HH:mm:SS [âœ”] EKS cluster "&lt;cluster_name&gt;" in "&lt;region_name&gt;" region is ready 
 
Cluster creation might take between 15â€“30 minutes. After itâ€™s created, your local ~/.kube/config file gets updated with connection information to your cluster. 
Run the following command line to verify that the cluster is accessible: 
kubectl get nodes 
Install plugins, operators, and other dependencies 
In this step, you install the necessary plugins, operators and other dependencies on your EKS cluster. This is necessary to run the fine-tuning on the correct node and save the model. 
 
 Install the NVIDIA Kubernetes device plugin: 
 
 
 kubectl&nbsp;create&nbsp;-f&nbsp;https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.0/deployments/static/nvidia-device-plugin.yml 
 
 
 Install the AWS EFA Kubernetes device plugin: 
 
 
 helm&nbsp;repo&nbsp;add&nbsp;eks&nbsp;https://aws.github.io/eks-charts
git&nbsp;clone&nbsp;-b&nbsp;v0.0.190&nbsp;https://github.com/aws/eks-charts.git
cd&nbsp;&nbsp;eks-charts/stable
helm&nbsp;install&nbsp;efa&nbsp;./aws-efa-k8s-device-plugin&nbsp;-n&nbsp;kube-system
cd ../.. 
 
 
 Delete aws-efa-k8s-device-plugin-daemonset by running the following command: 
 
 
 kubectl delete daemonset aws-efa-k8s-device-plugin-daemonset -n kube-system 
 
 
 Clone the code locally that with help with setup and fine-tuning: 
 
 
 git clone https://github.com/aws-samples/aws-do-eks.git
cd aws-do-eks
git checkout f59007ee50117b547305f3b8475c8e1b4db5a1d5
curl -L -o patch-aws-do-eks.tar.gz https://github.com/aws/deep-learning-containers/raw/refs/heads/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-aws-do-eks.tar.gz
ftar -xzf patch-aws-do-eks.tar.gz
cd patch-aws-do-eks/
git am *.patch
cd ../.. 
 
 
 Install etcd for running distributed training with PyTorch: 
 
 
 kubectl apply -f aws-do-eks/Container-Root/eks/deployment/etcd/etcd-deployment.yaml 
 
 
 Deploy the FSx CSI driver for saving the model after fine-tuning: 
   
   Enter into the fsx folder: 
     
     cd aws-do-eks/Container-Root/eks/deployment/csi/fsx/ 
      
   Edit the fsx.conf file to modify the CLUSTER_NAME, CLUSTER_REGION, and CLUSTER_ZONE values to your cluster specific data: 
     
     vi fsx.conf 
      
   Deploy the FSX CSI driver: 
     
     ./deploy.sh 
      
    
 Deploy the Kubeflow Training Operator that will be used to run the fine-tuning job: 
   
   Change the location to the following: 
     
     cd aws-do-eks/Container-Root/eks/deployment/kubeflow/training-operator/ 
      
   Deploy the Kubeflow Training Operator: 
     
     ./deploy.sh 
      
    
 Deploy the Kubeflow MPI Operator for running NCCL tests: 
   
   Run deploy.sh from the following GitHub repo. 
   Change the location to the following: 
     
     cd aws-do-eks/Container-Root/eks/deployment/kubeflow/mpi-operator/ 
      
   Deploy the Kubeflow MPI Operator: 
     
     ./deploy.sh 
      
    
 
Fine-tune Meta Llama 3.2 Vision using DLCs on Amazon EKS 
This section outlines the process for fine-tuning the Meta Llama 3.2 Vision model using PyTorch FSDP on Amazon EKS. We use the DLCs as the base image to run our training jobs. 
Configure the setup needed for fine-tuning 
Complete the following steps to configure the setup for fine-tuning: 
 
 Create a Hugging Face account and get a Hugging Face security token. 
 Enter into the fsdp folder: 
 
 
 cd Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp 
 
 
 Create a Persistent Volume Claim (PVC) that will use the underlying FSx CSI driver that you installed earlier: 
 
 
 kubectl apply -f pvc.yaml 
 
Monitor kubectl get pvc fsx-claim and make sure it reached BOUND status. You can then go to the Amazon EKS console to see an unnamed volume created without a name. You can let this happen in the background, but before you run the ./run.sh command to run the fine-tuning job in a later step, make sure the BOUND status is achieved. 
 
 To configure the environment, open the .env file and modify the following variables: 
   
   HF_TOKEN: Add the Hugging Face token that you generated earlier. 
   S3_LOCATION: Add the Amazon Simple Storage Service (Amazon S3) location where you want to store the fine-tuned model after the training is complete. 
    
 Create the required resource YAMLs: 
 
 
 ./deploy.sh 
 
This line uses the values in the .env file to generate new YAML files that will eventually be used for model deployment. 
 
 Build and push the container image: 
 
 
 ./login-dlc.sh
./build.sh
./push.sh 
 
Run the fine-tuning job 
In this step, we use the upstream DLCs and add the training scripts within the image for running the training. 
Make sure that you have requested access to the Meta Llama 3.2 Vision model on Hugging Face. Continue to the next step after permission has been granted. 
Execute the fine-tuning job: 
 
 ./run.sh 
 
For our use case, the job took 1.5 hours to complete. The script uses the following PyTorch command thatâ€™s defined in the .env file within the fsdp folder: 
 
 ```
bash
torchrun --nnodes 1 --nproc_per_node 8 &nbsp;\
recipes/quickstart/finetuning/finetuning.py \
--enable_fsdp --lr 1e-5 &nbsp;--num_epochs 5 \
--batch_size_training 2 \
--model_name meta-llama/Llama-3.2-11B-Vision-Instruct \
--dist_checkpoint_root_folder ./finetuned_model \
--dist_checkpoint_folder fine-tuned &nbsp;\
--use_fast_kernels \
--dataset "custom_dataset" --custom_dataset.test_split "test" \
--custom_dataset.file "recipes/quickstart/finetuning/datasets/mind2web_dataset.py" &nbsp;\
--run_validation False --batching_strategy padding
``` 
 
You can use the ./logs.sh command to see the training logs in both FSDP workers. 
After a successful run, logs from fsdp-worker will look as follows: 
 
 Sharded state checkpoint saved to /workspace/llama-recipes/finetuned_model_mind2web/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct
Checkpoint Time = 85.3276

Epoch 5: train_perplexity=1.0214, train_epoch_loss=0.0211, epoch time 706.1626197730075s
training params are saved in /workspace/llama-recipes/finetuned_model_mind2web/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/train_params.yaml
Key: avg_train_prep, Value: 1.0532150745391846
Key: avg_train_loss, Value: 0.05118955448269844
Key: avg_epoch_time, Value: 716.0386156642023
Key: avg_checkpoint_time, Value: 85.34336999000224
fsdp-worker-1:78:5593 [0] NCCL INFO [Service thread] Connection closed by localRank 1
fsdp-worker-1:81:5587 [0] NCCL INFO [Service thread] Connection closed by localRank 4
fsdp-worker-1:85:5590 [0] NCCL INFO [Service thread] Connection closed by localRank 0
I0305 19:37:56.173000 140632318404416 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I0305 19:37:56.173000 140632318404416 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I0305 19:37:56.177000 140632318404416 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.0037238597869873047 seconds 
 
Additionally: 
 
 [rank8]:W0305 19:37:46.754000 139970058049344 torch/distributed/distributed_c10d.py:2429] _tensor_to_object size: 2817680 hash value: 9260685783781206407
fsdp-worker-0:84:5591 [0] NCCL INFO [Service thread] Connection closed by localRank 7
I0305 19:37:56.124000 139944709084992 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I0305 19:37:56.124000 139944709084992 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I0305 19:37:56.177000 139944709084992 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.05295562744140625 seconds 
 
 
 
Run the processing model and store output in Amazon S3 
After the jobs are complete, the fine-tuned model will exist in the FSx file system. The next step is to convert the model into Hugging Face format and save it in Amazon S3 so you can access and deploy the model in the upcoming steps:kubectl apply -f model-processor.yaml 
The preceding command deploys a pod on your instance that will read the model from FSx, convert it to Hugging Face type, and push it to Amazon S3. It takes approximately 8â€“10 minutes for this pod to run. You can monitor the logs for this using ./logs.sh or kubectl logs -l app=model-processor. 
Get the location where your model has been stored in Amazon S3. This is the same Amazon S3 location that was mentioned the .env file in an earlier step. Run the following command (provide the Amazon S3 location):aws s3 cp tokenizer_config.json &lt;S3_LOCATION&gt;://tokenizer_config.json 
This is the tokenizer config that is needed by Amazon Bedrock to import Meta Llama models so they work with the Amazon Bedrock Converse API. For more details, see Converse API code samples for custom model import. 
For this post, we use the Mind2Web dataset. We have implemented code that has been adapted from the Mind2Web code for fine-tuning. The adapted code is as follows: 
 
 git clone https://github.com/meta-llama/llama-cookbook &amp;&amp; \
cd llama-cookbook &amp;&amp; \
git checkout a346e19df9dd1a9cddde416167732a3edd899d09 &amp;&amp; \
curl -L -o patch-llama-cookbook.tar.gz https://raw.githubusercontent.com/aws/deep-learning-containers/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-llama-cookbook.tar.gz &amp;&amp; \
tar -xzf patch-llama-cookbook.tar.gz &amp;&amp; \
cd patch-llama-cookbook &amp;&amp; \
git config --global user.email "you@example.com" &amp;&amp; \
git am *.patch &amp;&amp;&nbsp;\
cd ..&nbsp;&amp;&amp;&nbsp;\
cat recipes/quickstart/finetuning/datasets/mind2web_dataset.py 
 
Deploy the fine-tuned model on Amazon Bedrock 
After you fine-tune your Meta Llama 3.2 Vision model, you have several options for deployment. This section covers one deployment method using Amazon Bedrock. With Amazon Bedrock, you can import and use your custom trained models seamlessly. Make sure your fine-tuned model is uploaded to an S3 bucket, and itâ€™s converted to Hugging Face format. Complete the following steps to import your fine-tuned Meta Llama 3.2 Vision model: 
 
 On the Amazon Bedrock console, under Foundation models in the navigation pane, choose Imported models. 
 Choose Import model. 
 For Model name, enter a name for the model. 
 
 
 
 For Model import source, select Amazon S3 bucket. 
 For S3 location, enter the location of the S3 bucket containing your fine-tuned model. 
 
 
 
 Configure additional model settings as needed, then import your model. 
 
The process might take 10â€“15 minutes depending on the model size to complete. 
After you import your custom model, you can invoke it using the same Amazon Bedrock API as the default Meta Llama 3.2 Vision model. Just replace the model name with your imported modelâ€™s Amazon Resource Name (ARN). For detailed instructions, refer to Amazon Bedrock Custom Model Import. 
You can follow the prompt formats mentioned in the following GitHub repo. For example: 
What are the steps to build a docker image?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; 
Run the agent workload using the hosted Amazon Bedrock model 
Running the agent workload involves using the SeeAct framework and browser automation to start an interactive session with the AI agent and perform the browser operations. We recommend completing the steps in this section on a local machine for browser access. 
Clone the SeeAct repository 
Clone the customized SeeAct repository, which contains example code that can work with Amazon Bedrock, as well as a couple of test scripts: 
 
 git clone https://github.com/OSU-NLP-Group/SeeAct.git 
 
Set up SeeAct in a local runtime environment 
Complete the following steps to set up SeeAct in a local runtime environment: 
 
 Create a Python virtual environment for this demo. We use Python 3.11 in the example, but you can change to other Python versions. 
 
 
 python3.11 -m venv seacct-python-3-11
source seacct-python-3-11/bin/activate 
 
 
 Apply a patch to add the code change needed for this demo: 
 
 
 cd SeeAct
curl -O https://raw.githubusercontent.com/aws/deep-learning-containers/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-seeact.patch
git checkout 2fdbf373f58a1aa5f626f7c5931fe251afc69c0a
git apply patch-seeact.patch 
 
 
 Run the following commands to install the SeeAct package and dependencies: 
 
 
 cd SeeAct/seeact_package
pip install .
pip install -r&nbsp;requirements.txt
pip install -U boto3
playwright install 
 
Make sure youâ€™re using the latest version of Boto3 for these steps. 
Validate the browser automation tool used by SeeAct 
We added a small Python script to verify the functionality of Playwright, the browser automation tool used by SeeAct: 
 
 cd SeeAct/src
python test_playwright.py 
 
You should see a browser launched and closed after a few seconds. You should also see a screenshot being captured in SeeAct/src/example.png showing google.com. 
 
Test Amazon Bedrock model availability 
Modify the content of test_bedrock.py. Update the MODEL_ID to be your hosted Amazon Bedrock model ARN and set up the AWS connection. 
 
 export AWS_ACCESS_KEY_ID="replace with your aws credential"
export AWS_SECRET_ACCESS_KEY="replace with your aws credential"
export AWS_SESSION_TOKEN="replace with your aws credential" 
 
Run the test: 
 
 cd SeeAct
python&nbsp;test_bedrock.py 
 
After a successful invocation, you should see a log similar to the following in your terminal: 
 
 The image shows a dog lying down inside a black pet carrier, with a leash attached to the dog's collar. 
 
If the botocore.errorfactory.ModelNotReadyException error occurs, retry the command in a few minutes. 
Run the agent workflow 
The branch has already added support for BedrockEngine and SGLang for running inference with the fine-tuned Meta Llama 3.2 Vision model. The default option uses Amazon Bedrock inference. 
To run the agent workflow, update self.model from src/demo_utils/inference_engine.py at line 229 to your Amazon Bedrock model ARN. Then run the following code: 
 
 cd SeeAct/src
python seeact.py -c config/demo_mode.toml&nbsp; 
 
This will launch a terminal prompt like the following code, so you can input the task you want the agent to do: 
 
 Please input a task, and press Enter. 
Or directly press Enter to use the default task: Find pdf of paper "GPT-4V(ision) is a Generalist Web Agent, if Grounded" from arXiv
Task:&nbsp; 
 
In the following screenshot, we asked the agent to search for the website for DLCs. 
 
Clean up 
Use the following code to clean the resources you created as part of this post: 
 
 cd Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp
kubectl delete&nbsp;-f ./fsdp.yaml&nbsp;## Deletes the training fsdp job
kubectl&nbsp;delete&nbsp;-f&nbsp;./etcd.yaml&nbsp;## Deletes etcd
kubectl delete&nbsp;-f ./model-processor.yaml&nbsp;## Deletes model processing YAML

cd aws-do-eks/Container-Root/eks/deployment/kubeflow/mpi-operator/
./remove.sh

cd aws-do-eks/Container-Root/eks/deployment/kubeflow/training-operator/
./remove.sh

## [VOLUME GETS DELETED] - If&nbsp;you want to delete&nbsp;the FSX volume
kubectl delete -f ./pvc.yaml ## Deletes persistent volume claim, persistent volume and actual volume 
 
To stop the P5 nodes and release them, complete the following steps: 
 
 On the Amazon EKS console, choose Clusters in the navigation pane. 
 Choose the cluster that contains your node group. 
 On the cluster details page choose the Compute tab. 
 In the Node groups section, select your node group, then choose Edit. 
 Set the desired size to 0. 
 
Conclusion 
In this post, we presented an end-to-end workflow for fine-tuning and deploying the Meta Llama 3.2 Vision model using the production-grade infrastructure of AWS. By using AWS DLCs on Amazon EKS, you can create a robust, secure, and scalable environment for model fine-tuning. The integration of advanced technologies like EFA support and FSDP training enables efficient handling of LLMs while optimizing resource usage. The deployment through Amazon Bedrock provides a streamlined path to production, and the integration with SeeAct demonstrates practical applications in web automation tasks. This solution serves as a comprehensive reference point for engineers to develop their own specialized AI applications, adapt the demonstrated approaches, and implement similar solutions for web automation, content analysis, or other domain-specific tasks requiring vision-language capabilities. 
To get started with your own implementation, refer to our GitHub repo. To learn more about AWS DLCs, see the AWS Deep Learning Containers Developer Guide. For more details about Amazon Bedrock, see Getting started with Amazon Bedrock. 
For deeper insights into related topics, refer to the following resources: 
 
 Scale LLMs with PyTorch 2.0 FSDP on Amazon EKS â€“ Part 2 
 Build high-performance ML models using PyTorch 2.0 on AWS â€“ Part 1 
 Mind2Web dataset 
 
Need help or have questions? Join our AWS Machine Learning community on Discord or reach out to AWS Support. You can also stay updated with the latest developments by following the AWS Machine Learning Blog. 
 
About the Authors 
Shantanu Tripathi is a Software Development Engineer at AWS with over 4 years of experience in building and optimizing large-scale AI/ML solutions. His experience spans developing distributed AI training libraries, creating and launching DLCs and Deep Learning AMIs, designing scalable infrastructure for high-performance AI workloads, and working on generative AI solutions. He has contributed to AWS services like Amazon SageMaker HyperPod, AWS DLCs, and DLAMIs, along with driving innovations in AI security. Outside of work, he enjoys theater and swimming. 
Junpu Fan is a Senior Software Development Engineer at Amazon Web Services, specializing in AI/ML Infrastructure. With over 5 years of experience in the field, Junpu has developed extensive expertise across the full cycle of AI/ML workflows. His work focuses on building robust systems that power ML applications at scale, helping organizations transform their data into actionable insights. 
Harish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He helps customers harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices. 
Arindam Paul is a Sr. Product Manager in SageMaker AI team at AWS responsible for Deep Learning workloads on SageMaker, EC2, EKS, and ECS. He is passionate about using AI to solve customer problems. In his spare time, he enjoys working out and gardening.

â¸»