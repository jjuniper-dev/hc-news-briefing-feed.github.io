âœ… Morning News Briefing â€“ August 01, 2025 10:50

ğŸ“… Date: 2025-08-01 10:50
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  7.9Â°C
  Temperature: 7.9&deg;C Pressure / Tendency: 102.7 kPa rising Humidity: 98 % Humidity : 98 % Dewpoint: 7 .6&deg:C Wind: WNW 5 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 1 August 2025 Temperature: . 7.6
â€¢ Friday: Mainly sunny. High 23.
  Sunny. Becoming a mix of sun and cloud this afternoon . Wind becoming northwest 20 km/h late this morning . High 23. UV index 7 or high. Wind becoming a factor in the forecast . High of 23.5 degrees Celsius in the morning of August 1, 2025 . Forecast issued 5:00 AM EDT Friday 1 August 2025 . For more information, visit www.

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ An FDA panel spread misinformation about SSRI use in pregnancy, alarming doctors
  A panel organized by the FDA cast doubts on the safety of antidepressants during pregnancy . Doctors who say SSRIs are a crucial treatment option for women with perinatal depression say they are crucial . The panel also drew ire from doctors who say they're crucial treatment options for pregnant women . The FDA is expected to make a decision on whether to approve the use of antidepressants in the future .
â€¢ Alaska ignored budget crisis signs. Now, it doesn't have money to fix schools.
  Alaska has long ignored warning signs of a budget crisis . Now, it has no money to fix something that is posing serious health and safety risks to students and staff . The state has no budget to fix crumbling rural schools . Alaska has been in a state of financial crisis for years . It has been hard to find a solution to the state's budget crisis, but now it is running out
â€¢ France, U.K., others plan to recognize a Palestinian state. What does that change?
  The push by several countries to recognize a state of Palestine is largely symbolic, but carries diplomatic and potentially legal weight . Several countries have pushed for recognition of Palestine, but it carries diplomatic, potentially legal and diplomatic weight . The push for recognition by countries has been largely symbolic but could carry diplomatic and legal weight, says Henrique Campos of the U.N. The U.S.
â€¢ What is the U.S. destroying $9.7 million worth of? Find out in the quiz
  This week was full of mysteries. If you're a super sleuth who followed the news, you'll be well on your way to a perfect score. This week's news was a case of a mystery. Follow us on Twitter @dailymail.com/Heroes for the latest updates from this week's top 10 news stories. Share your knowledge of the latest events in the news
â€¢ Songs of Love writes personalized music for kids â€” but can AI carry the tune?
  Songs of Love Foundation has been creating custom songs for kids with terminal illnesses for nearly 30 years . Now it has harnessed AI to expand its services to older adults with memory loss . The nonprofit has been using AI to create songs for children with terminal illness for 30 years, and now it has expanded its services for memory loss to older people with memory losses . The foundation is now using AI

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ Cybercrooks attached Raspberry Pi to bank network and drained ATM cash
  Criminals used undocumented techniques and well-placed insiders to remotely withdraw money . Ring of cybercriminals managed to physically implant a Raspberry Pi on a bank's network to steal cash from an Indonesian ATM . Criminals also managed to use undocumented techniques to withdraw cash from the bank's system . The operation was carried out by a ring of cyber criminals who used a network hack to steal money .
â€¢ Windows 10 @ 10: How Microsoft led developers round in circles
  Microsoft has just released Windows 10 . Developers are being pitched a new vision from Microsoft: the Universal Windows Platform (UWP) From one platform to rule them all to a carousel of half-baked visions . Microsoft has released a new version of Windows 10, with the UWP version of the operating system . The UWP will be Microsoft's first version of its own version of
â€¢ Servers hated Mondays until techie quit quaffing coffee in their company
  On Call is a reader-contributed column that tells your tales of tech support treachery and triumph . The Register gives readers a little kick along on the last day of the working week in the form of a new installment of On Call . On Call: A hotfix for a hotfix to survive early morning cold boots on Friday morning . Read this week's On Call from The Register
â€¢ Top spy says LinkedIn profiles that list defence work 'recklessly invite attention of foreign intelligence services'
  Workers on joint US/UK/Australia nuclear submarine program are painting a target on themselves . Director-General of Security at the Australian Security Intelligence Organization (ASIO) has lamented the fact that many people list their work in the intelligence community or on sensitive military projects in their LinkedIn profiles . ASIO: Many people list work in intelligence community, military projects on LinkedIn profiles in their profiles .
â€¢ Panic buying ahead of Trump tariffs added $825 million to Apple's sales last quarter
  iBiz warns import imposts set to rise World War Fee . The USAâ€™s evolving tariff policy wasn't all bad news for Apple, which manufactures most of its products overseas . Apple's iPhone, iPad, iPad and other products are made in the U.S. and sold in China, but not in the EU, China has been hit hard by the tariffs .

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ Carbon emissions from the increasing use of inhaled corticosteroid-LABA medications: a primary care used case from Singapore
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ How allergens make us cough and wheeze â€” by poking holes in airway cells
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Permissive gun laws in the USA linked to thousands of child deaths
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ The relationship between parental psychological well-being and preadolescentsâ€™ social media use
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ A cross-national analysis of demographic variation in self-rated mental health across 22 countries
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ How decades-old frozen embryos are changing the shape of families
  Baby Thaddeus Daniel Pierce was born from an embryo that was frozen in storage for 30 and a half years . The baby's parents were themselves only young children when the embryo was created in 1994 . Linda Archerd, who donated the embryo, described the experience as â€œsurreal.â€ She says there may be no limit to how long embryos can be stored in tanks . People who have embryos they wonâ€™t use can choose to donate them, either to potential parents or for research .
â€¢ The Download: OpenAIâ€™s future research, and US climate regulation is under threat
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The two people shaping the future of OpenAIâ€™s research



â€”Will Douglas HeavenFor the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firmâ€™s roster.But Altman is not the one building the technology on which its reputation rests. That responsibility falls to OpenAIâ€™s twin heads of researchâ€”chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.I recently sat down with Chen and Pachocki for an exclusive conversation which covered everything from how they manage the inherent tension between research and product, to what they really mean when they talk about AGI, to what happened to OpenAIâ€™s superalignment team.Â 



I also wanted to get a sense of where their heads are at in the run-up to OpenAIâ€™s biggest product release in months: GPT-5. Read the full story.







An EPA rule change threatens to gut US climate regulations



The mechanism that allows the US federal government to regulate climate change is on the chopping block.



On Tuesday, US Environmental Protection Agency administrator Lee Zeldin announced that the agency is taking aim at the endangerment finding, a 2009 rule thatâ€™s essentially the tentpole supporting federal greenhouse-gas regulations.



This might sound like an obscure legal situation, but itâ€™s a really big deal for climate policy in the US. So letâ€™s look at what this rule says now, what the proposed change looks like, and what it all means. Read the full story.



â€”Casey Crownhart



This story is part of MIT Technology Reviewâ€™s â€œAmerica Undoneâ€ series, examining how the foundations of US success in science and innovation are currently under threat. You can read the rest here.



It appeared first in The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The AI Hype Index: The White Houseâ€™s war on â€œwoke AIâ€



Separating AI reality from hyped-up fiction isnâ€™t always easy. Thatâ€™s why weâ€™ve created the AI Hype Indexâ€”a simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this monthâ€™s edition of the index here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Trump has announced a new US health care records systemÂ Experts warn the initiative could leave patientsâ€™ medical records open to abuse. (NYT $)+ Big Tech has pledged to work with providers and health systems. (The Hill)



2 China says itâ€™s worried Nvidiaâ€™s chips have serious security issuesJust as the company sought to resume sales in the country. (Reuters)+Experts reportedly found the chips featured location tracking tech. (FT $)



3 Mark Zuckerberg believes superintelligence â€œis now in sightâ€Although he didnâ€™t illuminate what it even means. (The Guardian)+ Zuckerberg has taken a leaf out of the Altman playbook. (NY Mag $)+ Donâ€™t expect Meta to open source any of those superintelligent models. (TechCrunch)+ Tech billionaires are making a risky bet with humanityâ€™s future. (MIT Technology Review)



4 NASA is in turmoilWithout a permanent leader, workers are leaving in their thousands. (WP $)



5 Google removed negative articles about a tech CEO from search resultsAfter someone made fraudulent requests using its Refresh Outdated Content Tool. (404 Media)+ They exploited a bug in the tool to get pages removed. (Ars Technica)



6 How AI has transformed data center designThey need to accommodate a lot more heat and power than they used to. (FT $)+ A proposed Wyoming data center would use more electricity than its homes. (Ars Technica)+ Apple manufacturer Foxconn wants to get involved in building data centers. (CNBC)+ Should we be moving data centers to space? (MIT Technology Review)



7 AI agents can probe websites for security weaknessesEspecially shoddily-constructed vibe-coded ones. (Wired $)+ Cyberattacks by AI agents are coming. (MIT Technology Review)



8 New forms of life have been filmed at the oceanâ€™s deepest pointsThe abundance of life was amazing, the Chinese-led research team says. (BBC)+ Meet the divers trying to figure out how deep humans can go. (MIT Technology Review)



9 TikTok is adding Footnotes to its clipsAs AI-generated videos become even harder to spot. (The Verge)+ This fake viral clip of rabbits on a trampoline is a great example. (404 Media)



10 What itâ€™s like to attend an Elon Musk fan festX Takeover promised to unite Tesla and SpaceX-heads alike. (Insider $)+ Some people who definitely arenâ€™t fans: neighbors of Teslaâ€™s diner. (404 Media)







Quote of the day



â€œPatients across America should be very worried that their medical records are going to be used in ways that harm them and their families.â€



â€”Lawrence Gostin, a Georgetown University law professor specializing in public health, warns of the potential repercussions of the Trump administrationâ€™s new health data tracking system, the Associated Press reports.







One more thing







The cost of building the perfect waveFor nearly as long as surfing has existed, surfers have been obsessed with the search for the perfect wave.While this hunt has taken surfers from tropical coastlines to icebergs, these days that search may take place closer to home. That is, at least, the vision presented by developers and boosters in the growing industry of surf pools, spurred by advances in wave-Â­generating technology that have finally created artificial waves surfers actually want to ride.But thereâ€™s a problem: some of these pools are in drought-ridden areas, and face fierce local opposition. At the core of these fights is a question thatâ€™s also at the heart of the sport: What is the cost of finding, or now creating, the perfect waveâ€”and who will have to bear it? Read the full story.



â€”Eileen Guo







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Maybe airplane food isnâ€™t so bad after all.+ An unwitting metal detectorist uncovered some ancient armor in the Czech Republic that may have been worn during the Trojan war.+ Talking of the siege of Troy, tickets for Christopher Nolanâ€™s retelling of The Odyssey are already selling out a year before itâ€™s released.+ This fun website refreshes every few seconds with a new picture of someone pointing at your mouse pointer.
â€¢ The two people shaping the future of OpenAIâ€™s research
  For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firmâ€™s roster. Even his bungled ouster ended with him back on topâ€”and more famous than ever. But look past the charismatic frontman and you get a clearer sense of where this company is going. After all, Altman is not the one building the technology on which its reputation rests.&nbsp;



That responsibility falls to OpenAIâ€™s twin heads of researchâ€”chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.



I sat down with Chen and Pachocki for an exclusive conversation during a recent trip the pair made to London, where OpenAI set up its first international office in 2023. We talked about how they manage the inherent tension between research and product. We also talked about why they think coding and math are the keys to more capable all-purpose models; what they really mean when they talk about AGI; and what happened to OpenAIâ€™s superalignment team, set up by the firmâ€™s cofounder and former chief scientist Ilya Sutskever to prevent a hypothetical superintelligence from going rogue, which disbanded soon after he quit.&nbsp;



In particular, I wanted to get a sense of where their heads are at in the run-up to OpenAIâ€™s biggest product release in months: GPT-5.



Reports are out that the firmâ€™s next-generation model will be launched in August. OpenAIâ€™s official lineâ€”well, Altmanâ€™sâ€”is that it will release GPT-5 â€œsoon.â€ Anticipation is high. The leaps OpenAI made with GPT-3 and then GPT-4 raised the bar of what was thought possible with this technology. And yet delays to the launch of GPT-5 have fueled rumors that OpenAI has struggled to build a model that meets its ownâ€”not to mention everyone elseâ€™sâ€”expectations.



But expectation management is part of the job for a company that for the last several years has set the agenda for the industry. And Chen and Pachocki set the agenda inside OpenAI.



Twin peaks&nbsp;



The firmâ€™s main London office is in St Jamesâ€™s Park, a few hundred meters east of Buckingham Palace. But I met Chen and Pachocki in a conference room in a coworking space near Kingâ€™s Cross, which OpenAI keeps as a kind of pied-Ã -terre in the heart of Londonâ€™s tech neighborhood (Google DeepMind and Meta are just around the corner). OpenAIâ€™s head of research communications, Laurance Fauconnet, sat with an open laptop at the end of the table.&nbsp;



Chen, who was wearing a maroon polo shirt, is clean-cut, almost preppy. Heâ€™s media trained and comfortable talking to a reporter. (Thatâ€™s him flirting with a chatbot in the â€œIntroducing GPT-4oâ€ video.) Pachocki, in a black elephant-logo tee, has more of a TV-movie hacker look. He stares at his hands a lot when he speaks.



But the pair are a tighter double act than they first appear. Pachocki summed up their roles. Chen shapes and manages the research teams, he said. â€œI am responsible for setting the research roadmap and establishing our long-term technical vision.â€





â€œBut thereâ€™s fluidity in the roles,â€ Chen said. â€œWeâ€™re both researchers, we pull on technical threads. Whatever we see that we can pull on and fix, thatâ€™s what we do.â€



Chen joined the company in 2018 after working as a quantitative trader at the Wall Street firm Jane Street Capital, where he developed machine-learning models for futures trading. At OpenAI he spearheaded the creation of DALL-E, the firmâ€™s breakthrough generative image model. He then worked on adding image recognition to GPTâ€‘4 and led the development of Codex, the generative coding model that powers GitHub Copilot.



Pachocki left an academic career in theoretical computer science to join OpenAI in 2017 and replaced Sutskever as chief scientist in 2024. He is the key architect of OpenAIâ€™s so-called reasoning modelsâ€”especially o1 and o3â€”which are designed to tackle complex tasks in science, math, and coding.&nbsp;



When we met they were buzzing, fresh off the high of two new back-to-back wins for their companyâ€™s technology.



On July 16, one of OpenAIâ€™s large language models came in second in the AtCoder World Tour Finals, one of the worldâ€™s most hardcore programming competitions. On July 19, OpenAI announced that one of its models had achieved gold-medal-level results on the 2025 International Math Olympiad, one of the worldâ€™s most prestigious math contests.



The math result made headlines, not only because of OpenAIâ€™s remarkable achievement, but because rival Google DeepMind revealed two days later that one of its models had achieved the same score in the same competition. Google DeepMind had played by the competitionâ€™s rules and waited for its results to be checked by the organizers before making an announcement; OpenAI had in effect marked its own answers.



For Chen and Pachocki, the result speaks for itself. Anyway, itâ€™s the programming win theyâ€™re most excited about. â€œI think thatâ€™s quite underrated,â€ Chen told me. A gold medal result in the International Math Olympiad puts you somewhere in the top 20 to 50 competitors, he said. But in the AtCoder contest OpenAIâ€™s model placed in the top two: â€œTo break into a really different tier of human performanceâ€”thatâ€™s unprecedented.â€



Ship, ship, ship!



People at OpenAI still like to say they work at a research lab. But the company is very different from the one it was before the release of ChatGPT three years ago. The firm is now in a race with the biggest and richest technology companies in the world and valued at $300 billion. Envelope-pushing research and eye-catching demos no longer cut it. It needs to ship products and get them into peopleâ€™s handsâ€”and boy, it does.&nbsp;



OpenAI has kept up a run of new releasesâ€”putting out major updates to its GPT-4 series, launching a string of generative image and video models, and introducing the ability to talk to ChatGPT with your voice. Six months ago it kicked off a new wave of so-called reasoning models with its o1 release, soon followed by o3. And last week it released its browser-using agent Operator to the public. It now claims that more than 400 million people use its products every week and submit 2.5 billion prompts a day.&nbsp;



OpenAIâ€™s incoming CEO of applications, Fidji Simo, plans to keep up the momentum. In a memo to the company, she told employees she is looking forward to â€œhelping get OpenAIâ€™s technologies into the hands of more people around the world,â€ where they will â€œunlock more opportunities for more people than any other technology in history.â€ Expect the products to keep coming.



I asked how OpenAI juggles open-ended research and product development. â€œThis is something we have been thinking about for a very long time, long before ChatGPT,â€ Pachocki said. â€œIf we are actually serious about trying to build artificial general intelligence, clearly there will be so much that you can do with this technology along the way, so many tangents you can go down that will be big products.â€ In other words, keep shaking the tree and harvest what you can.



A talking point that comes up with OpenAI folks is that putting experimental models out into the world was a necessary part of research. The goal was to make people aware of how good this technology had become. â€œWe want to educate people about whatâ€™s coming so that we can participate in what will be a very hard societal conversation,â€ Altman told me back in 2022. The makers of this strange new technology were also curious what it might be for: OpenAI was keen to get it into peopleâ€™s hands to see what they would do with it.



Is that still the case? They answered at the same time. â€œYeah!â€ Chen said. â€œTo some extent,â€ Pachocki said. Chen laughed: â€œNo, go ahead.â€&nbsp;



â€œI wouldnâ€™t say research iterates on product,â€ said Pachocki. â€œBut now that models are at the edge of the capabilities that can be measured by classical benchmarks and a lot of the long-standing challenges that weâ€™ve been thinking about are starting to fall, weâ€™re at the point where it really is about what the models can do in the real world.â€



Like taking on humans in coding competitions. The person who beat OpenAIâ€™s model at this yearâ€™s AtCoder contest, held in Japan, was a programmer named PrzemysÅ‚aw DÄ™biak, also known as Psyho. The contest was a puzzle-solving marathon in which competitors had 10 hours to find the most efficient way to solve a complex coding problem. After his win, Psyho posted on X: â€œIâ€™m completely exhausted &#8230; Iâ€™m barely alive.â€&nbsp;&nbsp;



Chen and Pachocki have strong ties to the world of competitive coding. Both have competed in international coding contests in the past and Chen coaches the USA Computing Olympiad team. I asked whether that personal enthusiasm for competitive coding colors their sense of how big a deal it is for a model to perform well at such a challenge.





They both laughed. â€œDefinitely,â€ said Pachocki. â€œSo: Psyho is kind of a legend. Heâ€™s been the number one competitor for many years. Heâ€™s also actually a friend of mineâ€”we used to compete together in these contests.â€ DÄ™biak also used to work with Pachocki at OpenAI.



When Pachocki competed in coding contests he favored those that focused on shorter problems with concrete solutions. But DÄ™biak liked longer, open-ended problems without an obvious correct answer.



â€œHe used to poke fun at me, saying that the kind of contest I was into will be automated long before the ones he liked,â€ Pachocki recalled. â€œSo I was seriously invested in the performance of this model in this latest competition.â€



Pachocki told me he was glued to the late-night livestream from Tokyo, watching his model come in second: â€œPsyho resists for now.â€&nbsp;



â€œWeâ€™ve tracked the performance of LLMs on coding contests for a while,â€ said Chen. â€œWeâ€™ve watched them become better than me, better than Jakub. It feels something like Lee Sedol playing Go.â€



Lee is the master Go player who lost a series of matches to DeepMindâ€™s game-playing model AlphaGo in 2016. The results stunned the international Go community and led Lee to give up professional play. Last year he told the New York Times: â€œLosing to AI, in a sense, meant my entire world was collapsing &#8230; I could no longer enjoy the game.â€ And yet, unlike Lee, Chen and Pachocki are thrilled to be surpassed.&nbsp;&nbsp;&nbsp;



But why should the rest of us care about these niche wins? Itâ€™s clear that this technologyâ€”designed to mimic and, ultimately, stand in for human intelligenceâ€”is being built by people whose idea of peak intelligence is acing a math contest or holding your own against a legendary coder. Is it a problem that this view of intelligence is skewed toward the mathematical, analytical end of the scale?



â€œI mean, I think you are right thatâ€”you know, selfishly, we do want to create models which accelerate ourselves,â€ Chen told me. â€œWe see that as a very fast factor to progress.â€&nbsp;&nbsp;



The argument researchers like Chen and Pachocki make is that math and coding are the bedrock for a far more general form of intelligence, one that can solve a wide range of problems in ways we might not have thought of ourselves. â€œWeâ€™re talking about programming and math here,â€ said Pachocki. â€œBut itâ€™s really about creativity, coming up with novel ideas, connecting ideas from different places.â€



Look at the two recent competitions: â€œIn both cases, there were problems which required very hard, out-of-the-box thinking. Psyho spent half the programming competition thinking and then came up with a solution that was really novel and quite different from anything that our model looked at.â€



â€œThis is really what weâ€™re after,â€ Pachocki continued. â€œHow do we get models to discover this sort of novel insight? To actually advance our knowledge? I think they are already capable of that in some limited ways. But I think this technology has the potential to really accelerate scientific progress.â€&nbsp;



I returned to the question about whether the focus on math and programming was a problem, conceding that maybe itâ€™s fine if what weâ€™re building are tools to help us do science. We don&#8217;t necessarily want large language models to replace politicians and have people skills, I suggested.



Chen pulled a face and looked up at the ceiling: â€œWhy not?â€



Whatâ€™s missing



OpenAI was founded with a level of hubris that stood out even by Silicon Valley standards, boasting about its goal of building AGI back when talk of AGI still sounded kooky. OpenAI remains as gung-ho about AGI as ever, and it has done more than most to make AGI a mainstream multibillion-dollar concern. Itâ€™s not there yet, though. I asked Chen and Pachocki what they think is missing.



â€œI think the way to envision the future is to really, deeply study the technology that we see today,â€ Pachocki said. â€œFrom the beginning, OpenAI has looked at deep learning as this very mysterious and clearly very powerful technology with a lot of potential. Weâ€™ve been trying to understand its bottlenecks. What can it do? What can it not do?â€&nbsp;&nbsp;



At the current cutting edge, Chen said, are reasoning models, which break down problems into smaller, more manageable steps, but even they have limits: â€œYou know, you have these models which know a lot of things but canâ€™t chain that knowledge together. Why is that? Why canâ€™t it do that in a way that humans can?â€



OpenAI is throwing everything at answering that question.



â€œWe are probably still, like, at the very beginning of this reasoning paradigm,â€ Pachocki told me. â€œReally, we are thinking about how to get these models to learn and explore over the long term and actually deliver very new ideas.â€



Chen pushed the point home: â€œI really donâ€™t consider reasoning done. Weâ€™ve definitely not solved it. You have to read so much text to get a kind of approximation of what humans know.â€



OpenAI wonâ€™t say what data it uses to train its models or give details about their size and shapeâ€”only that it is working hard to make all stages of the development process more efficient.



Those efforts make them confident that so-called scaling lawsâ€”which suggest that models will continue to get better the more compute you throw at themâ€”show no sign of breaking down.



â€œI donâ€™t think thereâ€™s evidence that scaling laws are dead in any sense,â€ Chen insisted. â€œThere have always been bottlenecks, right? Sometimes theyâ€™re to do with the way models are built. Sometimes theyâ€™re to do with data. But fundamentally itâ€™s just about finding the research that breaks you through the current bottleneck.â€&nbsp;



The faith in progress is unshakeable. I brought up something Pachocki had said about AGI in an interview with Nature in May: â€œWhen I joined OpenAI in 2017, I was still among the biggest skeptics at the company.â€ He looked doubtful.&nbsp;



â€œIâ€™m not sure I was skeptical about the concept,â€ he said. â€œBut I think I wasâ€”â€ He paused, looking at his hands on the table in front of him. â€œWhen I joined OpenAI, I expected the timelines to be longer to get to the point that we are now.â€





â€œThereâ€™s a lot of consequences of AI,â€ he said. â€œBut the one I think the most about is automated research. When we look at human history, a lot of it is about technological progress, about humans building new technologies. The point when computers can develop new technologies themselves seems like a very important, um, inflection point.



â€œWe already see these models assist scientists. But when they are able to work on longer horizonsâ€”when theyâ€™re able to establish research programs for themselvesâ€”the world will feel meaningfully different.â€



For Chen, that ability for models to work by themselves for longer is key. â€œI mean, I do think everyone has their own definitions of AGI,â€ he said. â€œBut this concept of autonomous timeâ€”just the amount of time that the model can spend making productive progress on a difficult problem without hitting a dead endâ€”thatâ€™s one of the big things that weâ€™re after.â€



Itâ€™s a bold visionâ€”and far beyond the capabilities of todayâ€™s models. But I was nevertheless struck by how Chen and Pachocki made AGI sound almost mundane. Compare this with how Sutskever responded when I spoke to him 18 months ago. â€œItâ€™s going to be monumental, earth-shattering,â€ he told me. â€œThere will be a before and an after.â€ Faced with the immensity of what he was building, Sutskever switched the focus of his career from designing better and better models to figuring out how to control a technology that he believed would soon be smarter than himself.



Two years ago Sutskever set up what he called a superalignment team that he would co-lead with another OpenAI safety researcher, Jan Leike. The claim was that this team would funnel a full fifth of OpenAIâ€™s resources into figuring out how to control a hypothetical superintelligence. Today, most of the people on the superalignment team, including Sutskever and Leike, have left the company and the team no longer exists.&nbsp;&nbsp;&nbsp;



When Leike quit, he said it was because the team had not been given the support he felt it deserved. He posted this on X: â€œBuilding smarter-than-human machines is an inherently dangerous endeavor. OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.â€ Other departing researchers shared similar statements.



I asked Chen and Pachocki what they make of such concerns. â€œA lot of these things are highly personal decisions,â€ Chen said. â€œYou know, a researcher can kind of, you knowâ€”â€



He started again. â€œThey might have a belief that the field is going to evolve in a certain way and that their research is going to pan out and is going to bear fruit. And, you know, maybe the company doesnâ€™t reshape in the way that you want it to. Itâ€™s a very dynamic field.â€



â€œA lot of these things are personal decisions,â€ he repeated. â€œSometimes the field is just evolving in a way that is less consistent with the way that youâ€™re doing research.â€



But alignment, both of them insist, is now part of the core business rather than the concern of one specific team. According to Pachocki, these models donâ€™t work at all unless they work as you expect them to. Thereâ€™s also little desire to focus on aligning a hypothetical superintelligence with your objectives when doing so with existing models is already enough of a challenge.



â€œTwo years ago the risks that we were imagining were mostly theoretical risks,â€ Pachocki said. â€œThe world today looks very different, and I think a lot of alignment problems are now very practically motivated.â€



Still, experimental technology is being spun into mass-market products faster than ever before. Does that really never lead to disagreements between the two of them?



â€œI am often afforded the luxury of really kind of thinking about the long term, where the technology is headed,â€ Pachocki said. â€œContending with the reality of the processâ€”both in terms of people and also, like, the broader company needsâ€”falls on Mark. Itâ€™s not really a disagreement, but there is a natural tension between these different objectives and the different challenges that the company is facing that materializes between us.â€



Chen jumped in: â€œI think itâ€™s just a very delicate balance.â€&nbsp;&nbsp;



Correction: we have removed a line referring to an Altman message on X about GPT-5.
â€¢ The AI Hype Index: The White Houseâ€™s war on â€œwoke AIâ€
  The Trump administration recently declared war on so-called â€œwoke AI,â€ issuing an executive order aimed at preventing companies whose models exhibit a liberal bias from landing federal contracts . The Pentagon inked a deal with Elon Muskâ€™s xAI just days after its chatbot, Grok, spouted harmful antisemitic stereotypes . The White House has partnered with an anti-DEI nonprofit to create AI slop videos .
â€¢ An EPA rule change threatens to gut US climate regulations
  This story is part of MIT Technology Reviewâ€™s â€œAmerica Undoneâ€ series, examining how the foundations of US success in science and innovation are currently under threat.Â You can read the rest here.



The mechanism that allows the US federal government to regulate climate change is on the chopping block.



On Tuesday, US Environmental Protection Agency administrator Lee Zeldin announced that the agency is taking aim at the endangerment finding, a 2009 rule thatâ€™s essentially the tentpole supporting federal greenhouse-gas regulations.



This might sound like an obscure legal situation, but itâ€™s a really big deal for climate policy in the US. So buckle up, and letâ€™s look at what this rule says now, what the proposed change looks like, and what it all means.



To set the stage, we have to go back to the Clean Air Act of 1970, the law that essentially gave the EPA the power to regulate air pollution. (Stick with meâ€”I promise Iâ€™ll keep this short and not get too into the legal weeds.)



There were some pollutants explicitly called out in this law and its amendments, including lead and sulfur dioxide. But it also required the EPA to regulate new pollutants that were found to be harmful. In the late 1990s and early 2000s, environmental groups and states started asking for the agency to include greenhouse-gas pollution.



In 2007, the Supreme Court ruled that greenhouse gases qualify as air pollutants under the Clean Air Act, and that the EPA should study whether theyâ€™re a danger to public health. In 2009, the incoming Obama administration looked at the science and ruled that greenhouse gases pose a threat to public health because they cause climate change. Thatâ€™s the endangerment finding, and itâ€™s what allows the agency to pass rules to regulate greenhouse gases.Â Â 





The original case and argument were specifically about vehicles and the emissions from tailpipes, but this finding was eventually used to allow the agency to set rules around power plants and factories, too. It essentially underpins climate regulations in the US.



Fast-forward to today, and the Trump administration wants to reverse the endangerment finding. In a proposed rule released on Tuesday, the EPA argues that the Clean Air Act does not, in fact, authorize the agency to set emissions standards to address global climate change. Zeldin, in an appearance on the conservative politics and humor podcast Ruthless that preceded the official announcement, called the proposal the â€œlargest deregulatory action in the history of America.â€



The administration was already moving to undermine the climate regulations that rely on this rule. But this move directly targets a â€œfundamental building block of EPAâ€™s climate policy,â€ says Deborah Sivas, an environmental-law professor at Stanford University.



The proposed rule will go up for public comment, and the agency will then take that feedback and come up with a final version. Itâ€™ll almost certainly get hit with legal challenges and will likely wind up in front of the Supreme Court.



One note here is that the EPA makes a mostly legal argument in the proposed rule reversal rather than focusing on going after the science of climate change, says Madison Condon, an associate law professor at Boston University. That could make it easier for the Supreme Court to eventually uphold it, she says, though this whole process is going to take a while.Â 



If the endangerment finding goes down, it would have wide-reaching ripple effects. â€œWe could find ourselves in a couple years with no legal tools to try and address climate change,â€ Sivas says.



To take a step back for a moment, itâ€™s wild that weâ€™ve ended up in this place where a single rule is so central to regulating emissions. US climate policy is held up by duct tape and a dream. Congress could have, at some point, passed a law that more directly allows the EPA to regulate greenhouse-gas emissions (the last time we got close was a 2009 bill that passed the House but never made it to the Senate). But here we are.



This move isnâ€™t a surprise, exactly. The Trump administration has made it very clear that it is going after climate policy in every way that it can. But whatâ€™s most striking to me is that weâ€™re not operating in a shared reality anymore when it comes to this subject.Â 



While top officials tend to acknowledge that climate change is real, thereâ€™s often a â€œbutâ€ followed by talking points from climate denialâ€™s list of greatest hits. (One of the more ridiculous examples isÂ the statement that carbon dioxide is good, actually, because it helps plants.)Â 



Climate change is real, and itâ€™s a threat. And the US has emitted more greenhouse gases into the atmosphere than any other country in the world. It shouldnâ€™t be controversial to expect the government to be doing something about it.&nbsp;



This article is from The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.

ğŸ”’ Cybersecurity & Privacy
â€¢ Scammers Unleash Flood of Slick Online Gaming Sites
  Fraudsters are flooding Discord and other social media platforms with ads for hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. Here&#8217;s a closer look at the social engineering tactics and remarkable traits of this sprawling network of more than 1,200 scam sites.
The scam begins with deceptive ads posted on social media that claim the wagering sites are working in partnership with popular social media personalities, such as Mr. Beast, who recently launched a gaming business called Beast Games. The ads invariably state that by using a supplied &#8220;promo code,&#8221; interested players can claim a $2,500 credit on the advertised gaming website.
An ad posted to a Discord channel for a scam gamblingÂ website that the proprietors falsely claim was operating in collaboration with the Internet personality Mr. Beast. Image: Reddit.com.
The gaming sites all require users to create a free account to claim their $2,500 credit, which they can use to play any number of extremely polished video games that ask users to bet on each action. At the scam website gamblerbeast[.]com, for example, visitors can pick from dozens of games like B-Ball Blitz, in which you play a basketball pro who is taking shots from the free throw line against a single opponent, and you bet on your ability to sink each shot.
The financial part of this scam begins when users try to cash out any &#8220;winnings.&#8221; At that point, the gaming site will reject the request and prompt the user to make a &#8220;verification deposit&#8221; of cryptocurrency &#8212; typically around $100 &#8212; before any money can be distributed. Those who deposit cryptocurrency funds are soon asked for additional payments.

However, any &#8220;winnings&#8221; displayed by these gaming sites are a complete fantasy, and players who deposit cryptocurrency funds will never see that money again. Compounding the problem, victims likely will soon be peppered with come-ons from &#8220;recovery experts&#8221; who peddle dubious claims on social media networks about being able to retrieve funds lost to such scams.
KrebsOnSecurity first learned about this network of phony betting sites from a Discord user who asked to be identified only by their screen name: &#8220;Thereallo&#8221; is a 17-year-old developer who operates multiple Discord servers and said they began digging deeper after users started complaining of being inundated with misleading spam messages promoting the sites.
&#8220;We were being spammed relentlessly by these scam posts from compromised or purchased [Discord] accounts,&#8221; Thereallo said. &#8220;I got frustrated with just banning and deleting, so I started to investigate the infrastructure behind the scam messages. This is not a one-off site, it&#8217;s a scalable criminal enterprise with a clear playbook, technical fingerprints, and financial infrastructure.&#8221;
After comparing the code on the gaming sites promoted via spam messages, Thereallo found they all invoked the same API key for an online chatbot that appears to be in limited use or else is custom-made. Indeed, a scan for that API key at the threat hunting platform Silent Push reveals at least 1,270 recently-registered and active domains whose names all invoke some type of gaming or wagering theme.
The &#8220;verification deposit&#8221; stage of the scam requires the user to deposit cryptocurrency in order to withdraw their &#8220;winnings.&#8221;
Thereallo said the operators of this scam empire appear to generate a unique Bitcoin wallet for each gaming domain they deploy.
&#8220;This is a decoy wallet,&#8221; Thereallo explained. &#8220;Once the victim deposits funds, they are never able to withdraw any money. Any attempts to contact the &#8216;Live Support&#8217; are handled by a combination of AI and human operators who eventually block the user. The chat system is self-hosted, making it difficult to report to third-party service providers.&#8221;
Thereallo discovered another feature common to all of these scam gambling sites [hereafter referred to simply as &#8220;scambling&#8221; sites]: If you register at one of them and then very quickly try to register at a sister property of theirs from the same Internet address and device, the registration request is denied at the second site.
&#8220;I registered on one site, then hopped to another to register again,&#8221; Thereallo said. Instead, the second site returned an error stating that a new account couldn&#8217;t be created for another 10 minutes.
The scam gaming site spinora dot cc shares the same chatbot API as more than 1,200 similar fake gaming sites.
&#8220;They&#8217;re tracking my VPN IP across their entire network,&#8221; Thereallo explained. &#8220;My password manager also proved it. It tried to use my dummy email on a site I had never visited, and the site told me the account already existed. So it&#8217;s definitely one entity running a single platform with 1,200+ different domain names as front-ends. This explains how their support works, a central pool of agents handling all the sites. It also explains why they&#8217;re so strict about not giving out wallet addresses; it&#8217;s a network-wide policy.&#8221;
In many ways, these scambling sites borrow from the playbook of &#8220;pig butchering&#8221; schemes, a rampant and far more elaborate crime in which people are gradually lured by flirtatious strangers online into investing in fraudulent cryptocurrency trading platforms.
Pig butchering scams are typically powered by people in Asia who have been kidnapped and threatened with physical harm or worse unless they sit in a cubicle and scam Westerners on the Internet all day. In contrast, these scambling sites tend to steal far less money from individual victims, but their cookie-cutter nature and automated support components may enable their operators to extract payments from a large number of people in far less time, and with considerably less risk and up-front investment.
Silent Push&#8217;s Zach Edwards said the proprietors of this scambling empire are spending big money to make the sites look and feel like some fancy new type of casino.
&#8220;That&#8217;s a very odd type of pig butchering network and not like what we typically see, with much lower investments in the sites and lures,&#8221; Edwards said.
Here is a list of all domains that Silent Push found were using the scambling network&#8217;s chat API.

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Introducing AWS Batch Support for Amazon SageMaker Training jobs
  Picture this: your machine learning (ML) team has a promising model to train and experiments to run for their generative AI project, but theyâ€™re waiting for GPU availability. The ML scientists spend time monitoring instance availability, coordinating with teammates over shared resources, and managing infrastructure allocation. Simultaneously, your infrastructure administrators spend significant time trying to maximize utilization and minimize idle instances that lead to cost-inefficiency. 
This isnâ€™t a unique story. We heard from customers that instead of managing their own infrastructure and job ordering, they wanted a way to queue, submit, and retry training jobs while using Amazon SageMaker AI to perform model training. 
AWS Batch now seamlessly integrates with Amazon SageMaker Training jobs. This integration delivers intelligent job scheduling and automated resource management while preserving the fully managed SageMaker experience your teams are familiar with. ML scientists can now focus more on model development and less on infrastructure coordination. At the same time, your organization can optimize the usage of costly accelerated instances, increasing productivity and decreasing costs. The following example comes from Toyota Research Institute (TRI): 

 â€œWith multiple variants of Large Behavior Models (LBMs) to train, we needed a sophisticated job scheduling system. AWS Batchâ€™s priority queuing, combined with SageMaker AI Training Jobs, allowed our researchers to dynamically adjust their training pipelinesâ€”enabling them to prioritize critical model runs, balance demand across multiple teams, and efficiently utilize reserved capacity. The result was ideal for TRI: we maintained flexibility and speed while being responsible stewards of our resources.â€ â€“Peter Richmond, Director of Information Engineering
 
In this post, we discuss the benefits of managing and prioritizing ML training jobs to use hardware efficiently for your business. We also walk you through how to get started using this new capability and share suggested best practices, including the use of SageMaker training plans. 
Solution overview 
AWS Batch is a fully managed service for developers and researchers to efficiently run batch computing workloads at different scales without the overhead of managing underlying infrastructure. AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the volume and specific requirements of submitted batch jobs. The service automatically handles the heavy lifting of capacity planning, job scheduling, and resource allocation, so you can focus on your application logic rather than managing underlying infrastructure. 
When you submit a job, AWS Batch evaluates the jobâ€™s resource requirements, queues it appropriately, and launches the necessary compute instances to run the job, scaling up during peak demand and scaling down to zero when no jobs are running. Beyond basic orchestration, AWS Batch includes intelligent features like automatic retry mechanisms that restart failed jobs based on configurable retry strategies, and fair share scheduling to manage equitable resource distribution among different users or projects by preventing a single entity from monopolizing compute resources. This can be especially useful if your organization has production workloads that should be prioritized. AWS Batch has been used by many customers with submit-now, run-later semantics for scheduling jobs and achieving high utilization of compute resources on Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), AWS Fargate, and now SageMaker Training jobs. 
AWS Batch for SageMaker Training jobs consists of the following key components that work together to deliver seamless batch processing: 
 
 Training jobs serve as blueprints that specify how jobs should run, including Docker container images, instance types, AWS Identity and Access Management (IAM) roles, and environment variables 
 Job queues act as holding areas where jobs wait to be executed, with configurable priority levels that determine execution order 
 Service environments define the underlying infrastructure maximum capacity 
 
With these foundations, AWS Batch can retry for transient failures and provide comprehensive queue visualization, addressing critical pain points that have been challenging to address with ML workflows. The integration provides automatic retry for transient failures, bulk job submission, enabling scientists to focus on model improvements instead of infrastructure management. 
To use an AWS Batch queue for SageMaker Training jobs, you must have a service environment and a job queue. The service environment represents the Amazon SageMaker AI capacity limits available to schedule, expressed through maximum number of instances. The job queue is the scheduler interface researchers interact with to submit jobs and interrogate job status. You can use the AWS Batch console, or AWS Command Line Interface (AWS CLI) to create these resources. In this example, we create a First-In-First-Out (FIFO) job queue and a service environment pool with a limit of five ml.g5.xlarge instances using the AWS Batch console. The following diagram illustrates the solution architecture. 
 
Prerequisites 
Before you deploy this solution, you must have an AWS account with permissions to create and manage AWS Batch resources. For this example, you can use these&nbsp;Sample IAM Permissions along with your SageMaker AI execution role. 
Create a service environment 
Complete the following steps to create the service environment you will associate with the training job queue: 
 
 On the AWS Batch console, choose Environments in the navigation pane. 
 Choose Create environment, then choose Service environment. 
 
 
 
 Provide a name for your service environment (for this post, we name it ml-g5-xl-se). 
 Specify the maximum number of compute instances that will be available to this environment for model training (for this post, we set it to 5). You can update the value for your capacity limit later as needed. 
 Optionally, specify tags for your service environment. 
 Create your service environment. 
 
Create a job queue 
Complete the following steps to create your job queue: 
 
 On the AWS Batch console, choose Job queues in the navigation pane. 
 Choose Create job queue. 
 For Orchestration type, select SageMaker Training. 
 
 
 
 Provide a name for your job queue (for this post, we name it my-sm-training-fifo-jq). 
 For Connected service environment, choose the service environment you created. 
 Leave the remaining settings as default and choose Create job queue. 
 
 
You can explore fair-share queues by reading more about the scheduling policy parameter. Additionally, you can use job state limits to configure your job queue to take automatic action to unblock itself in the event that a user submitted jobs that are misconfigured or remain capacity constrained beyond a configurable period of time. These are workload-specific parameters that you can tune to help optimize your throughput and resource utilization. 
Submit SageMaker Training jobs to AWS Batch from the SageMaker Python SDK 
The newly added aws_batch module within the SageMaker Python SDK allows you to programmatically create and submit SageMaker Training jobs to an AWS Batch queue using Python. This includes helper classes to submit both Estimators and ModelTrainers. You can see an example of this in action by reviewing the sample Jupyter notebooks. The following code snippets summarize the key pieces. 
Complete the basic setup steps to install a compatible version of the SageMaker Python SDK: 
 
 !pip install sagemaker 
 
To use the job queue you configured earlier, you can refer to it by name. The Python SDK has built-in support for the integration within the TrainingQueue class: 
 
 from sagemaker.aws_batch.training_queue import TrainingQueue

JOB_QUEUE_NAME = 'my-sm-training-fifo-jq'
training_queue = TrainingQueue(JOB_QUEUE_NAME) 
 
For this example, we focus on the simplest job that you can run, either a class that inherits from EstimatorBase or ModelTrainer, a hello world job. You can use a ModelTrainer or Estimator, such as PyTorch, instead of the placeholder: 
 
 from sagemaker.session import Session
from sagemaker import image_uris
session = Session()

image_uri = image_uris.retrieve(
&nbsp;&nbsp; &nbsp;framework="pytorch",
&nbsp;&nbsp; &nbsp;region=session.boto_session.region_name,
&nbsp;&nbsp; &nbsp;version="2.5",
&nbsp;&nbsp; &nbsp;instance_type=INSTANCE_TYPE,
&nbsp;&nbsp; &nbsp;image_scope="training"
)
from sagemaker.estimator import Estimator

EXECUTION_ROLE = get_execution_role()
INSTANCE_TYPE = 'ml.g5.xlarge'
TRAINING_JOB_NAME = 'hello-world-simple-job'

estimator = Estimator(
&nbsp;&nbsp; &nbsp;image_uri=image_uri,
&nbsp;&nbsp; &nbsp;role=EXECUTION_ROLE,
&nbsp;&nbsp; &nbsp;instance_count=1,
&nbsp;&nbsp; &nbsp;instance_type=INSTANCE_TYPE,
&nbsp;&nbsp; &nbsp;volume_size=1,
&nbsp;&nbsp; &nbsp;base_job_name=TRAINING_JOB_NAME,
&nbsp;&nbsp; &nbsp;container_entry_point=['echo', 'Hello', 'World'],
&nbsp;&nbsp; &nbsp;max_run=300,
)

training_queued_job = training_queue.submit(training_job=estimator, inputs=None) 
 
Submitting an estimator job is as straightforward as creating the estimator and then calling queue.submit. This particular estimator doesnâ€™t require any data, but in general, data should be provided by specifying inputs. Alternatively, you can queue a ModelTrainer using AWS Batch by calling queue.submit, shown in the following code: 
 
 from sagemaker.modules.train import ModelTrainer
from sagemaker.modules.configs import SourceCode

source_code = SourceCode(command="echo 'Hello World'")

model_trainer = ModelTrainer(
&nbsp;&nbsp; &nbsp;training_image=image_uri,
&nbsp;&nbsp; &nbsp;source_code=source_code,
&nbsp;&nbsp; &nbsp;base_job_name=TRAINING_JOB_NAME,
&nbsp;&nbsp; &nbsp;compute={"instance_type": INSTANCE_TYPE, "instance_count": 1},
&nbsp;&nbsp; &nbsp;stopping_condition={"max_runtime_in_seconds": 300}
)

training_queued_job = training_queue.submit(training_job=model_trainer, inputs=None) 
 
Monitor job status 
In this section, we demonstrate two methods to monitor the job status. 
Display the status of jobs using the Python SDK 
The TrainingQueue can list jobs by status, and each job can be described individually for more details: 
 
 submitted_jobs = training_queue.list_jobs(status="SUBMITTED")
pending_jobs = training_queue.list_jobs(status="PENDING")
runnable_jobs = training_queue.list_jobs(status="RUNNABLE")
scheduled_jobs = training_queue.list_jobs(status="SCHEDULED")
starting_jobs = training_queue.list_jobs(status="STARTING")
running_jobs = training_queue.list_jobs(status="RUNNING")
completed_jobs = training_queue.list_jobs(status="SUCCEEDED")
failed_jobs = training_queue.list_jobs(status="FAILED")

all_jobs = submitted_jobs + pending_jobs + runnable_jobs + scheduled_jobs + starting_jobs + running_jobs + completed_jobs + failed_jobs

for job in all_jobs:
&nbsp;&nbsp; &nbsp;job_status = job.describe().get("status", "")
&nbsp;&nbsp; &nbsp;print(f"Job : {job.job_name}&nbsp;is {job_status}") 
 
After a TrainingQueuedJob has reached the STARTING status, the logs can be printed from the underlying SageMaker AI training job: 
 
 import time

while True:
&nbsp;&nbsp; &nbsp;job_status = training_queued_job.describe().get("status", "")

&nbsp;&nbsp; &nbsp;if job_status in {"STARTING", "RUNNING", "SUCCEEDED", "FAILED"}:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;break

&nbsp;&nbsp; &nbsp;print(f"Job : {training_queued_job.job_name} is {job_status}")
&nbsp;&nbsp; &nbsp;time.sleep(5)

training_queued_job.get_estimator().logs() 
 
Display the status of jobs on the AWS Batch console 
The AWS Batch console also provides a convenient way to view the status of running and queued jobs. To get started, navigate to the overview dashboard, as shown in the following screenshot. 
 
From there, you can choose on the number underneath the AWS Batch job state youâ€™re interested in to see the jobs in your queue that are in the given state. 
 
Choosing an individual job in the queue will bring you to the job details page. 
 
You can also switch to the SageMaker Training job console for a given job by choosing the View in SageMaker link on the AWS Batch job details page. You will be redirected to the corresponding job details page on the SageMaker Training console. 
 
Whether you use the AWS Batch console or a programmatic approach to inspecting the jobs in your queue, it is generally useful to know how AWS Batch job states map to SageMaker Training job states. To learn how that mapping is defined, refer to the Batch service job status overview page found within the Batch user guide. 
Best practices 
We recommend creating dedicated service environments for each job queue in a 1:1 ratio. FIFO queues deliver basic fire-and-forget semantics, whereas fair share scheduling queues provide more sophisticated scheduling, balancing utilization within a share identifier, share weights, and job priority. If you donâ€™t need multiple shares but want to assign a priority on job submission, we recommend creating a fair share scheduling queue and using a single share within it for all submissions. 
This integration works seamlessly with SageMaker Flexible Training Plans (FTP); simply set the TrainingPlanArn as part of the CreateTrainingJob JSON request, which is passed to AWS Batch. If the goal is for a single job queue to keep that FTP fully utilized, setting capacityLimits on the service environment to match the capacity allocated to the flexible training plan will allow the queue to maintain high utilization of all the capacity. 
If the same FTP needs to be shared among many teams, each with a firm sub-allocation of capacity (for example, dividing a 20-instance FTP into 5 instances for a research team and 15 instances for a team serving production workloads), then we recommend creating two job queues and two service environments. The first job queue, research_queue, would be connected to the research_environment service environment with a capacityLimit set to 5 instances. The second job queue, production_queue, would be connected to a production_environment service environment with a capacity limit of 15. Both research and production team members would submit their requests using the same FTP. 
Alternatively, if a strict partition isnâ€™t necessary, both teams can share a single fair share scheduling job queue with separate share identifiers, which allows the queue to better utilize available capacity. 
We recommend not using the SageMaker warm pool feature, because this can cause capacity to be idle. 
Conclusion 
In this post, we covered the new capability to use AWS Batch with SageMaker Training jobs and how to get started setting up your queues and submitting your jobs. This can help your organization schedule and prioritize jobs, freeing up time for your infrastructure admins and ML scientists. By implementing this functionality, your teams can focus on their workloads and not waste time managing and coordinating infrastructure. This capability is especially powerful using SageMaker training plans so that your organization can reserve capacity in the quantity you need, during the time you need it. By using AWS Batch with SageMaker AI, you can fully utilize the training plan for the most efficiency. We encourage you to try out this new capability so it can make a meaningful impact in your operations! 
 
About the Authors 
James Park is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In his spare time he enjoys seeking out new cultures, new experiences, and staying up to date with the latest technology trends. 
David Lindskog is a Senior Software Engineer at AWS Batch. David has worked across a broad spectrum of projects at Amazon, and specializes in designing and implementing complex, scalable distributed systems and APIs that solve challenging technical problems. 
Mike Moore is a Software Development Manager at AWS Batch. He works in high performance computing, with a focus on the application of simulation to the analysis and design of spacecraft and robotic systems. Prior to joining AWS, Mike worked with NASA to build spacecraft simulators to certify SpaceX Dragon and CST-100â€™s ascent abort systems for crew flight readiness. He lives in Seattle with his wife and daughter, where they enjoy hiking, biking, and sailing. 
Mike Garrison&nbsp;is a Global Solutions Architect based in Ypsilanti, Michigan. Utilizing his twenty years of experience, he helps accelerate tech transformation of automotive companies. In his free time, he enjoys playing video games and travel. 
Michelle Goodstein is a Principal Engineer on AWS Batch. She focuses on scheduling improvements for AI/ML to drive utilization, efficiency, and cost optimization, as well as improved observability into job execution lifecycle and efficiency. She enjoys building innovative solutions to distributed systems problems spanning data, compute, and AI/ML. 
Michael Oguike is a Product Manager for Amazon SageMaker AI. He is passionate about using technology and AI to solve real-world problems. At AWS, he helps customers across industries build, train, and deploy AI/ML models at scale. Outside of work, Michael enjoys exploring behavioral science and psychology through books and podcasts. 
Angel Pizarro is a Principal Developer Advocate for HPC and scientific computing. His background is in bioinformatics application development and building system architectures for scalable computing in genomics and other high throughput life science domains. 
Tom Burggraf is the Head of Product for AWS Batch, where he champions innovative features that help research platform builders achieve unprecedented scale and operational efficiency. He specializes in identifying novel ways to evolve AWS Batch capabilities, particularly in democratizing high-performance computing for complex scientific and analytical workloads. Prior to AWS, he was a product leader in FinTech and served as a consultant for product organizations across multiple industries, bringing a wealth of cross-industry expertise to cloud computing challenges.
â€¢ Structured outputs with Amazon Nova: A guide for builders
  Developers building AI applications face a common challenge: converting unstructured data into structured formats. Structured output is critical for machine-to-machine communication use cases, because this enables downstream use cases to more effectively consume and process the generated outputs. Whether itâ€™s extracting information from documents, creating assistants that fetch data from APIs, or developing agents that take actions, these tasks require foundation models to generate outputs in specific structured formats. 
We launched constrained decoding to provide reliability when using tools for structured outputs. Now, tools can be used with Amazon Nova foundation models (FMs) to extract data based on complex schemas, reducing tool use errors by over 95%. 
In this post, we explore how you can use Amazon Nova FMs for structured output use cases. 
Techniques for implementing structured outputs 
When addressing the requirements for structured outputs use cases, there are two common approaches for implementation. You can modify the system prompt or take advantage of tool calling. For example, in a customer support use case, you might want the model to output a JSON with its response to the user and the current sentiment. So, the system prompt would be modified to include the expected structure: 
 
 Make sure your final response is valid JSON that follows the below response schema: 

##Response schema
```json
{
   "response": "the response to the customer",
   "sentiment": "the current customer sentiment"
}``` 
 
The other option is to provide a tool configuration. Tool calling is the act of providing an API, code function, or schema (or structure) required by your end application to the model through the request schema with the Converse API. This is most used when building agentic applications but is also frequently used in structured output use cases because of the ability to define a set schema that the model should adhere to. 
 
 tool_config = {
    "tools": [ 
        {
            "toolSpec": {
                "name": "respondToUser",
                "description": "the formatted response to the customer",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "response": {
                            "description": "the response to the customer",
                            "type": "string"
                        },
                        "sentiment": {
                            "description": "the current customer sentiment",
                            "type": "string"
                        }
                    },
                    "required": [
                        "response",
                        "sentiment"
                    ]
                }
            }
        }
    ]
} 
 
Both approaches can be effective prompting techniques to influence the model output. However, the output is still non-deterministic and there is room for failure. In our work with customers to implement use cases such as agentic workflows and applications and structured extraction, weâ€™ve observed that the accuracy of the model tends to decrease as the schema becomes more complex. 
Structured output with Amazon Nova models 
Based on these learnings, we have implemented constrained decoding in our system to help ensure high model reliability in the output generated and to allow the model to handle complex schemas with ease. Constrained decoding relies on a grammar to constrain the possible tokens a model can output at each step. This is differentiated from the prompting techniques historically used, because this changes the actual tokens a model can choose from when generating an output. For example, when closing a JSON object, the model would be constrained to just a } token to select. Constrained decoding is used every time a tool configuration is passed. Because tool use provides us a specific schema already, we can use that to generate a grammar dynamically, based on the schema desired by the developer. Constrained decoding prevents the model from generating invalid keys and enforces correct data types based on the defined schema. 
Schema definition process 
A key step in using structured outputs with Amazon Nova is to create a tool configuration. The tool configuration provides a standard interface to define the expected output schema. While the primary intent of a tool configuration is to provide external functionality to the model, this JSON interface is used in structured output use cases as well. This can be illustrated using a use case that extracts recipes from online content. To start the integration, we create a tool configuration representing the specific fields we want extracted from the invoices. When creating a tool configuration, it is important to be clear and concise because the property names and descriptions are what inform the model how the fields should be populated. 
 
 tool_config = {
   "tools": [
        {
            "toolSpec": {
                "name": "extract_recipe",
                "description": "Extract recipe for cooking instructions",
                "inputSchema": {
                    "json": {
                        "type": "object",
                        "properties": {
                            "recipe": {
                                "type": "object",
                                "properties": {
                                    "name": {
                                        "type": "string",
                                        "description": "Name of the recipe"
                                    },
                                    "description": {
                                        "type": "string",
                                        "description": "Brief description of the dish"
                                    },
                                    "prep_time": {
                                        "type": "integer",
                                        "description": "Preparation time in minutes"
                                    },
                                    "cook_time": {
                                        "type": "integer",
                                        "description": "Cooking time in minutes"
                                    },
                                    "servings": {
                                        "type": "integer",
                                        "description": "Number of servings"
                                    },
                                    "difficulty": {
                                        "type": "string",
                                        "enum": [
                                            "easy",
                                            "medium",
                                            "hard"
                                        ],
                                        "description": "Difficulty level of the recipe"
                                    },
                                    "ingredients": {
                                        "type": "array",
                                        "items": {
                                            "type": "object",
                                            "properties": {
                                                "name": {
                                                    "type": "string",
                                                    "description": "Name of ingredient"
                                                },
                                                "amount": {
                                                    "type": "number",
                                                    "description": "Quantity of ingredient"
                                                },
                                                "unit": {
                                                    "type": "string",
                                                    "description": "Unit of measurement"
                                                }
                                            },
                                            "required": [
                                                "name",
                                                "amount",
                                                "unit"
                                            ]
                                        }
                                    },
                                    "instructions": {
                                        "type": "array",
                                        "items": {
                                            "type": "string",
                                            "description": "Step-by-step cooking instructions"
                                        }
                                    },
                                    "tags": {
                                        "type": "array",
                                        "items": {
                                            "type": "string",
                                            "description": "Categories or labels for the recipe"
                                        }
                                    }
                                },
                                "required": [
                               ]
                            }
                        },
                        "required": [
                        ]
                    }
                }
            }
        }
    ]
} 
 
After the tool configuration has been created, we can pass it through the Converse API along with the recipe, which will be contained in the user prompt. A system prompt is historically required for structured output use cases to guide the model in how to output the content, in this case we can use it to pass details about the system role and persona. 
 
 import boto3

model_response = client.converse(
    modelId="us.amazon.nova-lite-v1:0",
   system=[{"text": "You are an expert recipe extractor that compiles recipe details from blog posts"}],
    messages=[{"role": "user", "content": content}],
    inferenceConfig={"temperature": 0},
    toolConfig=tool_config
) 
 
By using the native tool use support with constrained decoding, we get a parsed tool call that will follow the correct syntax and expected schema as set in the tool configuration. 
 
 {
    "toolUse": {
        "toolUseId": "tooluse_HDCl-Y8gRa6yWTU-eE97xg",
        "name": "extract_recipe",
        "input": {
            "recipe": {
                "name": "Piacenza Tortelli",
                "description": "Piacenza tortelli, also known as 'tortelli with the tail' due to their elongated shape, are a delicious fresh pasta, easy to make at home!",
                "prep_time": 60,
                "cook_time": 10,
                "servings": 4,
                "difficulty": "hard",
                "ingredients": [
                    {
                        "name": "Type 00 flour",
                        "amount": 2.3,
                        "unit": "cups"
                    },
                    {
                        "name": "Eggs",
                        "amount": 3,
                        "unit": ""
                    },
                    {
                        "name": "Fine salt",
                        "amount": 1,
                        "unit": "pinch"
                    },
                    {
                        "name": "Spinach",
                        "amount": 13.3,
                        "unit": "cups"
                    },
                    {
                        "name": "Cow's milk ricotta cheese",
                        "amount": 1.3,
                        "unit": "cups"
                    },
                    {
                        "name": "Parmigiano Reggiano PDO cheese",
                        "amount": 4.2,
                        "unit": "oz"
                    },
                    {
                        "name": "Fine salt",
                        "amount": 1,
                        "unit": "to taste"
                    },
                    {
                        "name": "Nutmeg",
                        "amount": 1,
                        "unit": "to taste"
                    },
                    {
                        "name": "Butter",
                        "amount": 80,
                        "unit": "g"
                    },
                    {
                        "name": "Sage",
                        "amount": 2,
                        "unit": "sprigs"
                    }
                ],
                "instructions": [
                    "Arrange the flour in a mound and pour the eggs into the center 1; add a pinch of salt and start working with a fork 2, then knead by hand 3.",
                    "You should obtain a smooth dough 4; wrap it in plastic wrap and let it rest for half an hour in a cool place.",
                    "Meanwhile, prepare the filling starting with the spinach: immerse them in boiling salted water 5 and blanch them for a few minutes until wilted 6.",
                    "Drain the spinach and transfer them to cold water 7, preferably with ice. Then squeeze them very well 8 and chop them finely with a knife 9.",
                    "Place the chopped spinach in a bowl, add the ricotta 10, salt, pepper, and nutmeg 11. Also add the grated Parmigiano Reggiano DOP 12.",
                    "Mix well until you get a homogeneous consistency 13.",
                    "At this point, take the dough that has now rested 14, take a portion of it keeping the rest covered. Lightly flatten the dough with a rolling pin 15.",
                    "Roll it out with a pasta machine 16; as you reduce the thickness, fold the dough over itself 17 and roll it out again 18.",
                    "You should get a very thin rectangle, about 0.04-0.08 inches thick 19. Cut 2 strips of dough by dividing the rectangle in half lengthwise 20, then cut out diamonds of 4 inches 21.",
                    "Fill the diamonds with the spinach filling 22 and close them. To do this, bring one of the two longer points inward 23, then fold the two side points towards the center 24.",
                    "Now close the tortello by pinching the dough in the center and moving gradually towards the outside 25. The movement is similar to the closure of culurgiones. Continue in this way until the dough and filling are finished 26; you will get about 40-45 pieces.",
                    "Place a pot full of salted water on the stove. Meanwhile, in a pan, pour the butter and sage 27. Turn on the heat and let it flavor.",
                    "Then cook the tortelli for 5-6 minutes 28, then drain them and toss them in the butter and sage sauce 29.",
                    "Plate and serve the Piacenza tortelli with plenty of grated Parmigiano Reggiano DOP 30!"
                ],
                "tags": [
                    "vegetarian",
                    "Italian"
                ]
            }
        }
    }
} 
 
Now, with constrained decoding, we can use a smaller model such as Amazon Nova Lite to output a large and complex JSON schema to use in our application. For image-based use cases with complex schemas, we recommend that you use Nova Pro or Nova Premier for the best performance. 
Conclusion 
By using structured output with Amazon Nova through tool calling, you can take advantage of the key benefits of constrained decoding and build a reliable system. We encourage you to try this out in your applications today. Learn more at the Amazon Nova User Guide. Get started building your AI applications with Amazon Nova in the&nbsp;Amazon Bedrock console. 
 
About the authors 
Jean Farmer is a Generative AI Solutions Architect on the Amazon Artificial General Intelligence (AGI) team, specializing in agentic applications. Based in Seattle, Washington, she works at the intersection of autonomous AI systems and practical business solutions, helping to shape the future of AGI at Amazon. 
Mukund Birje is a Sr. Product Marketing Manager on the AIML team at AWS. In his current role heâ€™s focused on driving adoption of Amazon Nova Foundation Models. He has over 10 years of experience in marketing and branding across a variety of industries. Outside of work you can find him hiking, reading, and trying out new restaurants. You can connect with him on LinkedIn.
â€¢ AI agents unifying structured and unstructured data: Transforming support analytics and beyond with Amazon Q Plugins
  As organizations seek to derive greater value from their AWS Support data, operational teams are looking for ways to transform raw support cases and health events into actionable insights. While traditional analytics tools can provide basic reporting capabilities, teams need more sophisticated solutions that can understand and process natural language queries about their operational data. Retrieval-Augmented Generation (RAG) architecture forms the foundation for optimizing large language model outputs by referencing authoritative knowledge bases outside of their training data before generating responses. This architecture uses the power of semantic search and information retrieval capabilities to enhance accuracy. 
In our previous blog post, Derive meaningful and actionable operational insights from AWS Using Amazon Q Business, we introduced a RAG-based solution using Amazon Q Business. However, while this approach excels at semantic search, it can face challenges with precise numerical analysis and aggregations. In this post, we address these limitations by showing how you can enhance Amazon Q with custom plugins to improve support analytics and incident response. Our solution combines the strengths of RAG with structured data querying, enabling more accurate answers to analytical questions about your AWS Support data. Detailed deployment instructions for the solution in this post are available in our AWS Samples GitHub repository. 
Understanding RAGâ€™s design for analytics 
RAG architecture excels at finding relevant information through context-based matching, enabling natural language interactions with external data. However, the fundamental architecture of vector similarity search, which prioritizes semantic relevance over analytical aggregation capabilities, can introduce significant inaccuracies. Letâ€™s examine this limitation with a simple query: 
 
 Give me total count of support cases in Feb 2025 
 
The following is a RAG-only response indicating that the system attempts to piece together information from various documents, reporting 190 cases through potentially hallucinated aggregations. 
 
 Based on the provided JSON files, there were 190 AWS support cases documented in 
February 2025, calculated from the following case counts: 17 cases from Feb 1-3, 
12 cases from Feb 4-5, 21 cases from Feb 7-8, 21 cases from Feb 8-10, 16 cases from 
Feb 10-11, 10 cases from Feb 11-12, 23 cases from Feb 19-20, 22 cases from Feb 20-21,
27 cases from Feb 23-24, and 21 cases from Feb 25-26. 
 
However, when enhanced with structured data querying capabilities, the same query yields 958 support cases, the accurate count verified against the actual dataset: 
 
 Based on the query results, there were a total of 958 support cases in February 2025. 
 
This limitation becomes even more apparent with moderately complex queries that require categorical analysis: 
 
 Give me OpenSearch Support cases, distribution based on severity, opened in Jan 2025? 
 
The following is a RAG-only response showing limited or inaccurate categorical breakdown: 
 
  
  Based on the support cases data from January 2025, here is the severity distribution 
for OpenSearch cases: 

Urgent: One case was opened regarding cluster health degradation and production system 
downtime

Normal: Two cases were identified:
1. A case about OpenSearch RI Exchange
2. A case concerning blue/green deployment upgrade from version 1.3 to 2.17

The cases demonstrate varying levels of urgency, with one critical production issue 
and two normal severity cases requiring standard response times. 
  
 
With structured data querying, we get precise severity distributions: 
 
 
  
  Based on the query results for OpenSearch Support cases opened in January 2025,
here's the distribution by severity:
1. Normal severity: 4 cases
2. Critical severity: 4 cases
3. High severity: 3 cases
4. Low severity: 2 cases
5. Urgent severity: 1 case

In total, there were 14 OpenSearch Support cases opened in January 2025. 
The distribution shows that Normal and Critical severity cases were the most common, 
followed by High severity cases. There were fever Low and Urgent severity cases 
during this period. 
  
 
The gap widens further when dealing with multi-dimensional analysis. Consider the following multi-dimensional query expecting detailed analysis with account-service combinations and temporal patterns. 
Query: 
 
 Find accounts with frequent high-priority support cases since January 2024, 
excluding limit increase requests. For each account and AWS service combination, 
show the total number of cases and how many different days had issues. 
Only include results where there were more than 3 cases, and sort results by the 
highest number of cases first. 
 
Response: 
 
  
  Based on the analysis of high-priority support cases since January 2024, excluding
limit increase requests, here are the key findings:

1. The account with the most frequent high-priority cases is 111111111111, with 
60 cases for the Connect service.
2. The top 5 account and service combinations with the highest number of cases are:
* Account 111111111111 (Connect): 60 cases
* Account 222222222222 (RDS Oracle): 52 cases
* Account 333333333333 (OpenSearch Service): 38 cases
* Account 444444444444 (Aurora PostgreSQL): 25 cases
* Account 555555555555 (EMR): 23 cases

The services that appear most frequently in the top results are:
* Aurora PostgreSQL
* OpenSearch Service
* RDS Oracle
* EMR
* DynamoDB

Interestingly, the number of total cases matches the number of unique days with 
cases for each account-service combination. This suggests that these accounts 
are experiencing issues on different days rather than multiple issues on the 
same day.

There are 89 account-service combinations that have more than 3 high-priority 
cases since January 2024.

Some accounts appear multiple times with different services, indicating they 
may be experiencing issues across various AWS offerings. For example:
* Account 222222222222 appears with RDS Oracle (52 cases), DynamoDB (18 cases), 
and Aurora PostgreSQL (6 cases)
* Account 111111111111 appears with Connect (60 cases) and other services

This information can be useful for identifying accounts that may need additional 
support or proactive engagement to address recurring issues. Would you like to 
focus on a specific aspect of this data or explore further details about 
particular accounts or services? 
  
 
The following is a RAG-only response showing inability to process the same complex query: 
 
 Based on the provided data source, I cannot provide a complete analysis of 
high-priory support cases and their frequency patterns. While there are support 
cases mentioned in the data from January 2024, the data does not contain 
sufficient information about case priorities or enough cases per account 
to identity patterns meeting the threshold of more than 3 cases. 
Additionally, while some technical and customer service cases are shown, 
the data is fragmented across different dates without providing a comprehensive 
view of case volumes or frequencies per account and service. 
 
These examples demonstrate how RAG alone struggles with analytical queries of increasing complexity, while our plugin-enhanced solution delivers precise, structured analysis at every level. When working with scenarios requiring precise numerical analysis, we can enhance RAGâ€™s capabilities through structured approaches: 
 
 Aggregation and pattern analysis: When user prompts include aggregation queries (such as counts, totals, or distributions), they require exact numerical computation through structured querying to provide precise results. Vector similarity search alone cannot guarantee accurate numerical aggregations, making structured metadata querying essential for these analytical use cases. 
 Context and correlation analysis: External unstructured data requires thoughtful data engineering to extract and maintain structured metadata (such as creation dates, categories, severity levels, and service types). While RAG excels at finding semantically similar content, having well-defined metadata enables precise filtering and querying capabilities. For example, when analyzing system performance issues, structured metadata about incident timing, affected services, and their dependencies enables comprehensive impact analysis through exact querying rather than relying solely on semantic matching. 
 
Enhancing Q Support-Insights with agentic AI 
Building on the Q Support-Insights (QSI) solution introduced in Derive meaningful and actionable operational insights from AWS Using Amazon Q Business,&nbsp;weâ€™ll demonstrate how to enhance analytical capabilities through agentic AI by creating custom plugins. This enhancement preserves QSIâ€™s base implementation while adding precise analytical processing through structured metadata querying. 
QSI overview 
The Amazon Q Support Insights (QSI) solution consists of two main components: 
 
 Data collection Pipeline 
   
   Support Collector module using AWS Lambda functions 
   The Support data consists of AWS Support cases, Health events, and&nbsp;Trusted Advisor checks 
   Amazon EventBridge for automated data&nbsp;collection.&nbsp;The data pipeline enables two synchronization mechanisms: 
     
     Real-time case updates: Processes AWS Support cases through event-based triggers (CreateCase, AddCommunicationToCase, ResolveCase, ReopenCase). 
     Historical and daily sync: Performs initial historical data sync and refreshes AWS Trusted Advisor data daily. 
      
   Stores data in JSON format in centralized Amazon Simple Storage Service (Amazon S3) bucket 
   Supports multi-account data aggregation through AWS Organizations 
    
 Amazon Q Business application environment 
   
   Amazon Q Business application deployment 
   Amazon S3 connector for data source integration 
   Web experience configuration for user interaction 
   Authentication through AWS IAM Identity Center 
    
 
Enabling query aggregation with custom plugins for Amazon Q Business&nbsp; 
Custom plugins extend Amazon Q Business to combine semantic search with precise analytics capabilities. The following implementation details outline how weâ€™ve augmented the base QSI solution: 
 
 Augments QSIâ€™s natural language processing with structured query capabilities 
 Converts analytical requests into precise Amazon Athena SQL using an Amazon Bedrock large language model (LLM) 
 Executes queries against structured metadata tables 
 Provides exact numerical results alongside semantic search responses 
 
Metadata processing 
To enable precise querying capabilities, weâ€™ve implemented a robust metadata processing framework that structures and indexes support data: 
 
 Using QSIâ€™s existing data collection in Amazon S3 
 Extracts structured metadata from support cases into two queryable tables 
 Maintains critical fields like dates, categories, severity levels 
 Enables precise filtering and aggregation capabilities 
 
Cross-source analysis 
The solution enables comprehensive analysis across multiple data sources through intelligent correlation capabilities: 
 
 Correlates support cases with health events 
 Enables comprehensive operational assessment 
 Supports pattern detection across multiple data sources 
 
Architecture overview 
The following illustration shows how the enhanced Amazon Q Business solution integrates custom plugins with the existing QSI architecture. The diagram demonstrates the flow from user query through analytical processing to response generation, highlighting how structured data querying complements the RAG-based semantic search capabilities. 
 
When a user submits an analytical query like Show me OpenSearch case distribution based on severity, the following process occurs: 
 
 The custom plugin detects the analytical intent and triggers a Lambda function 
 The Lambda function uses an Amazon Bedrock LLM to transform the natural language query into precise Athena SQL 
 The query executes against structured metadata in Athena 
 Results are synthesized back into natural language and presented alongside traditional RAG responses 
 
This approach enables precise analytics while maintaining the conversational interface users expect. 
Implementation example 
Hereâ€™s a sample interaction demonstrating the enhanced capabilities. Letâ€™s start with a basic query to understand the overall OpenSearch case volume, using structured data querying: 
Query: 
 
 Give me OpenSearch cases in Jan 2025 
 
Response: 
 
 Based on the query results for OpenSearch cases in January 2025, here's a summary:
1. Total cases: 15
2. Severity breakdown:
Critical: 4 cases
High: 3 cases
Urgent: 1 case
Normal: 3 cases
Low: 2 cases
3. Status: All cases are resolved
4. Most common issues:
Cluster health problems (yellow status)
Blue/Green deployment issues
Performance concerns (high CPU utilization)
5. Services involved:
OpenSearch Service Managed Cluster: 13 cases
OpenSearch Service Ingestion: 2 cases

The cases span various topics including cluster health, deployments, performance, 
and configuration issues. Is there a specific aspect of these cases you'd like 
more information on? 
 
Now, using RAG capabilities to analyze unstructured case content for architectural insights: 
Query: 
 
 Out of these, what are the OpenSearch issues that are related to architecture 
deficiencies?  
 
Response: 
 
 Based on the available data, several architectural issues have been identified in OpenSearch deployments:

* The shard count per node exceeding recommended limits is a significant 
architectural concern, with validation errors occurring when scaling up 
node counts. The recommended limit is 1,000 shards per node and exceeding 
this requires careful architectural planning. 

* Master node scaling is another architectural challenge, where insufficient 
master node capacity can limit cluster expansion. This requires proper 
configuration of master node instance types according to the number of nodes 
and shards in the domain. 

* Resource utilization issues, particularly around JVM Memory Pressure and 
CPU utilization, indicate potential architectural shortcomings. 
The system architecture should be designed to maintain JVM Memory Pressure 
below 80% and CPU Utilization below 85% for optimal performance. 

* The system's handling of blue/green deployments shows architectural limitations, 
as these operations cause performance impacts and increased latency during data 
migration to new nodes.  
 
Continuing with RAG-based analysis of case details to evaluate implementation patterns: 
Query: 
 
 Given the high shard count, JVM, or other data can we make inferences about whether 
they are using this correctly? 
 
Response: 
 
  
  Based on the data discussed, there are strong indicators that the OpenSearch system 
is not being used optimally:

JVM Memory Management Issues:
1. Memory pressure consistently exceeded recommended thresholds (above 78% and 
eventually over 90%)
2. The 64GB initial heap size configuration proved inadequate for the workload
3. Circuit breaking exceptions and cluster blocks occurred due to memory issues

Shard Management Problems:
1. The system appears to have exceeded the recommended limit of 1,000 shards 
per node
2. Too many small shards created unnecessary system strain
Large shards complicated failure recovery processes

Resource Utilization:
1. Failed to maintain recommended thresholds:
2. JVM Memory Pressure should be below 80%
3. CPU Utilization should be below 85%
4. Continuous memory pressure issues

Failed snapshots
These indicators strongly suggest improper sizing and resource allocation for 
the workload, pointing to incorrect implementation or configuration of the 
OpenSearch system. Better capacity planning and adherence to best practices 
would be necessary for optimal performance. 
  
 
Finally, hereâ€™s another example of structured data analysis with a complex query demonstrating multi-dimensional aggregation capabilities: 
Query: 
 
 For high-severity support cases since January 1, 2024, show the number of 
total cases and after-hours cases for each account and service combination. 
Define after-hours as before 1 PM UTC, after 1 AM UTC, or times falling within 
the weekend. Only include results with more than 3 after-hours cases. Sort by 
the highest number of after-hours cases. Exclude limit increase requests. 
 
Response: 
 
 Based on the analysis of high-severity support cases since January 1, 2024, 
here's a summary of the results, focusing on accounts and services with more 
than 3 after-hours cases:

1. The account with the highest number of after-hours cases (48) is 111111111111, 
for the Connect service. They have a total of 66 high-severity cases.
2. The second highest is account 333333333333 with 40 after-hours cases for the 
OpenSearch Service Managed Cluster, out of 49 total cases.
3. Account 222222222222 has 38 after-hours cases for RDS Oracle, out of 64 
total cases.
4. EMR service for account 555555555555 has 19 after-hours cases out of 23 
total cases.
5. Bedrock service for account 666666666666 has 17 after-hours cases out of 
24 total cases.

The results are sorted by the highest number of after-hours cases, and limit 
increase requests have been excluded as requested. After-hours cases are 
defined as those created before 1 PM UTC, after 1 AM UTC, or times falling 
within the weekend.

Would you like more details on a specific account or service, or do you need 
a further analysis of this data? 
 
The response combines precise numerical analysis with semantic understanding, providing actionable insights for operational reviews. 
Benefits and impact 
This plugin architecture delivers several key improvements: 
 
 Precise analytics: Exact counts and distributions replace approximate semantic matching 
 Contextual analysis: Maintains analytical context across conversation threads 
 Architectural understanding: Better correlation of related issues through structured analysis 
 
Deploy the Amazon Q Business application 
The following is a simplified deployment process. For detailed instructions, see the Amazon Q Business application creation module. 
Prerequisites 
 
 AWS CloudShell is recommended since, it comes pre-installed with the required libraries and tools. Alternatively, you can use a local machine with the AWS Command Line Interface (AWS CLI) installed and configured with valid credentials. 
 Two S3 buckets: 
   
   Support data bucket for storing AWS Support case data 
   Resource bucket as temporary storage for Lambda resources for deployment (can be deleted after deployment) 
    
 IAM Identity Center instance configured 
 The solution needs AWS Support data collected using the&nbsp;Support Data Pipeline. You can deploy now and add data later, but functionality depends on data availability in your S3 bucket. 
 Access to Anthropicâ€™s Cloud 3-5 Sonnet through Amazon Bedrock. See Add or remove access to Amazon Bedrock foundation models 
 The default database should exist in Athena. If not, you can create one using Athena Query Editor to create the database. 
 
Deployment steps 
You can us the following script to deploy the Q solution. No manual steps are neededâ€”the script handles stack creation and configuration automatically. 
 
 # Clone the repository
git clone https://github.com/aws-samples/support-insights-with-amazon-q.git
cd q_application
chmod&nbsp;+x deploy_q_stacks.sh
./deploy_q_stacks.sh 
 
Clean up 
To remove the resources, delete the S3 buckets and CloudFormation stacks. Delete the CloudFormation stacks in the following order: 
 
 case-metadata-stack 
 amazon-q-stack 
 custom-plugin-stack 
 
Note that this wonâ€™t delete the existing S3 buckets, you must manually delete the S3 buckets. 
Conclusion 
By combining RAGâ€™s semantic understanding with precise analytical capabilities through plugins, weâ€™ve transformed Amazon Q Business into a powerful operational analytics platform. In the examples in this post, you can see how organizations can use this enhancement to derive more accurate and actionable insights from their AWS Support data, supporting better operational decision-making and proactive issue resolution. While demonstrated through support data analytics for operational improvements, these patterns apply across domains that combine structured and unstructured data sources. 
Learn more 
 
 Explore the Amazon Q documentation to understand more about building custom plugins 
 Check out these related resources: 
   
   Getting Started with Amazon Q Business 
   Plugins for Amazon Q Business 
   Amazon Q Business FAQs 
   About the AWS Support API 
    
 
For questions and feedback, visit the AWS re:Post&nbsp;or contact AWS Support. 
 
About the authors 
Chitresh Saxena is a Sr. AI/ML specialist TAM specializing in generative AI solutions and dedicated to helping customers successfully adopt AI/ML on AWS. He excels at understanding customer needs and provides technical guidance to build, launch, and scale AI solutions that solve complex business problems. 
Kevin Morgan is a Sr. Enterprise Support Manager at AWS who helps customers accelerate their cloud adoption journey through hands-on leadership and technical guidance. As a member of the NextGen Developer Experience TFC, he specializes in Builder Experience, CloudOps and DevOps. Outside of work, Kevin enjoys being a Game Master for D&amp;D and is a retro computing enthusiast.
â€¢ Amazon Strands Agents SDK: A technical deep dive into agent architectures and observability
  The Amazon Strands Agents SDK is an open source framework for building AI agents that emphasizes a model-driven approach. Instead of hardcoding complex task flows, Strands uses the reasoning abilities of modern large language models (LLMs) to handle planning and tool usage autonomously. Developers can create an agent with a prompt (defining the agentâ€™s role or behavior) and a list of tools, and the LLM-powered agent will figure out how to chain its reasoning and invoke tools as needed. This dramatically simplifies agent development compared to traditional workflow-based frameworks. 
In this post, we first introduce the Strands Agents SDK and its core features. Then we explore how it integrates with AWS environments for secure, scalable deployments, and how it provides rich observability for production use. Finally, we discuss practical use cases, and present a step-by-step example to illustrate Strands in action. 
What is the Strands Agents SDK? 
The Strands Agents SDK is an open source framework designed to simplify the creation of robust LLM-powered AI agents. Rather than requiring developers to handcraft complex workflows, Strands embraces a model-driven approach centered around three key components: a language model, a system prompt, and a set of tools. This architecture empowers the LLM to perform the crucial reasoning, autonomously deciding the optimal actions and when to use tools based on the current context and task. This model-driven design allows agents to be flexible, intelligent, and autonomous, while minimizing the boilerplate code typically needed to support multi-step or multi-agent interactions. Its effectiveness is already proven â€“ Strands is actively used in production by multiple AWS teams for their AI agents in production, including Kiro, Amazon Q, and AWS Glue. 
Key capabilities of the Strands Agents SDK 
The Strands Agents SDK offers the following key capabilities: 
 
 Lightweight, flexible agent loop â€“ Strands implements a simple yet extensible agent loop that drives the interaction. The LLM behind the agent iteratively reads the conversation (and context), plans an action, possibly calls a tool, and then incorporates the toolâ€™s result before deciding the next step, until it reaches a final answer. This loop is fully customizable when needed, but works out of the box for most use cases. 
 Tool use and integration â€“ Tools are external functions or APIs the agent can call (for example, calculators, web search, or database queries). Strands makes it straightforward to define tools in Python with a @tool decorator and supply them to agents. During development, the SDK supports hot-reloading, so you can modify or add tools and have them picked up automatically without restarting the agent. This accelerates iteration and testing. The SDK also comes with an optional library of pre-built tools (strands-agents-tools) for common functionalities like arithmetic, web requests, and more. Strands supports both the Model Context Protocol (MCP) and A2A (Agent-to-Agent). MCP is an open standard that gives agents access to thousands of external tools hosted on model servers, greatly expanding their capabilities without custom coding. A2A allows agents to call each other as tools â€“ enabling powerful multi-agent collaboration and specialization with minimal overhead. 
 Model-agnostic and multi-model support â€“ Strands is not tied to a single LLM provider. It can work with models on Amazon Bedrock (for example, Anthropicâ€™s Claude or other Amazon Bedrock models) by default, but also supports Anthropicâ€™s API; open source models such as LlamaAPI, Ollama, OpenAI; and others through a pluggable provider interface. For example, you can switch the agentâ€™s model from Anthropicâ€™s Claude hosted on Amazon Bedrock to a local Meta Llama 3 or OpenAI GPT-4 by changing the model provider in the code. With this flexibility, you can choose the model that best fits your needs or swap models in different deployments. 
 Scalability from prototypes to production â€“ The same Strands agent code can run locally for quick testing and then be deployed to AWS for production use. The SDK is already used internally at AWS for agent-based features in services like Amazon Q (developer assistant), AWS Glue, and VPC Reachability Analyzer. It supports running agents in various environments â€“ including Amazon Elastic Compute Cloud (Amazon EC2), AWS Lambda, AWS Fargate, and Amazon Bedrock AgentCore â€“ and isolating tool execution from the agent for security and reliability. Strands agents can run anywhere and integrate with cloud services, but because itâ€™s open source, you can also run them on premises or in other clouds. 
 Advanced use cases â€“ Although Strands excels with simple single-agent assistants, it also supports more complex agent systems. You can compose multi-agent applications where multiple agents collaborate or coordinate (for example, an agent that delegates subtasks to other specialist agents). The SDK supports patterns like agent hierarchies, agent networks, and even swarm-style cooperation (discussed more in the next section). It also allows building fully autonomous agents that loop on tasks without human input, enabling multi-step workflows where the agentâ€™s chain-of-thought spans many tool calls or intermediate decisions. Additionally, features like streaming responses (token streaming) are supported for real-time agent interactions. 
 Open source and community contributions â€“ The Strands Agents SDK is Apache-2.0 licensed and open to contributions. Several companies (Accenture, Anthropic, Meta, PwC, and others) have already contributed to its development. For example, Anthropic contributed integration for their API, and Meta added support for their Llama models. This community-driven approach means the tool and model ecosystem is growing beyond AWS. Developers are encouraged to join the project on GitHub to report issues, add new tools or model providers, and help expand the framework. 
 
Strands provides a concise yet powerful way to build AI agents. With a few lines of Python, you define an agentâ€™s role and its available tools, and the SDK handles the rest â€“ from reasoning through a problem to invoking tools and producing answers. The next sections explore how Strands supports various agent architectures and how it makes these agents observable in production. 
Agent-architectural patterns supported by Strands 
Strands supports multiple agent architecture patterns, scaling from a single self-contained agent up to complex networks of cooperating agents. In this section, we explain the key patterns and how the Strands SDK enables them. 
Single-agent pattern 
The simplest scenario is a single AI agent endowed with an LLM and (optionally) some tools, which interacts with a user or performs a job without delegating to other agents. In Strands, a single-agent is represented by the Agent class â€“ you initialize it with a model (or accept the default) and tools it can use. The agent runs an internal event loop to decide how to answer each query: it might directly respond using the modelâ€™s knowledge, or choose to invoke one of its tools, incorporate the result, potentially call more tools, and so on until it finishes. This loop continues until the agent produces a final answer. 
In code, a single-agent usage is straightforward. In the following example code, we create a basic agent with a calculator tool and ask it a question: 
 
 from&nbsp;strands import&nbsp;Agent
from&nbsp;strands_tools import&nbsp;calculator 

# Create an agent that can use a calculator tool
agent&nbsp;=&nbsp;Agent(tools=[calculator])
result&nbsp;=&nbsp;agent("What is the square root of 1764?")
print(result) 
 
In this example, the agent uses its LLM to interpret the question. It recognizes that it might need to calculate a square root, so it calls the provided calculator tool (a simple arithmetic function) to get the result, and then returns the answer. Strands handles the prompt formatting, calling the tool, and inserting the toolâ€™s result back into the modelâ€™s context for the final answer. With a single agent pattern, reasoning and tool use happen within one agent process. 
A single agent is suitable for many tasks, such as question-answering, data retrieval, simple assistants. It keeps the logic self-contained. However, as tasks grow in complexity, you might reach the limits of what one agent (even with tools) can effectively manage (for example, if multiple different expertise or concurrent actions are needed). Thatâ€™s where the multi-agent patterns come in. 
Multi-agent networks (swarm or peer-to-peer agents) 
Strands supports agent networks where multiple agents operate and communicate to solve problems collaboratively. In such a network, there is no single orchestrator; instead, agents interact peer-to-peer or in an open topology. This pattern is sometimes referred to as a swarm of agents working together. Each agent in the network might have a specialized role or perspective, and they share information to converge on a solution. 
In a swarm-style network, communication patterns can vary. One common approach is a mesh communication where agents can talk to other agents freely. This is useful for brainstorming agents or consensus-building, where agents exchange ideas. Other communication schemes include using a shared memory or blackboard (a common repository where agents post and read information), or message-passing channels between specific agent pairs. 
Strands provides tools to implement these networks. You can create multiple agent instances (each with its own prompt, persona, and tools) and connect them together. For example, you might have a research agent, creative agent, and critic agent connected in a mesh. The research agent provides factual data, the creative agent proposes ideas, and the critic agent spots flaws; together they iterate towards an answer. The Strands SDK also includes a built-in agent_graph tool to help manage such networks programmatically (so you can define agents and connections, then send messages into the network). 
Communication and coordination in an agent swarm can be designed with different philosophies: 
 
 Collaborative swarms â€“ Agents actively build on each otherâ€™s contributions and aim for consensus. This might be ideal for creative problem solving or research, where combining perspectives yields the best result. 
 Competitive swarms â€“ Agents work in parallel on the task (perhaps with different methods or hypotheses) and might even critique each otherâ€™s results. This could be useful in scenarios like multiple agents trying independent strategies to see which is best. 
 Hybrid approaches â€“ A mix of cooperation on some subtasks and independent exploration on other. 
 
Strands doesnâ€™t force a particular style; you can implement the message exchange logic as needed (the SDK leaves the content of messages and timing up to the developer or even the agents themselves). The new agent_graph utility simplifies setting up networks by specifying a topology (for example, fully connected mesh) and then letting you broadcast or direct messages to agents. Each agent can run on a separate thread or process, providing scalability. Multi-agent networks excel in complex problem domains where different skills or viewpoints are needed concurrently. 
Supervisor-agent model (orchestrator with tool agents) 
Another pattern Strands supports is the supervisor-agent model, also known as the orchestrator and specialists architecture or agents as tools pattern. In this design, one agent acts as a primary orchestrator (supervisor) that interfaces with the user or high-level task, and it delegates subtasks to one or more specialist agents. Each specialist is effectively an agent wrapped as a callable tool that the orchestrator can invoke for specific needs. 
The manager agent decides which specialist agent is required for a given query and forwards the request, then integrates the results back into a final answer. For example, you might design an orchestrator agent that, when asked a complex question, can call a Research Assistant agent for factual lookup, or a Math Assistant agent for calculations, or a Travel Planner agent for itinerary tasks, depending on the query. Each assistant is an LLM-powered agent with its own system prompt and tools specialized to its domain. 
Strands makes it straightforward to implement this. You can create specialized agents and expose them as Python tools using the @tool decorator, as illustrated in the following code: 
 
 from&nbsp;strands import&nbsp;Agent, tool
from&nbsp;strands_tools import&nbsp;retrieve, http_request

# System prompt for a specialized research agent
RESEARCH_ASSISTANT_PROMPT&nbsp;=&nbsp;"""
You are a specialized research assistant. Focus on providing factual, well-sourced information for research questions.
Always cite sources in your answers.
"""

@tool
def&nbsp;research_assistant(query: str)&nbsp;-&gt;&nbsp;str:
&nbsp;&nbsp; &nbsp;"""Tool that uses a specialized agent to answer research queries."""
&nbsp;&nbsp; &nbsp;# Create a specialized agent for research tasks
&nbsp;&nbsp; &nbsp;research_agent&nbsp;=&nbsp;Agent(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;system_prompt=RESEARCH_ASSISTANT_PROMPT,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;tools=[retrieve, http_request] # this agent can use web retrieval tools
&nbsp;&nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp;return&nbsp;research_agent(query) # delegate the query to the research agent 
 
In this example, we defined research_assistant as a tool. The solution spins up an agent with a special prompt and a couple of tools for web research (like retrieve to fetch documents and http_request to call web APIs). It then queries that agent and returns the result. We could similarly define other specialist agent tools, such as math_assistant or trip_planner_assistant, each with their own prompt and domain-specific toolset. 
Now we can create the orchestrator agent that uses these specialist agents as its tools: 
 
 # Orchestrator agent that can delegate to specialized assistants
orchestrator_agent&nbsp;=&nbsp;Agent(
&nbsp;&nbsp; &nbsp;tools=[research_assistant, math_assistant, trip_planner_assistant]
)
# When a user question comes in, the orchestrator can decide which agent to invoke
response&nbsp;=&nbsp;orchestrator_agent(
&nbsp;&nbsp; &nbsp;"What are the latest NASA findings on Mars, and can you calculate the travel time to Mars at 20km/s?"
)
print&nbsp;(response) 
 
When the orchestrator_agent receives the complex question, it uses the LLMâ€™s reasoning (guided by its prompt, which we could customize as a general coordinator) to determine how to answer. It might decide this question has two parts â€“ a research part (latest NASA findings) and a calculation part (travel time) â€“ so it might call the research_assistant tool for the first part and the math_assistant tool for the second part. Each tool a full agent that carries out its subtask (for example, the research assistant might use http_request to fetch data from a NASA API or knowledge base). The orchestrator then assembles the final answer. This hierarchy creates a clear delegation chain: the top-level agent offloads work to experts and then merges their outputs. 
The benefits of the supervisor agent architecture include separation of concerns (each agent specializes, making the system straightforward to maintain) and modularity (you can add or remove specialist agents without rewriting the whole agent). It also mirrors human organizational structures â€“ a manager coordinating specialists â€“ which can be an intuitive way to scale up agent complexity. In Strands, this pattern is fully supported by treating agents as just another kind of tool. The orchestrator agentâ€™s built-in logic (through the LLM) reads tool docstrings and decides when to use which specialist tool, especially if you provide it guidance like â€œUse the Research Assistant for any questions about scientific facts or current dataâ€ in its system prompt. 
Hierarchical agent architectures 
The hierarchical pattern is an extension of the supervisor-agent idea to multiple levels of delegation. Instead of a single orchestrator and a list of specialists, you can have layers of agents forming a hierarchy or tree structure. For example, at the top might be an executive agent handling the broad objective; it delegates high-level tasks to a few manager agents, each of whom further breaks down tasks among their worker agents. This is useful when problems are very complex or naturally tree-structured (project management, multi-stage workflows). 
Strandsâ€™s agent graph concept generalizes this. In an agent graph, nodes are agents and edges define communication or supervisory links. A hierarchical topology is one of the supported graph patterns: a tree where each parent node directs its children. Information flows down the tree as tasks and up the tree as results or reports. The star topology (one central coordinator with many direct specialists) is actually a one-level hierarchy (the supervisor-agent model), whereas a true hierarchy might have multi-level oversight. 
Using the Strands SDK, you can manually create such structures by assembling agents and coordinating their calls, but the simpler route is to use the graph tool or similar utilities. As shown earlier, you can programmatically define nodes and edges of a graph (specifying each agentâ€™s role and prompt) and the tool will instantiate that network. Then you can send messages into the graph (for instance, give a top-level instruction to the executive agent) and the system will propagate tasks downward and solutions upward according to the defined edges. Each agent in the hierarchy can maintain its own state and focus â€“ for example, a mid-level manager agent might keep track of progress of its sub-tasks independently. 
Hierarchical agent architectures are ideal when you need layered processing or decision-making. For instance, consider an autonomous software engineering agent: an executive agent decides overall what feature to implement and delegates to a design agent and a coding agent, and the coding agent might further delegate tasks to a code generation agent and a testing agent. Each level adds oversight and can catch errors or refine requirements from the level below. This reduces the cognitive load on each individual agent. Strands facilitates this by providing the mechanisms to link agents in tree or graph structures and handle message passing along those links. The built-in support for maintaining sessions and state for each agent and controlling information flow means you can fine-tune what each agent knows, providing clarity and preventing agents from interfering with each otherâ€™s context. 
Finally, Strands doesnâ€™t lock you into a single pattern â€“ you can combine them. For example, you could have a swarm of peer agents at one level of the hierarchy and a supervisor above them, or an orchestrator that consults a small swarm of brainstorming agents as one of its tools. The SDKâ€™s flexible design (with agents as first-class objects and tools) lets you mix patterns to suit the problem. 
Observability in Strands agents 
In production, observability is crucial to understand and trust what your agents are doing. The Strands SDK was built with observability in mind, providing built-in instrumentation hooks, telemetry collection, and support for logging and metrics out of the box. 
Instrumentation and traces 
Strands can record agent trajectories â€“ the sequence of steps (for example, model calls, tool calls) an agent takes for each request. It uses OpenTelemetry (OTEL) standards to emit this data, meaning you can plug it into other OTEL-compatible monitoring backends (such as AWS X-Ray, Amazon CloudWatch, and Jaeger) to visualize and analyze agent behavior. 
Each run of an agent can produce a trace, which consists of spans for each significant action. For example, when the agent calls the LLM model, thatâ€™s a span, which can include metadata like the prompt, model parameters (for example, temperature or max tokens), and token usage counts. When the agent invokes a tool, thatâ€™s another span, recording which tool was called and the input and output. By stitching these spans together, you get an end-to-end timeline of how the agent arrived at its answer. 
This distributed tracing extends across components â€“ for instance, if your agent is calling a remote microservice as a tool, the trace can propagate into that service (provided it also uses OTEL), giving you a cross-system view of a request. Such insight is invaluable for debugging agent reasoning, optimizing performance, and identifying failure points in complex multi-agent workflows. 
Metrics tracking 
Strands also tracks key metrics about agent operations. Metrics are aggregate measurements that help quantify performance and usage. You can capture metrics such as the number of times each tool was invoked (and perhaps success and failure rates of those calls), runtime of tool calls, how many turns or agent loops run per interaction, latency of model responses (time to first byte and time to complete), and token consumption (prompt tokens vs. completion tokens) per request. Strands can also surface system metrics (CPU, memory usage if relevant) and custom business metrics like how often users are happy with the agentâ€™s answer (if feedback is provided). 
By monitoring these metrics, developers and operations teams can make sure the agent is behaving efficiently and reliably. For example, a sudden spike in tool error rates or a jump in token usage per query might signal a regression that needs attention. Metrics can feed into dashboards or alerting systems to maintain the operational health of your AI agent service. 
Logging 
The SDK emits logs for important events â€“ for example, the full prompt being sent to the model, the modelâ€™s raw response, decisions it made about which tool to use, and errors encountered. These logs are timestamped and can be configured at various verbosity levels (debug, info, error) similar to other applications. Logs are useful for deep debugging or audit trails, and because they might include sensitive or verbose information, Strands allows structuring or redacting logs as needed. In production, you might integrate Strands logs with standard logging infrastructure (CloudWatch Logs, ELK stack) for centralized analysis. 
Observability 
Strands encourages end-to-end observability. The SDK documentation outlines a framework where agent developers, data engineers, and product owners all consume telemetry. For example, developers use traces to diagnose why an agent made a certain decision (such as visualizing a trace to see the chain of tool calls leading to a wrong answer). Data engineering teams might aggregate telemetry in a data warehouse to analyze usage patterns or costs over time. AI researchers could use logs and traces to identify failure modes and fine-tune prompts or models (treating trace data as feedback to improve the agent). 
Strands provides the raw capabilities (instrumentation points and OTEL integration) to enable this, but itâ€™s up to the implementing team to set up collectors and dashboards. Best practices include standardizing on open formats (like OTEL) for interoperability, using collectors to route telemetry to multiple sinks (for operations or business intelligence), and filtering or sampling data to manage volume and privacy. 
Observability is not an afterthought in Strands â€“ itâ€™s built into the agent loop so that when your agent is running in production, you can monitor its reasoning and actions. This focus on instrumentation sets Strands apart, especially compared to earlier agent frameworks where developers often had to add their own logging or tracing. 
Enterprise readiness and deployment best practices 
The Strands Agents SDK was designed with enterprise production use in mind, providing features and guidance to help agents run reliably, at scale, and securely in business-critical environments. This section discusses how Strands addresses key enterprise requirements that are essential for adopting agentic workflows in large organizations. 
Scalability and performance 
Strands can scale from quick prototypes to large-scale production deployments seamlessly. The same agent code running on a developerâ€™s laptop can be deployed to the cloud without changes. Strands is built in a lightweight manner (a Python framework orchestrating API calls to LLMs and tools), so it introduces minimal overhead. Agents can run concurrently â€“ each agent in a multi-agent configuration can execute on its own thread or process to utilize multiple cores or machines. This means an orchestrator agent can dispatch work to specialist agents running in parallel processes, achieving concurrency and faster throughput on multi-CPU systems. When integrated into distributed environments (like microservices or serverless architectures), you can run multiple agent instances behind a load balancer to handle high request volumes. The frameworkâ€™s model-agnostic nature also means you can choose more powerful model endpoints for heavier workloads or distribute calls across different model providers to avoid bottlenecks. 
In practice, performance is usually dominated by the LLMâ€™s response time and external API calls the agent makes; Strands makes sure it pipelines these operations efficiently (for example, by streaming responses when available, to start delivering output as soon as possible). There is no strict upper limit in Strands on the number of tools or steps an agent can handle, aside from compute resources and model limitations â€“ making it suitable for complex, long-running tasks if needed. 
Security and data protection 
Enterprise applications demand strong security measures, and Strands provides mechanisms and recommendations to build secure agents. Because agents can execute code or call external APIs through tools, itâ€™s critical to manage what tools are available to an agent. Strands supports fine-grained control over tool access â€“ you decide which tools to include for each agent. 
Sensitive data handling is also emphasized: AWS recommends implementing end-to-end encryption for sensitive data that agents handle, both at rest and in transit. For example, if an agent stores conversation history or intermediate results, those could be encrypted or kept in memory only. Itâ€™s also important to sanitize inputs and outputs; using Amazon Bedrock Guardrails or custom validation to filter the agentâ€™s responses can prevent the agent from returning confidential information or disallowed content. Strandsâ€™s logging can be configured to omit or redact sensitive details, so audit logs donâ€™t become a source of leakage. 
Authentication and authorization should be layered in front of agent endpoints: when deploying using Amazon API Gateway or Lambda, you can use AWS Identity and Access Management (IAM) roles, Amazon Cognito, or OAuth tokens to make sure only authorized systems or users can invoke the agent. Within the agent, you might also enforce role-based logic â€“ for example, certain tools only activate if the requesting user has appropriate permissions. For multi-agent systems, isolating each agentâ€™s context (which Strands supports using sessions and separate prompts) can enforce the principle of least privilege (each sub-agent only knows what it needs to). 
Threat modeling for AI agents is a new but important practice; AWS has published guidance like the MAESTRO framework for agentic AI threat modeling. This encourages teams to anticipate how an agent might be misused or attacked (for instance, prompt injection by malicious inputs, or attempts to make an agent reveal secure data) and to implement mitigations such as input validation, output filtering, and robust exception handling. 
Strands gives you the building blocks for powerful autonomous agents, but enterprises should wrap those agents with the same rigor applied to any application â€“ encryption, monitoring, access control, and testing against adversarial inputs â€“ to run them safely and responsibly. 
AWS service integration 
As an AWS originated project, the Strands Agents SDK integrates naturally with the AWS ecosystem. It can work with Amazon Bedrock out of the box (for access to foundation models), which simplifies using high-quality, scalable models with enterprise security (data is not left unencrypted or sent to external third-parties when using Amazon Bedrock). 
Beyond model hosting, Strandsâ€™s use of OTEL means you can pipe trace data into AWS X-Ray for distributed tracing visuals and send metrics to CloudWatch for real-time monitoring. For example, you could set up CloudWatch alarms on metrics like tool error rate or latency per agent call to alert operations teams of anomalies. 
Strands can also call AWS services as tools â€“ either using APIs or specialized tools. Community-contributed tools for AWS are already available (for instance, tools that can run AWS SDK commands or query AWS resources). In one scenario, an agent could use an AWS SDK tool to automatically remediate an AWS infrastructure issue (making it a DevOps assistant), or use an Amazon DynamoDB tool to store and retrieve information as part of its workflow. In fact, one of the Strands example use cases demonstrates an agent storing weather data into DynamoDB, showing how seamlessly an agent can incorporate AWS data services. Because Strands is Python, it also works smoothly with Lambda for serverless deployments (more on that later in this post) and with AWS Step Functions if you need to embed an agent call as one step in a larger orchestrated business workflow. Enterprises can use existing AWS identity and networking features like virtual private cloud (VPC), IAM, or AWS Key Management Service (AWS KMS) encryption to further secure and isolate their agents when running in the cloud. 
Conversely, Strands remains cloud-agnostic enough that if needed, you can run it on premises or in other cloud environments â€“ for example, using local models through Ollama or connecting to third-party APIs â€“ giving flexibility for hybrid deployments. 
Deployment best practices 
There are several proven patterns for deploying Strands agents in production, and the SDK provides a deployment toolkit with reference implementations. Depending on use case, you might choose one of the following deployment methods: 
 
 Serverless (Lambda) â€“ This is ideal for short-lived agent tasks or event-driven invocations. You can deploy an agent as a Lambda function, possibly using the Lambda function URL feature or API Gateway to trigger it using HTTPS. This offers scalability (Lambda will spawn concurrent executions as needed) and minimal operations overhead. Itâ€™s best for agents that complete within the Lambda runtime limit and donâ€™t require long-lived state (though you can use external storage for state if needed). AWS provides examples for deploying Strands this way, which also include using streaming using Lambda for real-time responses. For interactive or streaming agents, you might prefer using WebSockets or an asynchronous pattern, because Lambda invocations are stateless. 
 Containers (AWS Fargate and Amazon ECS or Amazon EKS) â€“ For long-running or stateful agent services, containerizing the agent logic is a common approach. You can host the agent loop in a container (for example, a microservice that listens for requests, invokes the agent, and returns results). Fargate (serverless containers) and Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS) are both supported in reference architectures. This approach is well-suited to streaming interactions (where an agent might keep a connection open to stream tokens) and high-concurrency scenarios. With containers, you can allocate more memory/CPU for larger models or use GPU-backed instances if running heavy local models. You can also horizontally scale the number of agent service instances and integrate with service meshes or mesh-like patterns if multiple agent services need to communicate. 
 Hybrid return-of-control pattern â€“ In some enterprise scenarios, part of the tool execution is done on the client side or in a separate environment for security. Strands supports an architecture where the agent is hosted in one environment (for example, in AWS) but some tools are executed in a different environment (like on a userâ€™s device or in a secured on-premises service). The return-of-control pattern lets the agent delegate certain tool calls back to the client application. For instance, a client application might register a local tool (for example, for accessing a local database or hardware device) with the agent. When the agent decides to use that tool, it returns a signal for the client to execute it and await the result. Meanwhile, other tools can be hosted in the cloud. This pattern gives maximum flexibility and can address data governance concerns (keeping some data processing local) while still benefiting from the agentâ€™s reasoning capabilities in the cloud. 
 Monolithic vs. microservices â€“ Strands agents can be deployed as a monolith (the agent loop and all tools in one process) or split into microservices (each tool as its own service that the agent calls through an API). Monolithic deployments are simpler and have less latency (function calls in memory), but splitting tools into separate services can improve fault isolation, allow independent scaling of expensive tools, and enable polyglot implementations (tools in other languages). A best practice is to start monolithic for simplicity, then refactor out critical tools as needed (for example, a data-intensive tool might be better as a separate service that can be scaled on its own). Strandsâ€™s observability will still capture cross-service calls if OTEL tracing is propagated, giving you a full picture of the distributed workflow. 
 Amazon Bedrock AgentCore â€“ For production-grade deployment of Strands agents with built-in support for identity, memory, observability, and tool integration, AWS offers Amazon Bedrock AgentCore. This is a secure, serverless runtime designed specifically for running AI agents in real-world applications. You can wrap a Strands agent using the BedrockAgentCoreApp wrapper and deploy it through the AWS Command Line Interface (AWS CLI) or container workflows. Amazon Bedrock AgentCore supports long-running tasks (up to 8 hours), asynchronous tool execution, and tool interoperability using MCP, A2A, or API Gateway based services. It also includes secure identity features such as OAuth, Amazon Cognito, and IAM, as well as native observability with CloudWatch and OTEL. This approach is ideal for teams looking for a scalable, secure, and fully managed agent infrastructure that integrates seamlessly with existing AWS services. Amazon Bedrock AgentCore is currently available in public preview as of July 2025. 
 
When operating agents in production, itâ€™s also recommended to implement robust error handling and monitoring. For example, you might wrap the agent invocation in a retry loop or fallback logic â€“ if the agent fails or returns an incomplete result, it will handle the exception and respond gracefully (perhaps return a default answer or a message that it will get back later). Define timeouts for tool calls and possibly limit the number of reasoning loops to avoid â€œrunawayâ€ agents. Use CloudWatch or a similar monitoring system to collect metrics like latency, error counts, token usage, and cost per request, and set up alerts for anomalies. In an enterprise, operational excellence is as important as the agentâ€™s accuracy â€“ Strands gives you the hooks (telemetry, logs, config options) to achieve this, but itâ€™s up to your DevOps team to wire them into your existing operations toolkit. 
By following these enterprise deployment best practices â€“ choosing the right architecture, enforcing security at multiple layers, and using the infrastructure of AWS â€“ companies can confidently deploy Strands agents that meet their scalability, security, and compliance requirements while delivering advanced AI capabilities to end-users. 
Practical usage example: Autonomous workflow in action 
To illustrate how you might use the Strands SDK for an autonomous workflow, letâ€™s walk through a scenario. Suppose we want to build an autonomous research assistant that can perform a multi-step task: the user asks a broad question, and the agent needs to gather information from the web, perform some analysis, and provide a well-formulated answer with citations. This involves decision-making (how to break down the task), tool usage (web search, reading documents, performing calculations), and synthesis of results. 
Define tools 
We need some tools for web research. Strands includes a retrieve tool (for searching and retrieving documents) and an http_request tool (for calling APIs or fetching URLs). If additional processing is needed, we could also include a Python execution tool or others. 
Create the agent with a suitable prompt 
We give our agent a system prompt that instructs it to be a diligent research assistant. For example: â€œYou have access to web search and browsing. Always find factual information and cite sources in your answer. If calculations are needed, do them step by step.â€ This prompt sets the context so the LLM knows how to behave (defining the role and guidelines for the agent). 
 
 from&nbsp;strands_tools import&nbsp;calculator

# Orchestrator agent that can both research and calculate
smart_agent&nbsp;=&nbsp;Agent(
&nbsp;&nbsp; &nbsp;system_prompt="You are an AI research assistant. You answer questions with facts and citations. You have tools for web research and math.",
&nbsp;&nbsp; &nbsp;tools=[research_assistant, calculator]
)
query&nbsp;=&nbsp;"What are the latest NASA findings on Mars, and how long would it take a spacecraft to travel from Earth to Mars at 20 km/s?"
answer&nbsp;=&nbsp;smart_agent(query)
print(answer) 
 
Agent reasoning and tool use 
When a query comes in, the agentâ€™s LLM will parse the question and decide on a plan. Thanks to Strandsâ€™s model-driven loop, the agent can internally reason something like: â€œThe question asks for the latest NASA findings on Mars and a travel time calculation. Plan: (1) use web search tool to find recent NASA announcements about Mars, (2) summarize findings, (3) use calculator (or internal math) to compute travel time, (4) compose final answer.â€ 
The agent will execute this plan step by step: call retrieve with a query like â€œlatest NASA Mars findings,â€ get results (perhaps through http_request to read an article), extract key information, do the calculation for travel time (maybe using the calculator tool if available, or basic arithmetic), and finally synthesize everything into a coherent answer, citing the article source for the Mars information. 
This happens in one agent() call, transparently. The user sees the final answer, but Strands orchestrated multiple tool calls and reasoning iterations to get there. The observability features described earlier make it possible to inspect this process after the fact (for example, we can see in the trace that the agent made two tool calls: one to retrieve for NASA info and one to calculator for computing travel time). 
Compose the final answer 
The agent combines the results of its tools and formulates the answer to return to the user. For instance: 
 
 ## Latest NASA Findings on Mars (2023-2025)

### Atmospheric Discoveries
1. **First Visible-Light Auroras** (March 2024)
- NASA observed visible-light auroras on Mars after a solar flare and coronal mass ejection
- This event occurred near the peak of the current solar cycle

2. **MAVEN's "Sputtering" Discovery** (2024)
- After a decade of searching, NASA's MAVEN mission made the first direct observation of atmospheric "sputtering"
- This process helps explain how Mars lost its atmosphere over time

### Geological Discoveries
1. **Arsia Mons Volcano Observation** (2025)
- NASA's 2001 Mars Odyssey orbiter captured new images of Arsia Mons, one of Mars' largest volcanoes
- The images show the massive volcano peeking through morning cloud cover

2. **Perseverance Rover's New Explorations** (2025)
- Perseverance began exploring "Krokodillen," a region potentially containing some of Mars' oldest rocks
- The rover continues collecting samples for the future Mars Sample Return mission

3. **Dust Devil Captured in Perseverance Selfie** (May 2024)
- A Martian dust devil was captured in Perseverance's fifth selfie since landing

### Other Notable Findings
- Curiosity rover was photographed in motion by the Mars Reconnaissance Orbiter
- NASA's Europa Clipper spacecraft captured infrared images of Mars during a flyby
- Ongoing updates to the Mars Relay Network for communication between rovers and orbiters

## Earth to Mars Travel Time at 20 km/s
The distance between Earth and Mars varies significantly due to their elliptical orbits:
- **Minimum distance**: 54.6 million km (when planets are at their closest)
- **Maximum distance**: 401 million km (when planets are at their furthest)

At a constant speed of 20 km/s:
- **Minimum travel time**:
- 2,730,000 seconds
- 758.3 hours
- **~31.6 days**

- **Maximum travel time**:
- 20,050,000 seconds
- 5,569.4 hours
- **~232.1 days** (about 7.7 months)

This significant variation in travel time (from approximately 1 month to 7.7 months) demonstrates why launch windows to Mars are carefully planned to coincide with favorable orbital alignments that minimize travel time, fuel consumption, and mission duration.
 
 
This answer illustrates how the agent retrieved factual data, specifically the minimum and maximum distances between Earth and Mars, and used that information to calculate travel time at a constant speed of 20 km/s. It expressed the results in multiple formats â€“ seconds, hours, and days â€“ offering a realistic travel range from about 1 month to 7.7 months depending on planetary positions â€“ all without the user needing to prompt those steps separately. 
By using Strands for this workflow, we didnâ€™t not have to script the sequence â€œfirst do search, then do calculationâ€ ourselves; the agent decided it intelligently. If the question had been different, the agent might have taken a different approach or called different tools, all based on its autonomous reasoning. By adjusting the provided tools and the system prompt, we can guide the agentâ€™s behavior for various autonomous workflows (from writing code with a code-execution tool to analyzing datasets with a data analysis tool). 
Error handling and reflection 
In practice, autonomous agents might make mistakes or need to recover (perhaps the first web search wasnâ€™t useful and it needs to try a different query). Strands agents have the ability to reflect and retry within the loop â€“ for instance, if a toolâ€™s result is unexpected or irrelevant, the agent can reformulate its approach. We can also program explicit guardrails or use the observability signals to intervene if something goes wrong (for example, if no useful answer after X loops, stop and respond with an apology or ask for clarification). The SDK provides callback hooks and the ability to integrate such logic, but by default, the LLMâ€™s own iterative reasoning often suffices for moderate tasks. 
Through this example, we see how Strands enables multi-step decision making in a single unified agent. It empowers the agent to act autonomously â€“ deciding which steps to take and in what order â€“ using the powerful reasoning of the LLM, all giving developers simple control points (which tools to allow, what the agentâ€™s role and prompt is, and the ability to analyze logs and traces afterward). 
Strands vs. other agent frameworks 
With the rapid rise of AI agents, a number of frameworks have emerged. LangChain is one of the most popular libraries for building LLM-driven applications and agents. In this section, we provide an overview of similarities and differences, and when to consider each. 
Core concept and philosophy 
Both Strands and LangChain enable the pattern of using LLMs to drive actions (the ReAct paradigm of reasoning and tool use). They let you define tools and functions and have the model call them to solve tasks. The big difference is in developer experience and emphasis. Strands preaches minimal orchestration: you supply the prompt and tools and trust the model to figure out the sequence. It consciously avoids requiring developers to write complex workflow code or state machines around the agent. 
LangChain started with a focus on giving developers building blocks to construct chains or sequences of LLM calls and tools. In early LangChain, you might manually stitch together a series of steps or use a predefined agent template. Over time, LangChain added agentic behavior (like its AgentExecutor with an LLM planning actions), but it still offers more low-level control if you want it. LangChainâ€™s latest versions introduced explicit workflow orchestration tools like LangGraph for DAG-based flows and a MultiAgent orchestration engine. This means LangChain can support very fine-grained, developer-defined workflows when needed (you can script each sub-agent and how they connect). Strands chooses to abstract that away by default â€“ the workflow emerges from the modelâ€™s decisions, not the developerâ€™s hardcoded logic. 
In short: Strands is â€œLLM-firstâ€ (model as planner) whereas LangChain is â€œdeveloper-firstâ€ (assemble your desired chain, though it can also delegate planning to the model if you use its agent abstractions). 
Tool ecosystem and integration 
Both frameworks recognize the importance of tools (also called skills or functions). LangChain provides a wide array of integrations â€“ not only generic tools like math or web search, but connectors to databases, vector stores, and APIs, often through community-contributed wrappers. 
Strandsâ€™s approach to tools embraces standards like MCP (model-provided tools). By supporting MCP, Strands can access a large library of tools (for example, QA over documents, coding helpers, and cloud resource manipulators) without each tool needing a custom integration script in your code. LangChain doesnâ€™t natively support MCP, but it has its own catalog of tools, and often you integrate by writing a Python function and adding it as a tool (similar to Strandsâ€™s @tool, which was likely inspired by patterns popularized by LangChain). LangChain has a wider array of built-in connectors (especially for things like data stores or model providers) given its maturity, whereas Strands is catching up quickly by using open protocols and community contributions (Anthropic and Meta have already added compatibility in Strands for their systems). 
Strands can naturally integrate well with AWS services (for example, Amazon Bedrock models), whereas LangChain has historically been used a lot with OpenAIâ€™s API, custom local models, and various third-party services. Both can be adapted to either use case, but the simplest mappings tend to be Amazon Bedrock with Strands and OpenAI with LangChain, even though Strands also works fine with OpenAI. 
Multi-agent and structuring of agents 
Strands comes with first-class support for multi-agent systems (you can make swarms, graphs, and hierarchical setups with built-ins like the agent_graph tool). LangChain has also moved into this space, providing things like MultiAgentManager/Executor and even experimental ecosystems like Microsoftâ€™s AutoGen integration, where multiple agents (like a user agent and a system agent) converse. However, LangChainâ€™s multi-agent patterns are not as unified under one concept; you often find examples or custom implementations (for instance, agents that critique each other or work in a loop). 
In 2025, LangChain introduced LangGraph and other features to let developers explicitly design multi-agent workflows with flows, shared memory, and parallelism. This gives a lot of power to the developer to orchestrate agents in LangChain, but requires you to plan those interactions. Strands provides more out-of-the-box constructs (like a ready-made way to turn agents into tools or create a mesh network) for common patterns, leaning on sensible defaults. 
AWS also has a separate library called Agent Squad, which focuses on multi-agent orchestration (routing to specialist agents). AWS suggests using Agent Squad when you need many specialized sub-agents with strict context isolation, and using Strands when you want a single-agent-with-tools approach that leans on the LLMâ€™s reasoning. That indicates Strands often works best with a single agent (or a contained hierarchy of agents) using tools, whereas if you wanted a large ensemble of disparate agents with their own models or knowledge, you might use a different orchestrator and possibly incorporate Strands agents within it. For most cases though, Strands is fully capable of multi-agent coordination. 
Memory and context handling 
Both frameworks allow for maintaining state between agent turns (conversation history, or more generally an agentâ€™s memory). LangChain has numerous utilities for short-term memory (chat message history) and long-term memory (vector store backed memory). Strands provides sessions and state management too â€“ you can choose where to store conversation history or other context (in-memory, persistent store) and how to truncate or retrieve it. The principles are similar, but LangChain has more pre-built variants (like summarizing memory or knowledge graphs as memory), reflecting its broader usage in conversational AI. 
For a solution architect, if memory beyond a simple history is needed (like integrating a vector database for knowledge), LangChain offers pluggable classes for that; with Strands, you might implement a custom tool to query a vector database (or use MCP if a tool exists for it). Both can achieve the goal, but the approach differs (LangChain would treat it as a retriever in a chain, Strands would treat it as a tool the agent can call). 
Observability and evaluation 
Strands focuses on production observability with its OTEL integration and guidance on metrics. It treats telemetry as a first-class concern for live agents. LangChain, although it provides some logging and callbacks (and there are third-party solutions like Langfuse to instrument LangChain agents), doesnâ€™t have a built-in distributed tracing or metrics system at the level Strands does. In practice, teams using LangChain have often needed to add their own logging or use wrappers to record agent steps. With Strands, much of that comes included or with a simple configuration. This makes Strands appealing for enterprise and production scenarios where monitoring is a must. 
On the evaluation side, both Strands and LangChain acknowledge the need to evaluate agent outputs. LangChain has an evaluation module (for grading responses), and Strands offers evaluation techniques for agents (such as collecting traces for analysis or using feedback loops). The telemetry advantage leans in favor of Strands due to its built-in OTEL support. 
Use case suitability 
Choosing between Strands and LangChain will depend on the projectâ€™s priorities. If you want a quick, boilerplate solution to get an agent working â€“ especially if you plan to use AWS infrastructure or Amazon Bedrock models â€“ Strands is a fantastic option. It offers a quick path from â€œprompt plus toolsâ€ to a working agent, and it excels when you are comfortable letting a powerful model orchestrate the process (which, given todayâ€™s LLMs like GPT-4 or Anthropicâ€™s Claude 2, is often a reasonable choice). Strands is also a strong choice if observability and reliability in production are top concerns; it was designed by AWS with production in mind (the fact that internal AWS teams replaced months of custom agent tinkering with Strands speaks to that focus). 
On the other hand, if your use case requires extensive custom control over each step, or involves a lot of integrations with external systems that LangChain already has modules for (for example, you need to interface with 10 different databases and proprietary APIs, and you want a library that has samples for all), LangChainâ€™s ecosystem might get you there faster. LangChain also has a larger community at the moment, so finding examples and support for niche scenarios could be faster. Additionally, for academic or very experimental multi-agent research, LangChainâ€™s flexibility to craft novel agent loops or logic might be useful. However, Strands is quickly evolving, and being open source, it might soon incorporate many of the needed integrations (and you can always use Strands in conjunction with other libraries â€“ for example, call a LangChain chain as a Strands tool). 
Performance 
There isnâ€™t a clear publicly documented performance benchmark between Strands and LangChain. Both primarily spend time waiting on LLM API calls or running the model, so raw performance differences likely come down to how efficiently they manage prompts and tools. Strands being lightweight might have less overhead in the agent loop, whereas LangChainâ€™s flexibility can sometimes introduce extra layers (for instance, parsing outputs with regex or validators). In practice, both are fast enough and any latency is dominated by the model inference and any external API calls. 
Conclusion 
In summary, Strands and LangChain have overlapping goals but different design centers. Strands, coming from AWS, emphasizes using the power of state-of-the-art models with minimal fuss and making sure you can deploy and monitor these agents reliably. LangChain offers a broader toolkit for constructing AI reasoning pipelines, which can be molded into agent behaviors but also requires more decision-making from the developer on how the workflow should look. Many solution architects might even use them together or for different layers of a system (you donâ€™t have to use one exclusively). Frameworks like Strands are simplifying agent development â€“ as LLMs improve, developers can focus more on what they want the agent to achieve and less on how to micromanage the agentâ€™s steps. As the ecosystem evolves, we can expect both Strands and LangChain (and others) to learn from each other and contribute to making AI agents more capable, robust, and simpler to build than ever before.For more information about the Strands Agents SDK, refer to the following: 
 
 Introducing Strands Agents, an Open Source AI Agents SDK 
 Strands Agents SDK 
 Agent samples built using the Strands Agents SDK 
 Introducing Strands Agents 1.0: Production-Ready Multi-Agent Orchestration Made Simple 
 
 
 
About the authors 
Jin Tan Ruan is a Senior Generative AI Developer on the AWS Industries Prototyping and Customer Engineering (PACE) team. He specializes in building multi-agent and multimodal applications using foundation models, with a focus on real-world implementations of AGI-like capabilities. Jin brings a strong background in software development and holds nine AWS certifications, including the Machine Learning Specialty. At AWS, Jin designs and builds advanced agentic systems that combine reasoning, planning, memory, and tool use â€“ using LLMs for autonomous decision-making and task orchestration. He is also deeply involved in fine-tuning and prompt engineering to optimize foundation models for enterprise-scale, domain-specific performance. Jin holds a Masterâ€™s in Computer Science and Software Engineering from Syracuse University. Outside of work, he enjoys exploring new places and diving into the world of horror films. Connect with Jin on LinkedIn.
â€¢ Build dynamic web research agents with the Strands Agents SDK and Tavily
  â€œTavily is now available on AWS Marketplace and integrates natively with Amazon Bedrock AgentCore Gateway. This makes it even faster for developers and enterprises to embed real-time web intelligence into secure, AWS-powered agents.â€ 
 
As enterprises accelerate their AI adoption, the demand for agent frameworks that can autonomously gather, process, and synthesize information has increased. Traditional approaches to building AI agents often require extensive orchestration code, explicit state management, and rigid architectures that are difficult to maintain and scale. 
Strands Agents simplifies agent development by addressing these challenges. It introduces a model-centric paradigm that shifts the complexity from hard-coded logic into the large language model (LLM) itself. This dramatically reduces development overhead while increasing agent flexibilityâ€”for example, minimizing the need to write explicit logic for each input or output type. By embedding logic directly into the model, agents can be significantly improved simply by swapping in more advanced models as they are released. 
In this post, we introduce how to combine Strands Agents with Tavilyâ€™s purpose-built web intelligence API, to create powerful research agents that excel at complex information gathering tasks while maintaining the security and compliance standards required for enterprise deployment. 
Strands Agents SDK: Model-centric agent framework 
The Strands Agents SDK is an open source framework that revolutionizes AI agent development by embracing a model-driven approach. It offers a code-first, lightweight yet powerful framework for building agentic workflows. Instead of requiring complex orchestration code, the Strands Agents SDK helps developers create sophisticated agents through three primary components: 
 
 Models â€“ Offers flexible integration with leading LLM providers, including Amazon Bedrock, Anthropic, Ollama, and LiteLLM, and provides an extensible interface for implementing custom model providers. 
 Tools â€“ Allows agents to interact with external systems, access data, and manipulate their environment. Strands Agents offers more than 20 built-in tool capabilities, and helps developers create custom tools using simple Python function decorators. 
 Prompts â€“ Supports natural language instructions that guide agent behavior and objectives. 
 
Strands Agents offers an advanced and rich feature set. With the Strands Agents SDK, developers can build intelligent agents with minimal code while maintaining enterprise-grade capabilities: 
 
 Security and responsible AI â€“ Provides seamless integration with guardrails for content filtering, personally identifiable information (PII) protection, and more 
 Streamlined agent development lifecycle â€“ Helps developers run agents locally and build complex evaluation workflows that can be automated as part of your continuous integration and delivery (CI/CD) pipelines 
 Flexible deployment â€“ Offers support for many deployment options, from dedicated servers to serverless 
 Observability â€“ Supports OpenTelemetry standard for transmitting logs, metrics, and traces 
 
Strands Agents abstracts away the complexity of building, orchestrating, and deploying intelligent agents, providing a natural language-based interaction and control coupled with dynamic output generation. The result is a more intuitive and powerful development experience. 
Tavily: Secure, modular web intelligence for AI agents 
Tavily is an API-first web intelligence layer designed specifically for LLM agents, powering real-time search, high-fidelity content extraction, and structured web crawling. Built for developers building AI-based systems, Tavily is engineered for precision, speed, and modularity. It offers a seamless integration experience for agent frameworks like Strands Agents.Tavilyâ€™s API is an enterprise-grade infrastructure layer trusted by leading AI companies. It combines robust capabilities with production-grade operational guarantees, such as: 
 
 SOC 2 Type II compliance â€“ Supports best-in-class security and privacy posture 
 Zero data retention â€“ No queries, payloads, or user data are stored, maintaining compliance with strict internal policies and regulatory frameworks 
 Plug-and-play with Amazon Bedrock and private LLMs â€“ Supports hybrid cloud deployments, private language model use, and latency-sensitive inference stacks 
 Modular endpoints â€“ Designed for agent-style interaction, Tavily provides purpose-built APIs for: 
   
   Search â€“ Retrieve semantically ranked links and content snippets across the public web, filtered by domain, recency, or count 
   Extract â€“ Pull raw content or cleaned markdown from known URLs for summarization, QA, or embedding 
   Crawl â€“ Traverse websites recursively through links to simulate exploratory behavior and build site maps 
    
 
Each endpoint is exposed as a standalone tool, meaning they can be quickly wrapped into your agent frameworkâ€™s tool schema (such as OpenAIâ€™s tool-calling, LangChain, Strands, or ReAct-based implementations). 
Combining Strands Agents with the Tavily web infrastructure 
By combining the flexibility of the Strands Agents SDK with Tavilyâ€™s real-time web intelligence capabilities, developers can build dynamic, LLM-powered agents that interact intelligently with the internet. These agents can reason over open-ended queries, make decisions based on natural language prompts, and autonomously gather, process, and deliver insights from the web.This integration can be appropriate for a wide range of agent-based applications. For example: 
 
 Customer success agents that proactively retrieve the latest product documentation, policy updates, or external FAQs to resolve support issues faster 
 Internal employee assistants that answer workplace questions by pulling from both internal tools and publicly available information, reducing dependency on knowledge silos 
 Sales and revenue agents that surface timely company news and industry shifts to support account planning and outreach 
 
Each use case benefits from the same foundation: a developer-friendly agent framework, composable web intelligence tools, and the decision-making power of LLMs.To demonstrate how this comes together in practice, we explore a focused implementation: a research agent designed for autonomous, high-fidelity web investigation. 
Research agent example 
Many research agent implementations require extensive development efforts and rely on deterministic logic or workflows with constrained inputs and outputs. Alternatively, Strands enables developers to build highly dynamic agents through natural language. Strands agents use prompt engineering to dynamically generate varied output types and accept diverse natural language inputs seamlessly.Combining Tavily with Strands unlocks a new class of agents purpose-built for deep, dynamic research. Unlike hardcoded research pipelines, this pairing helps developers accomplish the following: 
 
 Rapidly develop powerful research agents using Tavilyâ€™s endpoints (Search, Crawl, Extract) as tools within the Strands Agents framework, offering a developer-friendly interface 
 Offload complex decision-making relying on the LLMâ€™s native capabilities 
 Inherit performance boosts automatically with every new generation of model (for example, Anthropic Claudeâ€™s on Amazon Bedrock or Amazon Nova), as the flexible agent architecture dynamically improves with minimal code changes 
 Combine the enterprise security infrastructure of Amazon Bedrock with Tavilyâ€™s zero data retention policies to create a highly secure environment for sensitive research tasks 
 
With Strands Agents and Tavilyâ€™s capabilities combined, the agents excel in gathering industry intelligence and providing organizations with real-time insights into trends, competitor activities, and emerging opportunities. Agents can conduct comprehensive competitive analysis, scouring vast amounts of online data to identify strengths, weaknesses, and strategic positioning of industry players. In the realm of technical research, these agents can rapidly assimilate and synthesize complex information from multiple sources, which can help accelerate innovation and problem-solving processes. Additionally, such agents prove invaluable for regulatory compliance monitoring by continuously scanning and interpreting evolving legal landscapes to make sure organizations stay ahead of regulatory changes. The flexibility of the Strands Agents SDK allows for customization to specific industry needsâ€”itâ€™s equally effective for tasks ranging from customer service automation to sophisticated data analysis workflows. 
Solution overview 
To illustrate this combination, we created a deep researcher implementation (see the GitHub repo) that uses the agent loop capability at the core of the Strands Agents SDK to intelligently and autonomously choose from Tavilyâ€™s web intelligence capabilities. The following diagram illustrates this workflow. 
 
We configured the Strands Agents SDK to use Anthropicâ€™s Claude 4 Sonnet on Amazon Bedrock. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies through a unified API. The following diagram illustrates the solution architecture. 
 
This research agent is composed of three primary components: 
 
 Large language model â€“ Powers the agent to understand queries and generate responses 
 Tools â€“ Helps the agent gather information from the internet using Tavilyâ€™s APIs, format the response, and save the output in Markdown format 
 System prompt â€“ Guides the agentâ€™s behavior, outlining how and when to use each tool to achieve its research objectives 
 
In the following sections, we discuss the LLM and tools in more detail. 
Large language model 
The LLM influences the behavior of the agent as well as the quality of the generated response. We decided to use Anthropicâ€™s Claude 4 Sonnet on Amazon Bedrock for its ability to plan and execute complex tasks, but you can use one of the other models supported by Amazon Bedrock or another model provider. 
 
 from strands.models import BedrockModel
bedrock_model = BedrockModel(
    model_id="us.anthropic.claude-sonnet-4-20250514-v1:0",
    region_name="us-east-1",
)
agent = Agent(model=bedrock_model) 
 
Tools 
Tools help extend agentsâ€™ capabilities and interact with external services such as Tavily. We implemented the following tools to enable our agent to perform deep research over the internet and provide a formatted output: 
 
 web_search â€“ Search the web for relevant information 
 web_extract â€“ Extract the full page content from a webpage 
 web_crawl â€“ Crawl entire websites and scrape their content 
 format_research_response â€“ Transform raw research content into clear, well-structured, and properly cited responses 
 write_markdown_file â€“ Save the research output in Markdown forward on the local file system 
 
To define a tool with the Strands Agents SDK, you can simply wrap a Python function with the @tool decorator and provide a Python docstring with the tool description. Letâ€™s explore an example of how we implemented the web_search tool using Tavilyâ€™s search endpoint.The search endpoint lets agents discover relevant webpages based on a natural language query. Results include URLs, title, content snippets, semantic scores, or even the full content of matched pages. You can fine-tune searches with parameters such as: 
 
 Max number of results â€“ Limits the number of results to an upper bound 
 Time range filtering â€“ Limits the results to content published within a specific time frame 
 Domain restrictions â€“ Restricts results to specific domains 
 
See the following code: 
 
 @tool
def web_search(
    query: str, time_range: str | None = None, include_domains: str | None = None
) -&gt; str:
    """Perform a web search. Returns the search results as a string, with the title, url, and content of each result ranked by relevance.
    Args:
        query (str): The search query to be sent for the web search.
        time_range (str | None, optional): Limits results to content published within a specific timeframe.
            Valid values: 'd' (day - 24h), 'w' (week - 7d), 'm' (month - 30d), 'y' (year - 365d).
            Defaults to None.
        include_domains (list[str] | None, optional): A list of domains to restrict search results to.
            Only results from these domains will be returned. Defaults to None.
    Returns:
        formatted_results (str): The web search results
    """
    client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    formatted_results = format_search_results_for_agent(
        client.search(
            query=query,
            max_results=10,
            time_range=time_range,
            include_domains=include_domains
        )
    )
    return formatted_results 
 
LLMs rely heavily on the tool definition and description to determine how and when to use them. To improve tool accuracy, consider the following best practices: 
 
 Clearly explain when the tool should be used and its functionally 
 Use type hints in the function signature to describe the parameters, return types, and default values 
 Detail each parameter and provide examples of the accepted formats 
 
Each Tavily endpoint can be exposed to a language model as a distinct tool, giving AI agents flexible, granular access to the web. By combining these tools, agents become dramatically more capable at tasks like research, summarization, competitive intelligence, and decision-making. You can find the other tools implementation in the GitHub repository. 
Strategic value proposition 
AWS chose Tavily for the following benefits: 
 
 Shared vision â€“ Tavily and AWS both serve the next generation of AI-based builders, with a strong emphasis on enterprise-readiness, security, and privacy 
 Marketplace integration â€“ Tavily is available on AWS Marketplace, making integration and procurement seamless for enterprise customers 
 Go-to partner for web access â€“ AWS chose Tavily as the premier tool for real-time search integration within the Strands Agents SDK, providing the best web access experience for agent developers 
 Amazon Bedrock â€“ Amazon Bedrock is a fully managed, secure service that offers a choice of high-performing FMs from leading AI companies like Meta, Anthropic, AI21, and Amazon 
 
Conclusion 
The combination of the Strands Agents SDK and Tavily represents a significant advancement in enterprise-grade research agent development. This integration can help organizations build sophisticated, secure, and scalable AI agents while maintaining the highest standards of security and performance. To learn more, refer to the following resources: 
 
 Explore the Strands Agents SDK documentation 
 Sign up for Tavily API access 
 Join the developer community of Strands Agents 
 Start building your first Strands Agents Sample 
 
 
 
About the authors 
Akarsha Sehwag is a Generative AI Data Scientist in Amazon Bedrock Agents GTM team. With over six years of expertise in AI/ML product development, she has built Machine learning solutions across diverse customer segments. 
Lorenzo Micheli is a Principal Delivery Consultant at AWS Professional Services, focused on helping Global Financial Services and Healthcare organizations navigate their cloud journey. He develops strategic roadmaps for generative AI adoption and cloud-native architectures that drive innovation while ensuring alignment with their business objectives and regulatory requirements. 
Dean Sacoransky is a Forward Deployed Engineer at Tavily, specializing in applied AI. He helps enterprises and partners use Tavilyâ€™s web infrastructure technology to power and enhance their AI systems. 
Lee Tzanani is Head of GTM and Partnerships at Tavily. She leads strategic collaborations with Tavilyâ€™s most valuable partners and works with enterprise and Fortune 500 customers to integrate real-time web search into production AI systems. Lee drives Tavilyâ€™s go-to-market efforts across the AI landscape, advancing its mission to onboard the next billion AI agents to the web. 
Sofia Guzowski leads Partnerships and Community at Tavily, where she works with companies to integrate real-time web data into their AI products. She focuses on strategic collaborations, developer engagement, and bringing Tavilyâ€™s APIs to the broader AI landscape.

â¸»