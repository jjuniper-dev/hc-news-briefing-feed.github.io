‚úÖ Morning News Briefing ‚Äì August 31, 2025 10:41

üìÖ Date: 2025-08-31 10:41
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  9.6¬∞C
  Temperature: 9.6&deg;C Pressure / Tendency: 102.3 kPa rising Humidity: 91 % Dewpoint: 8.3&deg:C Wind: WNW 7 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Sunday 31 August 2025 . Weather forecast: 10,000 years from Pem
‚Ä¢ Sunday: Sunny. High 23.
  Fog patches dissipating this morning. Sunny. High 23. Humidex 26. UV index 6 or high. High 26.50C in the early morning of Sunday morning. Fog patches are dissipating today morning. Forecast issued 5:00 AM EDT Sunday 31 August 2025. For the rest of the day, see www.jenn.com/jennennenn .
‚Ä¢ Sunday night: A few clouds. Low 9.
  Fog patches developing overnight . Fog patches expected to develop overnight . A few clouds. Low 9.50 degrees Fahrenheit . Low 5 degrees Fahrenheit for the rest of the day . Forecast issued 5:00 AM EDT Sunday 31 August 2025 for the U.S. Low 41¬∞ FOURTH ON CNN.com Weather Forecast.com weather.com/News/weather.com .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Trump administration cancels $679 million for offshore wind projects at ports
  Ports across the country were planning to become economic hubs for the growing offshore wind industry . The Trump administration is cancelling grants to build the infrastructure for it . The wind industry was planning to be an economic hub for the U.S. offshore wind energy . The government has cancelled grants to help build infrastructure for the wind industry in the United States to build offshore wind infrastructure . The White House
‚Ä¢ 2025 Pok√©mon World Championships show how competitive the game still is
  The annual event pits some of the trading card and video game's most seasoned players against each other . It demonstrates how Pok√©mon has maintained its grip on pop culture . Pok√©mon is a popular video game and trading card game that has been a big part of pop culture for more than a decade . The event is held in New York City, New York, on October 1, at 8 p.
‚Ä¢ Can young Americans still have a better life than their parents? We want to know
  NPR wants to know how economic barriers are affecting you and your political views . For many Gen Z and millennial Americans, concerns about finances and the economy can feel ever present . NPR also wants to hear how you feel about the economy and politics in the U.S. For more information, visit NPR iReport.com/poll.com . Share your thoughts with us at iReport@
‚Ä¢ China's Xi and India's Modi vow to resolve border differences at meeting in Tianjin
  Modi is on his first visit to China since relations between the two countries deteriorated after deadly border clashes in 2020 . Relations between China and India deteriorated after Chinese and Indian soldiers engaged in deadly border clash in 2020. Modi is visiting China for the first time since his visit to the country since 2010 . Relations deteriorated after clashes between Indian and Chinese troops engaged in each other's border clashes, resulting in
‚Ä¢ Houthi rebels say Israeli airstrike killed their prime minister in Yemen's capital
  The Iranian-backed Houthis said an Israeli airstrike killed the prime minister of the rebel-controlled government in the Yemeni capital, Sanaa . Houthis say an Israeli-backed government in Yemen has been killed in an airstrike . The prime minister was killed in the airstrike, according to the Houthis . The Houthis are fighting in Yemen's capital, the capital of Yemen, which is

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ AI spies questionable science journals, with some human help
  About 1,000 of a set of 15,000 open access journals appear to exist mainly to extract fees from naive academics . "Louis, I think this is the beginning of a beautiful friendship," says Louis van Gaal. "Louis" says of his friendship with Louis, "This is the end of a wonderful friendship. Louis thinks this is a good start to a good relationship. "
‚Ä¢ Bring your own brain? Why local LLMs are taking off
  Running AIs on your own machine lets you stick it to the man and save some cash in the process . After a decade or two of the cloud, we're used to paying for our computing capability by the megabyte . As AI takes off, the whole cycle promises to repeat itself again . While AI might seem relatively cheap now, it might not always be so.‚Ä¶‚Ä¶‚Ä¶
‚Ä¢ Programmers: you have to watch your weight, too
  We are drowning in code, but at least some folks are swimming opinion . To fight the enshittification of software, the first step is to pinpoint why and how it happens . Some observers are trying to do that.‚Ä¶‚Ä¶‚Ä¶ Some observers have tried to . do that, but it's not always easy to do, it's a good thing to say. We need to
‚Ä¢ Kilopixel creator kills livestream switch before woodblock display hits Crysis point
  The Kilopixel has been pulled from the sky by its creator . The blocky, robotic marvel has reached the 200,000-pixel mark . All good things must come to an end, and so too must the blocky glory of the Kilopixels . The creator pulled the metaphorical plug on the robot after it reached the milestone of 200,00 pixels . The Kilops
‚Ä¢ Uncle Sam doesn't want Samsung, SK Hynix making memories in China
  Commerce Department moves to make it harder for South Korean memory vendors Samsung and SK Hynix to continue manufacturing in the region . US government already has a lot to say about what products chipmakers can and can't sell in China . End of verified end user status means South Korean vendors will need licenses to bring restricted chipmaking tech into Chinese fabs . South Korean chipmakers will now need

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Cultivating an equity-oriented data sharing culture for African health research initiatives
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Aluminum in childhood vaccines not linked to chronic diseases
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Educational inequalities in sun protection practices among Brazilian adults
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Rethinking suicide related searches and their association with suicide rates, attempts, and self harm hospitalisation
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ COVID-19 prevention behaviors are congruent within social networks of disenfranchised Hispanic and criminal legal involved community members
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: humans in space, and India‚Äôs thorium ambitions
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The case against humans in space



Elon Musk and Jeff Bezos are bitter rivals in the commercial space race, but they agree on one thing: Settling space is an existential imperative. Space is the place. The final frontier. It is our human destiny to transcend our home world and expand our civilization to extraterrestrial vistas.This belief has been mainstream for decades, but its rise has been positively meteoric in this new gilded age of astropreneurs.But as visions of giant orbital stations and Martian cities dance in our heads, a case against human space colonization has found its footing in a number of recent books, from doubts about the practical feasibility of off-Earth communities, to realism about the harsh environment of space and the enormous tax it would exact on the human body. Read the full story.‚ÄîBecky Ferreira



This story is from our new print edition, which is all about the future of security. Subscribe here to catch future copies when they land.







This American nuclear company could help India‚Äôs thorium dream



For just the second time in nearly two decades, the United States has granted an export license to an American company planning to sell nuclear technology to India, MIT Technology Review has learned.&nbsp;



The decision to greenlight Clean Core Thorium Energy‚Äôs license is a major step toward closer cooperation between the two countries on atomic energy and marks a milestone in the development of thorium as an alternative to uranium for fueling nuclear reactors. Read more about why it‚Äôs such a big deal.



‚ÄîAlexander C. Kaufman







RFK Jr‚Äôs plan to improve America‚Äôs diet is missing the point



A lot of Americans don‚Äôt eat well. And they‚Äôre paying for it with their health. A diet high in sugar, sodium, and saturated fat can increase the risk of problems like diabetes, heart disease, and kidney disease, to name a few. And those are among the leading causes of death in the US.



This is hardly news. But this week Robert F Kennedy Jr., who heads the US Department of Health and Human Services, floated a new solution to the problem: teaching medical students more about the role of nutrition in health could help turn things around.



It certainly sounds like a good idea. If more Americans ate a healthier diet, we could expect to see a decrease in those diseases.&nbsp;



But this framing of America‚Äôs health crisis is overly simplistic, especially given that plenty of the administration‚Äôs other actions have directly undermined health in multiple ways‚Äîincluding by canceling a vital nutrition education program. And at any rate, there are other, more effective ways to tackle the chronic-disease crisis. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 RFK Jr‚Äôs deputy has been chosen to be the new acting head of the CDCJim O‚ÄôNeill is likely to greenlight his boss‚Äôs federal vaccine policy plans. (WP $)+ The future of the department looks decidedly precarious. (The Atlantic $)+ Everything you need to know about Jim O‚ÄôNeill, the longevity enthusiast who is now RFK Jr.‚Äôs right-hand man. (MIT Technology Review)



2 A man killed his mother and himself after conversing with ChatGPTThe chatbot encouraged Stein-Erik Soelberg‚Äôs paranoia while repeatedly assuring him he was sane. (WSJ $)+ An AI chatbot told a user how to kill himself‚Äîbut the company doesn‚Äôt want to ‚Äúcensor‚Äù it. (MIT Technology Review)



3 China is cracking down on excess competition in its AI sectorThe country is hellbent on avoiding wasteful investment. (Bloomberg $)+ China is laser-focused on engineering, not so much on litigating. (Wired $)+ China built hundreds of AI data centers to catch the AI boom. Now many stand unused. (MIT Technology Review)



4 The EU should be prepared to walk away from a US trade dealIts competition commissioner worries Trump may act on his threats to target the bloc. (FT $)+ The French President had a similar warning for his ministers. (Politico)



5 xAI has released a new Grok agentic coding modelAt a significantly lower price than its rivals. (Reuters)+ This no-code website builder has been valued at $2 billion. (TechCrunch)+ The second wave of AI coding is here. (MIT Technology Review)



6 A US mail change has thrown online businesses into turmoilAll package deliveries are due to face duties from this week. (Insider $)



7 A former DOGE official is running America‚Äôs biggest MDMA companyAnd Antonio Gracias is not the only member of the department with ties to the psychedelics industry. (The Guardian)+ Other DOGE workers are joining Trump‚Äôs new National Design Studio. (Wired $)+ The FDA said no to the use of MDMA as a therapy last year. (MIT Technology Review)



8 How chatbots fake having personalitiesThey have no persistent self‚Äîdespite what they may tell you. (Ars Technica)+ What is AI? (MIT Technology Review)



9 The future of podcasting is murkyHundreds of shows have folded. The medium is in desperate need of an archive. (NY Mag $)+ The race to save our online lives from a digital dark age. (MIT Technology Review)10 Do we even know what we want to watch anymore?We‚Äôre so reliant on algorithms, it‚Äôs hard to know. (New Yorker $)







Quote of the day



‚ÄúWe‚Äôre scared for ourselves and for the country.‚Äù&nbsp;



‚ÄîAn anonymous CDC worker tells the New York Times about the mood inside the agency following the firing of their new director Susan Monarez.







One more thing







How a tiny Pacific Island became the global capital of cybercrimeTokelau, a string of three isolated atolls strung out across the Pacific, is so remote that it was the last place on Earth to be connected to the telephone‚Äîonly in 1997. Just three years later, the islands received a fax with an unlikely business proposal that would change everything.



It was from an early internet entrepreneur from Amsterdam, named Joost Zuurbier. He wanted to manage Tokelau‚Äôs country-code top-level domain, or ccTLD‚Äîthe short string of characters that is tacked onto the end of a URL‚Äîin exchange for money.



In the succeeding years, tiny Tokelau became an unlikely internet giant‚Äîbut not in the way it may have hoped. Until recently, its .tk domain had more users than any other country‚Äôs: a staggering 25 million‚Äîbut the vast majority were spammers, phishers, and cybercriminals.



Now the territory is desperately trying to clean up .tk. Its international standing, and even its sovereignty, may depend on it. Read the full story.¬†‚ÄîJacob Judah







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Scientists are using yeast to help save the bees.+ How to become super productive + Why North American mammoths were genetic freaks of nature.+ I love Seal‚Äôs steadfast refusal to explain his lyrics to Kiss from a Rose.
‚Ä¢ This American nuclear company could help India‚Äôs thorium dream
  For just the second time in nearly two decades, the United States has granted an export license to an American company planning to sell nuclear technology to India, MIT Technology Review has learned. The decision to greenlight Clean Core Thorium Energy‚Äôs license is a major step toward closer cooperation between the two countries on atomic energy and marks a milestone in the development of thorium as an alternative to uranium for fueling nuclear reactors.&nbsp;



Starting from the issuance last week, the thorium fuel produced by the Chicago-based company can be shipped to reactors in India, where it could be loaded into the cores of existing reactors. Once Clean Core receives final approval from Indian regulators, it will become one of the first American companies to sell nuclear technology to India, just as the world‚Äôs most populous nation has started relaxing strict rules that have long kept the US private sector from entering its atomic power industry.&nbsp;



‚ÄúThis license marks a turning point, not just for Clean Core but for the US-India civil nuclear partnership,‚Äù says Mehul Shah, the company&#8217;s chief executive and founder. ‚ÄúIt places thorium at the center of the global energy transformation.‚Äù



Thorium has long been seen as a good alternative to uranium because it‚Äôs more abundant, produces both smaller amounts of long-lived radioactive waste and fewer byproducts with centuries-long half-lives, and reduces the risk that materials from the fuel cycle will be diverted into weapons manufacturing.&nbsp;



But at least some uranium fuel is needed to make thorium atoms split, making it an imperfect replacement. It‚Äôs also less well suited for use in the light-water reactors that power the vast majority of commercial nuclear plants worldwide. And in any case, the complex, highly regulated nuclear industry is extremely resistant to change.



For India, which has scant uranium reserves but abundant deposits of thorium, the latter metal has been part of a long-term strategy for reducing dependence on imported fuels. The nation started negotiating a nuclear export treaty with the US in the early 2000s, and a 123 Agreement‚Äîa special, Senate-approved treaty the US requires with another country before sending it any civilian nuclear products‚Äîwas approved in 2008.



A new approach



While most thorium advocates have envisioned new reactors designed to run on this fuel, which would mean rebuilding the nuclear industry from the ground up, Shah and his team took a different approach. Clean Core created a new type of fuel that blends thorium with a more concentrated type of uranium called HALEU (high-assay low-enriched uranium). This blended fuel can be used in India‚Äôs pressurized heavy-water reactors, which make up the bulk of the country‚Äôs existing fleet and many of the new units under development now.&nbsp;



Thorium isn‚Äôt a fissile material itself, meaning its atoms aren‚Äôt inherently unstable enough for an extra neutron to easily split the nuclei and release energy. But the metal has what‚Äôs known as ‚Äúfertile properties,‚Äù meaning it can absorb neutrons and transform into the fissile material uranium-233. Uranium-233 produces fewer long-lived radioactive isotopes than the uranium-235 that makes up the fissionable part of traditional fuel pellets. Most commercial reactors run on low-enriched uranium, which is about 5% U-235. When the fuel is spent, roughly 95% of the energy potential is left in the metal. And what remains is a highly toxic cocktail of long-lived radioactive isotopes such as cesium-137 and plutonium-239, which keep the waste dangerous for tens of thousands of years. Another concern is that the plutonium could be extracted for use in weapons.&nbsp;



Enriched up to 20%, HALEU allows reactors to extract more of the available energy and thus reduce the volume of waste. Clean Core‚Äôs fuel goes further: The HALEU provides the initial spark to ignite fertile thorium and triggers a reaction that can burn much hotter and utilize the vast majority of the material in the core, as a study published last year in the journal Nuclear Engineering and Design showed.





‚ÄúThorium provides attributes needed to achieve higher burnups,‚Äù says Koroush Shirvan, an MIT professor of nuclear science and engineering who helped design Clean Core‚Äôs fuel assemblies. ‚ÄúIt is enabling technology to go to higher burnups, which reduces your spent fuel volume, increases your fuel efficiency, and reduces the amount of uranium that you need.‚Äù&nbsp;



Compared with traditional uranium fuel, Clean Core says, its fuel reduces waste by more than 85% while avoiding the most problematic isotopes produced during fission. ‚ÄúThe result is a safer, more sustainable cycle that reframes nuclear power not as a source of millennia-long liabilities but as a pathway to cleaner energy and a viable future fuel supply,‚Äù says Milan Shah, Clean Core‚Äôs chief operating officer and Mehul‚Äôs son.



Pressurized heavy-water reactors are particularly well suited to thorium because heavy water‚Äîa version of H2O that has an extra neutron on the hydrogen atom‚Äîabsorbs fewer neutrons during the fission process, increasing efficiency by allowing more neutrons to be captured by the thorium. 



There are 46 so-called PHWRs operating worldwide: 17 in Canada, 19 in India, three each in Argentina and South Korea, and two each in China and Romania, according to data from the International Atomic Energy Agency. In 1954, India set out a three-stage development plan for nuclear power that involved eventually phasing thorium into the fuel cycle for its fleet.¬†



Yet in the 56 years since India built its first commercial nuclear plant, its state-controlled industry has remained relatively shut off to the private sector and the rest of the world. When the US signed the 123 Agreement with India in 2008, the moment heralded an era in which the subcontinent could become a testing ground for new American reactor designs.&nbsp;



In 2010, however, India passed the Civil Liability for Nuclear Damage Act. The legislation was based on what lawmakers saw as legal shortcomings in the wake of the 1984 Bhopal chemical factory disaster, when a subsidiary of the American industrial giant Dow Chemical avoided major payouts to the victims of a catastrophe that killed thousands. Under this law, responsibility for an accident at an Indian nuclear plant would fall on suppliers. The statute effectively killed any exports to India, since few companies could shoulder that burden. Only Russia‚Äôs state-owned Rosatom charged ahead with exporting reactors to India.



But things are changing. In a joint statement issued after a February 2025 summit, Prime Minister Narendra Modi and President Donald Trump &#8220;announced their commitment to fully realise the US-India 123 Civil Nuclear Agreement by moving forward with plans to work together to build US-designed nuclear reactors in India through large scale localisation and possible technology transfer.‚Äù¬†



In March 2025, US federal officials gave the nuclear developer Holtec International an export license to sell Indian companies its as-yet-unbuilt small modular reactors, which are based on the light-water reactor design used in the US. In April, the Indian government suggested it would reform the nuclear liability law to relax rules on foreign companies in hopes of drawing more overseas developers. Last month, a top minister confirmed that the Modi administration would overhaul the law.¬†



‚ÄúFor India, the thing they need to do is get another international vendor in the marketplace,‚Äù says Chris Gadomski, the chief nuclear analyst at the consultancy BloombergNEF.



Path of least resistance



But Shah sees larger potential for Clean Core. Unlike Holtec, whose export license was endorsed by the two Mumbai-based industrial giants Larsen &amp; Toubro and Tata Consulting Engineers, Clean Core had its permit approved by two of India‚Äôs atomic regulators and its main state-owned nuclear company. By focusing on fuel rather than new reactors, Clean Core could become a vendor to the majority of the existing plants already operating in India.&nbsp;



Its technology diverges not only from that of other US nuclear companies but also from the approach used in China. Last year, China made waves by bringing its first thorium-fueled reactor online. This enabled it to establish a new foothold in a technology the US had invented and then abandoned, and it gave Beijing another leg up in atomic energy. 



But scaling that technology will require building out a whole new kind of reactor. That comes at a cost. A recent Johns Hopkins University study found that China‚Äôs success in building nuclear reactors stemmed in large part from standardization and repetition of successful designs, virtually all of which have been light-water reactors. Using thorium in existing heavy-water reactors lowers the bar for popularizing the fuel, according to the younger Shah.¬†



‚ÄúWe think ours is the path of least resistance,‚Äù Milan Shah says. ‚ÄúMaybe not being completely revolutionary in the way you look at nuclear today, but incredibly evolutionary to progress humanity forward.‚Äù&nbsp;



The company has plans to go beyond pressurized heavy-water reactors. Within two years, the elder Shah says, Clean Core plans to design a version of its fuel that could work in the light-water reactors that make up the entire US fleet of 94. But it‚Äôs not a simple conversion. For starters, there‚Äôs the size: While the PHWR fuel rods are about 50 centimeters in length, the rods that go into light-water reactors are roughly four meters long. Then there‚Äôs the history of challenges with light water‚Äôs absorption of neutrons that could otherwise be captured to induce fission in the thorium.&nbsp;



For Anil Kakodkar, the former chairman of India‚Äôs Atomic Energy Commission and a mentor to Shah, popularizing thorium could help rectify one of the darker chapters in his country‚Äôs nuclear development. In 1974, India became the first country since the signing of the first global Treaty on the Non-Proliferation of Nuclear Weapons to successfully test an atomic weapon. New Delhi was never a signatory to the pact. But the milestone prompted neighboring Pakistan to develop its own weapons.&nbsp;



In response, President Jimmy Carter tried to demonstrate Washington‚Äôs commitment to reversing the Cold War arms race by sacrificing the first US effort to commercialize nuclear waste recycling, since the technology to separate plutonium and other radioisotopes from uranium in spent fuel was widely seen as a potential new source of weapons-grade material. By running its own reactors on thorium, Kakodkar says, India can chart a new path for newcomer nations that want to harness the power of the atom without stoking fears that nuclear weapons capability will spread.&nbsp;



‚ÄúThe proliferation concerns will be dismissed to a significant extent, allowing more rapid growth of nuclear power in emerging countries,‚Äù he says. ‚ÄúThat will be a good thing for the world at large.‚Äù&nbsp;



Alexander C. Kaufman is a reporter who has covered energy, climate change, pollution, business, and geopolitics for more than a decade.&nbsp;
‚Ä¢ RFK Jr‚Äôs plan to improve America‚Äôs diet is missing the point
  A lot of Americans don‚Äôt eat well. And they‚Äôre paying for it with their health. A diet high in sugar, sodium, and saturated fat can increase the risk of problems like diabetes, heart disease, and kidney disease, to name a few. And those are among¬†the leading causes of death in the US.



This is hardly news. But this week Robert F Kennedy Jr., who heads the US Department of Health and Human Services, floated a new solution to the problem. Kennedy and education secretary Linda McMahon think that teaching medical students more about the role of nutrition in health could help turn things around.





‚ÄúI‚Äôm working with Linda on forcing medical schools ‚Ä¶ to put nutrition into medical school education,‚Äù Kennedy said during¬†a cabinet meeting on August 26. The next day, HHS released¬†a statement calling for ‚Äúincreased nutrition education‚Äù for medical students.



‚ÄúWe can reverse the chronic-disease epidemic simply by changing our diets and lifestyles,‚Äù Kennedy said in¬†an accompanying video statement. ‚ÄúBut to do that, we need nutrition to be a basic part of every doctor‚Äôs training.‚Äù



It certainly sounds like a good idea. If more Americans ate a healthier diet, we could expect to see a decrease in those diseases. But this framing of America‚Äôs health crisis is overly simplistic, especially given that plenty of the administration‚Äôs other actions have directly undermined health in multiple ways‚Äîincluding by canceling a vital nutrition education program.



At any rate, there are other, more effective ways to tackle the chronic-disease crisis.



The biggest killers, heart disease and stroke,¬†are responsible for more than a third of deaths, according to the US Centers for Disease Control and Prevention. A healthy diet can reduce your risk of developing those conditions. And it makes total sense to educate the future doctors of America about nutrition.



Medical bodies are on board with the idea, too. ‚ÄúThe importance of nutrition in medical education is increasingly clear, and we support expanded, evidence-based instruction to better equip physicians to prevent and manage chronic disease and improve patient outcomes,‚Äù David H. Aizuss, chair of the American Medical Association‚Äôs board of trustees, said in a statement.



But it‚Äôs not as though medical students aren‚Äôt getting any nutrition education. And that training has increased in the last five years, according to¬†surveys carried out by the American Association of Medical Colleges.



Kennedy has referred to¬†a 2021 survey suggesting that medical students in the US get only around one hour of nutrition education per year. But the AAMC argues that nutrition education increasingly happens through ‚Äúintegrated experiences‚Äù rather than stand-alone lectures.





‚ÄúMedical schools understand the critical role that nutrition plays in preventing, managing, and treating chronic health conditions, and incorporate significant nutrition education across their required curricula,‚Äù Alison J. Whelan, AAMC‚Äôs chief academic officer, said in a statement.



That‚Äôs not to say there isn‚Äôt room for improvement. Gabby Headrick, a food systems dietician and associate director of food and nutrition policy at George Washington University‚Äôs Institute for Food Safety &amp; Nutrition Security, thinks nutritionists could take a more prominent role in patient care, too.



But it‚Äôs somewhat galling for the administration to choose medical education as its focus given the recent cuts in federal funding that will affect health. For example, funding for the National Diabetes Prevention Program, which offers support and guidance to help thousands of people adopt healthy diets and exercise routines, was canceled by the Trump administration in March.



The focus on medical schools also overlooks one of the biggest factors behind poor nutrition in the US: access to healthy food. A recent¬†survey by the Pew Research Center found that increased costs make it harder for most Americans to eat well. Twenty percent of the people surveyed acknowledged that their diets were not healthy.



‚ÄúSo many people know what a healthy diet is, and they know what should be on their plate every night,‚Äù says Headrick, who has researched this issue. ‚ÄúBut the vast majority of folks just truly do not have the money or the time to get the food on the plate.‚Äù



The Supplemental Nutrition Assistance Program (SNAP) has been helping low-income Americans afford some of those healthier foods. It¬†supported over 41 million people in 2024. But under the Trump administration‚Äôs tax and spending bill,¬†the program is set to lose around $186 billion in funding over the next 10 years.



Kennedy‚Äôs focus is on education. And it just so happens that there is a nutrition education program in place‚Äîone that helps people of all ages learn not only what healthy foods are, but how to source them on a budget and use them to prepare meals.



SNAP-Ed, as it‚Äôs known, has already provided this support to millions of Americans. Under the Trump administration, it is¬†set to be eliminated.



It is difficult to see how these actions are going to help people adopt healthier diets. What might be a better approach? I put the question to Headrick: If she were in charge, what policies would she enact?



‚ÄúUniversal health care,‚Äù she told me. Being able to access health care without risking financial hardship not only¬†improves health outcomes and life expectancy; it also spares people from medical debt‚Äîsomething that¬†affects around 40% of adults in the US, according to a recent survey.



And the Trump administration‚Äôs plans to¬†cut federal health spending by about a trillion dollars over the next decade certainly aren‚Äôt going to help with that. All told, around 16 million people could lose their health insurance by 2034, according to¬†estimates by the Congressional Budget Office.



‚ÄúThe evidence suggests that if we cut folks‚Äô social benefit programs, such as access to health care and food, we are going to see detrimental impacts,‚Äù says Headrick. ‚ÄúAnd it‚Äôs going to cause an increased burden of preventable disease.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ The Download: Google‚Äôs AI energy use, and the AI Hype Index
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Google‚Äôs still not giving us the full picture on AI energy use&nbsp;



‚ÄîCasey Crownhart



Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That‚Äôs about the same as running a microwave for one second‚Äîsomething that feels insignificant. I run the microwave for many more seconds than that most days.I welcome more openness from major AI players about their estimated energy use per query. But I‚Äôve noticed that some folks are taking this number and using it to conclude that we don‚Äôt need to worry about AI‚Äôs energy demand. That‚Äôs not the right takeaway here. Let‚Äôs dig into why.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.



+ If you‚Äôre interested in AI‚Äôs energy footprint, earlier this year, MIT Technology Review published Power Hungry: a comprehensive series on AI and energy.







The AI Hype Index: AI-designed antibiotics show promise



Separating AI reality from hyped-up fiction isn‚Äôt always easy. That‚Äôs why we‚Äôve created the AI Hype Index‚Äîa simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month‚Äôs edition here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The White House has fired the director of the CDCBut Susan Monarez is refusing to go quietly. (WP $)+ Monarez is said to have clashed with RFK Jr over vaccine policy. (NYT $)+ She was confirmed by the Senate to the position just last month. (The Guardian)+ Vaccine consensus is splintering across the US. (Vox)



2 A Chinese hacking campaign hit at least 200 US organizationsIntelligence agencies say the breaches are among the most significant ever. (WP $)+ AI-generated ransomware is on the rise. (Wired $)



3 Ukraine‚Äôs new Flamingo cruise missile took just months to buildRussia‚Äôs air defenses are weakening. Can this missile exploit the gaps? (Economist $)+ 14 people were killed in an overnight bombardment of Kyiv. (BBC)+ On the ground in Ukraine‚Äôs largest Starlink repair shop. (MIT Technology Review)



4 AI infrastructure spending is boosting the US economyCompanies are throwing so much money at AI hardware it‚Äôs lifting the real economy, not just the stock market. (NYT $)+ How to fine-tune AI for prosperity. (MIT Technology Review)5 OpenAI and Anthropic safety-tested each other‚Äôs AIThey found Claude is a lot more cautious than OpenAI‚Äôs mini models. (Engadget)+ Sycophancy was a repeated issue among OpenAI‚Äôs models. (TechCrunch)+ This benchmark used Reddit‚Äôs AITA to test how much AI models suck up to us. (MIT Technology Review)



6 Climate change exacerbated Europe‚Äôs deadly wildfiresAnd fires across the Mediterranean are likely to become more frequent and severe. (BBC)+ What the collapse of a glacier can teach us. (New Yorker $)+ How AI can help spot wildfires. (MIT Technology Review)



7 911 centers are using AI to answer callsIt‚Äôs helping to triage anything that isn‚Äôt urgent. (TechCrunch)



8 Wikipedia has compiled a list of AI writing tropesBut their presence still isn‚Äôt a dead giveaway a text has been written by AI. (Fast Company $)+ AI-text detection tools are really easy to fool. (MIT Technology Review)



9 Melania Trump has launched the Presidential AI Challenge¬†But it‚Äôs not all that clear what the competition actually is. (NY Mag $)



10 Netflix‚Äôs algorithm-appeasing movies are bland and boringBut millions of people will watch them anyway. (The Guardian)







Quote of the day



&#8220;The more you buy, the more you grow.&#8221;



‚ÄîNvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.







One more thinghttps://www.technologyreview.com/2025/01/13/1109922/inside-the-strange-limbo-facing-ivf-embryos/?utm_source=the_download&amp;utm_medium=email&amp;utm_campaign=the_download.unpaid.engagement&amp;utm_term=*|SUBCLASS|*&amp;utm_content=*|DATE:m-d-Y|*







Inside the strange limbo facing millions of IVF embryosMillions of embryos created through IVF sit frozen in time, stored in cryopreservation tanks around the world, and the number is only growing.At a basic level, an embryo is simply a tiny ball of a hundred or so cells. But unlike other types of body tissue, it holds the potential for life. Many argue that this endows embryos with a special moral status, one that requires special protections.The problem is that no one can really agree on what that status is. What do these embryos mean to us? And who should be responsible for them? Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Wow, that is one seriously orange shark!+ TikTok is a proven way to introduce younger generations to older music‚Äîand now it‚Äôs Radiohead‚Äôs turn.+ Why we‚Äôre still going bananas for Donkey Kong after all these years+ This photo perfectly captures the joy of letting loose at a wedding.
‚Ä¢ Creating a qubit fit for a quantum future
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üîí Cybersecurity & Privacy
‚Ä¢ Affiliates Flock to ‚ÄòSoulless‚Äô Scam Gambling Machine
  Last month, KrebsOnSecurity tracked the sudden emergence of hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. We&#8217;ve since learned that these scam gambling sites have proliferated thanks to a new Russian affiliate program called &#8220;Gambler Panel&#8221; that bills itself as a &#8220;soulless project that is made for profit.&#8221;
A machine-translated version of Gambler Panel&#8217;s affiliate website.
The scam begins with deceptive ads posted on social media that claim the wagering sites are working in partnership with popular athletes or social media personalities. The ads invariably state that by using a supplied &#8220;promo code,&#8221; interested players can claim a $2,500 credit on the advertised gaming website.
The gaming sites ask visitors to create a free account to claim their $2,500 credit, which they can use to play any number of extremely polished video games that ask users to bet on each action. However, when users try to cash out any &#8220;winnings&#8221; the gaming site will reject the request and prompt the user to make a ‚Äúverification deposit‚Äù of cryptocurrency ‚Äî typically around $100 ‚Äî before any money can be distributed.
Those who deposit cryptocurrency funds are soon pressed into more wagering and making additional deposits. And &#8212; shocker alert &#8212; all players eventually lose everything they&#8217;ve invested in the platform.
The number of scam gambling or &#8220;scambling&#8221; sites has skyrocketed in the past month, and now we know why: The sites all pull their gaming content and detailed strategies for fleecing players straight from the playbook created by Gambler Panel, a Russian-language affiliate program that promises affiliates up to 70 percent of the profits.

Gambler Panel&#8217;s website gambler-panel[.]com links to a helpful wiki that explains the scam from cradle to grave, offering affiliates advice on how best to entice visitors, keep them gambling, and extract maximum profits from each victim.
&#8220;We have a completely self-written from scratch FAKE CASINO engine that has no competitors,&#8221; Gambler Panel&#8217;s wiki enthuses. &#8220;Carefully thought-out casino design in every pixel, a lot of audits, surveys of real people and test traffic floods were conducted, which allowed us to create something that has no doubts about the legitimacy and trustworthiness even for an inveterate gambling addict with many years of experience.&#8221;
Gambler Panel explains that the one and only goal of affiliates is to drive traffic to these scambling sites by any and all means possible.
A machine-translated portion of Gambler Panel&#8217;s singular instruction for affiliates: Drive traffic to these scambling sites by any means available.
&#8220;Unlike white gambling affiliates, we accept absolutely any type of traffic, regardless of origin, the only limitation is the CIS countries,&#8221; the wiki continued, referring to a common prohibition against scamming people in Russia and former Soviet republics in the Commonwealth of Independent States.
The program&#8217;s website claims it has more than 20,000 affiliates, who earn a minimum of $10 for each verification deposit. Interested new affiliates must first get approval from the group&#8217;s Telegram channel, which currently has around 2,500 active users.
The Gambler Panel channel is replete with images of affiliate panels showing the daily revenue of top affiliates, scantily-clad young women promoting the Gambler logo, and fast cars that top affiliates claimed they bought with their earnings.
A machine-translated version of the wiki for the affiliate program Gambler Panel.
The apparent popularity of this scambling niche is a consequence of the program&#8217;s ease of use and detailed instructions for successfully reproducing virtually every facet of the scam. Indeed, much of the tutorial focuses on advice and ready-made templates to help even novice affiliates drive traffic via social media websites, particularly on Instagram and TikTok.
Gambler Panel also walks affiliates through a range of possible responses to questions from users who are trying to withdraw funds from the platform. This section, titled &#8220;Rules for working in Live chat,&#8221; urges scammers to respond quickly to user requests (1-7 minutes), and includes numerous strategies for keeping the conversation professional and the user on the platform as long as possible.
A machine-translated version of the Gambler Panel&#8217;s instructions on managing chat support conversations with users.
The connection between Gambler Panel and the explosion in the number of scambling websites was made by a 17-year-old developer who operates multiple Discord servers that have been flooded lately with misleading ads for these sites.
The researcher, who asked to be identified only by the nickname &#8220;Thereallo,&#8221; said Gambler Panel has built a scalable business product for other criminals.
&#8220;The wiki is kinda like a &#8216;how to scam 101&#8217; for criminals written with the clarity you would expect from a legitimate company,&#8221; Thereallo said. &#8220;It&#8217;s clean, has step by step guides, and treats their scam platform like a real product. You could swap out the content, and it could be any documentation for startups.&#8221;
&#8220;They&#8217;ve minimized their own risk &#8212; spreading the links on Discord / Facebook / YT Shorts, etc. &#8212; and outsourced it to a hungry affiliate network, just like a franchise,&#8221; Thereallo wrote in response to questions.
&#8220;A centralized platform that can serve over 1,200 domains with a shared user base, IP tracking, and a custom API is not at all a trivial thing to build,&#8221; Thereallo said. &#8220;It&#8217;s a scalable system designed to be a resilient foundation for thousands of disposable scam sites.&#8221;
The security firm Silent Push has compiled a list of the latest domains associated with the Gambler Panel, available here (.csv).
‚Ä¢ DSLRoot, Proxies, and the Threat of ‚ÄòLegal Botnets‚Äô
  The cybersecurity community on Reddit responded in disbelief this month when a self-described Air National Guard member with top secret security clearance began questioning the arrangement they&#8217;d made with company called DSLRoot, which was paying $250 a month to plug a pair of laptops into the Redditor&#8217;s high-speed Internet connection in the United States. This post examines the history and provenance of DSLRoot, one of the oldest &#8220;residential proxy&#8221; networks with origins in Russia and Eastern Europe.

The query about DSLRoot came from a Reddit user &#8220;Sacapoopie,&#8221; who did not respond to questions. This user has since deleted the original question from their post, although some of their replies to other Reddit cybersecurity enthusiasts remain in the thread. The original post was indexed here by archive.is, and it began with a question:
&#8220;I have been getting paid 250$ a month by a residential IP network provider named DSL root to host devices in my home,&#8221; Sacapoopie wrote. &#8220;They are on a separate network than what we use for personal use. They have dedicated DSL connections (one per host) to the ISP that provides the DSL coverage. My family used Starlink. Is this stupid for me to do? They just sit there and I get paid for it. The company pays the internet bill too.&#8221;
Many Redditors said they assumed Sacapoopie&#8217;s post was a joke, and that nobody with a cybersecurity background and top-secret (TS/SCI) clearance would agree to let some shady residential proxy company introduce hardware into their network. Other readers pointed to a slew of posts from Sacapoopie in the Cybersecurity subreddit over the past two years about their work on cybersecurity for the Air National Guard.
When pressed for more details by fellow Redditors, Sacapoopie described the equipment supplied by DSLRoot as &#8220;just two laptops hardwired into a modem, which then goes to a dsl port in the wall.&#8221;

&#8220;When I open the computer, it looks like [they] have some sort of custom application that runs and spawns several cmd prompts,&#8221; the Redditor explained. &#8220;All I can infer from what I see in them is they are making connections.&#8221;
When asked how they became acquainted with DSLRoot, Sacapoopie told another user they discovered the company and reached out after viewing an advertisement on a social media platform.
&#8220;This was probably 5-6 years ago,&#8221; Sacapoopie wrote. &#8220;Since then I just communicate with a technician from that company and I help trouble shoot connectivity issues when they arise.&#8221;
Reached for comment, DSLRoot said its brand has been unfairly maligned thanks to that Reddit discussion. The unsigned email said DSLRoot is fully transparent about its goals and operations, adding that it operates under full consent from its &#8220;regional agents,&#8221; the company&#8217;s term for U.S. residents like Sacapoopie.
&#8220;As although we support honest journalism, we&#8217;re against of all kinds of &#8216;low rank/misleading Yellow Journalism&#8217; done for the sake of cheap hype,&#8221; DSLRoot wrote in reply. &#8220;It&#8217;s obvious to us that whoever is doing this, is either lacking a proper understanding of the subject or doing it intentionally to gain exposure by misleading those who lack proper understanding,&#8221; DSLRoot wrote in answer to questions about the company&#8217;s intentions.
&#8220;We monitor our clients and prohibit any illegal activity associated with our residential proxies,&#8221; DSLRoot continued. &#8220;We honestly didn&#8217;t know that the guy who made the Reddit post was a military guy. Be it an African-American granny trying to pay her rent or a white kid trying to get through college, as long as they can provide an Internet line or host phones for us &#8212; we&#8217;re good.&#8221;
WHAT IS DSLROOT?
DSLRoot is sold as a residential proxy service on the forum BlackHatWorld under the name DSLRoot and GlobalSolutions. The company is based in the Bahamas and was formed in 2012. The service is advertised to people who are not in the United States but who want to seem like they are. DSLRoot pays people in the United States to run the company&#8217;s hardware and software &#8212; including 5G mobile devices &#8212; and in return it rents those IP addresses as dedicated proxies to customers anywhere in the world &#8212; priced at $190 per month for unrestricted access to all locations.
The DSLRoot website.
The GlobalSolutions account on BlackHatWorld lists a Telegram account and a WhatsApp number in Mexico. DSLRoot&#8217;s profile on the marketing agency digitalpoint.com from 2010 shows their previous username on the forum was &#8220;Incorptoday.&#8221; GlobalSolutions user accounts at bitcointalk[.]org and roclub[.]com include the email clickdesk@instantvirtualcreditcards[.]com.
Passive DNS records from DomainTools.com show instantvirtualcreditcards[.]com shared a host back then &#8212; 208.85.1.164 &#8212; with just a handful of domains, including dslroot[.]com, regacard[.]com, 4groot[.]com, residential-ip[.]com, 4gemperor[.]com, ip-teleport[.]com, proxysource[.]net and proxyrental[.]net.
Cyber intelligence firm Intel 471 finds GlobalSolutions registered on BlackHatWorld in 2016 using the email address prepaidsolutions@yahoo.com. This user shared that their birthday is March 7, 1984.
Several negative reviews about DSLRoot on the forums noted that the service was operated by a BlackHatWorld user calling himself &#8220;USProxyKing.&#8221; Indeed, Intel 471 shows this user told fellow forum members in 2013 to contact him at the Skype username &#8220;dslroot.&#8221;
USProxyKing on BlackHatWorld, soliciting installations of his adware via torrents and file-sharing sites.
USProxyKing had a reputation for spamming the forums with ads for his residential proxy service, and he ran a &#8220;pay-per-install&#8221; program where he paid affiliates a small commission each time one of their websites resulted in the installation of his unspecified &#8220;adware&#8221; programs &#8212; presumably a program that turned host PCs into proxies. On the other end of the business, USProxyKing sold that pay-per-install access to others wishing to distribute questionable software &#8212; at $1 per installation.
Private messages indexed by Intel 471 show USProxyKing also raised money from nearly 20 different BlackHatWorld members who were promised shareholder positions in a new business that would offer robocalling services capable of placing 2,000 calls per minute.
Constella Intelligence, a platform that tracks data exposed in breaches, finds that same IP address GlobalSolutions used to register at BlackHatWorld was also used to create accounts at a handful of sites, including a GlobalSolutions user account at WebHostingTalk that supplied¬†the email address incorptoday@gmail.com. Also registered to incorptoday@gmail.com are the domains dslbay[.]com, dslhub[.]net, localsim[.]com, rdslpro[.]com, virtualcards[.]biz/cc, and virtualvisa[.]cc.
Recall that DSLRoot&#8217;s profile on digitalpoint.com was previously named Incorptoday. DomainTools says incorptoday@gmail.com is associated with almost two dozen domains going back to 2008, including incorptoday[.]com, a website that offers to incorporate businesses in several states, including Delaware, Florida and Nevada, for prices ranging from $450 to $550.
As we can see in this archived copy of the site from 2013, IncorpToday also offered a premiere service for $750 that would allow the customer&#8217;s new company to have a retail checking account, with no questions asked.
Global Solutions is able to provide access to the U.S. banking system by offering customers prepaid cards that can be loaded with a variety of virtual payment instruments that were popular in Russian-speaking countries at the time, including WebMoney. The cards are limited to $500 balances, but non-Westerners can use them to anonymously pay for goods and services at a variety of Western companies. Cardnow[.]ru, another domain registered to incorptoday@gmail.com, demonstrates this in action.
A copy of Incorptoday&#8217;s website from 2013 offers non-US residents a service to incorporate a business in Florida, Delaware or Nevada, along with a no-questions-asked checking account, for $750.
WHO IS ANDREI HOLAS?
The oldest domain (2008) registered to incorptoday@gmail.com is andrei[.]me; another is called andreigolos[.]com. DomainTools says these and other domains registered to that email address include the registrant name Andrei Holas, from Huntsville, Ala.
Public records indicate Andrei Holas has lived with his brother &#8212; Aliaksandr Holas &#8212; at two different addresses in Alabama. Those records state that Andrei Holas&#8217; birthday is in March 1984, and that his brother is slightly younger. The younger brother did not respond to a request for comment.
Andrei Holas maintained an account on the Russian social network Vkontakte under the email address ryzhik777@gmail.com, an address that shows up in numerous records hacked and leaked from Russian government entities over the past few years.
Those records indicate Andrei Holas and his brother are from Belarus and have maintained an address in Moscow for some time (that address is roughly three blocks away from the main headquarters of the Russian FSB, the successor intelligence agency to the KGB). Hacked Russian banking records show Andrei Holas&#8217; birthday is March 7, 1984 &#8212; the same birth date listed by GlobalSolutions on BlackHatWorld.
A 2010 post by ryzhik777@gmail.com at the Russian-language forum Ulitka explains that the poster was having trouble getting his B1/B2 visa to visit his brother in the United States, even though he&#8217;d previously been approved for two separate guest visas and a student visa. It remains unclear if one, both, or neither of the Holas brothers still lives in the United States. Andrei explained in 2010 that his brother was an American citizen.
LEGAL BOTNETS
We can all wag our fingers at military personnel who should undoubtedly know better than to install Internet hardware from strangers, but in truth there is an endless supply of U.S. residents who will resell their Internet connection if it means they can make a few bucks out of it. And these days, there are plenty of residential proxy providers who will make it worth your while.
Traditionally, residential proxy networks have been constructed using malicious software that quietly turns infected systems into traffic relays that are then sold in shadowy online forums. Most often, this malware gets bundled with popular cracked software and video files that are uploaded to file-sharing networks and that secretly turn the host device into a traffic relay. In fact, USPRoxyKing bragged that he routinely achieved thousands of installs per week via this method alone.
These days, there a number of residential proxy networks that entice users to monetize their unused bandwidth (inviting you to violate the terms of service of your ISP in the process); others, like DSLRoot, act as a communal VPN, and by using the service you gain access to the connections of other proxies (users) by default, but you also agree to share your connection with others.
Indeed, Intel 471&#8217;s archives show the GlobalSolutions and DSLRoot accounts routinely received private messages from forum users who were college students or young people trying to make ends meet. Those messages show that many of DSLRoot&#8217;s &#8220;regional agents&#8221; often sought commissions to refer friends interested in reselling their home Internet connections (DSLRoot would offer to cover the monthly cost of the agent&#8217;s home Internet connection).
But in an era when North Korean hackers are relentlessly posing as Western IT workers by paying people to host laptop farms in the United States, letting strangers run laptops, mobile devices or any other hardware on your network seems like an awfully risky move regardless of your station in life. As several Redditors pointed out in Sacapoopie&#8217;s thread, an Arizona woman was sentenced in July 2025 to 102 months in prison for hosting a laptop farm that helped North Korean hackers secure jobs at more than 300 U.S. companies, including Fortune 500 firms.
Lloyd Davies is the founder of Infrawatch, a London-based security startup that tracks residential proxy networks. Davies said he reverse engineered the software that powers DSLRoot&#8217;s proxy service, and found it phones home to the aforementioned domain proxysource[.]net, which sells a service that promises to &#8220;get your ads live in multiple cities without getting banned, flagged or ghosted&#8221; (presumably a reference to CraigsList ads).
Davies said he found the DSLRoot installer had capabilities to remotely control residential networking equipment across multiple vendor brands.
Image: Infrawatch.app.
&#8220;The software employs vendor-specific exploits and hardcoded administrative credentials, suggesting DSLRoot pre-configures equipment before deployment,&#8221; Davies wrote in an analysis published today. He said the software performs WiFi network enumeration to identify nearby wireless networks, thereby &#8220;potentially expanding targeting capabilities beyond the primary internet connection.&#8221;
It&#8217;s unclear exactly when the USProxyKing was usurped from his throne, but DSLRoot and its proxy offerings are not what they used to be. Davies said the entire DSLRoot network now has fewer than 300 nodes nationwide, mostly systems on DSL providers like CenturyLink and Frontier.
On Aug. 17, GlobalSolutions posted to BlackHatWorld saying, &#8220;We&#8217;re restructuring our business model by downgrading to &#8216;DSL only&#8217; lines (no mobile or cable).&#8221; Asked via email about the changes, DSLRoot blamed the decline in his customers on the proliferation of residential proxy services.
&#8220;These days it has become almost impossible to compete in this niche as everyone is selling residential proxies and many companies want you to install a piece of software on your phone or desktop so they can resell your residential IPs on a much larger scale,&#8221; DSLRoot explained. &#8220;So-called &#8216;legal botnets&#8217; as we see them.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Crescent library brings privacy to digital identity systems
  Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance.&nbsp;



One of these risks is linkability, the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver&#8217;s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior.&nbsp;&nbsp;



To address this, we have released Crescent (opens in new tab), a cryptographic library that adds unlinkability to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver&#8217;s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems. &nbsp;



The protection goes beyond existing privacy features. Some digital identity systems already offer selective disclosure, allowing users to share only specific pieces of information in each interaction. &nbsp;



But even with selective disclosure, credentials can still be linked through serial numbers, cryptographic signatures, or embedded identifiers. Crescent&#8217;s unlinkability feature is designed to prevent anything in the credential, beyond what a user explicitly chooses to reveal, from being used to connect their separate digital interactions.



Figure 1: Unlinkability between a credential issuance and presentation



Two paths to unlinkability&nbsp;



To understand how Crescent works, it helps to examine the two main approaches researchers have developed for adding unlinkability to identity systems:&nbsp;




Specialized cryptographic signature schemes. These schemes can provide unlinkability but require extensive changes to existing infrastructure. New algorithms must be standardized, implemented, and integrated into software and hardware platforms. For example, the BBS (opens in new tab) signature scheme is currently being standardized by the Internet Engineering Task Force (IETF), but even after completion, adoption may be slow.&nbsp;&nbsp;&nbsp;





Zero-knowledge proofs with existing credentials. This approach, used by Crescent (opens in new tab), allows users to prove specific facts about their credentials without revealing the underlying data that could enable tracking. For example, someone could prove they hold a valid driver&#8217;s license and live in a particular ZIP code without exposing any other personal information or identifiers that could link this interaction to future ones.&nbsp;




Zero-knowledge proofs have become more practical since they were first developed 40 years ago but they are not as efficient as the cryptographic algorithms used in today‚Äôs credentials. Crescent addresses this computational challenge through preprocessing, performing the most complex calculations once in advance so that later proof generation is quick and efficient for mobile devices.&nbsp;



Beyond unlinkability, Crescent supports selective disclosure, allowing users to prove specific facts without revealing unnecessary details. For example, it can confirm that a credential is valid and unexpired without disclosing the exact expiration date, which might otherwise serve as a unique identifier. These privacy protections work even when credentials are stored in a phone&#8217;s secure hardware, which keeps them tied to the device and prevents unauthorized access.



	
		

		
		Spotlight: Event Series
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Forum
				
								Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.
				
								
					
						
							Watch on-demand						
					
				
							
	
Opens in a new tab	
	


Behind the cryptographic curtain&nbsp;



At its core, Crescent uses a sophisticated form of cryptographic proof called a zero-knowledge SNARK (Zero-Knowledge Succinct Noninteractive Argument of Knowledge). This method allows one party to prove possession of information or credentials without revealing the underlying data itself.&nbsp;



Crescent specifically uses the Groth16 proof system, one of the first practical implementations of this technology. What makes Groth16 particularly useful is that its proofs are small in size, quick to verify, and can be shared in a single step without back-and-forth communication between the user and verifier.&nbsp;



The system works by first establishing shared cryptographic parameters based on a credential template. Multiple organizations issuing similar credentials, such as different state motor vehicle departments issuing mobile driver&#8217;s licenses, can use the same parameters as long as they follow compatible data formats and security standards.&nbsp;



The mathematical rules that define what each proof will verify are written using specialized programming tools that convert them into a Rank-1 Constraint System (R1CS), a mathematical framework that describes exactly what needs to be proven about a credential.&nbsp;



To make the system fast enough for real-world use, Crescent splits the proof generation into two distinct stages:&nbsp;




Prepare stage. This step runs once and generates cryptographic values that can be stored on the user&#8217;s device for repeated use.&nbsp;





Show stage. When a user needs to present their credential, this quicker step takes the stored values and randomizes them to prevent any connection to previous presentations. It also creates a compact cryptographic summary that reveals only the specific information needed for that particular interaction.&nbsp;




Figures 2 and 3 illustrate this credential-proving workflow and the division between the prepare and show steps.



Figure 2: Crescent‚Äôs credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier. 



Figure 3: The Crescent presentation steps show the division between prepare and show steps.



A sample application&nbsp;



To demonstrate how Crescent works, we created a sample application covering two real-world scenarios: verifying employment and proving age for online access. The application includes sample code for setting up fictional issuers and verifiers as Rust servers, along with a browser-extension wallet for the user. The step numbers correspond to the steps in Figure 4.&nbsp;



Setup&nbsp;




A Crescent service pre-generates the zero-knowledge parameters for creating and verifying proofs from JSON Web Tokens and mobile driver‚Äôs licenses.&nbsp;





The user obtains a mobile driver‚Äôs license from their Department of Motor Vehicles.&nbsp;





The user obtains a proof-of-employment JSON Web Token from their employer, Contoso.&nbsp;





These credentials and their private keys are stored in the Crescent wallet.&nbsp;




Scenarios&nbsp;




Employment verification: The user presents their JSON Web Token to Fabrikam, an online health clinic, to prove they are employed at Contoso and eligible for workplace benefits. Fabrikam learns that the user works at Contoso but not the user&#8217;s identity, while Contoso remains unaware of the interaction.&nbsp;





Age verification: The user presents their mobile driver‚Äôs license to a social network, proving they are over 18. The proof confirms eligibility without revealing their age or identity.&nbsp;




Across both scenarios, Crescent ensures that credential presentations remain unlinkable, preventing any party from connecting them to the user.&nbsp;



For simplicity, the sample defines its own issuance and presentation protocol, but it could be integrated into higher-level identity frameworks such as OpenID/OAuth, Verifiable Credentials, or the mobile driver‚Äôs license ecosystem.



Figure 4. The sample architecture, from credential issuance to presentation.



To learn more about the project, visit the Crescent project GitHub (opens in new tab) page, or check out our recent presentations given at the Real-Word Crypto 2025 (opens in new tab) and North Sec 2025 (opens in new tab) conferences.¬†




Opens in a new tabThe post Crescent library brings privacy to digital identity systems appeared first on Microsoft Research.
‚Ä¢ Detect Amazon Bedrock misconfigurations with Datadog Cloud Security
  This post was co-written with Nick Frichette and Vijay George from Datadog.&nbsp; 
As organizations increasingly adopt Amazon Bedrock for generative AI applications, protecting against misconfigurations that could lead to data leaks or unauthorized model access becomes critical. The AWS Generative AI Adoption Index, which surveyed 3,739 senior IT decision-makers across nine countries, revealed that 45% of organizations selected generative AI tools as their top budget priority in 2025. As more AWS and Datadog customers accelerate their adoption of AI, building AI security into existing processes will become essential, especially as more stringent regulations emerge. But looking at AI risks in a silo isn‚Äôt enough; AI risks must be contextualized alongside other risks such as identity exposures and misconfigurations. The combination of Amazon Bedrock and Datadog‚Äôs comprehensive security monitoring helps organizations innovate faster while maintaining robust security controls. 
Amazon Bedrock delivers enterprise-grade security by incorporating built-in protections across data privacy, access controls, network security, compliance, and responsible AI safeguards. Customer data is encrypted both in transit using TLS 1.2 or above and at rest with AWS Key Management Service (AWS KMS), and organizations have full control over encryption keys. Data privacy is central: your input, prompts, and outputs are not shared with model providers nor used to train or improve foundation models (FMs). Fine-tuning and customizations occur on private copies of models, providing data confidentiality. Access is tightly governed through AWS Identity and Access Management (IAM) and resource-based policies, supporting granular authorization for users and roles. Amazon Bedrock integrates with AWS PrivateLink and supports virtual private cloud (VPC) endpoints for private, internal communication, so traffic doesn‚Äôt leave the Amazon network. The service complies with key industry standards such as ISO, SOC, CSA STAR, HIPAA eligibility, GDPR, and FedRAMP High, making it suitable for regulated industries. Additionally, Amazon Bedrock includes configurable guardrails to filter sensitive or harmful content and promote responsible AI use. Security is structured under the AWS Shared Responsibility Model, where AWS manages infrastructure security and customers are responsible for secure configurations and access controls within their Amazon Bedrock environment. 
Building on these robust AWS security features, Datadog and AWS have partnered to provide a holistic view of AI infrastructure risks, vulnerabilities, sensitive data exposure, and other misconfigurations. Datadog Cloud Security employs both agentless and agent-based scanning to help organizations identify, prioritize, and remediate risks across cloud resources. This integration helps AWS users prioritize risks based on business criticality, with security findings enriched by observability data, thereby enhancing their overall security posture in AI implementations. 
We‚Äôre excited to announce new security capabilities in Datadog Cloud Security that can help you detect and remediate Amazon Bedrock misconfigurations before they become security incidents. This integration helps organizations embed robust security controls and secure their use of the powerful capabilities of Amazon Bedrock by offering three critical advantages: holistic AI security by integrating AI security into your broader cloud security strategy, real-time risk detection through identifying potential AI-related security issues as they emerge, and simplified compliance to help meet evolving AI regulations with pre-built detections. 
AWS and Datadog: Empowering customers to adopt AI securely 
The partnership between AWS and Datadog is focused on helping customers operate their cloud infrastructure securely and efficiently. As organizations rapidly adopt AI technologies, extending this partnership to include Amazon Bedrock is a natural evolution. Amazon Bedrock is a fully managed service that makes high-performing FMs from leading AI companies and Amazon available through a unified API, making it an ideal starting point for Datadog‚Äôs AI security capabilities. 
The decision to prioritize Amazon Bedrock integration is driven by several factors, including strong customer demand, comprehensive security needs, and the existing integration foundation. With over 900 integrations and a partner-built Marketplace, Datadog‚Äôs long-standing partnership with AWS and deep integration capabilities have helped Datadog quickly develop comprehensive security monitoring for Amazon Bedrock while using their existing cloud security expertise. 
Throughout Q4 2024, Datadog Security Research observed increasing threat actor interest in cloud AI environments, making this integration particularly timely. By combining the powerful AI capabilities of AWS with Datadog‚Äôs security expertise, you can safely accelerate your AI adoption while maintaining robust security controls. 
How Datadog Cloud Security helps secure Amazon Bedrock resources 
After adding the AWS integration to your Datadog account and enabling Datadog Cloud Security, Datadog Cloud Security continuously monitors your AWS environment, identifying misconfigurations, identity risks, vulnerabilities, and compliance violations. These detections use the Datadog Severity Scoring system to prioritize them based on infrastructure context. The scoring considers a variety of variables, including if the resource is in production, is publicly accessible, or has access to sensitive data. This multi-layer analysis can help you reduce noise and focus your attention to the most critical misconfigurations by considering runtime behavior. 
Partnering with AWS, Datadog is excited to offer detections for Datadog Cloud Security customers, such as: 
 
 Amazon Bedrock custom models should not output model data to publicly accessible S3 buckets 
 Amazon Bedrock custom models should not train from publicly writable S3 buckets 
 Amazon Bedrock guardrails should have a prompt attack filter enabled and block prompt attacks at high sensitivity 
 Amazon Bedrock agent guardrails should have the sensitive information filter enabled and block highly sensitive PII entities 
 
Detect AI misconfigurations with Datadog Cloud Security 
To understand how these detections can help secure your Amazon Bedrock infrastructure, let‚Äôs look at a specific use case, in which Amazon Bedrock custom models should not train from publicly writable Amazon Simple Storage Service (Amazon S3) buckets. 
With Amazon Bedrock, you can customize AI models by fine-tuning on domain specific data. To do this, that data is stored in an S3 bucket. Threat actors are constantly evaluating the configuration of S3 buckets, looking for the potential to access sensitive data or even the ability to write to S3 buckets. 
If a threat actor finds an S3 bucket that was misconfigured to permit public write access, and that same bucket contained data that was used to train an AI model, a bad actor could poison that dataset and introduce malicious behavior or output to the model. This is known as a data poisoning attack. 
Normally, detecting these types of misconfigurations requires multiple steps: one to identify the S3 bucket misconfigured with write access, and one to identify that the bucket is being used by Amazon Bedrock. With Datadog Cloud Security, this detection is one of hundreds that are activated out of the box. 
In the Datadog Cloud Security system, you can view this issue alongside surrounding infrastructure using Cloud Map. This provides live diagrams of your cloud architecture, as shown in the following screenshot. AI risks are then contextualized alongside sensitive data exposure, identity risks, vulnerabilities, and other misconfigurations to give you a 360-view of risks. 
 
For example, you might see that your application is using Anthropic‚Äôs Claude 3.7 on Amazon Bedrock and accessing training or prompt data stored in an S3 bucket that also has public write access. This could inadvertently impact model integrity by introducing unapproved data to the large language model (LLM), so you will want to update this configuration. Though basic, the first step for most security initiatives is identifying the issue. With agentless scanning, Datadog scans your AWS environment at intervals between 15 minutes and 2 hours, so users can identify misconfigurations as they are introduced to their environment. The next step is to remediate this risk. Datadog Cloud Security offers automatically generated remediation guidance, specifically for each risk (see the following screenshot). You will get a step-by-step explanation of how to fix each finding. In this situation, we can remediate this issue by modifying the S3 bucket‚Äôs policy, helping prevent public write access. You can do this directly in AWS, create a JIRA ticket, or use the built-in workflow automation tools. From here, you can apply remediation steps directly within Datadog and confirm that the misconfiguration has been resolved. 
 
Resolving this issue will positively impact your compliance posture, as illustrated by the posture score in Datadog Cloud Security, helping teams meet internal benchmarks and regulatory standards. Teams can also create custom frameworks or iterate on existing ones for tailored compliance controls. 
 
As generative AI is embraced across industries, the regulatory environment will evolve. Datadog will continue partnering with AWS to expand their detection library and support secure AI adoption and compliance. 
How Datadog Cloud Security detects misconfigurations in your cloud environment 
You can deploy Datadog Cloud Security either with the Datadog agent, agentlessly, or both to maximize security coverage in your cloud environment. Datadog customers can start monitoring their AWS accounts for misconfigurations by first adding the AWS integration to Datadog. This enables Datadog to crawl cloud resources in customer AWS accounts. 
As the Datadog system finds resources, it runs through a catalog of hundreds of out-of-the-box detection rules against these resources, looking for misconfigurations and threat paths that adversaries can exploit. 
Secure your AI infrastructure with Datadog 
Misconfigurations in AI systems can be risky, but with the right tools, you can have the visibility and context needed to manage them. With Datadog Cloud Security, teams gain visibility into these risks, detect threats early, and remediate issues with confidence. In addition, Datadog has also released numerous agentic AI security features, designed to help teams gain visibility into the health and security of critical AI workload, which includes new announcements made to Datadog‚Äôs LLM observability features. 
Lastly, Datadog announced Bits AI Security Analyst alongside other Bits AI agents at DASH. Included as part of Cloud SIEM, Bits is an agentic AI security analyst that automates triage for AWS CloudTrail signals. Bits investigates each alert like a seasoned analyst: pulling in relevant context from across your Datadog environment, annotating key findings, and offering a clear recommendation on whether the signal is likely benign or malicious. By accelerating triage and surfacing real threats faster, Bits helps reduce mean time to remediation (MTTR) and frees analysts to focus on important threat hunting and response initiatives. This helps across different threats, including AI-related threats. 
To learn more about how Datadog helps secure your AI infrastructure, see Monitor Amazon Bedrock with Datadog or check out our security documentation. If you‚Äôre not already using Datadog, you can get started with Datadog Cloud Security with a 14-day free trial. 
 
About the Authors 
Nina Chen is a Customer Solutions Manager at AWS specializing in leading software companies to use the power of the AWS Cloud to accelerate their product innovation and growth. With over 4 years of experience working in the strategic independent software vendor (ISV) vertical, Nina enjoys guiding ISV partners through their cloud transformation journeys, helping them optimize their cloud infrastructure, driving product innovation, and delivering exceptional customer experiences. 
 Sujatha Kuppuraju is a Principal Solutions Architect at AWS, specializing in cloud and generative AI security. She collaborates with software companies‚Äô leadership teams to architect secure, scalable solutions on AWS and guide strategic product development. Using her expertise in cloud architecture and emerging technologies, Sujatha helps organizations optimize offerings, maintain robust security, and bring innovative products to market in an evolving tech landscape. 
Nick Frichette is a Staff Security Researcher for Cloud Security Research at Datadog. 
Vijay George is a Product Manager for AI Security at Datadog.
‚Ä¢ Set up custom domain names for Amazon Bedrock AgentCore Runtime agents
  When deploying AI agents to Amazon Bedrock AgentCore Runtime (currently in preview), customers often want to use custom domain names to create a professional and seamless experience. 
By default, AgentCore Runtime agents use endpoints like https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{EncodedAgentARN}/invocations. 
In this post, we discuss how to transform these endpoints into user-friendly custom domains (like https://agent.yourcompany.com) using Amazon CloudFront as a reverse proxy. The solution combines CloudFront, Amazon Route 53, and AWS Certificate Manager (ACM) to create a secure, scalable custom domain setup that works seamlessly with your existing agents. 
Benefits of Amazon Bedrock AgentCore Runtime 
If you‚Äôre building AI agents, you have probably wrestled with hosting challenges: managing infrastructure, handling authentication, scaling, and maintaining security. Amazon Bedrock AgentCore Runtime helps address these problems. 
Amazon Bedrock AgentCore Runtime is framework agnostic; you can use it with LangGraph, CrewAI, Strands Agents, or custom agents you have built from scratch. It supports extended execution times up to 8 hours, perfect for complex reasoning tasks that traditional serverless functions can‚Äôt handle. Each user session runs in its own isolated microVM, providing security that‚Äôs crucial for enterprise applications. 
The consumption-based pricing model means you only pay for what you use, not what you provision. And unlike other hosting solutions, Amazon Bedrock AgentCore Runtime includes built-in authentication and specialized observability for AI agents out of the box. 
Benefits of custom domains 
When using Amazon Bedrock AgentCore Runtime with Open Authorization (OAuth) authentication, your applications make direct HTTPS requests to the service endpoint. Although this works, custom domains offer several benefits: 
 
 Custom branding ‚Äì Client-side applications (web browsers, mobile apps) display your branded domain instead of AWS infrastructure details in network requests 
 Better developer experience ‚Äì Development teams can use memorable, branded endpoints instead of copying and pasting long AWS endpoints across code bases and configurations 
 Simplified maintenance ‚Äì Custom domains make it straightforward to manage endpoints when deploying multiple agents or updating configurations across environments 
 
Solution overview 
In this solution, we use CloudFront as a reverse proxy to transform requests from your custom domain into Amazon Bedrock AgentCore Runtime API calls. Instead of using the default endpoint, your applications can make requests to a user-friendly URL like https://agent.yourcompany.com/. 
The following diagram illustrates the solution architecture. 
 
The workflow consists of the following steps: 
 
 A client application authenticates with Amazon Cognito and receives a bearer token. 
 The client makes an HTTPS request to your custom domain. 
 Route 53 resolves the DNS request to CloudFront. 
 CloudFront forwards the authenticated request to the Amazon Bedrock Runtime agent. 
 The agent processes the request and returns the response through the same path. 
 
You can use the same CloudFront distribution to serve both your frontend application and backend agent endpoints, avoiding cross-origin resource sharing (CORS) issues because everything originates from the same domain. 
Prerequisites 
To follow this walkthrough, you must have the following in place: 
 
 An AWS account with appropriate permissions 
 The AWS Cloud Development Kit (AWS CDK) version 2.x or later 
 An AWS Identity and Access Management (IAM) execution role with appropriate permissions for Amazon Bedrock AgentCore Runtime 
 
Although Amazon Bedrock AgentCore Runtime can be in other supported AWS Regions, CloudFront requires SSL certificates to be in the us-east-1 Region. 
You can choose from the following domain options: 
 
 Use an existing domain ‚Äì Add a subdomain like agent.yourcompany.com 
 Register a new domain ‚Äì Use Route 53 to register a domain if you don‚Äôt have one 
 Use the default URL from CloudFront ‚Äì No domain registration or configuration required 
 
Choose the third option if you want to test the solution quickly before setting up a custom domain. 
Create an agent with inbound authentication 
If you already have an agent deployed with OAuth authentication, you can skip to the next section to set up the custom domain. Otherwise, follow these steps to create a new agent using Amazon Cognito as your OAuth provider: 
 
 Create a new directory for your agent with the following structure: 
 
 
 your_project_directory/
‚îú‚îÄ‚îÄ agent_example.py # Your main agent code
‚îú‚îÄ‚îÄ requirements.txt # Dependencies for your agent
‚îî‚îÄ‚îÄ __init__.py # Makes the directory a Python package 
 
 
 Create the main agent code in agent_example.py: 
 
 
 # agent_example.py
from strands import Agent
from bedrock_agentcore.runtime import BedrockAgentCoreApp

agent = Agent()
app = BedrockAgentCoreApp()
@app.entrypoint
def invoke(payload):
    """Process user input and return a response"""
    user_message = payload.get("prompt", "Hello")
    response = agent(user_message)
    return str(response) # response should be json serializable
if __name__ == "__main__":
    app.run() 
 
 
 Add dependencies to requirements.txt: 
 
 
 # requirements.txt
strands-agents
bedrock-agentcore 
 
 
 Run the following commands to create an Amazon Cognito user pool and test user: 
 
 
 # Create User Pool and capture Pool ID
export POOL_ID=$(aws cognito-idp create-user-pool \
  --pool-name "MyUserPool" \
  --policies '{"PasswordPolicy":{"MinimumLength":8}}' \
  --region us-east-1 | jq -r '.UserPool.Id')

# Create App Client and capture Client ID
export CLIENT_ID=$(aws cognito-idp create-user-pool-client \
  --user-pool-id $POOL_ID \
  --client-name "MyClient" \
  --no-generate-secret \
  --explicit-auth-flows "ALLOW_USER_PASSWORD_AUTH" "ALLOW_REFRESH_TOKEN_AUTH" \
  --region us-east-1 | jq -r '.UserPoolClient.ClientId')

# Create and configure a test user
aws cognito-idp admin-create-user \
  --user-pool-id $POOL_ID \
  --username "testuser" \
  --temporary-password "Temp1234" \
  --region us-east-1 \
  --message-action SUPPRESS

aws cognito-idp admin-set-user-password \
  --user-pool-id $POOL_ID \
  --username "testuser" \
  --password "MyPassword123" \
  --region us-east-1 \
  --permanent

echo "Pool ID: $POOL_ID"
echo "Discovery URL: https://cognito-idp.us-east-1.amazonaws.com/$POOL_ID/.well-known/openid-configuration"
echo "Client ID: $CLIENT_ID" 
 
 
 Deploy the agent using the Amazon Bedrock AgentCore command line interface (CLI) provided by the starter toolkit: 
 
 
 pip install bedrock-agentcore-starter-toolkit #install the starter toolkit

agentcore configure --entrypoint agent_example.py \
--name my_agent \
--execution-role your-execution-role-arn \
--requirements-file requirements.txt \
--authorizer-config "{\"customJWTAuthorizer\":{\"discoveryUrl\":\"https://cognito-idp.us-east-1.amazonaws.com/$POOL_ID/.well-known/openid-configuration\",\"allowedClients\":[\"$CLIENT_ID\"]}}"

agentcore launch 
 
Make note of your agent runtime Amazon Resource Name (ARN) after deployment. You will need this for the custom domain configuration. 
For additional examples and details, see Authenticate and authorize with Inbound Auth and Outbound Auth. 
Set up the custom domain solution 
Now let‚Äôs implement the custom domain solution using the AWS CDK. This section shows you how to create the CloudFront distribution that proxies your custom domain requests to Amazon Bedrock AgentCore Runtime endpoints. 
 
 Create a new directory and initialize an AWS CDK project: 
 
 
 mkdir agentcore-custom-domain
cd agentcore-custom-domain
cdk init app --language python
source .venv/bin/activate
pip install aws-cdk-lib constructs 
 
 
 Encode the agent ARN and prepare the CloudFront origin configuration: 
 
 
 # agentcore_custom_domain_stack.py 
import urllib.parse

agent_runtime_arn = "arn:aws:bedrock-agentcore:us-east-1:accountId:runtime/my_agent-xbcDkz4FR9"
encoded_arn = urllib.parse.quote(agent_runtime_arn, safe='') # URL-encode the ARN
region = agent_runtime_arn.split(':')[3]  # Extract region from ARN 
 
If your frontend application runs on a different domain than your agent endpoint, you must configure CORS headers. This is common if your frontend is hosted on a different domain (for example, https://app.yourcompany.com calling https://agent.yourcompany.com), or if you‚Äôre developing locally (for example, http://localhost:3000 calling your production agent endpoint). 
 
 To handle CORS requirements, create a CloudFront response headers policy: 
 
 
 # agentcore_custom_domain_stack.py 
from aws_cdk.aws_cloudfront import ResponseHeadersPolicy, ResponseHeadersCorsBehavior

# Create CORS response headers policy
cors_policy = ResponseHeadersPolicy(self, 'CorsPolicy',
    cors_behavior=ResponseHeadersCorsBehavior(
        access_control_allow_origins=['*'], # Or specify your frontend domains
        access_control_allow_headers=[
            'Authorization',
            'Content-Type', 
            'X-Amzn-*',
            'X-Requested-With'
        ],
        access_control_allow_methods=['GET', 'POST', 'OPTIONS'],
        access_control_allow_credentials=False,
        access_control_expose_headers=['*'],
        origin_override=True # Overrides CORS headers from origin
    )
) 
 
 
 Create a CloudFront distribution to act as a reverse proxy for your agent endpoints: 
 
 
 # agentcore_custom_domain_stack.py
 from aws_cdk.aws_cloudfront import (
    Distribution, BehaviorOptions, CachePolicy, 
    AllowedMethods, ViewerProtocolPolicy,
    OriginProtocolPolicy, OriginRequestPolicy
)
from aws_cdk.aws_cloudfront_origins import HttpOrigin

bedrock_agentcore_hostname = f"bedrock-agentcore.{region}.amazonaws.com"
origin_path = f"/runtimes/{encoded_arn}/invocations"

distribution = Distribution(self, 'Distribution',
    default_behavior=BehaviorOptions(
        origin=HttpOrigin(
            bedrock_agentcore_hostname,
            origin_path=origin_path, 
            protocol_policy=OriginProtocolPolicy.HTTPS_ONLY,
            read_timeout=Duration.seconds(120) # Optional: for responses &gt;30s, adjust as needed
        ),
        viewer_protocol_policy=ViewerProtocolPolicy.REDIRECT_TO_HTTPS,
        cache_policy=CachePolicy.CACHING_DISABLED,  # Critical for dynamic APIs
        allowed_methods=AllowedMethods.ALLOW_ALL,
        response_headers_policy=cors_policy,  # Add CORS policy if created
        origin_request_policy=OriginRequestPolicy.ALL_VIEWER,  # Forward headers for MCP
    ),
    # Add domain configuration if using custom domains
    domain_names=[domain_name] if domain_name else None,
    certificate=certificate if domain_name else None,
) 
 
Set cache_policy=CachePolicy.CACHING_DISABLED to make sure your agent responses remain dynamic and aren‚Äôt cached by CloudFront. 
 
 If you‚Äôre using a custom domain, add an SSL certificate and DNS configuration to your stack: 
 
 
 # agentcore_custom_domain_stack.py 
from aws_cdk.aws_certificatemanager import Certificate, CertificateValidation
from aws_cdk.aws_route53 import HostedZone, ARecord, RecordTarget
from aws_cdk.aws_route53_targets import CloudFrontTarget

# For existing domains
hosted_zone = HostedZone.from_lookup(self, 'HostedZone',
    domain_name='yourcompany.com'
)
# SSL certificate with automatic DNS validation
certificate = Certificate(self, 'Certificate',
    domain_name='my-agent.yourcompany.com',
    validation=CertificateValidation.from_dns(hosted_zone),
)
# DNS record pointing to CloudFront
ARecord(self, 'AliasRecord',
    zone=hosted_zone,
    record_name='my-agent.yourcompany.com',
    target=RecordTarget.from_alias(CloudFrontTarget(distribution)),
) 
 
The following code is the complete AWS CDK stack that combines all the components: 
 
 # agentcore_custom_domain_stack.py
import urllib.parse
from aws_cdk import Stack, CfnOutput, Duration
from aws_cdk.aws_cloudfront import (
    Distribution, BehaviorOptions,
    CachePolicy, AllowedMethods,
    ViewerProtocolPolicy, OriginProtocolPolicy,
    ResponseHeadersPolicy, ResponseHeadersCorsBehavior,
    OriginRequestPolicy
)
from aws_cdk.aws_cloudfront_origins import HttpOrigin
from aws_cdk.aws_certificatemanager import Certificate, CertificateValidation
from aws_cdk.aws_route53 import HostedZone, ARecord, RecordTarget
from aws_cdk.aws_route53_targets import CloudFrontTarget
from constructs import Construct

class AgentcoreCustomDomainStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -&gt; None:
        super().__init__(scope, construct_id, **kwargs)

        # Configuration - Update these for your setup
        agent_runtime_arn = "arn:aws:bedrock-agentcore:us-east-1:accountId:runtime/my_agent-xbcDkz4FR9"
        region = agent_runtime_arn.split(':')[3]  # Extract region from ARN
        domain_name = "agent.yourcompany.com"  # Using your hosted zone
        hosted_zone_id = "Z1234567890ABC"  # Your hosted zone ID
        enable_cors = True  # Set to False if serving frontend and backend from same domain

        # Encode the agent ARN for the origin path
        encoded_arn = urllib.parse.quote(agent_runtime_arn, safe='')
        bedrock_agentcore_hostname = f"bedrock-agentcore.{region}.amazonaws.com"
        origin_path = f"/runtimes/{encoded_arn}/invocations"

        # Create CORS response headers policy if needed
        cors_policy = None
        if enable_cors:
            cors_policy = ResponseHeadersPolicy(self, 'CorsPolicy',
                cors_behavior=ResponseHeadersCorsBehavior(
                    access_control_allow_origins=['*'],  # Or specify your frontend domains
                    access_control_allow_headers=[
                        'Authorization',
                        'Content-Type', 
                        'X-Amzn-*',
                        'X-Requested-With'
                    ],
                    access_control_allow_methods=['GET', 'POST', 'OPTIONS'],
                    access_control_expose_headers=['*'],
                    access_control_allow_credentials=False,
                    origin_override=True  # Overrides CORS headers from origin
                )
            )

        # Base distribution configuration
        distribution_props = {
            "default_behavior": BehaviorOptions(
                origin=HttpOrigin(
                    bedrock_agentcore_hostname,
                    origin_path=origin_path,  # Direct path to agent endpoint
                    protocol_policy=OriginProtocolPolicy.HTTPS_ONLY,
                    read_timeout=Duration.seconds(120) # Optional: for responses &gt;30s, adjust as needed
                ),
                viewer_protocol_policy=ViewerProtocolPolicy.REDIRECT_TO_HTTPS,
                cache_policy=CachePolicy.CACHING_DISABLED,
                allowed_methods=AllowedMethods.ALLOW_ALL,
                response_headers_policy=cors_policy,  # Add CORS policy if enabled
                origin_request_policy=OriginRequestPolicy.ALL_VIEWER,  # Forward headers for MCP
            )
        }

        # Optional: Add custom domain
        if domain_name:
            # Use from_hosted_zone_attributes for specific zone
            hosted_zone = HostedZone.from_hosted_zone_attributes(self, 'HostedZone',
                                                                 zone_name='yourcompany.com',  # Your root domain
                                                                 hosted_zone_id=hosted_zone_id
                                                                 )

            certificate = Certificate(self, 'Certificate',
                                      domain_name=domain_name,
                                      validation=CertificateValidation.from_dns(
                                          hosted_zone),
                                      )

            # Add custom domain to distribution
            distribution_props["domain_names"] = [domain_name]
            distribution_props["certificate"] = certificate

        distribution = Distribution(self, 'Distribution', **distribution_props)

        # Create DNS record if using custom domain
        if domain_name:
            ARecord(self, 'AliasRecord',
                    zone=hosted_zone,
                    record_name=domain_name,
                    target=RecordTarget.from_alias(
                        CloudFrontTarget(distribution)),
                    )

        # Outputs
        if domain_name:
            domain_url = f"https://{domain_name}/"
            CfnOutput(self, "AgentEndpoint",
                      value=domain_url,
                      description="Your custom domain endpoint"
                      )

        CfnOutput(self, "CloudFrontDistribution",
                  value=f"https://{distribution.distribution_domain_name}/",
                  description="CloudFront default domain (works without custom domain)"
                  ) 
 
 
 Configure the AWS CDK app entry point: 
 
 
 # app.py
#!/usr/bin/env python3
import aws_cdk as cdk
from agentcore_custom_domain.agentcore_custom_domain_stack import AgentCoreCustomDomainStack

app = cdk.App()
AgentcoreCustomDomainStack(app, "AgentCoreCustomDomainStack",
    # CloudFront requires certificates in us-east-1
    env=cdk.Environment(region='us-east-1'),
)
app.synth() 
 
Deploy your custom domain 
Now you can deploy the solution and verify it works with both custom and default domains. Complete the following steps: 
 
 Update the following values in agentcore_custom_domain_stack.py: 
   
   Your Amazon Bedrock AgentCore Runtime ARN 
   Your domain name (if using a custom domain) 
   Your hosted zone ID (if using a custom domain) 
    
 Deploy using the AWS CDK: 
 
 
 cdk deploy 
 
Test your endpoint 
After you deploy the custom domain, you can test your endpoints using either the custom domain or the CloudFront default domain.First, get a JWT token from Amazon Cognito: 
 
 export TOKEN=$(aws cognito-idp initiate-auth \
  --client-id "your-client-id" \
  --auth-flow USER_PASSWORD_AUTH \
  --auth-parameters USERNAME='testuser',PASSWORD='MyPassword123' \
  --region us-east-1 | jq -r '.AuthenticationResult.AccessToken') 
 
Use the following code to test with your custom domain: 
 
 curl -X POST "https://my-agent.yourcompany.com/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -H "X-Amzn-Bedrock-AgentCore-Runtime-Session-Id: session-12345678901234567890123456789012345" \
  -d '{"prompt": "Hello, how can you help me today?"}' 
 
Alternatively, use the following code to test with the CloudFront default domain: 
 
 curl -X POST "https://d1234567890123.cloudfront.net/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -H "X-Amzn-Bedrock-AgentCore-Runtime-Session-Id: session-12345678901234567890123456789012345" \
  -d '{"prompt": "Hello, how can you help me today?"}' 
 
 
 If everything works correctly, you will receive a response from your agent through either endpoint. You‚Äôve successfully created a custom domain for your Amazon Bedrock AgentCore Runtime agent! 
 
Considerations 
As you implement this solution in production, the following are some important considerations: 
 
 Cost implications ‚Äì CloudFront adds costs for data transfer and requests. Review Amazon CloudFront pricing to understand the impact for your usage patterns. 
 Security enhancements ‚Äì Consider implementing the following security measures: 
   
   AWS WAF rules to help protect against common web exploits. 
   Rate limiting to help prevent abuse. 
   Geo-restrictions if your agent should only be accessible from specific Regions. 
    
 Monitoring ‚Äì Enable CloudFront access logs and set up Amazon CloudWatch alarms to monitor error rates, latency, and request volume. 
 
Clean up 
To avoid ongoing costs, delete the resources when you no longer need them: 
 
 cdk destroy 
 
You might need to manually delete the Route 53 hosted zones and ACM certificates from their respective service consoles. 
Conclusion 
In this post, we showed you how to create custom domain names for your Amazon Bedrock AgentCore Runtime agent endpoints using CloudFront as a reverse proxy. This solution provides several key benefits: simplified integration for development teams, custom domains that align with your organization, cleaner infrastructure abstraction, and straightforward maintenance when endpoints need updates. By using CloudFront as a reverse proxy, you can also serve both your frontend application and backend agent endpoints from the same domain, avoiding common CORS challenges. 
We encourage you to explore this solution further by adapting it to your specific needs. You might want to enhance it with additional security features, set up monitoring, or integrate it with your existing infrastructure. 
To learn more about building and deploying AI agents, see the Amazon Bedrock AgentCore Developer Guide. For advanced configurations and best practices with CloudFront, refer to the Amazon CloudFront documentation. You can find detailed information about SSL certificates in the AWS Certificate Manager documentation, and domain management in the Amazon Route 53 documentation. 
Amazon Bedrock AgentCore is currently in preview and subject to change. Standard AWS pricing applies to additional services used, such as CloudFront, Route 53, and Certificate Manager. 
 
About the authors 
Rahmat Fedayizada is a Senior Solutions Architect with the AWS Energy and Utilities team. He works with energy companies to design and implement scalable, secure, and highly available architectures. Rahmat is passionate about translating complex technical requirements into practical solutions that drive business value. 
Paras Bhuva is a Senior Manager of Solutions Architecture at AWS, where he leads a team of solution architects helping energy customers innovate and accelerate their transformation. Having started as a Solution Architect in 2012, Paras is passionate about architecting scalable solutions and building organizations focused on application modernization and AI initiatives.
‚Ä¢ Introducing auto scaling on Amazon SageMaker HyperPod
  Today, we‚Äôre excited to announce that Amazon SageMaker HyperPod now supports managed node automatic scaling with Karpenter, so you can efficiently scale your SageMaker HyperPod clusters to meet your inference and training demands. Real-time inference workloads require automatic scaling to address unpredictable traffic patterns and maintain service level agreements (SLAs). As demand spikes, organizations must rapidly adapt their GPU compute without compromising response times or cost-efficiency. Unlike self-managed Karpenter deployments, this service-managed solution alleviates the operational overhead of installing, configuring, and maintaining Karpenter controllers, while providing tighter integration with the resilience capabilities of SageMaker HyperPod. This managed approach supports scale to zero, reducing the need for dedicated compute resources to run the Karpenter controller itself, improving cost-efficiency. 
SageMaker HyperPod offers a resilient, high-performance infrastructure, observability, and tooling optimized for large-scale model training and deployment. Companies like Perplexity, HippocraticAI, H.AI, and Articul8 are already using SageMaker HyperPod for training and deploying models. As more customers transition from training foundation models (FMs) to running inference at scale, they require the ability to automatically scale their GPU nodes to handle real production traffic by scaling up during high demand and scaling down during periods of lower utilization. This capability necessitates a powerful cluster auto scaler. Karpenter, an open source Kubernetes node lifecycle manager created by AWS, is a popular choice among Kubernetes users for cluster auto scaling due to its powerful capabilities that optimize scaling times and reduce costs. 
This launch provides a managed Karpenter-based solution for automatic scaling that is installed and maintained by SageMaker HyperPod, removing the undifferentiated heavy lifting of setup and management from customers. The feature is available for SageMaker HyperPod EKS clusters, and you can enable auto scaling to transform your SageMaker HyperPod cluster from static capacity to a dynamic, cost-optimized infrastructure that scales with demand. This combines Karpenter‚Äôs proven node lifecycle management with the purpose-built and resilient infrastructure of SageMaker HyperPod, designed for large-scale machine learning (ML) workloads. In this post, we dive into the benefits of Karpenter, and provide details on enabling and configuring Karpenter in your SageMaker HyperPod EKS clusters. 
New features and benefits 
Karpenter-based auto scaling in your SageMaker HyperPod clusters provides the following capabilities: 
 
 Service managed lifecycle ‚Äì SageMaker HyperPod handles Karpenter installation, updates, and maintenance, alleviating operational overhead 
 Just-in-time provisioning ‚Äì Karpenter observes your pending pods and provisions the required compute for your workloads from an on-demand pool 
 Scale to zero ‚Äì You can scale down to zero nodes without maintaining dedicated controller infrastructure 
 Workload-aware node selection ‚Äì Karpenter chooses optimal instance types based on pod requirements, Availability Zones, and pricing to minimize costs 
 Automatic node consolidation ‚Äì Karpenter regularly evaluates clusters for optimization opportunities, shifting workloads to avoid underutilized nodes 
 Integrated resilience ‚Äì Karpenter uses the built-in fault tolerance and node recovery mechanisms of SageMaker HyperPod 
 
These capabilities are built on top of recently launched continuous provisioning capabilities, which enables SageMaker HyperPod to automatically provision remaining capacity in the background while workloads start immediately on available instances. When node provisioning encounters failures due to capacity constraints or other issues, SageMaker HyperPod automatically retries in the background until clusters reach their desired scale, so your auto scaling operations remain resilient and non-blocking. 
Solution overview 
The following diagram illustrates the solution architecture. 
 
Karpenter works as a controller in the cluster and operates in the following steps: 
 
 Watching ‚Äì Karpenter watches for un-schedulable pods in the cluster through the Kubernetes API server. These could be pods that go into pending state when deployed or automatically scaled to increase the replica count. 
 Evaluating ‚Äì When Karpenter finds such pods, it computes the shape and size of a NodeClaim to fit the set of pods requirements (GPU, CPU, memory) and topology constraints, and checks if it can pair them with an existing NodePool. For each NodePool, it queries the SageMaker HyperPod APIs to get the instance types supported by the NodePool. It uses the information about instance type metadata (hardware requirements, zone, capacity type) to find a matching NodePool. 
 Provisioning ‚Äì If Karpenter finds a matching NodePool, it creates a NodeClaim and tries to provision a new instance to be used as the new node. Karpenter internally uses the sagemaker:UpdateCluster API to increase the capacity of the selected instance group. 
 Disrupting ‚Äì Karpenter periodically checks if a new node is needed or not. If it‚Äôs not needed, Karpenter deletes it, which internally translates to a delete node request to the SageMaker HyperPod cluster. 
 
Prerequisites 
Verify you have the required quotas for the instances you will create in the SageMaker HyperPod cluster. To review your quotas, on the Service Quotas console, choose AWS services in the navigation pane, then choose SageMaker. For example, the following screenshot shows the available quota for g5.12xlarge instances (three). 
 
To update the cluster, you must first create AWS Identity and Access Management (IAM) permissions for Karpenter. For instructions, see Create an IAM role for HyperPod autoscaling with Karpenter. 
Create and configure a SageMaker HyperPod cluster 
To begin, launch and configure your SageMaker HyperPod EKS cluster and verify that continuous provisioning mode is enabled on cluster creation. Complete the following steps: 
 
 On the SageMaker AI console, choose HyperPod clusters in the navigation pane. 
 Choose Create HyperPod cluster and Orchestrated on Amazon EKS. 
 For Setup options, select Custom setup. 
 For Name, enter a name. 
 For Instance recovery, select Automatic. 
 For Instance provisioning mode, select Use continuous provisioning. 
 Choose Submit. 
 
 
This setup creates the necessary configuration such as virtual private cloud (VPC), subnets, security groups, and EKS cluster, and installs operators in the cluster. You can also provide existing resources such as an EKS cluster if you want to use an existing cluster instead of creating a new one. This setup will take around 20 minutes. 
Verify that each InstanceGroup is limited to one zone by opting for the OverrideVpcConfig and selecting only one subnet per each InstanceGroup. 
 
After you create the cluster, you must update it to enable Karpenter. You can do this using Boto3 or the AWS Command Line Interface (AWS CLI) using the UpdateCluster API command (after configuring the AWS CLI to connect to your AWS account). 
The following code uses Python Boto3: 
 
 import boto3
client = boto3.client('sagemaker')
response = client.update_cluster(
    ClusterName=&lt;Your_Cluster_Name&gt;,
    AutoScaling = { "Mode": "Enable", "AutoScalerType": "Karpenter" },
    ClusterRole = &lt;Cluster_Role_ARN&gt;,
) 
 
 
 The following code uses the AWS CLI: 
 
 
 aws sagemaker update-cluster \
&nbsp; &nbsp; --cluster-name &lt;clustername&gt;&nbsp;\
&nbsp; &nbsp; --auto-scaling '{ "Mode": "Enable", "AutoScalerType": "Karpenter" }` \
&nbsp; &nbsp; --cluster-role &lt;clusterrole&gt; 
 
After you run this command and update the cluster, you can verify that Karpenter has been enabled by running the DescribeCluster API. 
The following code uses Python: 
 
 import&nbsp;boto3
client&nbsp;=&nbsp;boto3.client('sagemaker')
print(sagemaker_client.describe_cluster(ClusterName=&lt;Your_Cluster_Name&gt;).get("AutoScaling")) 
 
The following code uses the AWS CLI: 
 
 aws sagemaker describe-cluster --cluster-name &lt;clustername&gt; --query AutoScaling 
 
The following code shows our output: 
 
 {'Mode': 'Enable',
&nbsp;'AutoScalerType': 'Karpenter',
&nbsp;'Status': 'Enabled'} 
 
Now you have a working cluster. The next step is to set up some custom resources in your cluster for Karpenter. 
Create HyperpodNodeClass 
HyperpodNodeClass is a custom resource that maps to pre-created instance groups in SageMaker HyperPod, defining constraints around which instance types and Availability Zones are supported for Karpenter‚Äôs auto scaling decisions. To use HyperpodNodeClass, simply specify the names of the InstanceGroups of your SageMaker HyperPod cluster that you want to use as the source for the AWS compute resources to use to scale up your pods in your NodePools. 
The HyperpodNodeClass name that you use here is carried over to the NodePool in the next section where you reference it. This tells the NodePool which HyperpodNodeClass to draw resources from. To create a HyperpodNodeClass, complete the following steps: 
 
 Create a YAML file (for example, nodeclass.yaml) similar to the following code. Add InstanceGroup names that you used at the time of the SageMaker HyperPod cluster creation. You can also add new instance groups to an existing SageMaker HyperPod EKS cluster. 
 Reference the HyperPodNodeClass name in your NodePool configuration. 
 
The following is a sample HyperpodNodeClass that uses ml.g6.xlarge and ml.g6.4xlarge instance types: 
 
 apiVersion: karpenter.sagemaker.amazonaws.com/v1
kind: HyperpodNodeClass
metadata:
&nbsp;&nbsp;name:&nbsp;multiazg6
spec:
&nbsp;&nbsp;instanceGroups:
&nbsp; &nbsp; #&nbsp;name of&nbsp;InstanceGroup in HyperPod cluster. InstanceGroup needs to pre-created
&nbsp; &nbsp; # before this step can be completed.
&nbsp; &nbsp; #&nbsp;MaxItems: 10
&nbsp;&nbsp; &nbsp;- auto-g6-az1
&nbsp;&nbsp; &nbsp;- auto-g6-4xaz2 
 
 
 Apply the configuration to your EKS cluster using kubectl: 
 
 
 kubectl apply -f nodeclass.yaml 
 
 
 Monitor the HyperpodNodeClass status to verify the Ready condition in status is set to True to ensure it was successfully created: 
 
 
 kubectl get hyperpodnodeclass multiazc5 -oyaml 
 
The SageMaker HyperPod cluster must have AutoScaling enabled and the AutoScaling status must change to InService before the HyperpodNodeClass can be applied. 
For more information and key considerations, see Autoscaling on SageMaker HyperPod EKS. 
Create NodePool 
The NodePool sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The NodePool can be set to perform various actions, such as: 
 
 Define labels and taints to limit the pods that can run on nodes Karpenter creates 
 Limit node creation to certain zones, instance types, and computer architectures, and so on 
 
For more information about NodePool, refer to NodePools. SageMaker HyperPod managed Karpenter supports a limited set of well-known Kubernetes and Karpenter requirements, which we explain in this post. 
To create a NodePool, complete the following steps: 
 
 Create a YAML file named nodepool.yaml with your desired NodePool configuration. 
 
The following code is a sample configuration to create a sample NodePool. We specify the NodePool to include our ml.g6.xlarge SageMaker instance type, and we additionally specify it for one zone. Refer to NodePools for more customizations. 
 
 apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
&nbsp;name:&nbsp;gpunodepool
spec:
&nbsp;template:
&nbsp;&nbsp; spec:
&nbsp; &nbsp; &nbsp;nodeClassRef:
&nbsp;&nbsp; &nbsp; &nbsp;group: karpenter.sagemaker.amazonaws.com
&nbsp;&nbsp; &nbsp; &nbsp;kind: HyperpodNodeClass
&nbsp;&nbsp; &nbsp; &nbsp;name: multiazg6
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expireAfter:&nbsp;Never
&nbsp;&nbsp; &nbsp; requirements:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: node.kubernetes.io/instance-type
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: Exists
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "node.kubernetes.io/instance-type"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["ml.g6.xlarge"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "topology.kubernetes.io/zone"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["us-west-2a"] 
 
 
 Apply the NodePool to your cluster: 
 
 
 kubectl apply -f nodepool.yaml 
 
 
 Monitor the NodePool status to ensure the Ready condition in the status is set to True: 
 
 
 kubectl get nodepool gpunodepool -oyaml 
 
This example shows how a NodePool can be used to specify the hardware (instance type) and placement (Availability Zone) for pods. 
Launch a simple workload 
The following workload runs a Kubernetes deployment where the pods in deployment are requesting for 1 CPU and 256 MB memory per replica, per pod. The pods have not been spun up yet. 
 
 kubectl apply -f https://raw.githubusercontent.com/aws/karpenter-provider-aws/refs/heads/main/examples/workloads/inflate.yaml 
 
When we apply this, we can see a deployment and a single node launch in our cluster, as shown in the following screenshot. 
 
To scale this component, use the following command: 
 
 kubectl scale deployment inflate --replicas 10 
 
Within a few minutes, we can see Karpenter add the requested nodes to the cluster. 
 
Implement advanced auto scaling for inference with KEDA and Karpenter 
To implement an end-to-end auto scaling solution on SageMaker HyperPod, you can set up Kubernetes Event-driven Autoscaling (KEDA) along with Karpenter. KEDA enables pod-level auto scaling based on a wide range of metrics, including Amazon CloudWatch metrics, Amazon Simple Queue Service (Amazon SQS) queue lengths, Prometheus queries, and resource utilization patterns. By configuring Keda ScaledObject resources to target your model deployments, KEDA can dynamically adjust the number of inference pods based on real-time demand signals. 
When integrating KEDA and Karpenter, this combination creates a powerful two-tier auto scaling architecture. As KEDA scales your pods up or down based on workload metrics, Karpenter automatically provisions or deletes nodes in response to changing resource requirements. This integration delivers optimal performance while controlling costs by making sure your cluster has precisely the right amount of compute resources available at all times. For effective implementation, consider the following key factors: 
 
 Set appropriate buffer thresholds in KEDA to accommodate Karpenter‚Äôs node provisioning time 
 Configure cooldown periods carefully to prevent scaling oscillations 
 Define clear resource requests and limits to help Karpenter make optimal node selections 
 Create specialized NodePools tailored to specific workload characteristics 
 
The following is a sample spec of a KEDA ScaledObject file that scales the number of pods based on CloudWatch metrics of Application Load Balancer (ALB) request count: 
 
 apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
&nbsp;&nbsp;name: nd-deepseek-llm-scaler
&nbsp;&nbsp;namespace: default
spec:
&nbsp;&nbsp;scaleTargetRef:
&nbsp;&nbsp; &nbsp;name: nd-deepseek-llm-r1-distill-qwen-1-5b
&nbsp;&nbsp; &nbsp;apiVersion: apps/v1
&nbsp;&nbsp; &nbsp;kind: Deployment
&nbsp;&nbsp;minReplicaCount: 1
&nbsp;&nbsp;maxReplicaCount: 3
&nbsp;&nbsp;pollingInterval: 30 &nbsp; &nbsp; # seconds between checks
&nbsp;&nbsp;cooldownPeriod: 300 &nbsp; &nbsp; # seconds before scaling down
&nbsp;&nbsp;triggers:
&nbsp;&nbsp; &nbsp;- type: aws-cloudwatch
&nbsp;&nbsp; &nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;namespace: AWS/ApplicationELB &nbsp; &nbsp; &nbsp; &nbsp;# or your metric namespace
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;metricName: RequestCount &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# or your metric name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;dimensionName: LoadBalancer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # or your dimension key
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;dimensionValue: app/k8s-default-albnddee-cc02b67f20/0991dc457b6e8447
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;statistic: Sum
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;threshold: "3" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# change to your desired threshold
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;minMetricValue: "0" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # optional floor
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;region: us-east-2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # your AWS region
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;identityOwner: operator &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use the IRSA SA bound to keda-operator 
 
Clean up 
To clean up your resources to avoid incurring more charges, delete your SageMaker HyperPod cluster. 
Conclusion 
With the launch of Karpenter node auto scaling on SageMaker HyperPod, ML workloads can automatically adapt to changing workload requirements, optimize resource utilization, and help control costs by scaling precisely when needed. You can also integrate it with event-driven pod auto scalers such as KEDA to scale based on custom metrics. 
To experience these benefits for your ML workloads, enable Karpenter in your SageMaker HyperPod clusters. For detailed implementation guidance and best practices, refer to Autoscaling on SageMaker HyperPod EKS. 
 
About the authors 
Vivek Gangasani is a Worldwide Lead GenAI Specialist Solutions Architect for SageMaker Inference. He drives Go-to-Market (GTM) and Outbound Product strategy for SageMaker Inference. He also helps enterprises and startups deploy, manage, and scale their GenAI models with SageMaker and GPUs. Currently, he is focused on developing strategies and content for optimizing inference performance and GPU efficiency for hosting Large Language Models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines. 
Adam Stanley is a Solution Architect for Software, Internet and Model Provider customers at Amazon Web Services (AWS). He supports customers adopting all AWS services, but focuses primarily on Machine Learning training and inference infrastructure. Prior to AWS, Adam went to the University of New South Wales and graduated with degrees in Mathematics and Accounting. You can connect with him on LinkedIn. 
Kunal Jha is a Principal Product Manager at AWS, where he focuses on building Amazon SageMaker HyperPod to enable scalable distributed training and fine-tuning of foundation models. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. You can connect with him on LinkedIn. 
Ty Bergstrom is a Software Engineer at Amazon Web Services. He works on the HyperPod Clusters platform for Amazon SageMaker.
‚Ä¢ Meet Boti: The AI assistant transforming how the citizens of Buenos Aires access government information with Amazon Bedrock
  This post is co-written with Julieta Rappan, Macarena Blasi, and Mar√≠a Candela Blanco from the Government of the City of Buenos Aires. 
The Government of the City of Buenos Aires continuously works to improve citizen services. In February 2019, it introduced an AI assistant named Boti available through WhatsApp, the most widely used messaging service in Argentina. With Boti, citizens can conveniently and quickly access a wide variety of information about the city, such as renewing a driver‚Äôs license, accessing healthcare services, and learning about cultural events. This AI assistant has become a preferred communication channel and facilitates more than 3 million conversations each month. 
As Boti grows in popularity, the Government of the City of Buenos Aires seeks to provide new conversational experiences that harness the latest developments in generative AI. One challenge that citizens often face is navigating the city‚Äôs complex bureaucratic landscape. The City Government‚Äôs website includes over 1,300 government procedures, each of which has its own logic, nuances, and exceptions. The City Government recognized that Boti could improve access to this information by directly answering citizens‚Äô questions and connecting them to the right procedure. 
To pilot this new solution, the Government of the City of Buenos Aires partnered with the AWS Generative AI Innovation Center (GenAIIC). The teams worked together to develop an agentic AI assistant using LangGraph and Amazon Bedrock. The solution includes two main components: an input guardrail system and a government procedures agent. The input guardrail uses a custom LLM classifier to analyze incoming user queries, determining whether to approve or block requests based on their content. Approved requests are handled by the government procedures agent, which retrieves relevant procedural information and generates responses. Since most user queries focus on a single procedure, we developed a novel reasoning retrieval system to improve retrieval accuracy. This system initially retrieves comparative summaries that disambiguate similar procedures and then applies a large language model (LLM) to select the most relevant results. The agent uses this information to craft responses in Boti‚Äôs characteristic style, delivering short, helpful, and expressive messages in Argentina‚Äôs Rioplatense Spanish dialect. We focused on distinctive linguistic features of this dialect including the voseo (using ‚Äúvos‚Äù instead of ‚Äút√∫‚Äù) and periphrastic future (using ‚Äúir a‚Äù before verbs). 
In this post, we dive into the implementation of the agentic AI system. We begin with an overview of the solution, explaining its design and main features. Then, we discuss the guardrail and agent subcomponents and assess their performance. Our evaluation shows that the guardrails effectively block harmful content, including offensive language, harmful opinions, prompt injection attempts, and unethical behaviors. The agent achieves up to 98.9% top-1 retrieval accuracy using the reasoning retriever, which marks a 12.5‚Äì17.5% improvement over standard retrieval-augmented generation (RAG) methods. Subject matter experts found that Boti‚Äôs responses were 98% accurate in voseo usage and 92% accurate in periphrastic future usage. The promising results of this solution establish a new era of citizen-government interaction. 
Solution overview 
The Government of the City of Buenos Aires and the GenAIIC built an agentic AI assistant using Amazon Bedrock and LangGraph that includes an input guardrail system to enable safe interactions and a government procedures agent to respond to user questions. The workflow is shown in the following diagram. 
 
The process begins when a user submits a question. In parallel, the question is passed to the input guardrail system and government procedures agent. The input guardrail system determines whether the question contains harmful content. If triggered, it stops graph execution and redirects the user to ask questions about government procedures. Otherwise, the agent continues to formulate its response. The agent either calls a retrieval tool, which allows it to obtain relevant context and metadata from government procedures stored in Amazon Bedrock Knowledge Bases, or responds to the user. Both the input guardrail and government procedures agent use the Amazon Bedrock Converse API for LLM inference. This API provides access to a wide selection of LLMs, helping us optimize performance and latency across different subtasks. 
Input guardrail system 
Input guardrails help prevent the LLM system from processing harmful content. Although Amazon Bedrock Guardrails offers one implementation approach with filters for specific words, content, or sensitive information, we developed a custom solution. This provided us greater flexibility to optimize performance for Rioplatense Spanish and monitor specific types of content. The following diagram illustrates our approach, in which an LLM classifier assigns a primary category (‚Äúapproved‚Äù or ‚Äúblocked‚Äù) as well as a more detailed subcategory. 
 
Approved queries are within the scope of the government procedures agent. They consist of on-topic requests, which focus on government procedures, and off-topic requests, which are low-risk conversation questions that the agent responds to directly. Blocked queries contain high-risk content that Boti should avoid, including offensive language, harmful opinions, prompt injection attacks, or unethical behaviors. 
We evaluated the input guardrail system on a dataset consisting of both normal and harmful user queries. The system successfully blocked 100% of harmful queries, while occasionally flagging normal queries as harmful. This performance balance makes sure that Boti can provide helpful information while maintaining safe and appropriate interactions for users. 
Agent system 
The government procedures agent is responsible for answering user questions. It determines when to retrieve relevant procedural information using its retrieval tool and generates responses in Boti‚Äôs characteristic style. In the following sections, we examine both processes. 
Reasoning retriever 
The agent can use a retrieval tool to provide accurate and up-to-date information about government procedures. Retrieval tools typically employ a RAG framework to perform semantic similarity searches between user queries and a knowledge base containing document chunks stored as embeddings, and then provide the most relevant samples as context to the LLM. Government procedures, however, present challenges to this standard approach. Related procedures, such as renewing and reprinting drivers‚Äô licenses, can be difficult to disambiguate. Additionally, each user question typically requires information from one specific procedure. The mixture of chunks returned from standard RAG approaches increases the likelihood of generating incorrect responses. 
To better disambiguate government procedures, the Buenos Aires and GenAIIC teams developed a reasoning retrieval method that uses comparative summaries and LLM selection. An overview of this approach is shown in the following diagram. 
 
A necessary preprocessing step before retrieval is the creation of a government procedures knowledge base. To capture both the key information contained in procedures and how they related to each other, we created comparative summaries. Each summary contains basic information, such as the procedure‚Äôs purpose, intended audience, and content, such as costs, steps, and requirements. We clustered the base summaries into small groups, with an average cluster size of 5, and used an LLM to generate descriptions about what made each procedure different from its neighbors. We appended the distinguishing descriptions to the base information to create the final summary. We note that this approach shares similarities to Anthropic‚Äôs Contextual Retrieval, which prepends explanatory context to document chunk. 
With the knowledge base in place, we are able to retrieve relevant government procedures based on the user query. The reasoning retriever completes three steps: 
 
 Retrieve M Summaries: We retrieve between 1 and M comparative summaries using semantic search. 
 Optional Reasoning: In some cases, the initial retrieval surfaces similar procedures. To make sure that the most relevant procedures are returned to the agent, we apply an optional LLM reasoning step. The condition for this step occurs when the ratio of the first and second retrieval scores falls below a threshold value. An LLM follows a chain-of-thought (CoT) process in which it compares the user query to the retrieved summaries. It discards irrelevant procedures and reorders the remaining ones based on relevance. If the user query is specific enough, this process typically returns one result. By applying this reasoning step selectively, we minimize latency and token usage while maintaining high retrieval accuracy. 
 Retrieve N Full-Text Procedures: After the most relevant procedures are identified, we fetch their complete documents and metadata from an Amazon DynamoDB table. The metadata contains information like the source URL and the sentiment of the procedure. The agent typically receives between 1 and N results, where N ‚â§ M. 
 
The agent receives the retrieved full text procedures in its context window. It follows its own CoT process to determine the relevant content and URL source attributions when generating its answer. 
We evaluated our reasoning retriever against standard RAG techniques using a synthetic dataset of 1,908 questions derived from known source procedures. The performance was measured by determining whether the correct procedure appeared in the top-k retrieved results for each question. The following plot compares the top-k retrieval accuracy for each approach across different models, arranged in order of ascending performance from left to right. The metrics are proportionally weighted based on each procedure‚Äôs webpage visit frequency, making sure that our evaluation reflects real-world usage patterns. 
 
The first three approaches represent standard vector-based retrieval methods. The first method, Section Titan, involved chunking procedures by document sections, targeting approximately 250 words per chunk, and then embedding the chunks using Amazon Titan Text Embeddings v2. The second method, Summaries Titan, consisted of embedding the procedure summaries using the same embedding model. By embedding summaries rather than document text, the retrieval accuracy improved by 7.8‚Äì15.8%. The third method, Summaries Cohere, involved embedding procedure summaries using Cohere Multilingual v3 on Amazon Bedrock. The Cohere Multilingual embedding model provided a noticeable improvement in retrieval accuracy compared to the Amazon Titan embedding models, with all top-k values above 90%. 
The next three approaches use the reasoning retriever. We embedded the procedure summaries using the Cohere Multilingual model, retrieved 10 summaries during the initial retrieval step, and optionally applied the LLM-based reasoning step using either Anthropic‚Äôs Haiku 3, Claude 3 Sonnet, or Claude 3.5 Sonnet on Amazon Bedrock. All three reasoning retrievers consistently outperform standard RAG techniques, achieving 12.5‚Äì17.5% higher top-k accuracies. Anthropic‚Äôs Claude 3.5 Sonnet delivered the highest performance with 98.9% top-1 accuracy. These results demonstrate how combining embedding-based retrieval with LLM-powered reasoning can improve RAG performance. 
Answer generation 
After collecting the necessary information, the agent responds using Boti‚Äôs distinctive communication style: concise, helpful messages in Rioplatense Spanish. We maintained this voice through prompt engineering that specified the following: 
 
 Personality ‚Äì Convey a warm and friendly tone, providing quick solutions to everyday problems 
 Response length ‚Äì Limit responses to a few sentences 
 Structure ‚Äì Organize content using lists and highlights key information using bold text 
 Expression ‚Äì Use emojis to mark important requirements and add visual cues 
 Dialect ‚Äì Incorporate Rioplatense linguistic features, including voseo, periphrastic future, and regional vocabulary (for example, ‚Äúacordate,‚Äù ‚Äúentrar,‚Äù ‚Äúac√°,‚Äù and ‚Äúall√°‚Äù). 
 
Government procedures often address sensitive topics, like accidents, health, or security. To facilitate appropriate responses, we incorporated sentiment analysis into our knowledge base as metadata. This allows our system to route to different prompt templates. Sensitive topics are directed to prompts with reduced emoji usage and more empathetic language, whereas neutral topics receive standard templates. 
The following figure shows a sample response to a question about borrowing library books. It has been translated to English for convenience. 
 
To validate our prompt engineering approach, subject matter experts at the Government of the City of Buenos Aires reviewed a sample of Boti‚Äôs responses. Their analysis confirmed high fidelity to Rioplatense Spanish, with 98% accuracy in voseo usage and 92% in periphrastic future usage. 
Conclusion 
This post described the agentic AI assistant built by the Government of the City of Buenos Aires and the GenAIIC to respond to citizens‚Äô questions about government procedures. The solution consists of two primary components: an input guardrail system that helps prevent the system from responding to harmful user queries and a government procedures agent that retrieves relevant information and generates responses. The input guardrails effectively block harmful content, including queries with offensive language, harmful opinions, prompt injection, and unethical behaviors. The government procedures agent employs a novel reasoning retrieval method that disambiguates similar government procedures, achieving up to 98.9% top-1 retrieval accuracy and a 12.5‚Äì17.5% improvement over standard RAG methods. Through prompt engineering, responses are delivered in Rioplatense Spanish using Boti‚Äôs voice. Subject matter experts rated Boti‚Äôs linguistic performance highly, with 98% accuracy in voseo usage and 92% in periphrastic future usage. 
As generative AI advances, we expect to continuously improve our solution. The expanding catalog of LLMs available in Amazon Bedrock makes it possible to experiment with newer, more powerful models. This includes models that process text, as explored in the solution in this post, as well as models that process speech, allowing for direct speech-to-speech interactions. We might also explore the fine-tuning capabilities of Amazon Bedrock to customize models so that they better capture the linguistic features of Rioplatense Spanish. Beyond model improvements, we can iterate on our agent framework. The agent‚Äôs tool set can be expanded to support other tasks associated with government procedures like account creation, form completion, and appointment scheduling. As the City Government develops new experiences for citizens, we can consider implementing multi-agent frameworks in which specialist agents, like the government procedures agent, handle specific tasks. 
To learn more about Boti and AWS‚Äôs generative AI capabilities, check out the following resources: 
 
 Boti: The City Chatbot 
 Government of the City of Buenos Aires: Procedures 
 Amazon Bedrock 
 Amazon Bedrock Knowledge Bases 
 
 
 
About the authors 
Julieta Rappan is Director of the Digital Channels Department of the Buenos Aires City Government, where she coordinates the landscape of digital and conversational interfaces. She has extensive experience in the comprehensive management of strategic and technological projects, as well as in leading high-performance teams focused on the development of digital products and services. Her leadership drives the implementation of technological solutions with a focus on scalability, coherence, public value, and innovation‚Äîwhere generative technologies are beginning to play a central role. 
Macarena Blasi is Chief of Staff at the Digital Channels Department of the Buenos Aires City Government, working across the city‚Äôs main digital services, including Boti‚Äîthe WhatsApp-based virtual assistant‚Äîand the official Buenos Aires website. She began her journey working in conversational experience design, later serving as product owner and Operations Manager and then as Head of Experience and Content, leading multidisciplinary teams focused on improving the quality, accessibility, and usability of public digital services. Her work is driven by a commitment to building clear, inclusive, and human-centered experiences in the public sector. 
Mar√≠a Candela Blanco is Operations Manager for Quality Assurance, Usability, and Continuous Improvement at the Buenos Aires Government, where she leads the content, research, and conversational strategy across the city‚Äôs main digital channels, including the Boti AI assistant and the official Buenos Aires website. Outside of tech, Candela studies literature at UNSAM and is deeply passionate about language, storytelling, and the ways they shape our interactions with technology. 
Leandro Micchele is a Software Developer focused on applying AI to real-world use cases, with expertise in AI assistants, voice, and vision solutions. He serves as the technical lead and consultant for the Boti AI assistant at the Buenos Aires Government and works as a Software Developer at Telecom Argentina. Beyond tech, his discipline extends to martial arts: he has over 20 years of experience and currently teaches Aikido. 
Hugo Albuquerque is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, Hugo had extensive experience working as a data scientist in the media and entertainment and marketing sectors. In his free time, he enjoys learning other languages like German and practicing social dancing, such as Brazilian Zouk. 
Enrique Balp is a Senior Data Scientist at the AWS Generative AI Innovation Center working on cutting-edge AI solutions. With a background in the physics of complex systems focused on neuroscience, he has applied data science and machine learning across healthcare, energy, and finance for over a decade. He enjoys hikes in nature, meditation retreats, and deep friendships. 
Diego Galaviz is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, he had over 8 years of expertise as a data scientist across diverse sectors, including financial services, energy, big tech, and cybersecurity. He holds a master‚Äôs degree in artificial intelligence, which complements his practical industry experience. 
Laura Kulowski is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where she works with customers to build generative AI solutions. Before joining Amazon, Laura completed her PhD at Harvard‚Äôs Department of Earth and Planetary Sciences and investigated Jupiter‚Äôs deep zonal flows and magnetic field using Juno data. 
Rafael Fernandes is the LATAM leader of the AWS Generative AI Innovation Center, whose mission is to accelerate the development and implementation of generative AI in the region. Before joining Amazon, Rafael was a co-founder in the financial services industry space and a data science leader with over 12 years of experience in Europe and LATAM.

‚∏ª