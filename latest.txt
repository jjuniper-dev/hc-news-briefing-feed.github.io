‚úÖ Morning News Briefing ‚Äì September 11, 2025 10:42

üìÖ Date: 2025-09-11 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions: Mostly Cloudy, 15.0¬∞C
  Temperature: 15.0&deg;C Pressure: 102.2 kPa Visibility: 24 km Humidity: 88 % Dewpoint: 13.1&deg:C Wind: NNW 11 km/h Air Quality Health Index: n/a . Observed at: Garrison Petawawa 6:00 AM EDT Thursday 11 September 2025 Condition: Mostly Cloudy Temperature:
‚Ä¢ Thursday: Clearing. High 20.
  Mainly cloudy. Clearing early this afternoon . High 20. UV index 6 or high . Clearing later this afternoon. High 20 or low for highs of 20.50 degrees Celsius . Forecast issued 5:00 AM EDT Thursday 11 September 2025. Forecast also issued Thursday 11 Sept. 11 September 11 September 10. Weather forecast: Sunny, cloudy, breezy, cloudy .
‚Ä¢ Thursday night: Clear. Low plus 5.
  Fog patches developing after midnight . Clear. Clear. Low plus 5.80s . Clear . Fog patches expected to develop after midnight. Clear . Forecast issued 5:00 AM EDT Thursday 11 September 2025. Forecast: "Clear, clear, bright, cold, sunny, cloudy, warm, cloudy" Forecast forecast: "Fog patches" will develop in the morning .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ What we know about the shooting death of conservative activist Charlie Kirk
  Kirk was killed Wednesday while speaking at a campus event in Utah . There is no suspect in custody in the death of Kirk . Kirk was speaking at an event at a Utah campus event . No suspect is in custody at the scene of the shooting . Kirk is believed to have been killed by a self-inflicted gunshot wound to the head in the head on Wednesday morning . No suspects have
‚Ä¢ Lab results confusing? Some patients use AI to interpret them, for better or worse
  Many patients wait to hear back from their doctors about test results . Many turn to AI assistants for answers for answers . There are issues with privacy and accuracy of data collected by AI assistants . There is also an issue with accuracy and privacy of the data collected about the results . The AI assistants can be controlled by humans and can be programmed to be more precise and more accurate. There are concerns
‚Ä¢ After 10 years of black hole science, Stephen Hawking proven right
  Researchers have spent ten years improving the massive detectors they use to catch shockwaves from colliding black holes . Now the science is precise enough to test one of Stephen Hawking's key ideas . The new technique will test the theory of black holes colliding supermassive black holes hitting black holes in the universe, which is thought to be one of the most powerful black holes to hit black holes ever
‚Ä¢ Israel has hunted its top enemies around the Middle East. What has it achieved?
  Israel's surprise attack in Qatar targeting Hamas' top political leaders was the latest in a campaign aimed at hunting down Israel's top enemies since the 2023 Hamas-led attack on Israel . Hamas' political leaders have been targeted by Israel in the past . Israel has vowed to hunt down Hamas leaders since Hamas' 2023 terror strike on Israel is expected to take place in the coming years .
‚Ä¢ The broke college student's guide to managing money
  A financial educator offers advice for first-year college students . Student loans, credit cards and a tight budget can be tough for first year students . A financial expert offers advice on how to deal with student loans and credit cards . The financial expert says it's important to keep a tight eye on student loans, debt and credit card accounts . For more information, visit CNN.com/Her

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Attacker steals customer data from Brit rail operator LNER during break-in at supplier
  LNER is the latest organization to spill user data via a third-party data breach . Major UK player cagey on specifics but latest attack follows string blamed on 'third party' suppliers . LNER was the UK's largest rail operator, LNER, which is cagey about specifics of the attack on the data breach but says it will not be affected by third party data breach.
‚Ä¢ Experts scrutinized Ofcom's Online Safety Act governance. They're concerned
  Academics and OSA stakeholders say watchdog needs to amend how controversial legislation is enforced . Ofcom is overseeing the regulator overseeing the Online Safety Act . Questions mount over the effectiveness of the controversial legislation . OSA is the UK's version of the online safety act, and Ofcom has been criticised for its lack of transparency and effectiveness in enforcement of the act by the UK regulator . Of
‚Ä¢ BAE Systems surfaces autonomous submarine for military use
  Defense biz BAE Systems says it is readying an autonomous military submarine for the end of next year . Battery powered now, fuel-cell fuel-cells tomorrow - all packed in a shipping box . The autonomous submarine will be powered by a battery powered fuel cell and battery-powered battery cells . The submarine is expected to be fully powered by fuel cells and battery cells in 2018 .
‚Ä¢ Microsoft puts last remnants of original Edge browser on life support
  Microsoft has added a raft of web components to its list of depature features, including legacy Edge developer tools and hosted web apps . Not yet gone and not forgotten, but on their way Microsoft adds legacy Edge tools and web apps to the list of features that are no longer available . The list includes web components such as hosted apps and developer tools, but also legacy Edge Developer Tools and hosted
‚Ä¢ Dashboard anxiety plagues IT pros' nights, weekends, vacations
  A new survey confirms what many IT pros already know: downtime doesn't exist . Admins can't stop checking their portals, survey finds . Dashboards and alerts intruding on their free time, according to the survey . The survey was conducted by IT professionals in New York and Washington, D.C., New York, New York City, Washington, New Jersey, Washington and D.

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Functions of the global health system in a new era
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A call for action: closing the gap on ethnic disparities in oral cavity cancer care
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Home-based care improves blood pressure control
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Protests are infectious: mapping rural unrest in Revolutionary France
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Enhancing Mindfulness-Based Cognitive Therapy in a Virtual Reality: A Prospective Interventional Study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Texas banned lab-grown meat. What‚Äôs next for the industry?
  Last week, a legal battle over lab-grown meat kicked off in Texas. On September 1, a two-year ban on the technology went into effect across the state; the following day, two companies filed a lawsuit against state officials.



The two companies, Wildtype Foods and Upside Foods, are part of a growing industry that aims to bring new types of food to people‚Äôs plates. These products, often called cultivated meat by the industry, take live animal cells and grow them in the lab to make food products without the need to slaughter animals.





Texas joins six other US states and the country of Italy in banning these products. These legal challenges are adding barriers to an industry that‚Äôs still in its infancy and already faces plenty of challenges before it can reach consumers in a meaningful way.



The agriculture sector makes up a hefty chunk of global greenhouse-gas emissions, with livestock alone accounting for somewhere between 10% and 20% of climate pollution. Alternative meat products, including those grown in a lab, could help cut the greenhouse gases from agriculture.



The industry is still in its early days, though. In the US, just a handful of companies can legally sell products including cultivated chicken, pork fat, and salmon. Australia, Singapore, and Israel also allow a few companies to sell within their borders.



Upside Foods, which makes cultivated chicken, was one of the first to receive the legal go-ahead to sell its products in the US, in 2022. Wildtype Foods, one of the latest additions to the US market, was able to start selling its cultivated salmon in June.



Upside, Wildtype, and other cultivated-meat companies are still working to scale up production. Products are generally available at pop-up events or on special menus at high-end restaurants. (I visited San Francisco to try Upside‚Äôs cultivated chicken at a Michelin-starred restaurant a few years ago.)



Until recently, the only place you could reliably find lab-grown meat in Texas was a sushi restaurant in Austin. Otoko featured Wildtype‚Äôs cultivated salmon on a special tasting menu starting in July. (The chef told local publication Culture Map Austin that the cultivated fish tastes like wild salmon, and it was included in a dish with grilled yellowtail to showcase it side-by-side with another type of fish.)



The as-yet-limited reach of lab-grown meat didn‚Äôt stop state officials from moving to ban the technology, effective from now until September 2027.



The office of state senator Charles Perry, the author of the bill, didn‚Äôt respond to requests for comment. Neither did the Texas and Southwestern Cattle Raisers Association, whose president, Carl Ray Polk Jr., testified in support of the bill in a March committee hearing.



‚ÄúThe introduction of lab-grown meat could disrupt traditional livestock markets, affecting rural communities and family farms,‚Äù Perry said during the meeting.





In an interview with the Texas Tribune, Polk said the two-year moratorium would help the industry put checks and balances in place before the products could be sold. He also expressed concern about how clearly cultivated-meat companies will be labeling their products.



‚ÄúThe purpose of these bans is to try to kill the cultivated-meat industry before it gets off the ground,‚Äù said Myra Pasek, general counsel of Upside Foods, via email. The company is working to scale up its manufacturing and get the product on the market, she says, ‚Äúbut that can&#8217;t happen if we‚Äôre not allowed to compete in the marketplace.‚Äù



Others in the industry have similar worries. ‚ÄúMoratoriums on sale like this not only deny Texans new choices and economic growth, but they also send chilling signals to researchers and entrepreneurs across the country,‚Äù said Pepin Andrew Tuma, the vice president of policy and government relations for the Good Food Institute, a nonprofit think tank focused on alternative proteins, in a statement. (The group isn‚Äôt involved in the lawsuit.)&nbsp;



One day after the moratorium took effect on September 1, Wildtype Foods and Upside Foods filed a lawsuit challenging the ban, naming Jennifer Shuford, commissioner of the Texas Department of State Health Services, among other state officials.



A lawsuit wasn‚Äôt necessarily part of the scale-up plan. ‚ÄúThis was really a last resort for us,‚Äù says Justin Kolbeck, cofounder and CEO of Wildtype.



Growing cells to make meat in the lab isn‚Äôt easy‚Äîsome companies have spent a decade or more trying to make significant amounts of a product that people want to eat. These legal battles certainly aren‚Äôt going to help.&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ The Download: AI‚Äôs energy future
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Video: AI and our energy future



In May, MIT Technology Review published an unprecedented and comprehensive look at how much energy the AI industry uses‚Äîdown to a single query. Our reporters and editors traced where AI‚Äôs carbon footprint stands now, and where it‚Äôs headed, as AI barrels towards billions of daily users.



We‚Äôve just produced a short video to accompany that investigation. You can read the original full story here, and check out‚Äîand share‚Äî the full video on YouTube here.







AI is changing the grid. Could it help more than it harms?



The rising popularity of AI is driving an increase in electricity demand so significant it has the potential to reshape the grid. Energy consumption by data centers has gone up by 80% from 2020 to 2025 and is likely to keep growing. Electricity prices are already rising, especially in places where data centers are most concentrated.&nbsp;



Yet many people, especially in Big Tech, argue that AI will be, on balance, a positive force for the grid. They claim that the technology could help get more clean power online faster, run our power system more efficiently, and predict and prevent failures that cause blackouts. How much merit is there to that argument?



‚ÄîCasey Crownhart







Three big things we still don‚Äôt know about AI‚Äôs energy burden



‚ÄîJames O‚ÄôDonnell



Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.¬†



We pestered Google, OpenAI, and Microsoft, but each company refused to provide its figure for our article. But then this summer, after we published, a strange thing started to happen. They finally started to release the numbers we‚Äôd been calling for.



So with this newfound transparency, is our job complete? Did we finally harpoon our white whale? I reached out to some of our old sources, and some new ones, to find out. Read the full story.







MIT Technology Review Narrated: Google DeepMind has a new way to look inside an AI‚Äôs ‚Äúmind‚Äù



We don‚Äôt know exactly how AI works, or why it works so well. That‚Äôs a problem: It could lead us to deploy an AI system in a highly sensitive field like medicine without understanding its critical flaws.But a team at Google DeepMind that studies something called mechanistic interpretability has been working on new ways to let us peer under the hood.&nbsp;



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta suppressed research into the harms young users face in VRTwo former employees told a Senate committee the firm did it to avoid regulatory scrutiny. (WP $)



2 The MAGA movement is full of AI skepticsBut the White House is ditching regulatory obstacles and trying to accelerate AI‚Äôs adoption. (FT $)



3 Pfizer says its new covid vaccine boosts immune responses fourfoldIf you can get one, that is. (Ars Technica)+ Americans who can‚Äôt access a booster are increasingly fearful. (The Guardian)+ Vaccine guidance is incredibly confusing these days. (Vox)+ Why limited access to covid vaccines isn‚Äôt all bad. (MIT Technology Review)



4 The EU will examine banning social media for under-16sFollowing governments across Europe pushing for mandatory age restrictions. (Bloomberg $)



5 RFK Jr is going all-in on ChatGPTAll US health department employees have been given access to the tool. (404 Media)+ Humans may be more likely to believe disinformation generated by AI. (MIT Technology Review)



6 An ‚ÄúAI-supported‚Äù coder won in a man vs machine hackathon¬†But AI tools seem to slow down some experienced human developers. (Wired $)+ The second wave of AI coding is here. (MIT Technology Review)



7 Mark Zuckerberg is suing MetaNo, not that Mark Zuckerberg. (NYT $)+ The bankruptcy lawyer is fed up with being mistaken for him. (The Guardian)



8 Apple‚Äôs new AirPods can translate languages in real timeVia a robotic voice in your ear. (Ars Technica)+ A new AI translation system for headphones clones multiple voices simultaneously. (MIT Technology Review)



9 AI is threatening Latin America‚Äôs diverse music scenesFake songs are flooding streaming platforms and depriving artists of an income. (Rest of World)+ How Pandora fumbled its streaming lead. (Fast Company $)+ How to break free of Spotify‚Äôs algorithm. (MIT Technology Review)



10 Auction house Christie‚Äôs is axing its digital art divisionBut don‚Äôt worry‚Äîit‚Äôll still sell you NFTs. (Cointelegraph)+ I tried to buy an Olive Garden NFT. All I got was heartburn. (MIT Technology Review)







Quote of the day



‚ÄúIf you don‚Äôt lay the groundwork culturally for bringing in these stars, you‚Äôre going to end up burning a bunch of them out and pissing them off, and a bunch of them are going to quit and you‚Äôre going to waste millions of dollars.‚Äù



‚ÄîLaszlo Bock, a tech industry adviser and former head of people operations at Google, points out where Meta‚Äôs AI division is going wrong to the Wall Street Journal.







One more thing







The $100 billion bet that a postindustrial US city can reinvent itself as a high-tech hub



On a day in late April 2023, a small drilling rig sits at the edge of the scrubby overgrown fields of Syracuse, New York, taking soil samples. It‚Äôs the first sign of construction on what could become the largest semiconductor manufacturing facility in the United States.



The CHIPS and Science Act was widely viewed by industry leaders and politicians as a way to secure supply chains, and make the United States competitive again in semiconductor chip manufacturing.&nbsp;



Now Syracuse is about to become an economic test of whether, over the next several decades, aggressive government policies‚Äîand the massive corporate investments they spur‚Äîcan both boost the country‚Äôs manufacturing prowess and revitalize neglected parts of the country. Read the full story.



‚ÄîDavid Rotman







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This 1981 Sony Trinitron TV is the last word in luxury.+ It‚Äôs not just you‚Äîas we age, we really do become less adventurous musically.+ It appears as though our human ancestors hibernated‚Äîbut weren‚Äôt very good at it.+ Did renowned painter Vermeer duplicate his own painting? You be the judge.
‚Ä¢ Adapting to new threats with proactive risk management
  In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail . The outage is estimated to have caused direct losses of more than $5 billion to Fortune 500 companies . As organizations become ever more interconnected, the expanding surface of networks and the rapid adoption of technologies like AI are exposing new vulnerabilities . Cyberattacks are also becoming increasingly sophisticated and damaging as AI-driven malware and malware-as-a-service platforms turbocharge attacks .
‚Ä¢ The Download: meet our AI innovators, and what happens when therapists use AI covertly
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Meet the AI honorees on our 35 Innovators Under 35 list for 2025



Each year, we select 35 outstanding individuals under the age of 35 who are using technology to tackle tough problems in their respective fields.Our AI honorees include people who steer model development at Silicon Valley‚Äôs biggest tech firms and academic researchers who develop new techniques to improve AI‚Äôs performance.Check out all of our AI innovators here, and the full list‚Äîincluding our innovator of the year‚Äîhere.







How Yichao ‚ÄúPeak‚Äù Ji became a global AI app hitmaker



When Yichao Ji‚Äîalso known as ‚ÄúPeak‚Äù‚Äîappeared in a launch video for Manus in March, he didn‚Äôt expect it to go viral. Speaking in fluent English, the 32-year-old introduced the AI agent built by Chinese startup Butterfly Effect, where he serves as chief scientist.&nbsp;



The video was not an elaborate production but something about Ji‚Äôs delivery, and the vision behind the product, cut through the noise. The product, then still an early preview available only through invite codes, spread across the Chinese internet to the world in a matter of days. Within a week of its debut, Manus had attracted a waiting list of around 2 million people.Despite his relative youth, Ji has over a decade of experience building products that merge technical complexity with real-world usability. That earned him credibility‚Äîand put him at the forefront of a rising class of Chinese technologists with global ambitions. Read the full story.



‚ÄîCaiwei Chen







Help! My therapist is secretly using ChatGPT



In Silicon Valley‚Äôs imagined future, AI models are so empathetic that we‚Äôll use them as therapists. They‚Äôll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&nbsp;



Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn‚Äôt subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time.As the writer of the story, Laurie Clarke, points out, it‚Äôs not a total pipe dream that AI could be therapeutically useful. But the secretive use by therapists of AI models that are not vetted for mental health is something very different. James O‚ÄôDonnell, our senior AI reporter, had a conversation with Clarke to hear more about what she found.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







What‚Äôs next in tech: the breakthroughs that matter



Some technologies reshape industries, whether we‚Äôre ready or not.Join us for our next LinkedIn Live event on September 10 as our editorial team explores the breakthroughs defining this moment and the ones on the horizon that demand our attention.¬†



From quantum computing to humanoid robotics, AI agents to climate tech, we‚Äôll explore the innovations that excite us, the challenges they may bring, and why they‚Äôre worth watching now. It kicks off at 12.30pm ET tomorrow‚Äîregister here to join us.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US is abandoning its international push against disinformation¬†The State Department will no longer collaborate with Europe to combat malicious information spread by foreign governments. (FT $)+ It comes as Russia is increasing its efforts to interfere overseas. (NYT $)



2 The judge overseeing Anthropic‚Äôs copyright case isn‚Äôt happyJudge William Alsup says a $1.5 billion out-of-court settlement may not be in the authors&#8217; best interests. (Bloomberg $)



3 WhatsApp‚Äôs former head of security is suing MetaAttaullah Baig is accusing the company of failing to protect user data. (WP $)+ He claims he uncovered systemic security failures, but was ignored. (Bloomberg $)+ Meta maintains that Baig was dismissed for poor performance, not whistleblowing. (NYT $)



4 DOGE‚Äôs acting head is urging the US government to start hiring again¬†Following months of widespread firings and resignations. (Fast Company $)+ How DOGE wreaked havoc in Social Security. (ProPublica)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



5 OpenAI is weighing up leaving CaliforniaIt‚Äôs worried that state regulators could derail its efforts to convert to a for-profit entity. (WSJ $)+ Rival Anthropic is backing California governor Gavin Newsom‚Äôs AI bill. (Politico)



6 ICE spends millions on facial recognition techIn an effort to pinpoint people it suspects have assaulted officers. (404 Media)+ The Supreme Court has given ICE the go-ahead to target people based on race. (Vox)+ ICE directors were told to triple their daily arrests for undocumented immigrants. (NY Mag $)



7 AI researchers are training AI to replace themThey‚Äôre recording every detail of their working days to help AI grasp their jobs. (The Information $)+ People are worried that AI will take everyone‚Äôs jobs. We‚Äôve been here before. (MIT Technology Review)



8 What comes after the smartphone?The rise of AI agents means we may not be staring at glass slabs forever. (NYT $)+ What‚Äôs next for smart glasses. (MIT Technology Review)



9 Social media‚Äôs obsession with ‚Äòlocking in‚Äô needs to dieHustle culture and maximizing productivity at all costs are the aims of the game. (Insider $)



10 What it‚Äôs like to receive a massage from a robotWhile it may not be quite as relaxing, it‚Äôs relatively cheap. (The Guardian)+ Will we ever trust robots? (MIT Technology Review)







Quote of the day



‚ÄúIt was hell on Earth.‚Äù



‚ÄîDuncan Okindo, who was enslaved in a Myanmar cyberscam compound and beaten for missing his targets, tells the Guardian about his harrowing experience.







One more thing







AI means the end of internet search as we‚Äôve known itWe all know what it means, colloquially, to google something. You pop a few words in a search box and in return get a list of blue links to the most relevant results. Fundamentally, it‚Äôs just fetching information that‚Äôs already out there on the internet and showing it to you, in a structured way.But all that is up for grabs. We are at a new inflection point. The biggest change to the way search engines deliver information to us since the 1990s is happening right now, thanks to generative AI.Not everyone is excited for the change. Publishers are completely freaked out. And people are also worried about what these new LLM-powered results will mean for our fundamental shared reality. Read the full story.



‚ÄîMat Honan







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Stephen King‚Äôs list of favorite movies doesn‚Äôt feature a whole lot of horror.+ Tune into a breathtaking livestream of Earth, beamed live from the International Space Station.+ Rodent thumbnails are way more important than I gave them credit for + Mark our words, actor Wagner Moura is going to be the next big thing.
‚Ä¢ Help! My therapist is secretly using ChatGPT
  In Silicon Valley‚Äôs imagined future, AI models are so empathetic that we‚Äôll use them as therapists. They‚Äôll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&nbsp;



Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn‚Äôt subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time. The model then suggested responses that his therapist parroted.&nbsp;



It‚Äôs my favorite AI story as of late, probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to.



As the writer of the story, Laurie Clarke, points out, it‚Äôs not a total pipe dream that AI could be therapeutically useful. Early this year, I wrote about the first clinical trial of an AI bot built specifically for therapy. The results were promising! But the secretive use by therapists of AI models that are not vetted for mental health is something very different. I had a conversation with Clarke to hear more about what she found.&nbsp;



I have to say, I was really fascinated that people called out their therapists after finding out they were covertly using AI. How did you interpret the reactions of these therapists? Were they trying to hide it?



In all the cases mentioned in the piece, the therapist hadn‚Äôt provided prior disclosure of how they were using AI to their patients. So whether or not they were explicitly trying to conceal it, that‚Äôs how it ended up looking when it was discovered. I think for this reason, one of my main takeaways from writing the piece was that therapists should absolutely disclose when they‚Äôre going to use AI and how (if they plan to use it). If they don‚Äôt, it raises all these really uncomfortable questions for patients when it‚Äôs uncovered and risks irrevocably damaging the trust that‚Äôs been built.





In the examples you‚Äôve come across, are therapists turning to AI simply as a time-saver? Or do they think AI models can genuinely give them a new perspective on what‚Äôs bothering someone?



Some see AI as a potential time-saver. I heard from a few therapists that notes are the bane of their lives. So I think there is some interest in AI-powered tools that can support this. Most I spoke to were very skeptical about using AI for advice on how to treat a patient. They said it would be better to consult supervisors or colleagues, or case studies in the literature. They were also understandably very wary of inputting sensitive data into these tools.



There is some evidence AI can deliver more standardized, &#8220;manualized&#8221; therapies like CBT [cognitive behavioral therapy] reasonably effectively. So it‚Äôs possible it could be more useful for that. But that is AI specifically designed for that purpose, not general-purpose tools like ChatGPT.



What happens if this goes awry? What attention is this getting from ethics groups and lawmakers?



At present, professional bodies like the American Counseling Association advise against using AI tools to diagnose patients. There could also be more stringent regulations preventing this in future. Nevada and Illinois, for example, have recently passed laws prohibiting the use of AI in therapeutic decision-making. More states could follow.



OpenAI‚Äôs Sam Altman said last month that ‚Äúa lot of people effectively use ChatGPT as a sort of therapist,‚Äù and that to him, that‚Äôs a good thing. Do you think tech companies are overpromising on AI‚Äôs ability to help us?



I think that tech companies are subtly encouraging this use of AI because clearly it‚Äôs a route through which some people are forming an attachment to their products. I think the main issue is that what people are getting from these tools isn‚Äôt really ‚Äútherapy‚Äù by any stretch. Good therapy goes far beyond being soothing and validating everything someone says. I‚Äôve never in my life looked forward to a (real, in-person) therapy session. They‚Äôre often highly uncomfortable, and even distressing. But that‚Äôs part of the point. The therapist should be challenging you and drawing you out and seeking to understand you. ChatGPT doesn‚Äôt do any of these things.&nbsp;



Read the full story from Laurie Clarke.&nbsp;



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.

üîí Cybersecurity & Privacy
‚Ä¢ Microsoft Patch Tuesday, September 2025 Edition
  Microsoft Corp. today issued security updates to fix more than 80 vulnerabilities in its Windows operating systems and software. There are no known &#8220;zero-day&#8221; or actively exploited vulnerabilities in this month&#8217;s bundle from Redmond, which nevertheless includes patches for 13 flaws that earned Microsoft&#8217;s most-dire &#8220;critical&#8221; label. Meanwhile, both Apple and Google recently released updates to fix zero-day bugs in their devices.

Microsoft assigns security flaws a &#8220;critical&#8221; rating when malware or miscreants can exploit them to gain remote access to a Windows system with little or no help from users. Among the more concerning critical bugs quashed this month is CVE-2025-54918. The problem here resides with Windows NTLM, or NT LAN Manager, a suite of code for managing authentication in a Windows network environment.
Redmond rates this flaw as &#8220;Exploitation More Likely,&#8221; and although it is listed as a privilege escalation vulnerability, Kev Breen at Immersive says this one is actually exploitable over the network or the Internet.
&#8220;From Microsoft‚Äôs limited description, it appears that if an attacker is able to send specially crafted packets over the network to the target device, they would have the ability to gain SYSTEM-level privileges on the target machine,&#8221; Breen said. &#8220;The patch notes for this vulnerability state that &#8216;Improper authentication in Windows NTLM allows an authorized attacker to elevate privileges over a network,&#8217; suggesting an attacker may already need to have access to the NTLM hash or the user&#8217;s credentials.&#8221;
Breen said another patch &#8212; CVE-2025-55234, a 8.8 CVSS-scored flaw affecting the Windows SMB client for sharing files across a network &#8212; also is listed as privilege escalation bug but is likewise remotely exploitable. This vulnerability was publicly disclosed prior to this month.
&#8220;Microsoft says that an attacker with network access would be able to perform a replay attack against a target host, which could result in the attacker gaining additional privileges, which could lead to code execution,&#8221; Breen noted.
CVE-2025-54916 is an &#8220;important&#8221; vulnerability in Windows NTFS &#8212; the default filesystem for all modern versions of Windows &#8212; that can lead to remote code execution. Microsoft likewise thinks we are more than likely to see exploitation of this bug soon: The last time Microsoft patched an NTFS bug was in March 2025 and it was already being exploited in the wild as a zero-day.
&#8220;While the title of the CVE says &#8216;Remote Code Execution,&#8217; this exploit is not remotely exploitable over the network, but instead needs an attacker to either have the ability to run code on the host or to convince a user to run a file that would trigger the exploit,&#8221; Breen said. &#8220;This is commonly seen in social engineering attacks, where they send the user a file to open as an attachment or a link to a file to download and run.&#8221;
Critical and remote code execution bugs tend to steal all the limelight, but Tenable Senior Staff Research Engineer Satnam Narang notes that nearly half of all vulnerabilities fixed by Microsoft this month are privilege escalation flaws that require an attacker to have gained access to a target system first before attempting to elevate privileges.
&#8220;For the third time this year, Microsoft patched more elevation of privilege vulnerabilities than remote code execution flaws,&#8221; Narang observed.
On Sept. 3, Google fixed two flaws that were detected as exploited in zero-day attacks, including¬†CVE-2025-38352, an elevation of privilege in the Android kernel, and CVE-2025-48543, also an elevation of privilege problem in the Android Runtime component.
Also, Apple recently patched its seventh zero-day (CVE-2025-43300) of this year. It was part of an exploit chain used along with a vulnerability in the WhatsApp (CVE-2025-55177) instant messenger to hack Apple devices. Amnesty International reports that the two zero-days have been used in &#8220;an advanced spyware campaign&#8221; over the past 90 days. The issue is fixed in iOS 18.6.2, iPadOS 18.6.2, iPadOS 17.7.10, macOS Sequoia 15.6.1, macOS Sonoma 14.7.8, and macOS Ventura 13.7.8.
The SANS Internet Storm Center has a clickable breakdown of each individual fix from Microsoft, indexed by severity and CVSS score. Enterprise Windows admins involved in testing patches before rolling them out should keep an eye on askwoody.com, which often has the skinny on wonky updates.
AskWoody also reminds us that we&#8217;re now just two months out from Microsoft discontinuing free security updates for Windows 10 computers. For those interested in safely extending the lifespan and usefulness of these older machines, check out last month&#8217;s Patch Tuesday coverage for a few pointers.
As ever, please don&#8217;t neglect to back up your data (if not your entire system) at regular intervals, and feel free to sound off in the comments if you experience problems installing any of these fixes.
‚Ä¢ 18 Popular Code Packages Hacked, Rigged to Steal Crypto
  At least 18 popular JavaScript code packages that are collectively downloaded more than two billion times each week were briefly compromised with malicious software today, after a developer involved in maintaining the projects was phished. The attack appears to have been quickly contained and was narrowly focused on stealing cryptocurrency. But experts warn that a similar attack with a slightly more nefarious payload could lead to a disruptive malware outbreak that is far more difficult to detect and restrain.
This phishing email lured a developer into logging in at a fake NPM website and supplying a one-time token for two-factor authentication. The phishers then used that developer&#8217;s NPM account to add malicious code to at least 18 popular JavaScript code packages.
Aikido is a security firm in Belgium that monitors new code updates to major open-source code repositories, scanning any code updates for suspicious and malicious code. In a blog post published today, Aikido said its systems found malicious code had been added to at least 18 widely-used code libraries available on NPM (short for) &#8220;Node Package Manager,&#8221; which acts as a central hub for JavaScript development and the latest updates to widely-used JavaScript components.
JavaScript is a powerful web-based scripting language used by countless websites to build a more interactive experience with users, such as entering data into a form. But there&#8217;s no need for each website developer to build a program from scratch for entering data into a form when they can just reuse already existing packages of code at NPM that are specifically designed for that purpose.
Unfortunately, if cybercriminals manage to phish NPM credentials from developers, they can introduce malicious code that allows attackers to fundamentally control what people see in their web browser when they visit a website that uses one of the affected code libraries.
According to Aikido, the attackers injected a piece of code that silently intercepts cryptocurrency activity in the browser, &#8220;manipulates wallet interactions, and rewrites payment destinations so that funds and approvals are redirected to attacker-controlled accounts without any obvious signs to the user.&#8221;
&#8220;This malware is essentially a browser-based interceptor that hijacks both network traffic and application APIs,&#8221; Aikido researcher Charlie Eriksen wrote. &#8220;What makes it dangerous is that it operates at multiple layers: Altering content shown on websites, tampering with API calls, and manipulating what users‚Äô apps believe they are signing. Even if the interface looks correct, the underlying transaction can be redirected in the background.&#8221;
Aikido said it used the social network Bsky to notify the affected developer, Josh Junon, who quickly replied that he was aware of having just been phished. The phishing email that Junon fell for was part of a larger campaign that spoofed NPM and told recipients they were required to update their two-factor authentication (2FA) credentials. The phishing site mimicked NPM&#8217;s login page, and intercepted Junon&#8217;s credentials and 2FA token. Once logged in, the phishers then changed the email address on file for Junon&#8217;s NPM account, temporarily locking him out.
Aikido notified the maintainer on Bluesky, who replied at 15:15 UTC that he was aware of being compromised, and starting to clean up the compromised packages.
Junon also issued a mea culpa on HackerNews, telling the community&#8217;s coder-heavy readership, &#8220;Hi, yep I got pwned.&#8221;
&#8220;It looks and feels a bit like a targeted attack,&#8221; Junon wrote. &#8220;Sorry everyone, very embarrassing.&#8221;
Philippe Caturegli, &#8220;chief hacking officer&#8221; at the security consultancy Seralys, observed that the attackers appear to have registered their spoofed website &#8212; npmjs[.]help &#8212; just two days before sending the phishing email. The spoofed website used services from dnsexit[.]com, a &#8220;dynamic DNS&#8221; company that also offers &#8220;100% free&#8221; domain names that can instantly be pointed at any IP address controlled by the user.
Junon&#8217;s mea cupla on Hackernews today listed the affected packages.
Caturegli said it&#8217;s remarkable that the attackers in this case were not more ambitious or malicious with their code modifications.
&#8220;The crazy part is they compromised billions of websites and apps just to target a couple of cryptocurrency things,&#8221; he said. &#8220;This was a supply chain attack, and it could easily have been something much worse than crypto harvesting.&#8221;
Aikido&#8217;s Eriksen agreed, saying countless websites dodged a bullet because this incident was handled in a matter of hours. As an example of how these supply-chain attacks can escalate quickly, Eriksen pointed to another compromise of an NPM developer in late August that added malware to &#8220;nx,&#8221; an open-source code development toolkit with as many as six million weekly downloads.
In the nx compromise, the attackers introduced code that scoured the user&#8217;s device for authentication tokens from programmer destinations like GitHub and NPM, as well as SSH and API keys. But instead of sending those stolen credentials to a central server controlled by the attackers, the malicious code created a new public repository in the victim&#8217;s GitHub account, and published the stolen data there for all the world to see and download.
Eriksen said coding platforms like GitHub and NPM should be doing more to ensure that any new code commits for broadly-used packages require a higher level of attestation that confirms the code in question was in fact submitted by the person who owns the account, and not just by that person&#8217;s account.
&#8220;More popular packages should require attestation that it came through trusted provenance and not just randomly from some location on the Internet,&#8221; Eriksen said. &#8220;Where does the package get uploaded from, by GitHub in response to a new pull request into the main branch, or somewhere else? In this case, they didn&#8217;t compromise the target&#8217;s GitHub account. They didn&#8217;t touch that. They just uploaded a modified version that didn&#8217;t come where it&#8217;s expected to come from.&#8221;
Eriksen said code repository compromises can be devastating for developers, many of whom end up abandoning their projects entirely after such an incident.
&#8220;It&#8217;s unfortunate because one thing we&#8217;ve seen is people have their projects get compromised and they say, &#8216;You know what, I don&#8217;t have the energy for this and I&#8217;m just going to deprecate the whole package,'&#8221; Eriksen said.
Kevin Beaumont, a frequently quoted security expert who writes about security incidents at the blog doublepulsar.com, has been following this story closely today in frequent updates to his account on Mastodon. Beaumont said the incident is a reminder that much of the planet still depends on code that is ultimately maintained by an exceedingly small number of people who are mostly overburdened and under-resourced.
&#8220;For about the past 15 years every business has been developing apps by pulling in 178 interconnected libraries written by 24 people in a shed in Skegness,&#8221; Beaumont wrote on Mastodon. &#8220;For about the past 2 years orgs have been buying AI vibe coding tools, where some exec screams &#8216;make online shop&#8217; into a computer and 389 libraries are added and an app is farted out. The output = if you want to own the world&#8217;s companies, just phish one guy in Skegness.&#8221;
Image: https://infosec.exchange/@GossiTheDog@cyberplace.social.
Aikido recently launched a product that aims to help development teams ensure that every code library used is checked for malware before it can be used or installed. Nicholas Weaver, a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif., said Aikido&#8217;s new offering exists because many organizations are still one successful phishing attack away from a supply-chain nightmare.
Weaver said these types of supply-chain compromises will continue as long as people responsible for maintaining widely-used code continue to rely on phishable forms of 2FA.
&#8220;NPM should only support phish-proof authentication,&#8221; Weaver said, referring to physical security keys that are phish-proof &#8212; meaning that even if phishers manage to steal your username and password, they still can&#8217;t log in to your account without also possessing that physical key.
&#8220;All critical infrastructure needs to use phish-proof 2FA, and given the dependencies in modern software, archives such as NPM are absolutely critical infrastructure,&#8221; Weaver said. &#8220;That NPM does not require that all contributor accounts use security keys or similar 2FA methods should be considered negligence.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ RenderFormer: How neural networks are reshaping 3D rendering
  3D rendering‚Äîthe process of converting three-dimensional models into two-dimensional images‚Äîis a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.



Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.



Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene‚Äîlimiting their generalizability.



RenderFormer: Toward a general-purpose neural rendering model



To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML‚Äîno traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. This work has been accepted at SIGGRAPH 2025 and is open-sourced on GitHub (opens in new tab).



Architecture overview



As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens‚Äîeach one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.



Figure 1. Architecture of RenderFormer



To describe the viewing direction, the model uses ray bundle tokens derived from a ray map‚Äîeach pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.



The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									The AI Revolution in Medicine, Revisited
				
								Join Microsoft‚Äôs Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


Dual-branch design for view-independent and view-dependent effects



The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.




The view-independent transformer captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.



The view-dependent transformer models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.




Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.



To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.



Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. 



To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.



Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. 



The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall‚Äîcapturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.



Figure 3. Visualization of attention outputs



Training methodology and dataset design



RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1‚Äì3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender‚Äôs Cycles renderer, under varied lighting conditions and camera angles.



The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:




500,000 steps at 256√ó256 resolution with up to 1,536 triangles



100,000 steps at 512√ó512 resolution with up to 4,096 triangles




The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.



Figure 4. Rendered results of different 3D scenes generated by RenderFormer 



RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.



3D animation sequence rendered by RenderFormer 



Looking ahead: Opportunities and challenges



RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination‚Äîall without any hand-coded graphics computations.



However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI.¬†



Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.
Opens in a new tabThe post RenderFormer: How neural networks are reshaping 3D rendering appeared first on Microsoft Research.
‚Ä¢ Breaking the¬†networking¬†wall¬†in¬†AI infrastructure
  Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&nbsp;utilization&nbsp;and overall efficiency,&nbsp;ultimately preventing&nbsp;infrastructure from reaching its full potential&nbsp;despite enormous investments.&nbsp;At the&nbsp;core&nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.



Datacenters typically deploy two types of physical cables&nbsp;for&nbsp;communication between&nbsp;GPUs.&nbsp;Traditional copper links&nbsp;are power-efficient and&nbsp;reliable,&nbsp;but&nbsp;limited to&nbsp;very short&nbsp;distances&nbsp;(simultaneously.&nbsp;This approach leverages a hardware-system co-design and adopts&nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&nbsp;microLEDs.&nbsp;



The fundamental trade-off&nbsp;among&nbsp;power, reliability, and reach&nbsp;stems from&nbsp;the&nbsp;narrow-and-fast&nbsp;architecture&nbsp;deployed&nbsp;in&nbsp;today&#8217;s copper and optical links,&nbsp;comprising&nbsp;a few channels&nbsp;operating&nbsp;at&nbsp;very high&nbsp;data rates. For example,&nbsp;an&nbsp;800 Gbps link&nbsp;consists of eight 100 Gbps channels.&nbsp;With&nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&nbsp;With optical&nbsp;links,&nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&nbsp;complex electronics&nbsp;to compensate for transmission impairments. These challenges&nbsp;grow&nbsp;as speeds increase&nbsp;with&nbsp;every&nbsp;generation&nbsp;of networks.&nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&nbsp;systems&nbsp;margins&nbsp;and increasing failure rates.&nbsp;



These limitations force systems designers to make unpleasant&nbsp;choices,&nbsp;limiting the scalability of AI infrastructure.&nbsp;For example,&nbsp;scale-up networks connecting AI accelerators at&nbsp;multi-Tbps&nbsp;bandwidth&nbsp;typically&nbsp;must&nbsp;rely on&nbsp;copper links&nbsp;to meet&nbsp;the&nbsp;power budget,&nbsp;requiring&nbsp;ultra-dense racks that&nbsp;consume&nbsp;hundreds of kilowatts&nbsp;per rack. This creates significant challenges in cooling&nbsp;and&nbsp;mechanical design,&nbsp;which constrain&nbsp;the practical scale of these networks and end-to-end performance. This imbalance&nbsp;ultimately&nbsp;erects&nbsp;a&nbsp;networking wall&nbsp;akin&nbsp;to the&nbsp;memory wall, in&nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.



A technology offering copper-like power efficiency and reliability over long distances can overcome this networking¬†wall,¬†enabling¬†multi-rack¬†scale-up domains and unlocking¬†new architectures. This is a highly active R&amp;D area, with many candidate technologies currently being developed across the industry.¬†In¬†our recent¬†paper,¬†‚ÄúMOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs‚Äù, which received the Best Paper award at ACM SIGCOMM (opens in new tab), we present¬†one such promising¬†approach¬†that is¬†the result of a multi-year collaboration between Microsoft Research,¬†Azure, and M365.¬†This¬†work is¬†centered around¬†an optical¬†wide-and-slow architecture, shifting from a small number of high-speed serial channels towards¬†hundreds of parallel low-speed channels.¬†This¬†would be impractical¬†to realize with today‚Äôs copper and optical technologies because of¬†i)¬†electromagnetic interference challenges in high-density copper cables and ii) the¬†high cost¬†and power consumption of lasers¬†in optical links,¬†as well as the increase in packaging complexity.¬†MOSAIC overcomes these issues by¬†leveraging¬†directly modulated¬†microLEDs, a technology originally developed for¬†screen¬†displays.¬†



MicroLEDs&nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&nbsp;microns) and, due to their&nbsp;small size,&nbsp;they&nbsp;can be modulated at several Gbps.&nbsp;They&nbsp;are manufactured in large arrays,&nbsp;with over half a million&nbsp;in a small physical footprint for high-resolution displays&nbsp;like&nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&nbsp;microLED&nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20√ó20&nbsp;microLED&nbsp;array, which can fit in less than 1 mm√ó1 mm&nbsp;silicon&nbsp;die.&nbsp;



MOSAIC‚Äôs&nbsp;wide-and-slow&nbsp;design&nbsp;provides four core benefits.




Operating&nbsp;at low speed improves power efficiency&nbsp;by&nbsp;eliminating&nbsp;the need for&nbsp;complex&nbsp;electronics&nbsp;and&nbsp;reducing optical power requirements.



By&nbsp;leveraging&nbsp;optical transmission (via&nbsp;microLEDs),&nbsp;MOSAIC&nbsp;sidesteps&nbsp;copper‚Äôs reach issues, supporting distances up to 50 meters,&nbsp;or&nbsp;> 10x&nbsp;further&nbsp;than copper.



MicroLEDs‚Äô&nbsp;simpler structure&nbsp;and temperature insensitivity&nbsp;make them more reliable than lasers. The parallel nature of&nbsp;wide-and-slow&nbsp;also&nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&nbsp;



The&nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&nbsp;1.6&nbsp;Tbps&nbsp;or 3.2&nbsp;Tbps) can be achieved by increasing the number of&nbsp;channels and/or raising per-channel speed&nbsp;(e.g., to 4-8 Gbps).&nbsp;




Further,&nbsp;MOSAIC is fully compatible with today‚Äôs pluggable transceivers‚Äô form&nbsp;factor&nbsp;and it provides a drop-in replacement for today‚Äôs copper and optical cables, without requiring any changes to existing server and network infrastructure.&nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&nbsp;terminating&nbsp;or inspecting the connection&nbsp;and, hence,&nbsp;it‚Äôs&nbsp;fully compatible with today‚Äôs protocols (e.g.,&nbsp;Ethernet, PCIe, CXL).&nbsp;We are currently working with our suppliers to&nbsp;productize&nbsp;this technology and&nbsp;scale&nbsp;to mass production.&nbsp;



While&nbsp;conceptually simple, realizing this architecture posed a few key challenges&nbsp;across the stack, which&nbsp;required&nbsp;a multi-disciplinary team with&nbsp;expertise&nbsp;spanning across integrated photonics, lens design, optical transmission, and&nbsp;analog&nbsp;and digital design.&nbsp;For example, using individual&nbsp;fibers&nbsp;per channel would be prohibitively complex and costly due to the&nbsp;large number&nbsp;of channels. We addressed this by employing imaging&nbsp;fibers,&nbsp;which are typically used for medical applications (e.g., endoscopy).&nbsp;They&nbsp;can support thousands of cores&nbsp;per&nbsp;fiber, enabling multiplexing&nbsp;of&nbsp;many channels within a single&nbsp;fiber.&nbsp;Also,&nbsp;microLEDs&nbsp;are a less pure light source&nbsp;than lasers,&nbsp;with&nbsp;a larger beam shape (which complicates&nbsp;fiber&nbsp;coupling) and&nbsp;a broader spectrum (which&nbsp;degrades&nbsp;fiber&nbsp;transmission due to chromatic dispersion).&nbsp;We tackled these issues through&nbsp;a novel&nbsp;microLED and&nbsp;optical lens design,&nbsp;and&nbsp;a power-efficient&nbsp;analog-only electronic back&nbsp;end, which does not require any expensive digital signal processing.&nbsp;&nbsp;



Based on our current estimates, this approach can save&nbsp;up to 68% of power, i.e., more&nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&nbsp;reaching into&nbsp;the tens of millions, this translates to over 100MW of power savings per year,&nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&nbsp;to rethink&nbsp;AI&nbsp;infrastructure from network and cluster architectures to compute and memory designs.



For example,&nbsp;by&nbsp;supporting&nbsp;low-power,&nbsp;high-bandwidth connectivity at long reach,&nbsp;MOSAIC&nbsp;removes the need for ultra-dense racks and&nbsp;enables&nbsp;novel network topologies, which would be impractical today. The resulting redesign could&nbsp;reduce&nbsp;resource fragmentation and&nbsp;simplify&nbsp;collective optimization.&nbsp;Similarly,&nbsp;on the&nbsp;compute&nbsp;front,&nbsp;the ability&nbsp;to&nbsp;connect&nbsp;silicon&nbsp;dies at low power over long distances&nbsp;could&nbsp;enable&nbsp;resource&nbsp;disaggregation, shifting from today‚Äôs&nbsp;large,&nbsp;multi-die packages to&nbsp;smaller, more cost-effective, ones.&nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&nbsp;GPU&nbsp;memory&nbsp;capacity and bandwidth,&nbsp;while&nbsp;facilitating&nbsp;adoption of&nbsp;novel memory technologies.&nbsp;



Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&nbsp;possible future&nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.
Opens in a new tabThe post Breaking the¬†networking¬†wall¬†in¬†AI infrastructure¬† appeared first on Microsoft Research.
‚Ä¢ TII Falcon-H1 models now available on Amazon Bedrock Marketplace and Amazon SageMaker JumpStart
  This post was co-authored with Jingwei Zuo from TII. 
We are excited to announce the availability of the Technology Innovation Institute (TII)‚Äôs Falcon-H1 models on&nbsp;Amazon Bedrock Marketplace and Amazon SageMaker JumpStart.&nbsp;With this launch, developers and data scientists can now use&nbsp;six instruction-tuned Falcon-H1 models (0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B) on AWS, and have access to a comprehensive suite of hybrid architecture models that combine traditional attention mechanisms with State Space Models (SSMs) to deliver exceptional performance with unprecedented efficiency. 
In this post, we present an overview of Falcon-H1 capabilities and show how to get started with TII‚Äôs&nbsp;Falcon-H1 models on both Amazon Bedrock Marketplace and SageMaker JumpStart. 
Overview of TII and AWS collaboration 
TII is a leading research institute based in Abu Dhabi. As part of UAE‚Äôs Advanced Technology Research Council (ATRC), TII focuses on advanced technology research and development across AI, quantum computing, autonomous robotics, cryptography,&nbsp;and more.&nbsp;TII employs international teams of scientists, researchers, and engineers in an open and agile environment, aiming to drive technological innovation and position Abu Dhabi and the UAE as a global research and development hub in&nbsp;alignment with the UAE National Strategy for Artificial Intelligence 2031. 
TII and Amazon Web Services (AWS) are collaborating to expand access to made-in-the-UAE AI models across the globe. By combining TII‚Äôs technical expertise in building large language models (LLMs) with AWS Cloud-based AI and machine learning (ML) services, professionals worldwide can now build and scale generative AI applications using the Falcon-H1 series of models. 
About Falcon-H1 models 
The Falcon-H1 architecture implements a parallel hybrid design, using elements from Mamba and Transformer architectures to combine the faster inference and lower memory footprint of SSMs like Mamba with the effectiveness of Transformers‚Äô attention mechanism in understanding context and enhanced generalization capabilities.&nbsp;The Falcon-H1 architecture scales across multiple configurations ranging from 0.5‚Äì34 billion parameters and provides native support for 18 languages. According to TII, the Falcon-H1 family demonstrates notable efficiency with published metrics indicating that smaller model variants achieve performance parity with larger models. Some of the benefits of Falcon-H1 series include: 
 
 Performance ‚Äì The hybrid attention-SSM model has optimized parameters with adjustable ratios between attention and SSM heads, leading to faster inference, lower memory usage, and strong generalization capabilities. According to TII benchmarks published in Falcon-H1‚Äôs&nbsp;technical blog post and technical report, Falcon-H1 models demonstrate superior performance across multiple scales against other leading Transformer models of similar or larger scales. For example, Falcon-H1-0.5B delivers performance similar to typical 7B models from 2024, and Falcon-H1-1.5B-Deep rivals many of the current leading 7B-10B models. 
 Wide range of model sizes ‚Äì The Falcon-H1 series includes six sizes: 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B, with both base and instruction-tuned variants. The Instruct models are now available in Amazon Bedrock Marketplace and SageMaker JumpStart. 
 Multilingual by design ‚Äì The models support 18 languages natively&nbsp;(Arabic, Czech, German, English, Spanish, French, Hindi, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Romanian, Russian, Swedish, Urdu, and Chinese) and can scale to&nbsp;over 100 languages according to TII, thanks to&nbsp;a multilingual tokenizer trained on diverse language datasets. 
 Up to 256,000 context length ‚Äì The Falcon-H1 series enables applications in long-document processing, multi-turn dialogue, and long-range reasoning, showing a distinct advantage over competitors in practical long-context applications like Retrieval Augmented Generation (RAG). 
 Robust data and training strategy ‚Äì Training of Falcon-H1 models employs an innovative approach that introduces complex data early on, contrary to traditional curriculum learning. It also implements strategic data reuse based on careful memorization window assessment. Additionally, the training process scales smoothly across model sizes through a customized Maximal Update Parametrization (¬µP) recipe, specifically adapted for this novel architecture. 
 Balanced performance in science and knowledge-intensive&nbsp;domains ‚Äì Through a carefully designed data mixture and regular evaluations during training, the model achieves strong general capabilities and broad world knowledge while minimizing unintended specialization or domain-specific biases. 
 
In line with their mission to foster AI accessibility and collaboration, TII have released Falcon-H1 models under the&nbsp;Falcon LLM license. It offers the following benefits: 
 
 Open source nature and accessibility 
 Multi-language capabilities 
 Cost-effectiveness compared to proprietary models 
 Energy-efficiency 
 
About Amazon Bedrock Marketplace and SageMaker JumpStart 
Amazon Bedrock Marketplace offers access to over 100 popular, emerging, specialized, and domain-specific models, so you can find the best proprietary and publicly available models for your use case based on factors such as accuracy, flexibility, and cost. On Amazon Bedrock Marketplace you can discover models in a single place and access them through unified and secure Amazon Bedrock APIs. You can also select your desired number of instances and the instance type to meet the demands of your workload and optimize your costs. 
SageMaker JumpStart helps you quickly get started with machine learning. It provides access to state-of-the-art model architectures, such as language models, computer vision models, and more, without having to build them from scratch. With SageMaker JumpStart you can deploy models in a secure environment by provisioning them on SageMaker inference instances and isolating them within your virtual private cloud (VPC). You can also use Amazon SageMaker AI to further customize and fine-tune the models and streamline the entire model deployment process. 
Solution overview 
This post demonstrates how to deploy a Falcon-H1 model using both Amazon Bedrock Marketplace and SageMaker JumpStart. Although we use Falcon-H1-0.5B as an example, you can apply these steps to other models in the Falcon-H1 series. For help determining which deployment option‚ÄîAmazon Bedrock Marketplace or SageMaker JumpStart‚Äîbest suits your specific requirements, see Amazon Bedrock or Amazon SageMaker AI? 
Deploy&nbsp;Falcon-H1-0.5B-Instruct with Amazon Bedrock Marketplace 
In this section, we show how to deploy the Falcon-H1-0.5B-Instruct model in Amazon Bedrock Marketplace. 
Prerequisites 
To try the Falcon-H1-0.5B-Instruct model in Amazon Bedrock Marketplace, you must have access to an AWS account&nbsp;that will contain your AWS resources.Prior to deploying Falcon-H1-0.5B-Instruct, verify that your AWS account has sufficient quota allocation for ml.g6.xlarge instances. The default quota for endpoints using several instance types and sizes is 0, so attempting to deploy the model without a higher quota will trigger a deployment failure. 
To request a quota increase, open the AWS Service Quotas console and search for Amazon SageMaker. Locate ml.g6.xlarge&nbsp;for endpoint usage and choose Request quota increase, then specify your required limit value. After the request is approved, you can proceed with the deployment. 
Deploy the model using the Amazon Bedrock Marketplace UI 
To deploy the model using Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, under Discover in the navigation pane, choose Model catalog. 
 Filter for Falcon-H1 as the model name and choose Falcon-H1-0.5B-Instruct. 
 
 
The model overview page includes information about&nbsp;the model‚Äôs&nbsp;license terms, features, setup instructions, and links to further resources. 
 
 Review the model license terms, and if you agree with the terms, choose Deploy. 
 
 
 
 For Endpoint name, enter an endpoint name or leave it as the default pre-populated name. 
 To minimize costs while experimenting, set the Number of instances to 1. 
 For Instance type, choose from the list of compatible instance types. Falcon-H1-0.5B-Instruct is an efficient model, so ml.m6.xlarge is sufficient for this exercise. 
 
 
Although the default configurations are typically sufficient for basic needs, you can customize advanced settings like VPC, service access permissions, encryption keys, and resource tags. These advanced settings might require adjustment for production environments to maintain compliance with your organization‚Äôs security protocols. 
 
 Choose Deploy. 
 A prompt asks you to stay on the page while the AWS Identity and Access Management (IAM) role is being created. If your AWS account lacks sufficient quota for the selected instance type, you‚Äôll receive an error message. In this case, refer to the preceding prerequisite section to increase your quota, then try the deployment again. 
 
While deployment is in progress, you can choose Marketplace model deployments in the navigation pane to monitor the deployment progress in the Managed deployment section. When the deployment is complete, the endpoint status will change from Creating to In Service. 
Interact with the model in the Amazon Bedrock Marketplace playground 
You can now test Falcon-H1&nbsp;capabilities directly in the Amazon Bedrock playground by selecting the managed deployment and choosing Open in playground. 
 
You can now use the Amazon Bedrock Marketplace playground to interact with Falcon-H1-0.5B-Instruct. 
Invoke the model using code 
In this section, we demonstrate to invoke the model using the Amazon Bedrock Converse API. 
Replace the placeholder code with the endpoint‚Äôs Amazon Resource Name (ARN), which begins with arn:aws:sagemaker. You can find this ARN on the endpoint details page in the Managed deployments section. 
 
 import boto3
bedrock_runtime = boto3.client("bedrock-runtime")
endpoint_arn = "{ENDPOINT ARN}" # Replace with endpoint ARN
response = bedrock_runtime.converse( modelId=endpoint_arn, messages=[{"role": "user", "content": [{"text": "What is generative AI?"}]}], inferenceConfig={"temperature": 0.1, "topP": 0.1})

print(response["output"]["message"]["content"][0]["text"]) 
 
To learn more about the detailed steps and example code for invoking the model using Amazon Bedrock APIs, refer to&nbsp;Submit prompts and generate response using the API. 
Deploy Falcon-H1-0.5B-Instruct with SageMaker JumpStart 
You can access FMs in SageMaker JumpStart through Amazon SageMaker Studio, the SageMaker SDK, and the AWS Management Console. In this walkthrough, we demonstrate how to deploy&nbsp;Falcon-H1-0.5B-Instruct using the SageMaker Python SDK. Refer to Deploy a model in Studio to learn how to deploy the model through SageMaker Studio. 
Prerequisites 
To deploy Falcon-H1-0.5B-Instruct with SageMaker JumpStart, you must have the following prerequisites: 
 
 An AWS account that will contain your AWS resources. 
 An IAM role to access SageMaker AI. To learn more about how IAM works with SageMaker AI, see Identity and Access Management for Amazon SageMaker AI. 
 Access to SageMaker Studio with a JupyterLab space, or an interactive development environment (IDE) such as Visual Studio Code or PyCharm. 
 
Deploy the model programmatically using the SageMaker Python SDK 
Before deploying Falcon-H1-0.5B-Instruct using the SageMaker Python SDK, make sure you have installed the SDK and configured your AWS credentials and permissions. 
The following code example demonstrates how to deploy the model: 
 
  
  import sagemakerfrom sagemaker.jumpstart.model
import JumpStartModelfrom sagemaker
import Session
import boto3
import json

# Initialize SageMaker session
session = sagemaker.Session()
role = sagemaker.get_execution_role()

# Specify model parameters
model_id = "huggingface-llm-falcon-h1-0-5b-instruct"
instance_type = "ml.g6.xlarge" # Choose appropriate instance based on your needs

# Create and deploy the model
model = JumpStartModel( model_id=model_id, role=role, instance_type=instance_type, model_version="*" # Latest version)

# Deploy the model
predictor = model.deploy( initial_instance_count=1, accept_eula=True # Required for deploying foundation models)

print("Endpoint name:")
print(predictor.endpoint_name) 
  
 Perform inference using the SageMaker Python API 
 
When the previous code segment completes successfully, the Falcon-H1-0.5B-Instruct model deployment is complete and available on a SageMaker endpoint. Note the endpoint name shown in the output‚Äîyou will replace the placeholder in the following code segment with this value.The following code demonstrates how to prepare the input data, make the inference API call, and process the model‚Äôs response: 
 
 import json
import boto3

session = boto3.Session() # Make sure your AWS credentials are configured
sagemaker_runtime = session.client("sagemaker-runtime")

endpoint_name = "{ENDPOINT_NAME}" # Replace with endpoint name from deployment output

payload = { "messages": [ { "role": "user", "content": "What is generative AI?" } ], "parameters": { "max_tokens": 256, "temperature": 0.1, "top_p": 0.1 } }

# Perform inference
response = sagemaker_runtime.invoke_endpoint( EndpointName=endpoint_name, ContentType="application/json", Body=json.dumps(payload))

# Parse the response
result = json.loads(response["Body"].read().decode("utf-8"))generated_text = result["choices"][0]["message"]["content"].strip()
print("Generated Response:")
print(generated_text) 
 
Clean up 
To avoid ongoing charges for AWS resources used while experimenting with Falcon-H1 models, make sure to delete all deployed endpoints and their associated resources when you‚Äôre finished. To do so, complete the following steps: 
 
 Delete Amazon Bedrock Marketplace resources: 
   
   On the Amazon Bedrock console, choose Marketplace model deployment in the navigation pane. 
   Under Managed deployments, choose the Falcon-H1 model endpoint you deployed earlier. 
   Choose Delete and confirm the deletion if you no longer need to use this endpoint in Amazon Bedrock Marketplace. 
    
 Delete SageMaker endpoints: 
   
   On the SageMaker AI console, in the navigation pane, choose Endpoints under Inference. 
   Select the endpoint associated with the&nbsp;Falcon-H1 models. 
   Choose Delete and confirm the deletion. This stops the endpoint and avoids further compute charges. 
    
 Delete SageMaker models: 
   
   On the SageMaker AI console, choose Models under Inference. 
   Select the model associated with your endpoint and choose Delete. 
    
 
Always verify that all endpoints are deleted after experimentation to optimize costs. Refer to the Amazon SageMaker documentation for additional guidance on managing resources. 
Conclusion 
The availability of Falcon-H1 models in Amazon Bedrock Marketplace and SageMaker JumpStart helps developers, researchers, and businesses build cutting-edge generative AI applications with ease.&nbsp;Falcon-H1 models offer multilingual support (18 languages) across various model sizes (from 0.5B to 34B parameters) and support up to 256K context length, thanks to their efficient hybrid attention-SSM architecture. 
By using the seamless discovery and deployment capabilities of Amazon Bedrock Marketplace and SageMaker JumpStart, you can accelerate your AI innovation while benefiting from the secure, scalable, and cost-effective AWS Cloud infrastructure. 
We encourage you to explore the Falcon-H1&nbsp;models in Amazon Bedrock Marketplace or SageMaker JumpStart. You can use these models in AWS Regions where Amazon Bedrock or SageMaker JumpStart and the required instance types are available. 
For further learning, explore the AWS Machine Learning Blog, SageMaker JumpStart GitHub repository, and Amazon Bedrock User Guide. Start building your next generative AI application with Falcon-H1 models and unlock new possibilities with AWS! 
Special thanks to everyone who contributed to the launch: Evan Kravitz, Varun Morishetty, and Yotam Moss. 
 
About the authors 
Mehran Nikoo leads the Go-to-Market strategy for Amazon Bedrock and agentic AI in EMEA at AWS, where he has been driving the development of AI systems and cloud-native solutions over the last four years. Prior to joining AWS, Mehran held leadership and technical positions at Trainline, McLaren, and Microsoft. He holds an MBA from Warwick Business School and an MRes in Computer Science from Birkbeck, University of London. 
Mustapha Tawbi is a Senior Partner Solutions Architect at AWS, specializing in generative AI and ML, with 25 years of enterprise technology experience across AWS, IBM, Sopra Group, and Capgemini. He has a PhD in Computer Science from Sorbonne and a Master‚Äôs degree in Data Science from Heriot-Watt University Dubai. Mustapha leads generative AI technical collaborations with AWS partners throughout the MENAT region. 
Jingwei Zuo is a Lead Researcher at the Technology Innovation Institute (TII) in the UAE, where he leads the Falcon Foundational Models team. He received his PhD in 2022 from University of Paris-Saclay, where he was awarded the Plateau de Saclay Doctoral Prize. He holds an MSc (2018) from the University of Paris-Saclay, an Engineer degree (2017) from Sorbonne Universit√©, and a BSc from Huazhong University of Science &amp; Technology. 
John Liu is a Principal Product Manager for Amazon Bedrock at AWS. Previously, he served as the Head of Product for AWS Web3/Blockchain. Prior to joining AWS, John held various product leadership roles at public blockchain protocols and financial technology (fintech) companies for 14 years. He also has nine years of portfolio management experience at several hedge funds. 
Hamza MIMI is a Solutions Architect for partners and strategic deals in the MENAT region at AWS, where he bridges cutting-edge technology with impactful business outcomes. With expertise in AI and a passion for sustainability, he helps organizations architect innovative solutions that drive both digital transformation and environmental responsibility, transforming complex challenges into opportunities for growth and positive change.
‚Ä¢ Oldcastle accelerates document processing with Amazon Bedrock
  This post was written with Avdhesh Paliwal of Oldcastle APG. 
Oldcastle APG, one of the largest global networks of manufacturers in the architectural products industry, was grappling with an inefficient and labor-intensive process for handling proof of delivery (POD) documents, known as ship tickets. The company was processing 100,000‚Äì300,000 ship tickets per month across more than 200 facilities. Their existing optical character recognition (OCR) system was unreliable, requiring constant maintenance and manual intervention. It could only accurately read 30‚Äì40% of the documents, leading to significant time and resource expenditure. 
This post explores how Oldcastle partnered with AWS to transform their document processing workflow using Amazon Bedrock with Amazon Textract. We discuss how Oldcastle overcame the limitations of their previous OCR solution to automate the processing of hundreds of thousands of POD documents each month, dramatically improving accuracy while reducing manual effort. This solution demonstrates a practical, scalable approach that can be adapted to your specific needs, such as similar challenges addressing document processing or using generative AI for business process optimization. 
Challenges with document processing 
The primary challenge for Oldcastle was to find a solution that could accomplish the following: 
 
 Accurately process a high volume of ship tickets (PODs) with minimal human intervention 
 Scale to handle 200,000‚Äì300,000 documents per month 
 Handle inconsistent inputs like rotated pages and variable formatting 
 Improve the accuracy of data extraction from the current 30‚Äì40% to a much higher rate 
 Add new capabilities like signature validation on PODs 
 Provide real-time visibility into outstanding PODs and deliveries 
 
Additionally, Oldcastle needed a solution for processing supplier invoices and matching them against purchase orders, which presented similar challenges due to varying document formats.The existing process required dispatchers at more than 200 facilities to spend 4‚Äì5 hours daily manually processing ship tickets. This consumed valuable human resources and led to delays in processing and potential errors in data entry. The IT team was burdened with constant maintenance and development efforts to keep the unreliable OCR system functioning. 
Solution overview 
AWS Solutions Architects worked closely with Oldcastle engineers to build a solution addressing these challenges. The end-to-end workflow uses Amazon Simple Email Service (Amazon SES) to receive ship tickets, which are sent directly from drivers in the field. The system processes emails at scale using an event-based architecture centered on Amazon S3 Event Notifications. The workflow sends ship ticket documents to an automatic scaling compute job orchestrator. Documents are processed with the following steps: 
 
 The system sends PDF files to Amazon Textract using the Start Document Analysis API with Layout and Signature features. 
 Amazon Textract results are processed by an AWS Lambda microservice. This microservice resolves rotation issues with page text and generates a collection of pages of markdown representation of the text. 
 The markdown is passed to Amazon Bedrock, which efficiently extracts key values from the markdown text. 
 The orchestrator saves the results to their Amazon Relational Database Service (Amazon RDS) for PostgreSQL database. 
 
The following diagram illustrates the solution architecture. 
 
In this architecture, Amazon Textract is an effective solution to handle large PDF files at scale. The output of Amazon Textract contains the necessary geometries used to calculate rotation and fix layout issues before generating markdown. Quality markdown layouts are critical for Amazon Bedrock in identifying the right key-value pairs from the content. We further optimized cost by extracting only the data needed to limit output tokens and by using Amazon Bedrock batch processing to get the lowest token cost. Amazon Bedrock was used for its cost-effectiveness and ability to process format shipping tickets where the fields that need to be extracted are the same. 
Results 
The implementation using this architecture on AWS brought numerous benefits to Oldcastle: 
 
 Business process improvement ‚Äì The solution accomplished the following: 
   
   Alleviated the need for manual processing of ship tickets at each facility 
   Automated document processing with minimal human intervention 
   Improved accuracy and reliability of data extraction 
   Enhanced ability to validate signatures and reject incomplete documents 
   Provided real-time visibility into outstanding PODs and deliveries 
    
 Productivity gains ‚Äì Oldcastle saw the following benefits: 
   
   Significantly fewer human hours were spent on manual data entry and document processing 
   Staff had more time for more value-added activities 
   The IT team benefited from reduced development and maintenance efforts 
    
 Scalability and performance ‚Äì The team experienced the following performance gains: 
   
   They seamlessly scaled from processing a few thousand documents to 200,000‚Äì300,000 documents per month 
   The team observed no performance issues with increased volume 
    
 User satisfaction ‚Äì The solution improved user sentiment in several ways: 
   
   High user confidence in the new system due to its accuracy and reliability 
   Positive feedback from business users on the ease of use and effectiveness 
    
 Cost-effective ‚Äì With this approach, Oldcastle can process documents at less than $0.04 per page 
 
Conclusion 
With the success of the AWS implementation, Oldcastle is exploring potential expansion to other use cases such as AP invoice processing, W9 form validation, and automated document approval workflows. This strategic move towards AI-powered document processing is positioning Oldcastle for improved efficiency and scalability in its operations. 
Review your current manual document processing procedures and identify where intelligent document processing can help you automate these workflows for your business. 
For further exploration and learning, we recommend checking out the following resources: 
 
 Intelligent Document Processing on AWS 
 Automate document processing with Amazon Bedrock Prompt Flows 
 Intelligent Document Processing with Generative AI 
 
 
 
About the authors 
Erik Cordsen is a Solutions Architect at AWS serving customers in Georgia. He is passionate about applying cloud technologies and ML to solve real life problems. When he is not designing cloud solutions, Erik enjoys travel, cooking, and cycling. 
Sourabh Jain is a Senior Solutions Architect with over 8 years of experience developing cloud solutions that drive better business outcomes for organizations worldwide. He specializes in architecting and implementing robust cloud software solutions, with extensive experience working alongside global Fortune 500 teams across diverse time zones and cultures. 
Avdhesh Paliwal is an accomplished Application Architect at Oldcastle APG with 29 years of extensive ERP experience. His expertise spans Manufacturing, Supply Chain, and Human Resources modules, with a proven track record of designing and implementing enterprise solutions that drive operational efficiency and business value.
‚Ä¢ How London Stock Exchange Group is detecting market abuse with their AI-powered Surveillance Guide on Amazon Bedrock
  London Stock Exchange Group (LSEG) is a global provider of financial markets data and infrastructure. It operates the London Stock Exchange and manages international equity, fixed income, and derivative markets. The group also develops capital markets software, offers real-time and reference data products, and provides extensive post-trade services. This post was co-authored with Charles Kellaway and Rasika Withanawasam of LSEG. 
Financial markets are remarkably complex, hosting increasingly dynamic investment strategies across new asset classes and interconnected venues. Accordingly, regulators place great emphasis on the ability of market surveillance teams to keep pace with evolving risk profiles. However, the landscape is vast; London Stock Exchange alone facilitates the trading and reporting of over ¬£1 trillion of securities by 400 members annually. Effective monitoring must cover all MiFID asset classes, markets and jurisdictions to detect market abuse, while also giving weight to participant relationships, and market surveillance systems must scale with volumes and volatility. As a result, many systems are outdated and unsatisfactory for regulatory expectations, requiring manual and time-consuming work. 
To address these challenges, London Stock Exchange Group (LSEG) has developed an innovative solution using Amazon Bedrock, a fully managed service that offers a choice of high-performing foundation models from leading AI companies, to automate and enhance their market surveillance capabilities. LSEG‚Äôs AI-powered Surveillance Guide helps analysts efficiently review trades flagged for potential market abuse by automatically analyzing news sensitivity and its impact on market behavior. 
In this post, we explore how LSEG used Amazon Bedrock and Anthropic‚Äôs Claude foundation models to build an automated system that significantly improves the efficiency and accuracy of market surveillance operations. 
The challenge 
Currently, LSEG‚Äôs surveillance monitoring systems generate automated, customized alerts to flag suspicious trading activity to the Market Supervision team. Analysts then conduct initial triage assessments to determine whether the activity warrants further investigation, which might require undertaking differing levels of qualitative analysis. This could involve manual collation of all and any evidence that might be applicable when methodically corroborating regulation, news, sentiment and trading activity. For example, during an insider dealing investigation, analysts are alerted to statistically significant price movements. The analyst must then conduct an initial assessment of related news during the observation period to determine if the highlighted price move has been caused by specific news and its likely price sensitivity. This initial step in assessing the presence, or absence, of price sensitive news guides the subsequent actions an analyst will take with a possible case of market abuse. 
Initial triaging can be a time-consuming and resource-intensive process and still necessitate a full investigation if the identified behavior remains potentially suspicious or abusive. 
Moreover, the dynamic nature of financial markets and evolving tactics and sophistication of bad actors demand that market facilitators revisit automated rules-based surveillance systems. The increasing frequency of alerts and high number of false positives adversely impact an analyst‚Äôs ability to devote quality time to the most meaningful cases, and such heightened emphasis on resources could result in operational delays. 
Solution overview 
To address these challenges, LSEG collaborated with AWS to improve insider dealing detection, developing a generative AI prototype that automatically predicts the probability of news articles being price sensitive. The system employs Anthropic‚Äôs Claude Sonnet 3.5 model‚Äîthe most price performant model at the time‚Äîthrough Amazon Bedrock to analyze news content from LSEG‚Äôs Regulatory News Service (RNS) and classify articles based on their potential market impact. The results support analysts to more quickly determine whether highlighted trading activity can be mitigated during the observation period. 
The architecture consists of three main components: 
 
 A data ingestion and preprocessing pipeline for RNS articles 
 Amazon Bedrock integration for news analysis using Claude Sonnet 3.5 
 Inference application for visualising results and predictions 
 
The following diagram illustrates the conceptual approach: 
 
The workflow processes news articles through the following steps: 
 
 Ingest raw RNS news documents in HTML format 
 Preprocess and extract clean news text 
 Fill the classification prompt template with text from the news documents 
 Prompt Anthropic‚Äôs Claude Sonnet 3.5 through Amazon Bedrock 
 Receive and process model predictions and justifications 
 Present results through the visualization interface developed using Streamlit 
 
Methodology 
The team collated a comprehensive dataset of approximately 250,000 RNS articles spanning 6 consecutive months of trading activity in 2023. The raw data‚ÄîHTML documents from RNS‚Äîwere initially pre-processed within the AWS environment by removing extraneous HTML elements and formatted to extract clean textual content. Having isolated substantive news content, the team subsequently carried out exploratory data analysis to understand distribution patterns within the RNS corpus, focused on three dimensions: 
 
 News categories: Distribution of articles across different regulatory categories 
 Instruments: Financial instruments referenced in the news articles 
 Article length: Statistical distribution of document sizes 
 
Exploration provided contextual understanding of the news landscape and informed the sampling strategy in creating a representative evaluation dataset. 110 articles were selected to cover major news categories, and this curated subset was presented to market surveillance analysts who, as domain experts, evaluated each article‚Äôs price sensitivity on a nine-point scale, as shown in the following image: 
 
 1‚Äì3: PRICE_NOT_SENSITIVE ‚Äì Low probability of price sensitivity 
 4‚Äì6: HARD_TO_DETERMINE ‚Äì Uncertain price sensitivity 
 7‚Äì9: PRICE_SENSITIVE ‚Äì High probability of price sensitivity 
 
 
The experiment was executed within Amazon SageMaker using Jupyter Notebooks as the development environment. The technical stack consisted of: 
 
 Instructor library: Provided integration capabilities with Anthropic‚Äôs Claude Sonnet 3.5 model in Amazon Bedrock 
 Amazon Bedrock: Served as the API infrastructure for model access 
 Custom data processing pipelines (Python): For data ingestion and preprocessing 
 
This infrastructure enabled systematic experimentation with various algorithmic approaches, including traditional supervised learning methods, prompt engineering with foundation models, and fine-tuning scenarios. 
The evaluation framework established specific technical success metrics: 
 
 Data pipeline implementation: Successful ingestion and preprocessing of RNS data 
 Metric definition: Clear articulation of precision, recall, and F1 metrics 
 Workflow completion: Execution of comprehensive exploratory data analysis (EDA) and experimental workflows 
 
The analytical approach was a two-step classification process, as shown in the following figure: 
 
 Step 1: Classify news articles as potentially price sensitive or other 
 Step 2: Classify news articles as potentially price not sensitive or other 
 
 
This multi-stage architecture was designed to maximize classification accuracy by allowing analysts to focus on specific aspects of price sensitivity at each stage. The results from each step were then merged to produce the final output, which was compared with the human-labeled dataset to generate quantitative results. 
To consolidate the results from both classification steps, the data merging rules followed were: 
 
  
   
   Step 1 Classification 
   Step 2 Classification 
   Final Classification 
   
  
  
   
   Sensitive 
   Other 
   Sensitive 
   
   
   Other 
   Non-sensitive 
   Non-sensitive 
   
   
   Other 
   Other 
   Ambiguous ‚Äì requires manual review i.e., Hard to Determine 
   
   
   Sensitive 
   Non-sensitive 
   Ambiguous ‚Äì requires manual review i.e., Hard to Determine 
   
  
 
Based on the insights gathered, prompts were optimized. The prompt templates elicited three key components from the model: 
 
 A concise summary of the news article 
 A price sensitivity classification 
 A chain-of-thought explanation justifying the classification decision 
 
The following is an example prompt: 
 
 system non sensitive = "*"
You are an expert financial analyst with deep knowledge of market dynamics, investor
    psychology, and the intricate relationships between news events and asset prices.
    Your core function is to analyze news articles and assess their likelihood of being
    non-price sensitive with unparalleled accuracy and insight.
Key aspects of your expertise include:
1. Market Dynamics: You have a comprehensive understanding of how financial markets
    operate, including the factors that typically drive price movements and those that
    are often overlooked by the market.
2. Investor Psychology: You possess keen insight into how different types of news affect
    investor sentiment and decision-making, particularly in distinguishing between
    information that causes reactions and information that doesn't.
3. News Analysis: You excel at dissecting financial news articles, identifying key
    elements, and determining their relevance (or lack thereof) to asset valuations and
    market movements.
4. Pattern Recognition: You can draw upon a vast knowledge of historical market 
    reactions to various types of news, allowing you to identify patterns of 
    non-impactful information.
5. Sector-Specific Knowledge: You understand the nuances of different industry sectors
    and how the importance of news can vary across them.
6. Regulatory Insight: You're well-versed in financial regulations and can identify when
    news does or doesn't meet thresholds for material information.
7. Macroeconomic Perspective: You can place company-specific news in the broader context
    of economic trends and assess whether it's likely to be overshadowed by larger market
    forces.
8. Quantitative Skills: You can evaluate financial metrics and understand when changes or
    announcements related to them are significant enough to impact prices.
Your primary task is to analyze given news articles and determine, with a high degree of
    confidence, whether they are likely to be non-price sensitive. This involves:
- Carefully examining the content and context of each news item
- Assessing its potential (or lack thereof) to influence investor decisions
- Considering both short-term and long-term implications
- Providing clear, well-reasoned justifications for your assessments
- Identifying key factors that support your conclusion
- Recommending further information that could enhance the analysis
- Offering insights that can help traders make more informed decisions
You should always maintain a conservative approach, erring on the side of caution. If
    there's any reasonable doubt about whether news could be price-sensitive, you should
    classify it as 'OTHER' rather than 'NOT_PRICE_SENSITIVE'.
Your analyses should be sophisticated yet accessible, catering to both experienced
    traders and those new to the market. Always strive for objectivity, acknowledging any
    uncertainties or limitations in your assessment.
Remember, your insights play a crucial role in helping traders filter out market noise
    and focus on truly impactful information, ultimately contributing to more effective
    and educated trading decisions. 
 
As shown in the following figure, the solution was optimized to maximize: 
 
 Precision for the NOT SENSITIVE class 
 Recall for the PRICE SENSITIVE class 
 
 
This optimization strategy was deliberate, facilitating high confidence in non-sensitive classifications to reduce unnecessary escalations to human analysts (in other words, to reduce false positives). Through this methodical approach, prompts were iteratively refined while maintaining rigorous evaluation standards through comparison against the expert-annotated baseline data. 
Key benefits and results 
Over a 6-week period, Surveillance Guide demonstrated remarkable accuracy when evaluated on a representative sample dataset. Key achievements include the following: 
 
 100% precision in identifying non-sensitive news, allocating 6 articles to this category that analysts confirmed were non price sensitive 
 100% recall in detecting price-sensitive content, allocating 36 hard to determine and 28 price sensitive articles labelled by analysts into one of these two categories (never misclassifying price sensitive content) 
 Automated analysis of complex financial news 
 Detailed justifications for classification decisions 
 Effective triaging of results by sensitivity level 
 
In this implementation, LSEG has employed Amazon Bedrock so that they can use secure, scalable access to foundation models through a unified API, minimizing the need for direct model management and reducing operational complexity. Because of the serverless architecture of Amazon Bedrock, LSEG can take advantage of dynamic scaling of model inference capacity based on news volume, while maintaining consistent performance during market-critical periods. Its built-in monitoring and governance features support reliable model performance and maintain audit trails for regulatory compliance. 
Impact on market surveillance 
This AI-powered solution transforms market surveillance operations by: 
 
 Reducing manual review time for analysts 
 Improving consistency in price-sensitivity assessment 
 Providing detailed audit trails through automated justifications 
 Enabling faster response to potential market abuse cases 
 Scaling surveillance capabilities without proportional resource increases 
 
The system‚Äôs ability to process news articles instantly and provide detailed justifications helps analysts focus their attention on the most critical cases while maintaining comprehensive market oversight. 
Proposed next steps 
LSEG plans to first enhance the solution, for internal use, by: 
 
 Integrating additional data sources, including company financials and market data 
 Implementing few-shot prompting and fine-tuning capabilities 
 Expanding the evaluation dataset for continued accuracy improvements 
 Deploying in live environments alongside manual processes for validation 
 Adapting to additional market abuse typologies 
 
Conclusion 
LSEG‚Äôs Surveillance Guide demonstrates how generative AI can transform market surveillance operations. Powered by Amazon Bedrock, the solution improves efficiency and enhances the quality and consistency of market abuse detection. 
As financial markets continue to evolve, AI-powered solutions architected along similar lines will become increasingly important for maintaining integrity and compliance. AWS and LSEG are intent on being at the forefront of this change. 
The selection of Amazon Bedrock as the foundation model service provides LSEG with the flexibility to iterate on their solution while maintaining enterprise-grade security and scalability. To learn more about building similar solutions with Amazon Bedrock, visit the Amazon Bedrock documentation or explore other financial services use cases in the AWS Financial Services Blog. 
 
About the authors 
 Charles Kellaway is a Senior Manager in the Equities Trading team at LSE plc, based in London. With a background spanning both Equity and Insurance markets, Charles specialises in deep market research and business strategy, with a focus on deploying technology to unlock liquidity and drive operational efficiency. His work bridges the gap between finance and engineering, and he always brings a cross-functional perspective to solving complex challenges. 
 Rasika Withanawasam is a seasoned technology leader with over two decades of experience architecting and developing mission-critical, scalable, low-latency software solutions. Rasika‚Äôs core expertise lies in big data and machine learning applications, focusing intently on FinTech and RegTech sectors. He has held several pivotal roles at LSEG, including Chief Product Architect for the flagship Millennium Surveillance and Millennium Analytics platforms, and currently serves as Manager of the Quantitative Surveillance &amp; Technology team, where he leads AI/ML solution development. 
Richard Chester&nbsp;is a Principal Solutions Architect at AWS, advising large Financial Services organisations. He has 25+ years‚Äô experience across the Financial Services Industry where he has held leadership roles in transformation programs, DevOps engineering, and Development Tooling. Since moving across to AWS from being a customer, Richard is now focused on driving the execution of strategic initiatives, mitigating risks and tackling complex technical challenges for AWS customers.

‚∏ª